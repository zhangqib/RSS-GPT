<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.11865</link>
<guid>https://arxiv.org/abs/2512.11865</guid>
<content:encoded><![CDATA[
<div> Smart farming, RGB cameras, photometric perturbations, adversarial attacks, Vision-Language-Action model<br><br>Summary:<br><br>Smart farming utilizes automation and intelligent control to enhance modern agriculture. However, current systems that depend on RGB cameras for perception and robotic manipulators for control are vulnerable to photometric perturbations like hue shifts, illumination changes, and noise, which may lead to malfunctions, especially under adversarial attacks. To mitigate these vulnerabilities, the paper proposes an explainable adversarial-robust Vision-Language-Action model built on the OpenVLA-OFT framework. A key innovation is the integration of the Evidence-3 module, designed to detect photometric perturbations and provide natural language explanations outlining their causes and effects. Experimental results demonstrate the effectiveness of the proposed model, showing a reduction in Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to a baseline model. This indicates that the approach not only enhances the accuracy of action prediction in smart farming systems but also improves the explainability of the model's decisions under challenging adversarial conditions, contributing to more reliable and transparent automation in agriculture. <div>
arXiv:2512.11865v1 Announce Type: new 
Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion</title>
<link>https://arxiv.org/abs/2512.11869</link>
<guid>https://arxiv.org/abs/2512.11869</guid>
<content:encoded><![CDATA[
<div> Monocular 3D lane detection, Temporal LSTM Fusion, Anchor3DLane, multi-task loss, temporal consistency<br><br>Summary:<br><br>1) The paper addresses significant challenges in monocular 3D lane detection, specifically depth ambiguity, occlusion, and temporal instability across video frames.<br>2) It builds upon the Anchor3DLane model, which performs regression of continuous 3D lane curves from multi-camera surround views but has limitations such as sensitivity to regression outliers, insufficient global curve supervision, difficulty balancing multiple loss terms, and underutilization of temporal continuity.<br>3) The authors propose Temporal-Anchor3DLane, which introduces several key improvements: enhanced multi-task loss functions including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, combined with focal and Dice losses for classification and visibility tasks.<br>4) A lightweight Temporal LSTM Fusion module is developed to aggregate per-anchor features across frames, replacing heavier Transformer-based temporal fusion to improve efficiency.<br>5) The training scheme is refined using ESCOP-style techniques that enforce curve-level supervision alongside temporal consistency, yielding improved robustness.<br>6) Experimental results on the OpenLane dataset show a significant F1 score improvement of +6.2 and smoother lane trajectory predictions.<br>7) These enhancements demonstrate that relatively small architectural and loss refinements can substantially boost 3D lane detection robustness without requiring additional sensors or larger model scaling. <div>
arXiv:2512.11869v1 Announce Type: new 
Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops</title>
<link>https://arxiv.org/abs/2512.11871</link>
<guid>https://arxiv.org/abs/2512.11871</guid>
<content:encoded><![CDATA[
<div> Keywords: agriculture, crop disease detection, cactus-fig, mobile-efficient architectures, offline inference<br><br>Summary:<br><br>1. The study focuses on the Tigray region of Ethiopia, where over 80% of the population depends on agriculture but faces challenges in accessing expert crop disease diagnosis due to infrastructural disruptions.  
2. Researchers created a new indigenous cactus-fig (Opuntia ficus-indica) dataset with 3,587 field images categorized into three symptom classes to support disease detection.  
3. Considering deployment constraints in resource-limited, post-conflict edge environments, three mobile-efficient models were evaluated: a custom lightweight CNN, EfficientNet-Lite1, and the hybrid CNN-Transformer MobileViT-XS.  
4. The research isolated cactus-fig model performance from other crops (potato, apple, corn) to specifically study attention sensitivity and inductive bias transfer relevant to the unique morphology of indigenous plants.  
5. Results revealed a Pareto trade-off between accuracy and deployment efficiency: EfficientNet-Lite1 achieved 90.7% test accuracy; the lightweight CNN offered 89.5% accuracy with the best latency (42 ms) and smallest model size (4.8 MB); MobileViT-XS attained the highest mean cross-validation accuracy of 97.3% by leveraging MHSA-based global reasoning, effectively differentiating pest clusters and fungal lesions.  
6. The trained ARM-compatible models are integrated into a Flutter application localized in Tigrigna and Amharic, capable of fully offline inference on Cortex-A53 devices, promoting inclusive, food security critical diagnostics in the region. <div>
arXiv:2512.11871v1 Announce Type: new 
Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training</title>
<link>https://arxiv.org/abs/2512.11874</link>
<guid>https://arxiv.org/abs/2512.11874</guid>
<content:encoded><![CDATA[
<div> Keywords: self-training, semantic segmentation, SegFormer, teacher-student loop, data augmentation  

<br><br>Summary:  
This extended abstract presents a solution developed for the Global Wheat Full Semantic Segmentation Competition. The authors designed a systematic self-training framework aimed at enhancing model performance through effective data utilization. Central to their approach is a two-stage hybrid training strategy combined with extensive data augmentation techniques to improve generalization. The core model employed is SegFormer, which uses a Mix Transformer (MiT-B4) backbone known for its strong feature representation abilities. To further refine accuracy, the team implements an iterative teacher-student loop, allowing the model to progressively improve by leveraging its own predictions as pseudo-labels for retraining. This iterative process maximizes the use of available data and helps mitigate overfitting issues. The proposed method demonstrated competitive performance metrics on both the Development and Testing Phase datasets of the competition. Overall, the framework effectively integrates advanced semantic segmentation techniques with iterative self-training to address the challenge of wheat segmentation in diverse visual conditions. <div>
arXiv:2512.11874v1 Announce Type: new 
Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors</title>
<link>https://arxiv.org/abs/2512.11884</link>
<guid>https://arxiv.org/abs/2512.11884</guid>
<content:encoded><![CDATA[
<div> Keywords: instance segmentation, SAM3, YOLO11, MinneApple dataset, zero-shot learning<br><br>Summary:<br><br>This paper presents a detailed comparison between the Segment Anything Model version 3 (SAM3) operating in zero-shot mode and three fine-tuned variants of Ultralytics YOLO11 (nano, medium, and large) for instance segmentation. The evaluation uses the MinneApple dataset featuring 670 orchard images with 28,179 apple instances, emphasizing high object density and occlusion challenges. The study reveals that Intersection over Union (IoU) threshold choices significantly influence performance gaps—up to 30% variation. At a low IoU threshold of 0.15, YOLO models outperform SAM3 in F1 scores, achieving 68.9%, 72.2%, and 71.9% versus SAM3’s 59.8% in zero-shot mode, highlighting YOLO’s superiority in detection completeness when fine-tuned. However, YOLO models experience steep performance degradation (48-50 points) over varying IoU thresholds, while SAM3’s performance decreases by only 4 points, demonstrating 12 times greater boundary stability and mask precision. This suggests a trade-off between SAM3’s mask accuracy and YOLO’s detection coverage. The authors provide open-source code, evaluation pipelines, and methodological recommendations for selecting between specialized fine-tuned or generalist foundation models in dense instance segmentation tasks, contributing valuable insights for practical applications. The project's repository is accessible on GitHub. <div>
arXiv:2512.11884v1 Announce Type: new 
Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description</title>
<link>https://arxiv.org/abs/2512.11894</link>
<guid>https://arxiv.org/abs/2512.11894</guid>
<content:encoded><![CDATA[
<div> mmWave radar, Implicit Neural Representations, hypernetworks, activity recognition, pose estimation<br><br>Summary:<br><br>This paper introduces mmWeaver, a novel framework designed to synthesize realistic, environment-specific mmWave radar signals efficiently. It addresses challenges in mmWave signal generation, which is traditionally complex, sparse, and computationally expensive to simulate physically. mmWeaver uses Implicit Neural Representations (INRs) to model signals as continuous functions, achieving up to 49-fold data compression. The framework employs hypernetworks that dynamically generate INR parameters conditioned on environmental context from RGB-D images and human motion features derived via text-to-pose generation through MotionGPT. By incorporating semantic and geometric priors, mmWeaver produces diverse in-phase/quadrature (I/Q) signals at multiple resolutions, retaining critical phase information essential for tasks like point cloud estimation and activity classification. Experimentally, mmWeaver attains a complex Structural Similarity Index Measure (SSIM) of 0.88 and a Peak Signal-to-Noise Ratio (PSNR) of 35 dB, surpassing prior methods in signal realism. Additionally, it enhances activity recognition accuracy by up to 7% and decreases human pose estimation error by up to 15%. Notably, mmWeaver operates significantly faster—between 6 and 35 times—compared to traditional simulation approaches, enabling efficient and adaptive mmWave signal synthesis tailored to real-world environments. <div>
arXiv:2512.11894v1 Announce Type: new 
Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hot H\'em: S\`ai G\`on Gi\~ua C\'ai N\'ong H\^ong C\`ong B\`ang -- Saigon in Unequal Heat</title>
<link>https://arxiv.org/abs/2512.11896</link>
<guid>https://arxiv.org/abs/2512.11896</guid>
<content:encoded><![CDATA[
<div> Pedestrian heat exposure, GeoAI, land surface temperature, semantic image segmentation, heat-aware routing<br><br>Summary:<br><br>1. The article addresses the critical health risk posed by pedestrian heat exposure in dense tropical cities, specifically focusing on Hô Chí Minh City (HCMC), Vietnam.  
2. It highlights the inadequacy of standard routing algorithms, which often ignore micro-scale thermal variations experienced at a pedestrian level.  
3. The authors introduce Hot Hém, a GeoAI workflow designed to estimate and operationalize pedestrian heat exposure by integrating multiple spatial data science techniques.  
4. The workflow combines Google Street View imagery, semantic image segmentation, and remote sensing data to generate detailed thermal information at a fine spatial resolution.  
5. Two XGBoost machine learning models are trained using a GSV-derived dataset from selected administrative wards (phường) to predict land surface temperature (LST).  
6. These models are applied in a patchwork manner across the entire pedestrian network derived from OSMnx, allowing the creation of a heat-aware pedestrian routing system.  
7. The developed model can help city planners and stakeholders identify corridors with disproportionately high temperatures and understand the underlying infrastructural factors contributing to heat exposure, enabling better urban climate resilience and public health mitigation strategies. <div>
arXiv:2512.11896v1 Announce Type: new 
Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot H\'em is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in H\^o Ch\'i Minh City (HCMC), Vi\d{e}t Nam, colloquially known as S\`ai G\`on. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as ph\u{o}ng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic</title>
<link>https://arxiv.org/abs/2512.11898</link>
<guid>https://arxiv.org/abs/2512.11898</guid>
<content:encoded><![CDATA[
<div> Microscopic vehicle trajectories, UAV data, heterogeneous traffic, urban traffic, Data from Sky<br><br>Summary: This paper introduces openly accessible microscopic vehicle trajectory (MVT) datasets gathered via unmanned aerial vehicles (UAVs) in heterogeneous and area-based urban traffic environments. Conventional roadside video capture often struggles with occlusions, limited viewpoints, and erratic vehicle movements in dense mixed traffic scenarios. In contrast, UAV-based recordings provide a top-down perspective that alleviates these limitations, enabling detailed capture of spatial and temporal traffic dynamics. The datasets were created using the Data from Sky (DFS) platform and rigorously validated against manual vehicle counts, space mean speeds, and probe trajectories documented in previous studies. Each dataset includes timestamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a high resolution of 30 frames per second. Data collection occurred at six mid-block urban locations in the national capital region of India, capturing a broad spectrum of traffic compositions and densities. Exploratory analyses of the datasets reveal key behavioral patterns such as lane-keeping tendencies, speed distributions, and typical lateral maneuvers inherent to heterogeneous traffic. These datasets aim to serve the global research community in traffic simulation modeling, safety analysis, and behavioral studies within complex urban traffic settings. By providing these empirical data openly, the study enables improved development, testing, and validation of models that better reflect real-world urban traffic complexities. <div>
arXiv:2512.11898v1 Announce Type: new 
Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.11899</link>
<guid>https://arxiv.org/abs/2512.11899</guid>
<content:encoded><![CDATA[
<div> typographic attacks, vision-language models, selective text use, visual question answering, RIO-Bench  

<br><br>Summary:  
This paper addresses the vulnerability of large vision-language models (LVLMs) to typographic attacks, where misleading text in images can override correct visual understanding. Existing evaluation protocols and defenses focus mainly on object recognition and often encourage ignoring text entirely for robustness. However, many real-world scenarios require models to reason jointly about both visual objects and embedded text, such as reading traffic signs while recognizing pedestrians. To tackle this gap, the authors introduce a new task named Read-or-Ignore VQA (RIO-VQA), which challenges models to selectively decide when to read text or ignore it based on context during visual question answering (VQA). For evaluation, the paper proposes the Read-or-Ignore Benchmark (RIO-Bench), a dataset containing real images paired with same-scene counterfactuals differing only in textual content and question types to test both reading and ignoring capabilities. Experimental results on RIO-Bench show that current strong LVLMs and defenses fail to effectively balance robustness to typographic attacks with the ability to read relevant text. To improve performance, the authors develop a novel data-driven defense that adaptively selects text use, moving beyond previous methods that simply ignore text non-adaptively. Overall, this work highlights a fundamental misalignment between existing evaluations and real-world requirements and offers a principled direction for developing more reliable LVLMs. Further details and resources are available on their project page. <div>
arXiv:2512.11899v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities</title>
<link>https://arxiv.org/abs/2512.11901</link>
<guid>https://arxiv.org/abs/2512.11901</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal fusion, Graph Attention Network, representation learning, multimodal robustness, contrastive loss<br><br>Summary:<br><br>1. CLARGA is a versatile multimodal fusion architecture designed to handle any number and type of input modalities without altering its framework.<br><br>2. It constructs sample-specific attention-weighted graphs over modality features, leveraging a multi-head Graph Attention Network to enable adaptive and efficient cross-modal interaction.<br><br>3. The model achieves computational efficiency with sub-quadratic complexity as the number of modalities increases and incorporates a learnable mask to manage missing modality inputs.<br><br>4. Training employs a hybrid loss combining supervised task objectives with contrastive InfoNCE loss, which enhances cross-modal consistency and resilience to noisy or incomplete data.<br><br>5. Extensive experiments across 7 diverse datasets—including finance, human-computer interaction, multimedia classification, and affective computing—show that CLARGA outperforms existing baselines and state-of-the-art models, demonstrating robustness and strong performance on niche tasks.<br><br>Overall, CLARGA offers a plug-and-play, efficient, and adaptive solution for multimodal representation learning across a wide range of machine learning applications. <div>
arXiv:2512.11901v1 Announce Type: new 
Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life</title>
<link>https://arxiv.org/abs/2512.11905</link>
<guid>https://arxiv.org/abs/2512.11905</guid>
<content:encoded><![CDATA[
<div> Keywords: subjective well-being, smile intensity, smartphone sensing, positive affect, deep learning<br><br>Summary:<br><br>
1. The study addresses the challenge of measuring subjective well-being, traditionally reliant on self-report methods that are vulnerable to recall bias and participant burden. <br>2. Researchers hypothesized that candid smiles, captured unobtrusively during natural smartphone interactions, could serve as an objective and scalable indicator of positive affect.<br>3. Analysis was performed on 405,448 video clips collected passively from 233 participants over one week, with a deep learning model quantifying smile intensity.<br>4. Results revealed distinct diurnal and daily patterns of smiling, with daily smile intensity correlating strongly with national happiness survey data (r=0.92) and diurnal rhythms aligning with day reconstruction method results (r=0.80).<br>5. Higher daily mean smile intensity was significantly linked to increased physical activity and light exposure, while no significant relationship was found with smartphone usage duration.<br>6. The findings indicate that passive smartphone sensing of smiles can provide a robust, ecologically valid approach to studying real-world affective behaviors, potentially enabling population-scale research on well-being dynamics. <div>
arXiv:2512.11905v1 Announce Type: new 
Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPath: Multimodal Pathology Report Generation from Whole Slide Images</title>
<link>https://arxiv.org/abs/2512.11906</link>
<guid>https://arxiv.org/abs/2512.11906</guid>
<content:encoded><![CDATA[
<div> Keywords: pathology report generation, whole slide images, multimodal framework, BioBART, visual-prefix prompting<br><br>Summary: Automated diagnostic pathology report generation from whole slide images (WSIs) is a challenging task due to high morphological variability and complex narrative structures in pathology. The paper introduces MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on visual embeddings derived from WSIs using a visual-prefix prompting mechanism. Unlike traditional end-to-end vision-language pretraining, MPath employs foundation-model WSI features from CONCH and Titan and integrates them into BioBART through a compact projection module, allowing the language model to remain frozen, which enhances stability and data efficiency. The approach was evaluated on the RED 2025 Grand Challenge dataset, where MPath secured 4th place in Test Phase 2 despite having limited chances to submit. The results demonstrate that prompt-based multimodal conditioning is a promising scalable and interpretable method for generating pathology reports directly from WSIs. This method leverages foundation models effectively while reducing training complexity and maintaining high performance in clinical text generation for computational pathology. <div>
arXiv:2512.11906v1 Announce Type: new 
Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</title>
<link>https://arxiv.org/abs/2512.11925</link>
<guid>https://arxiv.org/abs/2512.11925</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D plant models, procedural modeling, large language models, parametric geometry, computational phenotyping<br><br>Summary:<br><br>1. The paper introduces FloraForge, a novel framework that integrates large language models (LLMs) to assist domain experts in creating accurate, fully parametric 3D plant models using natural language refinements, reducing the need for programming expertise.<br><br>2. FloraForge leverages LLM-enabled co-design to iteratively refine Python scripts that generate hierarchical B-spline surface representations of plants, ensuring botanical accuracy through explicit control points and parametric deformation functions.<br><br>3. The resulting 3D models are mathematically continuous and can be tessellated into polygonal meshes with arbitrary precision, making them compatible with advanced plant functional structural analyses such as light simulation, CFD, and finite element analysis.<br><br>4. The framework was demonstrated on multiple plant species including maize, soybean, and mung bean, fitting procedural models to empirical point cloud data via manual refinement of human-readable Plant Descriptor files.<br><br>5. FloraForge outputs two types of meshes: one optimized for visualization and another augmented with parametric metadata for quantitative analysis, combining LLM-assistance, parametric control, and biological rigor to democratize sophisticated geometric modeling for plant science.<br><br> <div>
arXiv:2512.11925v1 Announce Type: new 
Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</title>
<link>https://arxiv.org/abs/2512.11926</link>
<guid>https://arxiv.org/abs/2512.11926</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D object detection, LiDAR, transformer, point cloud completion, autonomous driving  

<br><br>Summary:  
This paper addresses the challenge of detecting objects in distant regions with sparse LiDAR points, critical for autonomous driving. It proposes a joint completion and detection framework that enhances detection features in sparse areas without increasing computational cost. The core innovation is TransBridge, a transformer-based up-sampling block that fuses features from both detection and completion networks, allowing the detection network to leverage implicit completion features. Additionally, the Dynamic-Static Reconstruction (DSRecon) module is introduced to generate dense LiDAR data as ground truth for the completion network. The use of transformer mechanisms facilitates the connection of channel-wise and spatial relations, leading to a high-resolution feature map beneficial for completion tasks. Extensive evaluation on the nuScenes and Waymo datasets validates the framework's effectiveness, showing consistent improvements in end-to-end 3D object detection. Performance gains in mean average precision (mAP) range between 0.7 to 1.5 points across various methods, demonstrating strong generalization ability. Notably, for two-stage detection frameworks, the approach yields an mAP improvement of up to 5.78 points, highlighting its substantial impact on detection accuracy in sparse LiDAR scenarios. <div>
arXiv:2512.11926v1 Announce Type: new 
Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion</title>
<link>https://arxiv.org/abs/2512.11928</link>
<guid>https://arxiv.org/abs/2512.11928</guid>
<content:encoded><![CDATA[
<div> Cell Painting, Diffusion Model, Brightfield Imaging, Time-Lapse Videos, In-Context Learning  

<br><br>Summary: This paper addresses two major limitations of traditional cell painting techniques: the labor-intensive process and the requirement for chemical fixation, which prevents studying cell dynamics. To overcome these challenges, the authors develop MONET (Morphological Observation Neural Enhancement Tool), a diffusion model trained on a large dataset to predict cell paint channels directly from brightfield images. They demonstrate that increasing the scale of the model leads to improved prediction quality. MONET incorporates a consistency architecture that enables the generation of time-lapse videos, despite the lack of actual cell paint video training data. Furthermore, this architecture supports a form of in-context learning, allowing the model to partially generalize to out-of-distribution cell lines and diverse imaging protocols. The work suggests that virtual cell painting via MONET is not intended to fully replace physical cell painting but rather to serve as a complementary tool. This virtual approach can facilitate novel biological research workflows by providing high-contrast, interpretable morphological images without the constraints of chemical fixation and laborious imaging setups. <div>
arXiv:2512.11928v1 Announce Type: new 
Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains</title>
<link>https://arxiv.org/abs/2512.11939</link>
<guid>https://arxiv.org/abs/2512.11939</guid>
<content:encoded><![CDATA[
<div> Peano scan, hidden Markov chains, Bayesian segmentation, contextual PS, evidential Markov chains<br><br>Summary:<br><br>This article focuses on the transformation of two-dimensional image pixel data into one-dimensional sequences using a Peano scan (PS), facilitating the application of hidden Markov chains (HMCs) for unsupervised image segmentation. It highlights that Bayesian segmentation methods based on HMCs can rival those based on hidden Markov fields (HMFs) while being computationally more efficient. The study introduces an extension called the contextual Peano scan (CPS) and presents the associated HMC model, HMC-CPS, which has shown promising initial results in image segmentation tasks. Furthermore, it discusses the development of hidden evidential Markov chains (HEMCs), which enhance Bayesian segmentation performance beyond standard HMCs. The core contribution is the new HEMC-CPS model that integrates contextual PS with evidential HMC, demonstrating its effectiveness in maximum posterior mode (MPM) Bayesian segmentation on both synthetic and real images. The segmentation process is unsupervised, with parameters estimated through a stochastic expectation-maximization (SEM) algorithm. The paper suggests the HEMC-CPS model holds significant potential for handling complex imaging data, including three-dimensional and multi-sensor, multi-resolution images. Lastly, it notes that these models extend beyond image segmentation and may be applied to any spatially correlated data analysis. <div>
arXiv:2512.11939v1 Announce Type: new 
Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11941</link>
<guid>https://arxiv.org/abs/2512.11941</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot learning, skeleton-based action recognition, visual-semantic alignment, dynamic refinement, large language model<br><br>Summary:  
This paper addresses the limitations of zero-shot skeleton-based action recognition (ZS-SAR) methods that rely on static class-level semantic alignment, which struggle to generalize to unseen classes due to coarse visual-semantic correspondences. The proposed framework, DynaPURLS, improves this by creating multi-scale visual-semantic mappings and dynamically refining them during inference to adapt to domain shifts. DynaPURLS utilizes a large language model to generate hierarchical textual descriptions capturing both overall movements and fine-grained body-part dynamics, enriching semantic information. On the visual side, an adaptive partitioning module groups skeleton joints semantically to produce fine-grained visual representations that better correlate with the detailed textual descriptions. To further address distribution shifts between seen and unseen classes, a dynamic refinement module is introduced that adapts textual features through a lightweight learnable projection during inference. This process is stabilized using a confidence-aware, class-balanced memory bank which reduces error accumulation caused by noisy pseudo-labels. Extensive experiments on major benchmarks NTU RGB+D 60/120 and PKU-MMD show that DynaPURLS outperforms previous methods and sets new state-of-the-art results. The source code is publicly available, facilitating reproducibility and further research. <div>
arXiv:2512.11941v1 Announce Type: new 
Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer</title>
<link>https://arxiv.org/abs/2512.11977</link>
<guid>https://arxiv.org/abs/2512.11977</guid>
<content:encoded><![CDATA[
<div> Keywords: Predictive maintenance, Semiconductor wafer defects, Data-Efficient Image Transformer (DeiT), Convolutional Neural Networks, Imbalanced data<br><br>Summary:<br><br>This study focuses on improving defect detection in semiconductor wafer maintenance, a critical area requiring accurate fault prediction to reduce costs. Traditional convolutional neural networks (CNNs) like VGG-19, Xception, and SqueezeNet have been widely used for wafer defect classification but struggle with limited and imbalanced datasets common in this domain. To address this challenge, the research evaluates the Data-Efficient Image Transformer (DeiT) model's performance in these data-constrained scenarios. Experimental analysis demonstrates that DeiT outperforms the CNN baselines, achieving a classification accuracy of 90.83%, compared to VGG-19 (65%), SqueezeNet (82%), Xception (66%), and a Hybrid model (67%). DeiT also excels in the F1-score metric, scoring 90.78%, indicating balanced precision and recall, with particular strength in identifying minority defect classes, which is crucial for effective fault detection. Moreover, DeiT exhibits faster training convergence, improving computational efficiency. These results underline the potential of transformer-based architectures like DeiT in enhancing semiconductor wafer defect detection, supporting more reliable predictive maintenance strategies, and ultimately contributing to better operational efficiency and reduced downtime in semiconductor fabrication processes. <div>
arXiv:2512.11977v1 Announce Type: new 
Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction</title>
<link>https://arxiv.org/abs/2512.11988</link>
<guid>https://arxiv.org/abs/2512.11988</guid>
<content:encoded><![CDATA[
<div> Keywords: human-object interaction, monocular RGB video, 4D reconstruction, pose hypothesis selection, category-agnostic<br><br>Summary:<br><br>This paper addresses the challenge of reconstructing accurate 4D human-object interactions from single monocular RGB videos, an important task for applications such as human understanding, gaming, and robot learning. The problem is difficult due to unknown object and human information, depth ambiguity, occlusion, and complex motion that hinder consistent spatial and temporal 3D reconstructions. Unlike prior methods that rely on ground truth object templates or focus on limited object categories, the authors introduce CARI4D, the first category-agnostic method capable of metric-scale 4D reconstruction of human-object interactions from monocular RGB input. The method involves a pose hypothesis selection algorithm that integrates predictions from foundation models and further refines these via a learned render-and-compare approach to ensure spatial, temporal, and pixel-level consistency. Additionally, it reasons about detailed physical contacts to refine the interaction while enforcing physical constraints. Experimental results demonstrate that CARI4D outperforms state-of-the-art methods by 38% on known datasets and 36% on unseen datasets in terms of reconstruction error. Remarkably, the model generalizes well beyond training categories, enabling zero-shot application to in-the-wild internet videos. The authors plan to release their code and pretrained models publicly. <div>
arXiv:2512.11988v1 Announce Type: new 
Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</title>
<link>https://arxiv.org/abs/2512.11995</link>
<guid>https://arxiv.org/abs/2512.11995</guid>
<content:encoded><![CDATA[
<div> Visual Reasoning, Multi-step Exploration, Vision-language Models, Chain-of-Questions, Benchmark

<br><br>Summary: This paper addresses the limitations of current vision-language models (VLMs) that perform well on simple, well-defined questions but struggle with complex, open-ended visual reasoning tasks requiring multiple exploration steps. The authors introduce V-REX (Visual Reasoning with multi-step EXploration), a comprehensive evaluation suite consisting of a benchmark and protocol designed to test VLMs’ ability to engage in native multi-step exploratory reasoning across diverse real-world scenarios. V-REX reformulates multi-step reasoning as a Chain-of-Questions (CoQ), separating the process into two distinct capabilities: (1) Planning, where the model breaks down a complex task by selecting a sequence of exploratory questions, and (2) Following, where it sequentially answers these questions to gather necessary information for the final solution. By limiting the options for questions and answers at each step, V-REX enables fine-grained, quantitative evaluation of intermediate reasoning stages rather than only final outcomes. Experiments on state-of-the-art proprietary and open-source VLMs reveal consistent trends with model scaling and highlight notable gaps between planning and following skills. These findings demonstrate significant opportunities to advance multi-step exploratory visual reasoning in VLMs. <div>
arXiv:2512.11995v1 Announce Type: new 
Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title>
<link>https://arxiv.org/abs/2512.12012</link>
<guid>https://arxiv.org/abs/2512.12012</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous Vehicles, Long-Tail Data, Neuro-symbolic Framework, Open-Vocabulary Detector, Privacy-Preserving  

<br><br>Summary:  
The paper introduces Semantic-Drive, a novel local-first neuro-symbolic framework designed to address the scarcity of rare, safety-critical "Long-Tail" training data in Autonomous Vehicles (AVs). The approach separates perception into two key stages: (1) Symbolic Grounding, which uses a real-time open-vocabulary detector called YOLOE to focus attention on relevant objects, and (2) Cognitive Analysis, which employs a Reasoning Vision-Language Model (VLM) for detailed forensic scene evaluation. To counter hallucination and improve inference accuracy, Semantic-Drive implements a "System 2" alignment strategy featuring a multi-model "Judge-Scout" consensus mechanism. Evaluated on the nuScenes dataset with reference to the Waymo Open Dataset taxonomy, the framework achieves a high Recall of 0.966, significantly outperforming the 0.475 Recall of CLIP, and reduces Risk Assessment Error by 40% relative to single-model baselines. Importantly, Semantic-Drive runs fully on consumer-grade hardware (NVIDIA RTX 3090), thereby providing a privacy-preserving, cost-effective alternative to cloud-based video large models (VLMs). This system supports more precise and scalable semantic data mining without compromising user data privacy or incurring cloud operational costs. <div>
arXiv:2512.12012v1 Announce Type: new 
Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.12013</link>
<guid>https://arxiv.org/abs/2512.12013</guid>
<content:encoded><![CDATA[
<div> mmWave radar, human activity recognition, dynamic graph neural network, spatial-temporal features, resource-constrained platforms  

<br><br>Summary:  
This paper addresses the challenges of human activity recognition (HAR) using mmWave radar point clouds, which are inherently sparse and variable in size, unlike dense vision-based data. The authors propose a novel graph-based approach utilizing a discrete dynamic graph neural network (DDGNN) to capture spatial-temporal features of human movements effectively. A star graph structure is designed, incorporating a static center point connected to dynamic radar points within the same and consecutive frames, to model high-dimensional relative relationships. The DDGNN is specifically tailored to handle variable-sized input data from the mmWave radar. Experimental evaluations on real-world HAR datasets demonstrate that the proposed method achieves a high overall classification accuracy of 94.27%, which is close to the 97.25% accuracy attained by vision-based skeleton data. Additionally, the authors perform inference tests on a resource-constrained platform (Raspberry Pi 4), showcasing the method’s practical applicability in low-power environments. A comprehensive ablation study validates the design choices behind variable DDGNN structures. Importantly, the system outperforms three recent radar-specific HAR methods without needing data resampling or frame aggregation steps, highlighting its efficiency and effectiveness in handling mmWave radar data for HAR tasks. <div>
arXiv:2512.12013v1 Announce Type: new 
Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive federated learning for ship detection across diverse satellite imagery sources</title>
<link>https://arxiv.org/abs/2512.12053</link>
<guid>https://arxiv.org/abs/2512.12053</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Ship Detection, Satellite Imagery, YOLOv8, Privacy-Preserving<br><br>Summary:  
This study explores the use of Federated Learning (FL) for ship detection across multiple heterogeneous satellite datasets, enabling privacy-preserving collaboration without the need to share raw data or centralize collections. The approach is particularly useful when dealing with commercial satellite images or sensitive ship annotations, preserving data confidentiality. Four FL algorithms—FedAvg, FedProx, FedOpt, and FedMedian—are implemented and benchmarked against a local training baseline in which the YOLOv8 detection model is trained independently on each dataset without parameter sharing. Results demonstrate that FL models significantly enhance detection accuracy compared to local-only training on smaller datasets. Furthermore, FL performance approaches that of global training, which requires aggregating all datasets into a single training set. The investigation highlights the critical role of careful hyperparameter tuning, such as the number of communication rounds and the number of local training epochs per round, to balance detection performance with computational efficiency. The findings suggest that FL is a promising approach for improving ship detection accuracy across diverse satellite data sources while safeguarding sensitive information. <div>
arXiv:2512.12053v1 Announce Type: new 
Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management</title>
<link>https://arxiv.org/abs/2512.12056</link>
<guid>https://arxiv.org/abs/2512.12056</guid>
<content:encoded><![CDATA[
<div> Burned areas, semantic segmentation, SPOT-6/7 imagery, U-Net, Test-Time Augmentation  

<br><br>Summary: After wildfires, accurately mapping burned areas (BAs) is essential for damage assessment and ecosystem recovery. This study presents a supervised semantic segmentation workflow designed to improve both the accuracy and efficiency of BA delineation, focusing on high-resolution SPOT-6/7 satellite imagery known for its on-demand availability. Performance is measured using Dice score, Intersection over Union, and inference time. The experimentation reveals that U-Net and SegFormer models achieve comparable results when trained on limited data; however, SegFormer's greater computational resource needs limit its suitability for urgent emergency response situations. The integration of land cover data as an auxiliary task strengthens model robustness without increasing inference time. Additionally, the application of Test-Time Augmentation (TTA) enhances the quality of BA delineation but at the cost of longer inference times, which can be addressed through optimization strategies such as Mixed Precision computing. Overall, this workflow balances model performance with operational efficiency, making it pragmatic for time-sensitive wildfire management. <div>
arXiv:2512.12056v1 Announce Type: new 
Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos</title>
<link>https://arxiv.org/abs/2512.12060</link>
<guid>https://arxiv.org/abs/2512.12060</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, diffusion models, video restoration, structural artifacts, temporal coherence

<br><br>Summary:  
Modern text-to-video diffusion models generate compelling clips but often suffer from structural issues like distorted faces, warped backgrounds, and inconsistent motion, similar to artifacts seen in very low-quality real-world videos. Traditional video restoration and super-resolution methods mainly handle synthetic degradations like blur and downsampling but tend to stabilize rather than fix such structural defects. Diffusion-prior restorers, typically trained on photometric noise, lack fine control between perceptual quality and fidelity. To address these challenges, the authors introduce CreativeVR, a diffusion-prior-guided video restoration framework designed for AI-generated and real videos exhibiting severe structural and temporal artifacts. CreativeVR uses a deep-adapter-based approach featuring a single precision control knob that balances between faithful restoration of standard degradations and stronger corrective action on complex artifacts. A key innovation is a temporally coherent degradation module during training, which introduces realistic structural failures to help the model learn effective correction. For evaluation, the authors propose the AIGC54 benchmark incorporating FIQA, semantic, and perceptual metrics with multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts, performs competitively on standard benchmarks, and operates efficiently at about 13 frames per second at 720p on a single 80-GB A100 GPU. The project page is available at https://daveishan.github.io/creativevr-webpage/. <div>
arXiv:2512.12060v1 Announce Type: new 
Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.12080</link>
<guid>https://arxiv.org/abs/2512.12080</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive video models, exposure bias, Backwards Aggregation, causal diffusion transformers, long-horizon motion stability  

<br><br>Summary:  
This paper addresses the challenge of exposure bias in autoregressive video models, which arises due to the discrepancy between training on clean context frames and inference on self-generated frames, leading to progressive error accumulation and quality degradation over time. To mitigate this, the authors propose Backwards Aggregation (BAgger), a novel self-supervised method that generates corrective trajectories from the model's own rollouts, allowing it to learn recovery from its mistakes. Unlike traditional techniques that depend on few-step distillation or distribution-matching losses—which often compromise video quality and diversity—BAgger employs standard score or flow matching objectives, thereby eliminating the need for large teacher models and extensive backpropagation through time. The method is exemplified by integrating BAgger with causal diffusion transformers and tested across several tasks including text-to-video generation, video extension, and multi-prompt video creation. Experimental results demonstrate that BAgger enhances long-horizon motion stability and maintains superior visual consistency by reducing frame drift, thus advancing the performance and robustness of autoregressive video modeling approaches. <div>
arXiv:2512.12080v1 Announce Type: new 
Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.12083</link>
<guid>https://arxiv.org/abs/2512.12083</guid>
<content:encoded><![CDATA[
<div> VFM representations, latent diffusion models, RePack, Diffusion Transformers, dimensionality reduction  

<br><br>Summary: This paper addresses the challenge of incorporating high-dimensional Vision Foundation Model (VFM) representations into latent diffusion models (LDMs) for image generation. While injecting VFM features such as those from DINOv3 improves semantic understanding and generation quality, their large size can cause information overload and complicate decoding. To resolve this, the authors propose RePack (Representation Packing), a framework designed to compress VFM representations into compact, low-dimensional manifolds that are easier for Diffusion Transformers (DiTs) to decode. RePack effectively filters out non-semantic noise and retains the essential structural information necessary for high-fidelity image reconstruction. Experimental comparisons demonstrate that RePack accelerates the convergence of DiTs significantly, outperforming methods that use raw VFM features. Specifically, on the DiT-XL/2 architecture, RePack achieves a Fréchet Inception Distance (FID) score of 3.66 in only 64 training epochs—35% faster than the previous state-of-the-art. This shows that RePack not only preserves the semantic richness of VFM representations but also mitigates the drawbacks of their high dimensionality, leading to more efficient and effective diffusion model training and improved image generation outcomes. <div>
arXiv:2512.12083v1 Announce Type: new 
Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering</title>
<link>https://arxiv.org/abs/2512.12089</link>
<guid>https://arxiv.org/abs/2512.12089</guid>
<content:encoded><![CDATA[
<div> Keywords: Large vision-language models, hallucinations, visual attention, attention maps, VEGAS<br><br>Summary:<br><br>1. Large vision-language models (LVLMs) demonstrate strong capabilities in reasoning over combined visual and textual data but tend to generate hallucinated outputs—linguistically plausible yet visually inconsistent information. 2. The paper identifies that hallucinations occur when the LVLM's final visual-attention maps do not focus adequately on key objects within images, while attention maps from the vision encoder are more focused and reduce hallucinations. 3. Analysis reveals that vision-text conflicts, which contribute to hallucinations, peak in the middle layers of the language model during decoding. 4. By injecting the vision encoder’s attention maps into these middle layers, hallucinations can be effectively suppressed. 5. Based on these insights, the authors propose VEGAS, a straightforward inference-time approach that incorporates the vision encoder’s attention maps into the language model’s mid-layers, adaptively guiding the token generation process away from hallucinated content. 6. Extensive benchmark evaluations demonstrate that VEGAS achieves state-of-the-art results in reducing hallucinations in LVLMs, emphasizing the importance of vision encoder attention for reliable multimodal reasoning. <div>
arXiv:2512.12089v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPDMark: Selective Parameter Displacement for Robust Video Watermarking</title>
<link>https://arxiv.org/abs/2512.12090</link>
<guid>https://arxiv.org/abs/2512.12090</guid>
<content:encoded><![CDATA[
<div> Keywords: video watermarking, diffusion model, low-rank adaptation, temporal consistency, robustness<br><br>Summary:<br>1. SPDMark is a novel in-generation watermarking framework designed specifically for video diffusion models to embed watermarks into generated videos by selectively displacing model parameters.<br>2. The watermark embedding is performed by additive compositions of layer-wise basis shifts indexed by a watermarking key, which enables robustness and tractability.<br>3. Low-rank adaptation (LoRA) is used to efficiently implement these basis shifts, optimizing parameter usage.<br>4. During training, basis shifts and watermark extractors are jointly optimized by minimizing losses related to message recovery accuracy, perceptual similarity, and temporal consistency.<br>5. To handle temporal attacks, a cryptographic hashing function generates frame-specific watermark messages, while maximum bipartite matching during extraction helps recover correct frame order.<br>6. SPDMark is evaluated on both text-to-video and image-to-video generation, demonstrating imperceptible watermarks that can be accurately recovered.<br>7. Extensive testing confirms the method’s robustness against common video modifications, and its computational efficiency surpasses existing post-hoc and in-generation watermarking approaches.<br>Overall, SPDMark effectively balances imperceptibility, robustness, and computational practicality in video watermarking for diffusion-based generative models. <div>
arXiv:2512.12090v1 Announce Type: new 
Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging</title>
<link>https://arxiv.org/abs/2512.12101</link>
<guid>https://arxiv.org/abs/2512.12101</guid>
<content:encoded><![CDATA[
<div> Keywords: pollen recognition, digital in-line holographic microscopy, YOLOv8, MobileNetV3L, GAN augmentation<br><br>Summary:<br><br>1. This study focuses on fully automated pollen recognition using both conventional optical microscopy and digital in-line holographic microscopy (DIHM) images. <br>2. Recognizing pollen in unreconstructed holographic images is challenging due to speckle noise, twin-image artifacts, and significant visual differences compared to bright-field images.<br>3. The baseline performance was established by training YOLOv8s for object detection and MobileNetV3L for pollen classification on a dual-modality dataset comprising automatically annotated optical and affinely aligned DIHM images.<br>4. On optical images, detection achieved a mean average precision at IoU 0.5 (mAP50) of 91.3% and classification accuracy of 97%, while on DIHM images, detection mAP50 was only 8.15% and classification accuracy 50%.<br>5. Expanding DIHM bounding boxes based on aligned optical image annotations slightly improved detection to 13.3% mAP50 and classification accuracy to 54%.<br>6. To enhance DIHM detection, a Wasserstein GAN with spectral normalization (WGAN-SN) was used to generate synthetic DIHM images, producing an FID score of 58.246.<br>7. Mixing real DIHM images with synthetic GAN-generated images at a ratio of 1.0 : 1.5 improved detection mAP50 to 15.4%.<br>8. The results demonstrate that GAN-based data augmentation can modestly bridge the performance gap, moving automated DIHM-based veterinary imaging closer to practical application. <div>
arXiv:2512.12101v1 Announce Type: new 
Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography</title>
<link>https://arxiv.org/abs/2512.12107</link>
<guid>https://arxiv.org/abs/2512.12107</guid>
<content:encoded><![CDATA[
<div> Keywords: echocardiography, vision-language models, multimodal dataset, measurement-grounded, EchoVLM  

<br><br>Summary:  
1. Echocardiography is widely used in cardiology but its interpretation is complex, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning.  
2. Existing vision-language models (VLMs) have limited applicability to echocardiography due to the lack of large-scale, clinically grounded image-text datasets and measurement-based reasoning mechanisms.  
3. The authors introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, containing 19,065 image-text pairs from 1,572 patients, standardized views, structured measurements, measurement-grounded captions, and guideline-based disease labels.  
4. They propose EchoVLM, a vision-language model pretrained with two novel objectives: (i) a view-informed contrastive loss to encode the view-dependent structure of echo imaging, and (ii) a negation-aware contrastive loss to distinguish critical negative from positive clinical findings.  
5. EchoVLM shows state-of-the-art performance across 36 clinical tasks in five categories, including multimodal disease classification (86.5% AUC in zero-shot), image-text retrieval, view classification (95.1% accuracy), chamber segmentation, and landmark detection.  
6. The study demonstrates that clinically grounded multimodal pretraining creates transferable visual representations, positioning EchoVLM as a foundational model for end-to-end echocardiography interpretation.  
7. The authors will release EchoGround-MIMIC and data curation code to support reproducibility and future research in multimodal echocardiography analysis. <div>
arXiv:2512.12107v1 Announce Type: new 
Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Patch-Based TDA Approach for Computed Tomography</title>
<link>https://arxiv.org/abs/2512.12108</link>
<guid>https://arxiv.org/abs/2512.12108</guid>
<content:encoded><![CDATA[
<div> Topological Data Analysis, Persistent Homology, Computed Tomography, Patch-based Approach, Medical Imaging<br><br>Summary:<br><br>This article presents a novel patch-based method for constructing persistent homology (PH) from volumetric computed tomography (CT) images, a key technique in topological data analysis (TDA). Traditional approaches typically use a 3D cubical complex filtration suited for grid-structured data but often face challenges related to computational complexity and suboptimal performance with high-resolution CT images. The proposed patch-based PH method divides volumetric images into smaller patches for analysis, improving both efficiency and effectiveness in extracting topological features such as connected components, cycles, and voids. Extensive experiments on multiple 3D CT datasets demonstrate that the patch-based method consistently outperforms the standard cubical complex algorithm across various metrics, including accuracy, area under the curve (AUC), sensitivity, specificity, and F1 score, with improvements averaging 10.38%, 6.94%, 2.06%, 11.58%, and 8.51%, respectively. This approach offers a significant advancement in ML feature engineering for medical imaging by harnessing deeper structural insights while reducing computational costs. To promote accessibility and adoption, the authors provide Patch-TDA, a Python package implementing this patch-based PH method, facilitating its integration into medical imaging and machine learning workflows. <div>
arXiv:2512.12108v1 Announce Type: new 
Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery</title>
<link>https://arxiv.org/abs/2512.12128</link>
<guid>https://arxiv.org/abs/2512.12128</guid>
<content:encoded><![CDATA[
<div> Keywords: road damage assessment, post-disaster imagery, machine learning, spatial alignment, uncrewed aerial systems (sUAS)<br><br>Summary:<br><br>1. This paper introduces the largest benchmark dataset for road damage assessment and road alignment, named CRASAR-U-DRIODs, based on post-disaster sUAS imagery collected from 10 federally declared disasters.  
2. The dataset addresses three key limitations in previous datasets, which were either small-scale or used low-resolution imagery inadequate for detecting road damages critical to emergency management.  
3. The authors annotated 657.25 km of roads using a 10-class labeling schema to comprehensively categorize damage and conditions.  
4. They trained and tested 18 baseline machine learning models on this dataset and deployed these models operationally during the 2024 responses to Hurricanes Debby and Helene.  
5. Observing significant issues with misaligned road lines in practice, the dataset includes 9,184 spatial adjustments for road line alignment to improve model accuracy.  
6. Deployment experiments show that ignoring spatial alignment reduces model performance by an average of 5.596% in Macro IoU, with around 8% of adverse road conditions mislabeled and 9% of road lines misaligned from the actual roads.  
7. The study highlights essential gaps in machine learning, computer vision, and robotics fields that must be addressed to enhance disaster response through more accurate road damage assessment and alignment. <div>
arXiv:2512.12128v1 Announce Type: new 
Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater</title>
<link>https://arxiv.org/abs/2512.12142</link>
<guid>https://arxiv.org/abs/2512.12142</guid>
<content:encoded><![CDATA[
<div> Keywords: Greenland ice sheet, surface meltwater, deep learning, remote sensing, downscaling<br><br>Summary:<br><br>1. The Greenland ice sheet is melting faster due to complex, not fully understood processes that are difficult to measure directly. Surface meltwater distribution is critical for understanding these processes and can be observed through remote sensing.  
2. Existing surface meltwater maps face a resolution trade-off: they can be high resolution in time or space, but not both simultaneously.  
3. The authors develop a deep learning model that fuses data from remote sensing (synthetic aperture radar and passive microwave) and physics-based regional climate models along with digital elevation models to produce daily 100m resolution meltwater maps over Helheim Glacier, Eastern Greenland, covering 2017-2023.  
4. Using SAR-derived meltwater maps as ground truth, the deep learning approach significantly improves accuracy (95%) compared to non-deep learning methods relying solely on RCM outputs (83%) or passive microwave data (72%). A simpler SAR running window method also achieves good accuracy (90%) but underestimates extreme melt events and does not require deep learning.  
5. The study evaluates common deep learning architectures (UNet and DeepLabv3+), publishes the aligned dataset as MeltwaterBench for benchmarking, and provides code and data openly at github.com/blutjens/hrmelt to support further research in spatiotemporal downscaling of meltwater observations. <div>
arXiv:2512.12142v1 Announce Type: new 
Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Horizons: Evaluating Deep Models in the Wild</title>
<link>https://arxiv.org/abs/2512.12146</link>
<guid>https://arxiv.org/abs/2512.12146</guid>
<content:encoded><![CDATA[
<div> Open-set recognition, few-shot class-incremental learning, CIFAR-10, pretrained visual encoders, prototype-based methods  

<br><br>Summary:  
This paper investigates open-world deployment challenges by studying open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on the CIFAR-10 dataset. For OSR, the authors evaluate three pretrained frozen visual encoders—ResNet-50, ConvNeXt-Tiny, and CLIP ViT-B/16—using a linear probe combined with four post-hoc scoring functions (MSP, Energy, Mahalanobis, and kNN). The results show that CLIP excels in distinguishing known from unknown categories across various metrics, including AUROC, AUPR, FPR@95, and OSCR, with the Energy score offering the most consistent performance. In FSCIL, three methods—modified SPPR, OrCo, and ConCM—are compared, all using partially frozen ResNet-50 backbones, under 1-, 5-, and 10-shot conditions. ConCM achieves the highest accuracy (84.7%) in the 10-shot setting and produces the cleanest confusion matrix. However, all approaches experience performance saturation beyond five shots. The study highlights the significant impact of backbone choice and scoring strategies on unknown detection and demonstrates that prototype-based approaches like ConCM effectively mitigate catastrophic forgetting while adapting incrementally to novel classes. This controlled evaluation provides valuable insights for building reliable open-world recognition systems. <div>
arXiv:2512.12146v1 Announce Type: new 
Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video</title>
<link>https://arxiv.org/abs/2512.12165</link>
<guid>https://arxiv.org/abs/2512.12165</guid>
<content:encoded><![CDATA[
<div> camera motion, audio-visual integration, relative camera pose estimation, direction-of-arrival spectra, robustness<br><br>Summary:<br>1. This paper addresses the fundamental challenge of understanding camera motion, which is crucial in embodied perception and 3D scene understanding.<br>2. Traditional visual methods for camera pose estimation often fail in degraded visual conditions such as motion blur or occlusions.<br>3. The authors propose leveraging passive scene sounds as complementary cues for estimating relative camera pose in real-world, in-the-wild videos.<br>4. They introduce a novel audio-visual framework that incorporates direction-of-arrival (DOA) spectra and binauralized audio embeddings into a state-of-the-art vision-only pose estimation model.<br>5. Experiments on two large datasets demonstrate consistent improvements over strong visual-only baselines and increased robustness when visual information is corrupted.<br>6. This work marks the first successful use of audio signals for relative camera pose estimation in real-world videos.<br>7. The findings establish incidental, everyday audio as an unexpected yet promising signal for addressing spatial understanding challenges related to camera motion.<br>8. The project page provides additional resources and code for further research and application: http://vision.cs.utexas.edu/projects/av_camera_pose. <div>
arXiv:2512.12165v1 Announce Type: new 
Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation</title>
<link>https://arxiv.org/abs/2512.12193</link>
<guid>https://arxiv.org/abs/2512.12193</guid>
<content:encoded><![CDATA[
<div> Keywords: customized video generation, subject appearance, motion consistency, self-supervised encoder, LoRA fine-tuning<br><br>Summary:<br><br>Customized video generation aims to create videos that retain the subject's appearance from reference images while preserving motion consistency from reference videos. Existing methods face challenges balancing subject appearance similarity and motion pattern consistency due to insufficient object-level guidance. To overcome this, the authors propose SMRABooth, which integrates a self-supervised encoder and an optical flow encoder to extract object-level subject and motion representations. These representations are aligned with the generation model through LoRA (Low-Rank Adaptation) fine-tuning. The approach consists of three key stages: first, subject representations generated via a self-supervised encoder guide subject alignment, improving the capture of subject structure and enhancing semantic consistency. Second, motion representations derived from an optical flow encoder capture object-level motion trajectories independently of appearance, ensuring motion coherence. Third, a subject-motion association decoupling strategy employs sparse LoRA injections across spatial locations and time steps, reducing interference between subject and motion modulations. Extensive experiments demonstrate that SMRABooth excels at both subject and motion customization, effectively maintaining consistent subject appearance and motion patterns, thus proving its effectiveness in controllable text-to-video generation tasks. <div>
arXiv:2512.12193v1 Announce Type: new 
Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms</title>
<link>https://arxiv.org/abs/2512.12199</link>
<guid>https://arxiv.org/abs/2512.12199</guid>
<content:encoded><![CDATA[
<div> Keywords: micro UAV, wildfire tracking, thermal imaging, edge detection, inertial feedback<br><br>Summary:<br><br>This study proposes a lightweight perimeter tracking method tailored for micro UAV teams operating in wildfire environments where bandwidth is severely limited. It utilizes thermal image frames to generate initial coarse hot region masks via adaptive thresholding and morphological refinement, while RGB frames provide edge cues and help suppress false detections due to texture by applying gradient-based filtering. A rule-level merging strategy consolidates boundary candidates and refines them using the Ramer Douglas Peucker algorithm to simplify the shapes. The system incorporates periodic beacons and an inertial feedback loop to maintain stable UAV trajectories even when GPS signals degrade. Designed to run efficiently on embedded System on Chip (SoC) platforms, the guidance loop targets less than 50 ms latency by limiting per-frame pixel operations and precomputing gradient tables. Small-scale simulations show that this method reduces average path length and boundary jitter compared to pure edge tracking baselines, while effectively covering the environment as measured by intersection merge analysis. Additionally, battery consumption and computational utilization metrics indicate feasibility for achieving forward motion speeds of 10 to 15 m/s on standard micro UAV platforms. Overall, this approach enables rapid field deployment with robust sensing and minimal communication, suitable for emergency wildfire reconnaissance applications. <div>
arXiv:2512.12199v1 Announce Type: new 
Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection</title>
<link>https://arxiv.org/abs/2512.12205</link>
<guid>https://arxiv.org/abs/2512.12205</guid>
<content:encoded><![CDATA[
<div> Keywords: urban streetlights, visual drift, anomaly detection, CNN-VAE, smart city  

<br><br>Summary:  
1. This work introduces a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras across Bristol, U.K., spanning from 2021 to 2025.  
2. The dataset includes over 526,000 images taken hourly under various lighting, weather, and seasonal conditions, with comprehensive metadata such as timestamps, GPS coordinates, and device identifiers.  
3. The dataset facilitates detailed studies on visual drift, anomaly detection, and machine learning operations (MLOps) for smart city applications.  
4. To assist secondary analysis, the authors provide a self-supervised modeling framework using convolutional variational autoencoders (CNN-VAEs), trained separately for each camera and for day/night image sets.  
5. Two per-sample drift metrics are defined: relative centroid drift, which tracks latent space deviations from a baseline quarter, and relative reconstruction error, which quantifies normalized image-domain degradation.  
6. The dataset serves as a fine-grained, realistic benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems in urban environments.  
7. The images and structured metadata are publicly available in JPEG and CSV formats to ensure reproducibility and enable downstream applications such as streetlight monitoring, weather inference, and urban scene understanding.  
8. Dataset access is provided via the Zenodo repository at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120. <div>
arXiv:2512.12205v1 Announce Type: new 
Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</title>
<link>https://arxiv.org/abs/2512.12206</link>
<guid>https://arxiv.org/abs/2512.12206</guid>
<content:encoded><![CDATA[
<div> Keywords: distracted driving, impulse radio ultra-wideband radar, Vision Transformer, driver activity recognition, dataset

<br><br>Summary: This paper tackles two major challenges in driver activity recognition (DAR) using impulse radio ultra-wideband (IR-UWB) radar: the scarcity of large-scale real-world datasets for diverse distracted driving behaviors and the difficulty of adapting fixed-input Vision Transformers (ViTs) to radar data with non-standard dimensions. First, the authors introduce the ALERT dataset, comprising 10,220 radar samples gathered under real driving conditions that cover seven different distracted driving activities. Second, they propose the input-size-agnostic Vision Transformer (ISA-ViT), a novel framework specifically designed for UWB radar-based DAR. ISA-ViT effectively resizes radar data while preserving important radar-domain features such as Doppler shifts and phase information, overcoming the limitations of simple resizing methods. The approach adjusts patch configurations and uses pre-trained positional embedding vectors (PEVs), enabling flexible input dimensions for ViTs. Moreover, a domain fusion strategy combining range- and frequency-domain features is implemented to boost classification accuracy further. Extensive experiments demonstrate that ISA-ViT achieves a 22.68% improvement in accuracy compared to existing ViT-based methods for UWB radar DAR. By releasing the ALERT dataset and detailing the input-size-agnostic method, this work advances scalable, privacy-preserving distracted driving detection for real-world applications. <div>
arXiv:2512.12206v1 Announce Type: new 
Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction</title>
<link>https://arxiv.org/abs/2512.12208</link>
<guid>https://arxiv.org/abs/2512.12208</guid>
<content:encoded><![CDATA[
<div> Keywords: Autism Spectrum Disorder, emotion recognition, humanoid robot, deep learning, MediaPipe FaceMesh<br><br>Summary:<br><br>This study addresses the challenge of understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interactions, focusing on reactions to name-calling by a humanoid robot (NAO) in controlled settings. A substantial dataset of around 50,000 facial frames from 15 ASD children was collected for analysis. The proposed approach combines a fine-tuned ResNet-50 convolutional neural network (CNN) with a three-layer Graph Convolutional Network (GCN) that processes both visual data and geometric features extracted from MediaPipe FaceMesh landmarks. To annotate emotions, a weighted ensemble method using DeepFace and FER models generated probabilistic soft labels across seven emotion categories. The final classification utilized a fused embedding optimized by minimizing the Kullback-Leibler divergence, enhancing the recognition of subtle affective cues. This hybrid model demonstrated robust performance in capturing micro-emotions in neurodivergent children, addressing a significant gap in autism-specific human-robot interaction (HRI) research. Moreover, this work represents the first large-scale real-world dataset and deep learning pipeline from India dedicated to emotion analysis in ASD using social robotics. Consequently, it lays an essential foundation for future personalized assistive technologies and clinical applications involving affective profiling in ASD. <div>
arXiv:2512.12208v1 Announce Type: new 
Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CineLOG: A Training Free Approach for Cinematic Long Video Generation</title>
<link>https://arxiv.org/abs/2512.12209</link>
<guid>https://arxiv.org/abs/2512.12209</guid>
<content:encoded><![CDATA[
<div> Keywords: controllable video synthesis, camera trajectory, cinematic genres, CineLOG dataset, Trajectory Guided Transition Module<br><br>Summary:<br><br>1. The paper addresses the challenge of controllable video synthesis in computer vision, focusing on fine-grained control beyond textual prompts, specifically targeting cinematic attributes such as camera trajectory and film genre. <br>2. Existing datasets suffer from problems like data imbalance, noisy labels, and a gap between simulated and real videos, limiting progress in this area. <br>3. To overcome these issues, the authors introduce CineLOG, a novel dataset comprising 5,000 high-quality, balanced, and uncut video clips. Each video is accompanied by detailed scene descriptions, explicit camera instructions following a standardized cinematic taxonomy, and genre labels, covering 17 camera movements and 15 film genres evenly. <br>4. The paper presents a novel multi-stage pipeline that breaks down the complex text-to-video generation task into four simpler stages using more mature technology, improving generation quality and control. <br>5. A key innovation is the Trajectory Guided Transition Module, which produces smooth spatio-temporal interpolations enabling coherent multi-shot video sequences. Human evaluations demonstrate that this pipeline substantially outperforms state-of-the-art end-to-end text-to-video models in faithfully following camera and screenplay instructions while maintaining professional visual quality. <br>6. The dataset and code have been made publicly available to facilitate further research and development in controllable cinematic video synthesis. <div>
arXiv:2512.12209v1 Announce Type: new 
Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title>
<link>https://arxiv.org/abs/2512.12218</link>
<guid>https://arxiv.org/abs/2512.12218</guid>
<content:encoded><![CDATA[
<div> Reasoning-augmented vision language models, visual faithfulness, reasoning chains, perception steps, self-reflection procedure  

<br><br>Summary:  
This paper addresses new failure modes in reasoning-augmented vision language models (VLMs), where models may produce correct answers through visually unfaithful intermediate steps or reason faithfully but still err in the final prediction. It highlights that traditional evaluation metrics focusing solely on final-answer accuracy fail to capture these nuanced behaviors. To overcome this, the authors introduce the concept of visual faithfulness for reasoning chains, emphasizing the importance of perception steps being grounded in the image itself. They propose a novel, training- and reference-free evaluation framework that separates reasoning chains into perception and reasoning components, utilizing off-the-shelf VLM judges to assess step-level faithfulness. This method is further validated by a human meta-evaluation to ensure reliability. Building on this evaluation metric, the paper presents a lightweight self-reflection procedure that detects and selectively regenerates unfaithful perception steps without requiring additional training. Experimental results across multiple reasoning-trained VLMs and perception-intensive benchmarks demonstrate that this approach successfully reduces the rate of unfaithful perception steps while maintaining final-answer accuracy. Overall, the work enhances the reliability and interpretability of multimodal reasoning in VLMs by ensuring perception steps remain visually faithful. <div>
arXiv:2512.12218v1 Announce Type: new 
Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained Zero-Shot Learning with Attribute-Centric Representations</title>
<link>https://arxiv.org/abs/2512.12219</link>
<guid>https://arxiv.org/abs/2512.12219</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot learning, attribute disentanglement, mixture-of-experts, transformer, fine-grained recognition

<br><br>Summary:  
This paper addresses the challenge of recognizing unseen fine-grained categories by tackling attribute entanglement, where conventional models merge distinct visual attributes such as color, shape, and texture into a single embedding, causing interference and loss of critical distinctions. The authors propose a zero-shot learning framework called Attribute-Centric Representations (ACR) that explicitly enforces attribute disentanglement during representation learning. ACR employs two mixture-of-experts components: Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). MoPE is integrated into the transformer model via a dual-level routing mechanism that routes image patches to specialized experts, ensuring that coherent attribute families are processed separately. MoAE then projects these refined features into sparse, part-aware attribute maps, enabling more robust zero-shot classification. The method is evaluated on standard zero-shot learning benchmarks CUB, AwA2, and SUN, demonstrating consistent state-of-the-art performance. This approach surpasses prior post-hoc disentanglement methods since it prevents attributes from mixing during early representation learning rather than correcting mixed features afterward. Overall, ACR advances fine-grained zero-shot recognition through a novel transformer-based disentanglement framework using expert mixtures for improved interpretability and accuracy. <div>
arXiv:2512.12219v1 Announce Type: new 
Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation</title>
<link>https://arxiv.org/abs/2512.12220</link>
<guid>https://arxiv.org/abs/2512.12220</guid>
<content:encoded><![CDATA[
<div> Keywords: professional image generation, ProImage-Bench, scientific diagrams, rubric evaluation, iterative refinement<br><br>Summary:<br><br>1. This paper addresses professional image generation, focusing on creating scientifically precise and information-dense illustrations from technical descriptions rather than just visually plausible images.<br>2. The authors introduce ProImage-Bench, a detailed rubric-based benchmark aimed at biology schematics, engineering and patent drawings, and general scientific diagrams.<br>3. ProImage-Bench is built using 654 figures sourced from real textbooks and technical reports, with detailed image instructions and a hierarchical rubric system that breaks down correctness into 6,076 criteria and 44,131 binary checks.<br>4. The rubrics are developed by leveraging large multimodal models to extract criteria from surrounding texts and reference figures, and are evaluated using an automated large multimodal model (LMM)-based judge with a penalty scheme to generate interpretable scores.<br>5. Benchmarking existing text-to-image models on ProImage-Bench shows a significant performance gap, with the best model achieving only 0.791 rubric accuracy and 0.553 criterion score, indicating challenges in achieving fine-grained scientific fidelity.<br>6. The rubrics also function as actionable supervision; by feeding failed checks back into an editing model, an iterative refinement process substantially improved generation quality from 0.653 to 0.865 rubric accuracy and from 0.388 to 0.697 criterion score.<br>7. Overall, ProImage-Bench provides both a rigorous diagnostic tool and a scalable signal to improve the fidelity of specification-faithful scientific illustrations through professional image generation. <div>
arXiv:2512.12220v1 Announce Type: new 
Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs</title>
<link>https://arxiv.org/abs/2512.12222</link>
<guid>https://arxiv.org/abs/2512.12222</guid>
<content:encoded><![CDATA[
<div> infant brain MRI, segmentation accuracy, SynthSeg, SamSeg, fractal dimension<br><br>Summary:<br><br>1. Accurate segmentation of infant brain MRI is critical for quantifying developmental structural changes but is challenging due to ongoing myelination and reduced tissue contrast.<br>2. This study assessed segmentation performance on the Baby Open Brains (BOB) dataset comprising 71 scans from infants aged 1 to 9 months.<br>3. Two methods, SynthSeg and SamSeg, were compared against expert manual annotations using metrics including Dice coefficient, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information.<br>4. SynthSeg consistently outperformed SamSeg across all quality metrics, achieving a mean Dice score over 0.8 for major brain regions and volumetric estimates closely aligned with the manual references (+4% mean deviation).<br>5. SamSeg showed significant overestimation of ventricular and whole-brain volumes, with a mean bias of +76%.<br>6. Segmentation accuracy improved with increasing infant age, correlating with enhanced tissue contrast during myelination.<br>7. Fractal dimension (FD) analyses revealed notable regional differences between SynthSeg and expert segmentations, and variability related to segmentation error exceeded most group differences observed in developmental studies.<br>8. Volumetric and FD deviations were positively correlated, indicating that segmentation bias directly impacts FD estimation.<br>9. Overall, SynthSeg was identified as the most reliable method for volumetric and FD assessment in pediatric MRI, though small morphological variations should be interpreted cautiously due to inherent segmentation uncertainties. <div>
arXiv:2512.12222v1 Announce Type: new 
Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder</title>
<link>https://arxiv.org/abs/2512.12229</link>
<guid>https://arxiv.org/abs/2512.12229</guid>
<content:encoded><![CDATA[
<div> Keywords: ultra-low bitrate, image compression, shallow encoder, diffusion decoder, feature distillation  

<br><br>Summary:  
This paper addresses ultra-low bitrate image compression (below 0.05 bits per pixel), a critical need for bandwidth-limited and computation-constrained devices like edge hardware. Current methods rely heavily on large pretrained encoders, such as VAEs or tokenizer-based models, which restrict deployment on weak senders due to their computational demand. The authors propose a novel Asymmetric Extreme Image Compression (AEIC) framework that balances encoding simplicity and decoding quality by employing moderate or shallow encoder networks paired with a one-step diffusion decoder. This approach enables high-fidelity, realistic image reconstructions even at extremely low bitrates. To improve shallow encoder performance, a dual-side feature distillation technique is introduced, transferring knowledge from moderate encoder AEIC models to their shallow counterparts. Experimental results demonstrate that AEIC surpasses existing methods in rate-distortion-perception metrics at ultra-low bitrates. Moreover, the framework achieves exceptional encoding efficiency, reaching 35.8 frames per second on 1080p inputs, while maintaining competitive decoding speeds relative to existing approaches. Overall, AEIC offers a compelling solution for encoding-constrained scenarios, combining fast encoding, quality reconstruction, and resource efficiency. <div>
arXiv:2512.12229v1 Announce Type: new 
Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moment and Highlight Detection via MLLM Frame Segmentation</title>
<link>https://arxiv.org/abs/2512.12246</link>
<guid>https://arxiv.org/abs/2512.12246</guid>
<content:encoded><![CDATA[
<div> Detecting video moments, highlights, natural-language queries, segmentation losses, multimodal LLM  

<br><br>Summary:  
This paper presents a novel approach for detecting video moments and highlights from natural-language queries by leveraging generative multimodal large language models (MLLMs). Unlike previous transformer-based methods or RL approaches, the method treats the output tokens of the LLM as binary segmentation sequences, where each token corresponds to a frame labeled "0" (background) or "1" (foreground). Input to the LLM is limited to a fixed set of frames along with a prompt guiding this binary output format. This allows direct application of segmentation losses as training objectives alongside the standard causal language modeling loss, enabling frame-level supervision and stable complementary signals for learning. At inference, beam search is used to generate the sequence and corresponding logits, which serve as predicted moments and highlight saliency scores. The approach is computationally efficient, sampling only 25 frames per video—less than half what comparable methods require. Despite this, it achieves strong highlight detection performance with 56.74 HIT@1 on the QVHighlights dataset, and surpasses baseline scores (35.28 MAP) in moment retrieval tasks. Overall, the segmentation-driven training approach effectively exploits the LLM’s reasoning and language capabilities to improve video understanding tasks based on text queries. <div>
arXiv:2512.12246v1 Announce Type: new 
Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.12268</link>
<guid>https://arxiv.org/abs/2512.12268</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, test-time prompt tuning, meta-learning, domain shifts, self-supervised auxiliary task<br><br>Summary:<br><br>This paper addresses the issue of domain shift sensitivity in Vision-Language Models (VLMs) like CLIP, which, despite strong zero-shot generalization, struggle to maintain performance when the data distribution changes at test time. To overcome this, the authors propose Meta Test-Time Prompt Tuning (MetaTPT), a novel meta-learning framework designed to improve test-time adaptation. MetaTPT introduces a self-supervised auxiliary task that dynamically learns parameterized augmentations tailored to each sample, allowing the model to generate more expressive and informative transformations specific to the target domain. The framework uses a dual-loop optimization strategy: the inner loop focuses on learning the self-supervised task that produces consistent and meaningful views of the input, while the outer loop performs prompt tuning by enforcing consistency between these augmented views. By integrating augmentation learning directly with prompt tuning, MetaTPT enhances the model’s ability to generalize under challenging domain shifts. Extensive experimental results demonstrate that MetaTPT outperforms existing methods on multiple domain generalization and cross-dataset benchmarks, establishing new state-of-the-art performance in these areas and providing a robust approach for test-time adaptation in vision-language tasks. <div>
arXiv:2512.12268v1 Announce Type: new 
Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions</title>
<link>https://arxiv.org/abs/2512.12277</link>
<guid>https://arxiv.org/abs/2512.12277</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial Expression Recognition, Continual Learning, Catastrophic Forgetting, Bayesian Gaussian Mixture Models, Facial Action Units  

<br><br>Summary:  
This paper addresses the challenge of recognizing human emotions through facial expressions in AI systems that continuously learn over time. The authors propose a hybrid framework combining deep convolutional neural network features with Facial Action Units (AUs) derived from the Facial Action Coding System (FACS) to capture complementary information. To model the combined features, Bayesian Gaussian Mixture Models (BGMMs) are used, offering a probabilistic and lightweight approach that avoids the need for retraining while maintaining strong discriminative ability. The framework is tested on the Compound Facial Expression of Emotion (CFEE) dataset, demonstrating that the model can initially learn basic expressions and then progressively incorporate recognition of compound expressions. Experimental results show that this method improves overall accuracy, enhances knowledge retention, and significantly reduces catastrophic forgetting compared to traditional continual learning methods. The approach supports the development of emotionally intelligent AI applications, with potential uses in education, healthcare, and adaptive user interfaces where ongoing understanding of human emotions is critical. <div>
arXiv:2512.12277v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection</title>
<link>https://arxiv.org/abs/2512.12281</link>
<guid>https://arxiv.org/abs/2512.12281</guid>
<content:encoded><![CDATA[

arXiv:2512.12281v1 Announce Type: new 
Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealDrag: The First Dragging Benchmark with Real Target Image</title>
<link>https://arxiv.org/abs/2512.12287</link>
<guid>https://arxiv.org/abs/2512.12287</guid>
<content:encoded><![CDATA[

arXiv:2512.12287v1 Announce Type: new 
Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</title>
<link>https://arxiv.org/abs/2512.12296</link>
<guid>https://arxiv.org/abs/2512.12296</guid>
<content:encoded><![CDATA[

arXiv:2512.12296v1 Announce Type: new 
Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.12302</link>
<guid>https://arxiv.org/abs/2512.12302</guid>
<content:encoded><![CDATA[

arXiv:2512.12302v1 Announce Type: new 
Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.12303</link>
<guid>https://arxiv.org/abs/2512.12303</guid>
<content:encoded><![CDATA[

arXiv:2512.12303v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.12307</link>
<guid>https://arxiv.org/abs/2512.12307</guid>
<content:encoded><![CDATA[

arXiv:2512.12307v1 Announce Type: new 
Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeDetect: Fast Open-Vocabulary Object Detection as Retrieval</title>
<link>https://arxiv.org/abs/2512.12309</link>
<guid>https://arxiv.org/abs/2512.12309</guid>
<content:encoded><![CDATA[

arXiv:2512.12309v1 Announce Type: new 
Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Control for Inference-Time Guidance of Denoising Diffusion Models</title>
<link>https://arxiv.org/abs/2512.12339</link>
<guid>https://arxiv.org/abs/2512.12339</guid>
<content:encoded><![CDATA[

arXiv:2512.12339v1 Announce Type: new 
Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection</title>
<link>https://arxiv.org/abs/2512.12357</link>
<guid>https://arxiv.org/abs/2512.12357</guid>
<content:encoded><![CDATA[

arXiv:2512.12357v1 Announce Type: new 
Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2512.12360</link>
<guid>https://arxiv.org/abs/2512.12360</guid>
<content:encoded><![CDATA[

arXiv:2512.12360v1 Announce Type: new 
Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative</title>
<link>https://arxiv.org/abs/2512.12372</link>
<guid>https://arxiv.org/abs/2512.12372</guid>
<content:encoded><![CDATA[

arXiv:2512.12372v1 Announce Type: new 
Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping</title>
<link>https://arxiv.org/abs/2512.12375</link>
<guid>https://arxiv.org/abs/2512.12375</guid>
<content:encoded><![CDATA[

arXiv:2512.12375v1 Announce Type: new 
Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2512.12378</link>
<guid>https://arxiv.org/abs/2512.12378</guid>
<content:encoded><![CDATA[

arXiv:2512.12378v1 Announce Type: new 
Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speedrunning ImageNet Diffusion</title>
<link>https://arxiv.org/abs/2512.12386</link>
<guid>https://arxiv.org/abs/2512.12386</guid>
<content:encoded><![CDATA[

arXiv:2512.12386v1 Announce Type: new 
Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States</title>
<link>https://arxiv.org/abs/2512.12395</link>
<guid>https://arxiv.org/abs/2512.12395</guid>
<content:encoded><![CDATA[

arXiv:2512.12395v1 Announce Type: new 
Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
<link>https://arxiv.org/abs/2512.12410</link>
<guid>https://arxiv.org/abs/2512.12410</guid>
<content:encoded><![CDATA[

arXiv:2512.12410v1 Announce Type: new 
Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics</title>
<link>https://arxiv.org/abs/2512.12424</link>
<guid>https://arxiv.org/abs/2512.12424</guid>
<content:encoded><![CDATA[

arXiv:2512.12424v1 Announce Type: new 
Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation</title>
<link>https://arxiv.org/abs/2512.12425</link>
<guid>https://arxiv.org/abs/2512.12425</guid>
<content:encoded><![CDATA[

arXiv:2512.12425v1 Announce Type: new 
Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endless World: Real-Time 3D-Aware Long Video Generation</title>
<link>https://arxiv.org/abs/2512.12430</link>
<guid>https://arxiv.org/abs/2512.12430</guid>
<content:encoded><![CDATA[

arXiv:2512.12430v1 Announce Type: new 
Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields</title>
<link>https://arxiv.org/abs/2512.12459</link>
<guid>https://arxiv.org/abs/2512.12459</guid>
<content:encoded><![CDATA[

arXiv:2512.12459v1 Announce Type: new 
Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.12487</link>
<guid>https://arxiv.org/abs/2512.12487</guid>
<content:encoded><![CDATA[

arXiv:2512.12487v1 Announce Type: new 
Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings</title>
<link>https://arxiv.org/abs/2512.12492</link>
<guid>https://arxiv.org/abs/2512.12492</guid>
<content:encoded><![CDATA[

arXiv:2512.12492v1 Announce Type: new 
Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention</title>
<link>https://arxiv.org/abs/2512.12498</link>
<guid>https://arxiv.org/abs/2512.12498</guid>
<content:encoded><![CDATA[

arXiv:2512.12498v1 Announce Type: new 
Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Spatiotemporal Data Augmentation</title>
<link>https://arxiv.org/abs/2512.12508</link>
<guid>https://arxiv.org/abs/2512.12508</guid>
<content:encoded><![CDATA[

arXiv:2512.12508v1 Announce Type: new 
Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animus3D: Text-driven 3D Animation via Motion Score Distillation</title>
<link>https://arxiv.org/abs/2512.12534</link>
<guid>https://arxiv.org/abs/2512.12534</guid>
<content:encoded><![CDATA[

arXiv:2512.12534v1 Announce Type: new 
Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling</title>
<link>https://arxiv.org/abs/2512.12539</link>
<guid>https://arxiv.org/abs/2512.12539</guid>
<content:encoded><![CDATA[

arXiv:2512.12539v1 Announce Type: new 
Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Contrastive Frame Aggregation for Video Representation Learning</title>
<link>https://arxiv.org/abs/2512.12549</link>
<guid>https://arxiv.org/abs/2512.12549</guid>
<content:encoded><![CDATA[

arXiv:2512.12549v1 Announce Type: new 
Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
<link>https://arxiv.org/abs/2512.12560</link>
<guid>https://arxiv.org/abs/2512.12560</guid>
<content:encoded><![CDATA[

arXiv:2512.12560v1 Announce Type: new 
Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models</title>
<link>https://arxiv.org/abs/2512.12571</link>
<guid>https://arxiv.org/abs/2512.12571</guid>
<content:encoded><![CDATA[

arXiv:2512.12571v1 Announce Type: new 
Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis</title>
<link>https://arxiv.org/abs/2512.12586</link>
<guid>https://arxiv.org/abs/2512.12586</guid>
<content:encoded><![CDATA[

arXiv:2512.12586v1 Announce Type: new 
Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Wire-Harness Color Sequence Detector</title>
<link>https://arxiv.org/abs/2512.12590</link>
<guid>https://arxiv.org/abs/2512.12590</guid>
<content:encoded><![CDATA[

arXiv:2512.12590v1 Announce Type: new 
Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation</title>
<link>https://arxiv.org/abs/2512.12595</link>
<guid>https://arxiv.org/abs/2512.12595</guid>
<content:encoded><![CDATA[

arXiv:2512.12595v1 Announce Type: new 
Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models</title>
<link>https://arxiv.org/abs/2512.12596</link>
<guid>https://arxiv.org/abs/2512.12596</guid>
<content:encoded><![CDATA[

arXiv:2512.12596v1 Announce Type: new 
Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Aware Scene-Consistent Image Generation</title>
<link>https://arxiv.org/abs/2512.12598</link>
<guid>https://arxiv.org/abs/2512.12598</guid>
<content:encoded><![CDATA[

arXiv:2512.12598v1 Announce Type: new 
Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching</title>
<link>https://arxiv.org/abs/2512.12604</link>
<guid>https://arxiv.org/abs/2512.12604</guid>
<content:encoded><![CDATA[

arXiv:2512.12604v1 Announce Type: new 
Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching</title>
<link>https://arxiv.org/abs/2512.12610</link>
<guid>https://arxiv.org/abs/2512.12610</guid>
<content:encoded><![CDATA[

arXiv:2512.12610v1 Announce Type: new 
Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation</title>
<link>https://arxiv.org/abs/2512.12622</link>
<guid>https://arxiv.org/abs/2512.12622</guid>
<content:encoded><![CDATA[

arXiv:2512.12622v1 Announce Type: new 
Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space</title>
<link>https://arxiv.org/abs/2512.12623</link>
<guid>https://arxiv.org/abs/2512.12623</guid>
<content:encoded><![CDATA[

arXiv:2512.12623v1 Announce Type: new 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.12633</link>
<guid>https://arxiv.org/abs/2512.12633</guid>
<content:encoded><![CDATA[

arXiv:2512.12633v1 Announce Type: new 
Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Fundus Image Registration under Large FoV Disparity</title>
<link>https://arxiv.org/abs/2512.12657</link>
<guid>https://arxiv.org/abs/2512.12657</guid>
<content:encoded><![CDATA[

arXiv:2512.12657v1 Announce Type: new 
Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogDoc: Towards Unified thinking in Documents</title>
<link>https://arxiv.org/abs/2512.12658</link>
<guid>https://arxiv.org/abs/2512.12658</guid>
<content:encoded><![CDATA[

arXiv:2512.12658v1 Announce Type: new 
Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.12662</link>
<guid>https://arxiv.org/abs/2512.12662</guid>
<content:encoded><![CDATA[

arXiv:2512.12662v1 Announce Type: new 
Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation</title>
<link>https://arxiv.org/abs/2512.12664</link>
<guid>https://arxiv.org/abs/2512.12664</guid>
<content:encoded><![CDATA[

arXiv:2512.12664v1 Announce Type: new 
Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning</title>
<link>https://arxiv.org/abs/2512.12667</link>
<guid>https://arxiv.org/abs/2512.12667</guid>
<content:encoded><![CDATA[

arXiv:2512.12667v1 Announce Type: new 
Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation</title>
<link>https://arxiv.org/abs/2512.12673</link>
<guid>https://arxiv.org/abs/2512.12673</guid>
<content:encoded><![CDATA[

arXiv:2512.12673v1 Announce Type: new 
Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</title>
<link>https://arxiv.org/abs/2512.12675</link>
<guid>https://arxiv.org/abs/2512.12675</guid>
<content:encoded><![CDATA[

arXiv:2512.12675v1 Announce Type: new 
Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\beta$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2512.12678</link>
<guid>https://arxiv.org/abs/2512.12678</guid>
<content:encoded><![CDATA[

arXiv:2512.12678v1 Announce Type: new 
Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $\beta$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $\beta$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $\beta$-Contextualized Contrastive Alignment Loss ($\beta$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $\beta$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $\beta$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Vision-Language Reasoning via Adaptive Token Pruning</title>
<link>https://arxiv.org/abs/2512.12701</link>
<guid>https://arxiv.org/abs/2512.12701</guid>
<content:encoded><![CDATA[

arXiv:2512.12701v1 Announce Type: new 
Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Motion Generation using Part-level Reliable Data from Videos</title>
<link>https://arxiv.org/abs/2512.12703</link>
<guid>https://arxiv.org/abs/2512.12703</guid>
<content:encoded><![CDATA[

arXiv:2512.12703v1 Announce Type: new 
Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images</title>
<link>https://arxiv.org/abs/2512.12718</link>
<guid>https://arxiv.org/abs/2512.12718</guid>
<content:encoded><![CDATA[

arXiv:2512.12718v1 Announce Type: new 
Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</title>
<link>https://arxiv.org/abs/2512.12751</link>
<guid>https://arxiv.org/abs/2512.12751</guid>
<content:encoded><![CDATA[

arXiv:2512.12751v1 Announce Type: new 
Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning</title>
<link>https://arxiv.org/abs/2512.12756</link>
<guid>https://arxiv.org/abs/2512.12756</guid>
<content:encoded><![CDATA[

arXiv:2512.12756v1 Announce Type: new 
Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
<link>https://arxiv.org/abs/2512.12768</link>
<guid>https://arxiv.org/abs/2512.12768</guid>
<content:encoded><![CDATA[

arXiv:2512.12768v1 Announce Type: new 
Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior</title>
<link>https://arxiv.org/abs/2512.12774</link>
<guid>https://arxiv.org/abs/2512.12774</guid>
<content:encoded><![CDATA[

arXiv:2512.12774v1 Announce Type: new 
Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context</title>
<link>https://arxiv.org/abs/2512.12790</link>
<guid>https://arxiv.org/abs/2512.12790</guid>
<content:encoded><![CDATA[

arXiv:2512.12790v1 Announce Type: new 
Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</title>
<link>https://arxiv.org/abs/2512.12799</link>
<guid>https://arxiv.org/abs/2512.12799</guid>
<content:encoded><![CDATA[

arXiv:2512.12799v1 Announce Type: new 
Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Common and Salient Generative Factors Between Two Image Datasets</title>
<link>https://arxiv.org/abs/2512.12800</link>
<guid>https://arxiv.org/abs/2512.12800</guid>
<content:encoded><![CDATA[

arXiv:2512.12800v1 Announce Type: new 
Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.12822</link>
<guid>https://arxiv.org/abs/2512.12822</guid>
<content:encoded><![CDATA[

arXiv:2512.12822v1 Announce Type: new 
Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
<link>https://arxiv.org/abs/2512.12824</link>
<guid>https://arxiv.org/abs/2512.12824</guid>
<content:encoded><![CDATA[

arXiv:2512.12824v1 Announce Type: new 
Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal</title>
<link>https://arxiv.org/abs/2512.12875</link>
<guid>https://arxiv.org/abs/2512.12875</guid>
<content:encoded><![CDATA[

arXiv:2512.12875v1 Announce Type: new 
Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.12884</link>
<guid>https://arxiv.org/abs/2512.12884</guid>
<content:encoded><![CDATA[

arXiv:2512.12884v1 Announce Type: new 
Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</title>
<link>https://arxiv.org/abs/2512.12885</link>
<guid>https://arxiv.org/abs/2512.12885</guid>
<content:encoded><![CDATA[

arXiv:2512.12885v1 Announce Type: new 
Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification</title>
<link>https://arxiv.org/abs/2512.12887</link>
<guid>https://arxiv.org/abs/2512.12887</guid>
<content:encoded><![CDATA[

arXiv:2512.12887v1 Announce Type: new 
Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution</title>
<link>https://arxiv.org/abs/2512.12898</link>
<guid>https://arxiv.org/abs/2512.12898</guid>
<content:encoded><![CDATA[

arXiv:2512.12898v1 Announce Type: new 
Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2512.12906</link>
<guid>https://arxiv.org/abs/2512.12906</guid>
<content:encoded><![CDATA[

arXiv:2512.12906v1 Announce Type: new 
Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2512.12925</link>
<guid>https://arxiv.org/abs/2512.12925</guid>
<content:encoded><![CDATA[

arXiv:2512.12925v1 Announce Type: new 
Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
<link>https://arxiv.org/abs/2512.12929</link>
<guid>https://arxiv.org/abs/2512.12929</guid>
<content:encoded><![CDATA[

arXiv:2512.12929v1 Announce Type: new 
Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
<link>https://arxiv.org/abs/2512.12935</link>
<guid>https://arxiv.org/abs/2512.12935</guid>
<content:encoded><![CDATA[

arXiv:2512.12935v1 Announce Type: new 
Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content Adaptive based Motion Alignment Framework for Learned Video Compression</title>
<link>https://arxiv.org/abs/2512.12936</link>
<guid>https://arxiv.org/abs/2512.12936</guid>
<content:encoded><![CDATA[

arXiv:2512.12936v1 Announce Type: new 
Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</title>
<link>https://arxiv.org/abs/2512.12941</link>
<guid>https://arxiv.org/abs/2512.12941</guid>
<content:encoded><![CDATA[

arXiv:2512.12941v1 Announce Type: new 
Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer</title>
<link>https://arxiv.org/abs/2512.12963</link>
<guid>https://arxiv.org/abs/2512.12963</guid>
<content:encoded><![CDATA[

arXiv:2512.12963v1 Announce Type: new 
Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference</title>
<link>https://arxiv.org/abs/2512.12977</link>
<guid>https://arxiv.org/abs/2512.12977</guid>
<content:encoded><![CDATA[

arXiv:2512.12977v1 Announce Type: new 
Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes</title>
<link>https://arxiv.org/abs/2512.12982</link>
<guid>https://arxiv.org/abs/2512.12982</guid>
<content:encoded><![CDATA[

arXiv:2512.12982v1 Announce Type: new 
Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
<link>https://arxiv.org/abs/2512.12997</link>
<guid>https://arxiv.org/abs/2512.12997</guid>
<content:encoded><![CDATA[

arXiv:2512.12997v1 Announce Type: new 
Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Step Distillation for Text-to-Image Generation: A Practical Guide</title>
<link>https://arxiv.org/abs/2512.13006</link>
<guid>https://arxiv.org/abs/2512.13006</guid>
<content:encoded><![CDATA[

arXiv:2512.13006v1 Announce Type: new 
Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light Field Based 6DoF Tracking of Previously Unobserved Objects</title>
<link>https://arxiv.org/abs/2512.13007</link>
<guid>https://arxiv.org/abs/2512.13007</guid>
<content:encoded><![CDATA[

arXiv:2512.13007v1 Announce Type: new 
Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</title>
<link>https://arxiv.org/abs/2512.13008</link>
<guid>https://arxiv.org/abs/2512.13008</guid>
<content:encoded><![CDATA[

arXiv:2512.13008v1 Announce Type: new 
Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion</title>
<link>https://arxiv.org/abs/2512.13014</link>
<guid>https://arxiv.org/abs/2512.13014</guid>
<content:encoded><![CDATA[

arXiv:2512.13014v1 Announce Type: new 
Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Happens Next? Next Scene Prediction with a Unified Video Model</title>
<link>https://arxiv.org/abs/2512.13015</link>
<guid>https://arxiv.org/abs/2512.13015</guid>
<content:encoded><![CDATA[

arXiv:2512.13015v1 Announce Type: new 
Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</title>
<link>https://arxiv.org/abs/2512.13018</link>
<guid>https://arxiv.org/abs/2512.13018</guid>
<content:encoded><![CDATA[

arXiv:2512.13018v1 Announce Type: new 
Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SneakPeek: Future-Guided Instructional Streaming Video Generation</title>
<link>https://arxiv.org/abs/2512.13019</link>
<guid>https://arxiv.org/abs/2512.13019</guid>
<content:encoded><![CDATA[

arXiv:2512.13019v1 Announce Type: new 
Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motus: A Unified Latent Action World Model</title>
<link>https://arxiv.org/abs/2512.13030</link>
<guid>https://arxiv.org/abs/2512.13030</guid>
<content:encoded><![CDATA[

arXiv:2512.13030v1 Announce Type: new 
Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs</title>
<link>https://arxiv.org/abs/2512.13031</link>
<guid>https://arxiv.org/abs/2512.13031</guid>
<content:encoded><![CDATA[

arXiv:2512.13031v1 Announce Type: new 
Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13039</link>
<guid>https://arxiv.org/abs/2512.13039</guid>
<content:encoded><![CDATA[

arXiv:2512.13039v1 Announce Type: new 
Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</title>
<link>https://arxiv.org/abs/2512.13043</link>
<guid>https://arxiv.org/abs/2512.13043</guid>
<content:encoded><![CDATA[

arXiv:2512.13043v1 Announce Type: new 
Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</title>
<link>https://arxiv.org/abs/2512.13055</link>
<guid>https://arxiv.org/abs/2512.13055</guid>
<content:encoded><![CDATA[

arXiv:2512.13055v1 Announce Type: new 
Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models</title>
<link>https://arxiv.org/abs/2512.13072</link>
<guid>https://arxiv.org/abs/2512.13072</guid>
<content:encoded><![CDATA[

arXiv:2512.13072v1 Announce Type: new 
Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heart Disease Prediction using Case Based Reasoning (CBR)</title>
<link>https://arxiv.org/abs/2512.13078</link>
<guid>https://arxiv.org/abs/2512.13078</guid>
<content:encoded><![CDATA[

arXiv:2512.13078v1 Announce Type: new 
Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiRe: Diversity-promoting Regularization for Dataset Condensation</title>
<link>https://arxiv.org/abs/2512.13083</link>
<guid>https://arxiv.org/abs/2512.13083</guid>
<content:encoded><![CDATA[

arXiv:2512.13083v1 Announce Type: new 
Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
<link>https://arxiv.org/abs/2512.13089</link>
<guid>https://arxiv.org/abs/2512.13089</guid>
<content:encoded><![CDATA[

arXiv:2512.13089v1 Announce Type: new 
Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13095</link>
<guid>https://arxiv.org/abs/2512.13095</guid>
<content:encoded><![CDATA[

arXiv:2512.13095v1 Announce Type: new 
Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.13101</link>
<guid>https://arxiv.org/abs/2512.13101</guid>
<content:encoded><![CDATA[

arXiv:2512.13101v1 Announce Type: new 
Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection</title>
<link>https://arxiv.org/abs/2512.13104</link>
<guid>https://arxiv.org/abs/2512.13104</guid>
<content:encoded><![CDATA[

arXiv:2512.13104v1 Announce Type: new 
Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
<link>https://arxiv.org/abs/2512.13107</link>
<guid>https://arxiv.org/abs/2512.13107</guid>
<content:encoded><![CDATA[

arXiv:2512.13107v1 Announce Type: new 
Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
<link>https://arxiv.org/abs/2512.13122</link>
<guid>https://arxiv.org/abs/2512.13122</guid>
<content:encoded><![CDATA[

arXiv:2512.13122v1 Announce Type: new 
Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</title>
<link>https://arxiv.org/abs/2512.13130</link>
<guid>https://arxiv.org/abs/2512.13130</guid>
<content:encoded><![CDATA[

arXiv:2512.13130v1 Announce Type: new 
Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13144</link>
<guid>https://arxiv.org/abs/2512.13144</guid>
<content:encoded><![CDATA[

arXiv:2512.13144v1 Announce Type: new 
Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion</title>
<link>https://arxiv.org/abs/2512.13147</link>
<guid>https://arxiv.org/abs/2512.13147</guid>
<content:encoded><![CDATA[

arXiv:2512.13147v1 Announce Type: new 
Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Image Fusion for Multi-View 3D Material Reconstruction</title>
<link>https://arxiv.org/abs/2512.13157</link>
<guid>https://arxiv.org/abs/2512.13157</guid>
<content:encoded><![CDATA[

arXiv:2512.13157v1 Announce Type: new 
Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
<link>https://arxiv.org/abs/2512.13164</link>
<guid>https://arxiv.org/abs/2512.13164</guid>
<content:encoded><![CDATA[

arXiv:2512.13164v1 Announce Type: new 
Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.13175</link>
<guid>https://arxiv.org/abs/2512.13175</guid>
<content:encoded><![CDATA[

arXiv:2512.13175v1 Announce Type: new 
Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</title>
<link>https://arxiv.org/abs/2512.13177</link>
<guid>https://arxiv.org/abs/2512.13177</guid>
<content:encoded><![CDATA[

arXiv:2512.13177v1 Announce Type: new 
Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception</title>
<link>https://arxiv.org/abs/2512.13191</link>
<guid>https://arxiv.org/abs/2512.13191</guid>
<content:encoded><![CDATA[

arXiv:2512.13191v1 Announce Type: new 
Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling</title>
<link>https://arxiv.org/abs/2512.13192</link>
<guid>https://arxiv.org/abs/2512.13192</guid>
<content:encoded><![CDATA[

arXiv:2512.13192v1 Announce Type: new 
Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance</title>
<link>https://arxiv.org/abs/2512.13238</link>
<guid>https://arxiv.org/abs/2512.13238</guid>
<content:encoded><![CDATA[

arXiv:2512.13238v1 Announce Type: new 
Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits</title>
<link>https://arxiv.org/abs/2512.13247</link>
<guid>https://arxiv.org/abs/2512.13247</guid>
<content:encoded><![CDATA[

arXiv:2512.13247v1 Announce Type: new 
Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</title>
<link>https://arxiv.org/abs/2512.13250</link>
<guid>https://arxiv.org/abs/2512.13250</guid>
<content:encoded><![CDATA[

arXiv:2512.13250v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing</title>
<link>https://arxiv.org/abs/2512.13276</link>
<guid>https://arxiv.org/abs/2512.13276</guid>
<content:encoded><![CDATA[

arXiv:2512.13276v1 Announce Type: new 
Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</title>
<link>https://arxiv.org/abs/2512.13281</link>
<guid>https://arxiv.org/abs/2512.13281</guid>
<content:encoded><![CDATA[

arXiv:2512.13281v1 Announce Type: new 
Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</title>
<link>https://arxiv.org/abs/2512.13285</link>
<guid>https://arxiv.org/abs/2512.13285</guid>
<content:encoded><![CDATA[

arXiv:2512.13285v1 Announce Type: new 
Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13290</link>
<guid>https://arxiv.org/abs/2512.13290</guid>
<content:encoded><![CDATA[

arXiv:2512.13290v1 Announce Type: new 
Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</title>
<link>https://arxiv.org/abs/2512.13303</link>
<guid>https://arxiv.org/abs/2512.13303</guid>
<content:encoded><![CDATA[

arXiv:2512.13303v1 Announce Type: new 
Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KlingAvatar 2.0 Technical Report</title>
<link>https://arxiv.org/abs/2512.13313</link>
<guid>https://arxiv.org/abs/2512.13313</guid>
<content:encoded><![CDATA[

arXiv:2512.13313v1 Announce Type: new 
Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
<link>https://arxiv.org/abs/2512.13317</link>
<guid>https://arxiv.org/abs/2512.13317</guid>
<content:encoded><![CDATA[

arXiv:2512.13317v1 Announce Type: new 
Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated User Identification from Facial Thermograms with Siamese Networks</title>
<link>https://arxiv.org/abs/2512.13361</link>
<guid>https://arxiv.org/abs/2512.13361</guid>
<content:encoded><![CDATA[

arXiv:2512.13361v1 Announce Type: new 
Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"</title>
<link>https://arxiv.org/abs/2512.13376</link>
<guid>https://arxiv.org/abs/2512.13376</guid>
<content:encoded><![CDATA[

arXiv:2512.13376v1 Announce Type: new 
Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</title>
<link>https://arxiv.org/abs/2512.13392</link>
<guid>https://arxiv.org/abs/2512.13392</guid>
<content:encoded><![CDATA[

arXiv:2512.13392v1 Announce Type: new 
Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>rNCA: Self-Repairing Segmentation Masks</title>
<link>https://arxiv.org/abs/2512.13397</link>
<guid>https://arxiv.org/abs/2512.13397</guid>
<content:encoded><![CDATA[

arXiv:2512.13397v1 Announce Type: new 
Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $\beta_0$ errors by 60% and $\beta_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</title>
<link>https://arxiv.org/abs/2512.13402</link>
<guid>https://arxiv.org/abs/2512.13402</guid>
<content:encoded><![CDATA[

arXiv:2512.13402v1 Announce Type: new 
Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer vision training dataset generation for robotic environments using Gaussian splatting</title>
<link>https://arxiv.org/abs/2512.13411</link>
<guid>https://arxiv.org/abs/2512.13411</guid>
<content:encoded><![CDATA[

arXiv:2512.13411v1 Announce Type: new 
Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2512.13415</link>
<guid>https://arxiv.org/abs/2512.13415</guid>
<content:encoded><![CDATA[

arXiv:2512.13415v1 Announce Type: new 
Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Generate Cross-Task Unexploitable Examples</title>
<link>https://arxiv.org/abs/2512.13416</link>
<guid>https://arxiv.org/abs/2512.13416</guid>
<content:encoded><![CDATA[

arXiv:2512.13416v1 Announce Type: new 
Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RecTok: Reconstruction Distillation along Rectified Flow</title>
<link>https://arxiv.org/abs/2512.13421</link>
<guid>https://arxiv.org/abs/2512.13421</guid>
<content:encoded><![CDATA[

arXiv:2512.13421v1 Announce Type: new 
Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MineTheGap: Automatic Mining of Biases in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.13427</link>
<guid>https://arxiv.org/abs/2512.13427</guid>
<content:encoded><![CDATA[

arXiv:2512.13427v1 Announce Type: new 
Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification</title>
<link>https://arxiv.org/abs/2512.13428</link>
<guid>https://arxiv.org/abs/2512.13428</guid>
<content:encoded><![CDATA[

arXiv:2512.13428v1 Announce Type: new 
Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&amp;E whole slide images</title>
<link>https://arxiv.org/abs/2512.13440</link>
<guid>https://arxiv.org/abs/2512.13440</guid>
<content:encoded><![CDATA[

arXiv:2512.13440v1 Announce Type: new 
Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&amp;E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Modification: Inverse Domain Transformation for Robust Perception</title>
<link>https://arxiv.org/abs/2512.13454</link>
<guid>https://arxiv.org/abs/2512.13454</guid>
<content:encoded><![CDATA[

arXiv:2512.13454v1 Announce Type: new 
Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence</title>
<link>https://arxiv.org/abs/2512.13465</link>
<guid>https://arxiv.org/abs/2512.13465</guid>
<content:encoded><![CDATA[

arXiv:2512.13465v1 Announce Type: new 
Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$</title>
<link>https://arxiv.org/abs/2512.13492</link>
<guid>https://arxiv.org/abs/2512.13492</guid>
<content:encoded><![CDATA[

arXiv:2512.13492v1 Announce Type: new 
Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation</title>
<link>https://arxiv.org/abs/2512.13495</link>
<guid>https://arxiv.org/abs/2512.13495</guid>
<content:encoded><![CDATA[

arXiv:2512.13495v1 Announce Type: new 
Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
<link>https://arxiv.org/abs/2512.13507</link>
<guid>https://arxiv.org/abs/2512.13507</guid>
<content:encoded><![CDATA[

arXiv:2512.13507v1 Announce Type: new 
Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding</title>
<link>https://arxiv.org/abs/2512.13511</link>
<guid>https://arxiv.org/abs/2512.13511</guid>
<content:encoded><![CDATA[

arXiv:2512.13511v1 Announce Type: new 
Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains</title>
<link>https://arxiv.org/abs/2512.13534</link>
<guid>https://arxiv.org/abs/2512.13534</guid>
<content:encoded><![CDATA[

arXiv:2512.13534v1 Announce Type: new 
Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Human-Human Interaction Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.13560</link>
<guid>https://arxiv.org/abs/2512.13560</guid>
<content:encoded><![CDATA[

arXiv:2512.13560v1 Announce Type: new 
Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMhops-R1: Multimodal Multi-hop Reasoning</title>
<link>https://arxiv.org/abs/2512.13573</link>
<guid>https://arxiv.org/abs/2512.13573</guid>
<content:encoded><![CDATA[

arXiv:2512.13573v1 Announce Type: new 
Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lighting in Motion: Spatiotemporal HDR Lighting Estimation</title>
<link>https://arxiv.org/abs/2512.13597</link>
<guid>https://arxiv.org/abs/2512.13597</guid>
<content:encoded><![CDATA[

arXiv:2512.13597v1 Announce Type: new 
Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
<link>https://arxiv.org/abs/2512.13600</link>
<guid>https://arxiv.org/abs/2512.13600</guid>
<content:encoded><![CDATA[

arXiv:2512.13600v1 Announce Type: new 
Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVie 2: Multimodal Controllable Ultra-Long Video World Model</title>
<link>https://arxiv.org/abs/2512.13604</link>
<guid>https://arxiv.org/abs/2512.13604</guid>
<content:encoded><![CDATA[

arXiv:2512.13604v1 Announce Type: new 
Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis</title>
<link>https://arxiv.org/abs/2512.13608</link>
<guid>https://arxiv.org/abs/2512.13608</guid>
<content:encoded><![CDATA[

arXiv:2512.13608v1 Announce Type: new 
Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.13609</link>
<guid>https://arxiv.org/abs/2512.13609</guid>
<content:encoded><![CDATA[

arXiv:2512.13609v1 Announce Type: new 
Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13635</link>
<guid>https://arxiv.org/abs/2512.13635</guid>
<content:encoded><![CDATA[

arXiv:2512.13635v1 Announce Type: new 
Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13636</link>
<guid>https://arxiv.org/abs/2512.13636</guid>
<content:encoded><![CDATA[

arXiv:2512.13636v1 Announce Type: new 
Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All</title>
<link>https://arxiv.org/abs/2512.13639</link>
<guid>https://arxiv.org/abs/2512.13639</guid>
<content:encoded><![CDATA[

arXiv:2512.13639v1 Announce Type: new 
Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</title>
<link>https://arxiv.org/abs/2512.13665</link>
<guid>https://arxiv.org/abs/2512.13665</guid>
<content:encoded><![CDATA[

arXiv:2512.13665v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.13671</link>
<guid>https://arxiv.org/abs/2512.13671</guid>
<content:encoded><![CDATA[

arXiv:2512.13671v1 Announce Type: new 
Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Interactive Intelligence for Digital Humans</title>
<link>https://arxiv.org/abs/2512.13674</link>
<guid>https://arxiv.org/abs/2512.13674</guid>
<content:encoded><![CDATA[

arXiv:2512.13674v1 Announce Type: new 
Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</title>
<link>https://arxiv.org/abs/2512.13677</link>
<guid>https://arxiv.org/abs/2512.13677</guid>
<content:encoded><![CDATA[

arXiv:2512.13677v1 Announce Type: new 
Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedforward 3D Editing via Text-Steerable Image-to-3D</title>
<link>https://arxiv.org/abs/2512.13678</link>
<guid>https://arxiv.org/abs/2512.13678</guid>
<content:encoded><![CDATA[

arXiv:2512.13678v1 Announce Type: new 
Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.13680</link>
<guid>https://arxiv.org/abs/2512.13680</guid>
<content:encoded><![CDATA[

arXiv:2512.13680v1 Announce Type: new 
Abstract: Recent feed-forward reconstruction models like VGGT and $\pi^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</title>
<link>https://arxiv.org/abs/2512.13683</link>
<guid>https://arxiv.org/abs/2512.13683</guid>
<content:encoded><![CDATA[

arXiv:2512.13683v1 Announce Type: new 
Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent Video Masked Autoencoders</title>
<link>https://arxiv.org/abs/2512.13684</link>
<guid>https://arxiv.org/abs/2512.13684</guid>
<content:encoded><![CDATA[

arXiv:2512.13684v1 Announce Type: new 
Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Pre-training of Visual Tokenizers for Generation</title>
<link>https://arxiv.org/abs/2512.13687</link>
<guid>https://arxiv.org/abs/2512.13687</guid>
<content:encoded><![CDATA[

arXiv:2512.13687v1 Announce Type: new 
Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LitePT: Lighter Yet Stronger Point Transformer</title>
<link>https://arxiv.org/abs/2512.13689</link>
<guid>https://arxiv.org/abs/2512.13689</guid>
<content:encoded><![CDATA[

arXiv:2512.13689v1 Announce Type: new 
Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
<link>https://arxiv.org/abs/2512.13690</link>
<guid>https://arxiv.org/abs/2512.13690</guid>
<content:encoded><![CDATA[

arXiv:2512.13690v1 Announce Type: new 
Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights</title>
<link>https://arxiv.org/abs/2512.11802</link>
<guid>https://arxiv.org/abs/2512.11802</guid>
<content:encoded><![CDATA[

arXiv:2512.11802v1 Announce Type: cross 
Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
<link>https://arxiv.org/abs/2512.11811</link>
<guid>https://arxiv.org/abs/2512.11811</guid>
<content:encoded><![CDATA[

arXiv:2512.11811v1 Announce Type: cross 
Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images</title>
<link>https://arxiv.org/abs/2512.11817</link>
<guid>https://arxiv.org/abs/2512.11817</guid>
<content:encoded><![CDATA[

arXiv:2512.11817v1 Announce Type: cross 
Abstract: This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision</title>
<link>https://arxiv.org/abs/2512.11824</link>
<guid>https://arxiv.org/abs/2512.11824</guid>
<content:encoded><![CDATA[

arXiv:2512.11824v1 Announce Type: cross 
Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?</title>
<link>https://arxiv.org/abs/2512.11827</link>
<guid>https://arxiv.org/abs/2512.11827</guid>
<content:encoded><![CDATA[

arXiv:2512.11827v1 Announce Type: cross 
Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of One-step Diffusion via Shortcutting Flow Paths</title>
<link>https://arxiv.org/abs/2512.11831</link>
<guid>https://arxiv.org/abs/2512.11831</guid>
<content:encoded><![CDATA[

arXiv:2512.11831v1 Announce Type: cross 
Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Decision Tree classifier: explainable and extendable PyTorch implementation</title>
<link>https://arxiv.org/abs/2512.11833</link>
<guid>https://arxiv.org/abs/2512.11833</guid>
<content:encoded><![CDATA[

arXiv:2512.11833v1 Announce Type: cross 
Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Foundry: A System for Training Foundational Vision AI Models</title>
<link>https://arxiv.org/abs/2512.11837</link>
<guid>https://arxiv.org/abs/2512.11837</guid>
<content:encoded><![CDATA[

arXiv:2512.11837v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document</title>
<link>https://arxiv.org/abs/2512.11849</link>
<guid>https://arxiv.org/abs/2512.11849</guid>
<content:encoded><![CDATA[

arXiv:2512.11849v1 Announce Type: cross 
Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title>
<link>https://arxiv.org/abs/2512.11867</link>
<guid>https://arxiv.org/abs/2512.11867</guid>
<content:encoded><![CDATA[

arXiv:2512.11867v1 Announce Type: cross 
Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title>
<link>https://arxiv.org/abs/2512.11883</link>
<guid>https://arxiv.org/abs/2512.11883</guid>
<content:encoded><![CDATA[

arXiv:2512.11883v1 Announce Type: cross 
Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics</title>
<link>https://arxiv.org/abs/2512.11903</link>
<guid>https://arxiv.org/abs/2512.11903</guid>
<content:encoded><![CDATA[

arXiv:2512.11903v1 Announce Type: cross 
Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-training vision models for the classification of alerts from wide-field time-domain surveys</title>
<link>https://arxiv.org/abs/2512.11957</link>
<guid>https://arxiv.org/abs/2512.11957</guid>
<content:encoded><![CDATA[

arXiv:2512.11957v1 Announce Type: cross 
Abstract: Modern wide-field time-domain surveys facilitate the study of transient, variable and moving phenomena by conducting image differencing and relaying alerts to their communities. Machine learning tools have been used on data from these surveys and their precursors for more than a decade, and convolutional neural networks (CNNs), which make predictions directly from input images, saw particularly broad adoption through the 2010s. Since then, continually rapid advances in computer vision have transformed the standard practices around using such models. It is now commonplace to use standardized architectures pre-trained on large corpora of everyday images (e.g., ImageNet). In contrast, time-domain astronomy studies still typically design custom CNN architectures and train them from scratch. Here, we explore the affects of adopting various pre-training regimens and standardized model architectures on the performance of alert classification. We find that the resulting models match or outperform a custom, specialized CNN like what is typically used for filtering alerts. Moreover, our results show that pre-training on galaxy images from Galaxy Zoo tends to yield better performance than pre-training on ImageNet or training from scratch. We observe that the design of standardized architectures are much better optimized than the custom CNN baseline, requiring significantly less time and memory for inference despite having more trainable parameters. On the eve of the Legacy Survey of Space and Time and other image-differencing surveys, these findings advocate for a paradigm shift in the creation of vision models for alerts, demonstrating that greater performance and efficiency, in time and in data, can be achieved by adopting the latest practices from the computer vision field.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic search for 100M+ galaxy images using AI-generated captions</title>
<link>https://arxiv.org/abs/2512.11982</link>
<guid>https://arxiv.org/abs/2512.11982</guid>
<content:encoded><![CDATA[

arXiv:2512.11982v1 Announce Type: cross 
Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMV: An Automatic Multi-Agent System for Music Video Generation</title>
<link>https://arxiv.org/abs/2512.12196</link>
<guid>https://arxiv.org/abs/2512.12196</guid>
<content:encoded><![CDATA[

arXiv:2512.12196v1 Announce Type: cross 
Abstract: Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for "story" or "singer" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion</title>
<link>https://arxiv.org/abs/2512.12203</link>
<guid>https://arxiv.org/abs/2512.12203</guid>
<content:encoded><![CDATA[

arXiv:2512.12203v1 Announce Type: cross 
Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT</title>
<link>https://arxiv.org/abs/2512.12236</link>
<guid>https://arxiv.org/abs/2512.12236</guid>
<content:encoded><![CDATA[

arXiv:2512.12236v1 Announce Type: cross 
Abstract: Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.
  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2512.12284</link>
<guid>https://arxiv.org/abs/2512.12284</guid>
<content:encoded><![CDATA[

arXiv:2512.12284v1 Announce Type: cross 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JPEG-Inspired Cloud-Edge Holography</title>
<link>https://arxiv.org/abs/2512.12367</link>
<guid>https://arxiv.org/abs/2512.12367</guid>
<content:encoded><![CDATA[

arXiv:2512.12367v1 Announce Type: cross 
Abstract: Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $<$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.12663</link>
<guid>https://arxiv.org/abs/2512.12663</guid>
<content:encoded><![CDATA[

arXiv:2512.12663v1 Announce Type: cross 
Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.12683</link>
<guid>https://arxiv.org/abs/2512.12683</guid>
<content:encoded><![CDATA[

arXiv:2512.12683v1 Announce Type: cross 
Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning</title>
<link>https://arxiv.org/abs/2512.12690</link>
<guid>https://arxiv.org/abs/2512.12690</guid>
<content:encoded><![CDATA[

arXiv:2512.12690v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering</title>
<link>https://arxiv.org/abs/2512.12694</link>
<guid>https://arxiv.org/abs/2512.12694</guid>
<content:encoded><![CDATA[

arXiv:2512.12694v1 Announce Type: cross 
Abstract: Large-scale digitization initiatives have unlocked massive collections of historical newspapers, yet effective computational access remains hindered by OCR corruption, multilingual orthographic variation, and temporal language drift. We develop and evaluate a multilingual Retrieval-Augmented Generation pipeline specifically designed for question answering on noisy historical documents. Our approach integrates: (i) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion to improve retrieval robustness against vocabulary mismatch; (ii) a carefully engineered generation prompt that enforces strict grounding in retrieved evidence and explicit abstention when evidence is insufficient; and (iii) a modular architecture enabling systematic component evaluation. We conduct comprehensive ablation studies on Named Entity Recognition and embedding model selection, demonstrating the importance of syntactic coherence in entity extraction and balanced performance-efficiency trade-offs in dense retrieval. Our end-to-end evaluation framework shows that the pipeline generates faithful answers for well-supported queries while correctly abstaining from unanswerable questions. The hybrid retrieval strategy improves recall stability, particularly benefiting from RRF's ability to smooth performance variance across query formulations. We release our code and configurations at https://anonymous.4open.science/r/RAGs-C5AE/, providing a reproducible foundation for robust historical document question answering.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Feedback Alignment</title>
<link>https://arxiv.org/abs/2512.12762</link>
<guid>https://arxiv.org/abs/2512.12762</guid>
<content:encoded><![CDATA[

arXiv:2512.12762v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2512.12772</link>
<guid>https://arxiv.org/abs/2512.12772</guid>
<content:encoded><![CDATA[

arXiv:2512.12772v1 Announce Type: cross 
Abstract: Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</title>
<link>https://arxiv.org/abs/2512.12827</link>
<guid>https://arxiv.org/abs/2512.12827</guid>
<content:encoded><![CDATA[

arXiv:2512.12827v1 Announce Type: cross 
Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams</title>
<link>https://arxiv.org/abs/2512.12939</link>
<guid>https://arxiv.org/abs/2512.12939</guid>
<content:encoded><![CDATA[

arXiv:2512.12939v1 Announce Type: cross 
Abstract: We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(\alpha\) (trade-off between temporal misalignment and diagram discrepancy) and \(\beta\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework</title>
<link>https://arxiv.org/abs/2512.12945</link>
<guid>https://arxiv.org/abs/2512.12945</guid>
<content:encoded><![CDATA[

arXiv:2512.12945v1 Announce Type: cross 
Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Compression to Construct Transferable Bitrate Ladders</title>
<link>https://arxiv.org/abs/2512.12952</link>
<guid>https://arxiv.org/abs/2512.12952</guid>
<content:encoded><![CDATA[

arXiv:2512.12952v1 Announce Type: cross 
Abstract: Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs</title>
<link>https://arxiv.org/abs/2512.12984</link>
<guid>https://arxiv.org/abs/2512.12984</guid>
<content:encoded><![CDATA[

arXiv:2512.12984v1 Announce Type: cross 
Abstract: We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12987</link>
<guid>https://arxiv.org/abs/2512.12987</guid>
<content:encoded><![CDATA[

arXiv:2512.12987v1 Announce Type: cross 
Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning</title>
<link>https://arxiv.org/abs/2512.13131</link>
<guid>https://arxiv.org/abs/2512.13131</guid>
<content:encoded><![CDATA[

arXiv:2512.13131v1 Announce Type: cross 
Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.13262</link>
<guid>https://arxiv.org/abs/2512.13262</guid>
<content:encoded><![CDATA[

arXiv:2512.13262v1 Announce Type: cross 
Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging</title>
<link>https://arxiv.org/abs/2512.13434</link>
<guid>https://arxiv.org/abs/2512.13434</guid>
<content:encoded><![CDATA[

arXiv:2512.13434v1 Announce Type: cross 
Abstract: Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing</title>
<link>https://arxiv.org/abs/2512.13497</link>
<guid>https://arxiv.org/abs/2512.13497</guid>
<content:encoded><![CDATA[

arXiv:2512.13497v1 Announce Type: cross 
Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Diffusion Preview with Consistency Solver</title>
<link>https://arxiv.org/abs/2512.13592</link>
<guid>https://arxiv.org/abs/2512.13592</guid>
<content:encoded><![CDATA[

arXiv:2512.13592v1 Announce Type: cross 
Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
<link>https://arxiv.org/abs/2512.13641</link>
<guid>https://arxiv.org/abs/2512.13641</guid>
<content:encoded><![CDATA[

arXiv:2512.13641v1 Announce Type: cross 
Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Can Leverage Human Videos for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.13644</link>
<guid>https://arxiv.org/abs/2512.13644</guid>
<content:encoded><![CDATA[

arXiv:2512.13644v1 Announce Type: cross 
Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</title>
<link>https://arxiv.org/abs/2512.13660</link>
<guid>https://arxiv.org/abs/2512.13660</guid>
<content:encoded><![CDATA[

arXiv:2512.13660v1 Announce Type: cross 
Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directional Textual Inversion for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.13672</link>
<guid>https://arxiv.org/abs/2512.13672</guid>
<content:encoded><![CDATA[

arXiv:2512.13672v1 Announce Type: cross 
Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature</title>
<link>https://arxiv.org/abs/2106.05261</link>
<guid>https://arxiv.org/abs/2106.05261</guid>
<content:encoded><![CDATA[

arXiv:2106.05261v3 Announce Type: replace 
Abstract: Recently, object detection has proven vulnerable to adversarial patch attacks. The attackers holding a specially crafted patch can hide themselves from state-of-the-art detectors, e.g., YOLO, even in the physical world. This attack can bring serious security threats, such as escaping from surveillance cameras. How to effectively detect this kind of adversarial examples to catch potential attacks has become an important problem. In this paper, we propose two detection methods: the signature-based method and the signature-independent method. First, we identify two signatures of existing adversarial patches that can be utilized to precisely locate patches within adversarial examples. By employing the signatures, a fast signature-based method is developed to detect the adversarial objects. Second, we present a robust signature-independent method based on the \textit{content semantics consistency} of model outputs. Adversarial objects violate this consistency, appearing locally but disappearing globally, while benign ones remain consistently present. The experiments demonstrate that two proposed methods can effectively detect attacks both in the digital and physical world. These methods each offer distinct advantage. Specifically, the signature-based method is capable of real-time detection, while the signature-independent method can detect unknown adversarial patch attacks and makes defense-aware attacks almost impossible to perform.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer</title>
<link>https://arxiv.org/abs/2112.01330</link>
<guid>https://arxiv.org/abs/2112.01330</guid>
<content:encoded><![CDATA[

arXiv:2112.01330v2 Announce Type: replace 
Abstract: Interval and large invasive breast cancers, which are associated with worse prognosis than other cancers, are usually detected at a late stage due to false negative assessments of screening mammograms. The missed screening-time detection is commonly caused by the tumor being obscured by its surrounding breast tissues, a phenomenon called masking. To study and benchmark mammographic masking of cancer, in this work we introduce CSAW-M, the largest public mammographic dataset, collected from over 10,000 individuals and annotated with potential masking. In contrast to the previous approaches which measure breast image density as a proxy, our dataset directly provides annotations of masking potential assessments from five specialists. We also trained deep learning models on CSAW-M to estimate the masking level and showed that the estimated masking is significantly more predictive of screening participants diagnosed with interval and large invasive cancers -- without being explicitly trained for these tasks -- than its breast density counterparts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExReg: Wide-range Photo Exposure Correction via a Multi-dimensional Regressor with Attention</title>
<link>https://arxiv.org/abs/2212.14801</link>
<guid>https://arxiv.org/abs/2212.14801</guid>
<content:encoded><![CDATA[

arXiv:2212.14801v2 Announce Type: replace 
Abstract: Photo exposure correction is widely investigated, but fewer studies focus on correcting under- and over-exposed images simultaneously. Three issues remain open to handle and correct both under- and over-exposed images in a unified way. First, a locally-adaptive exposure adjustment may be more flexible instead of learning a global mapping. Second, it is an ill-posed problem to determine the suitable exposure values locally. Third, photos with the same content but different exposures may not reach consistent adjustment results. To this end, we proposed a novel exposure correction network, ExReg, to address the challenges by formulating exposure correction as a multi-dimensional regression process. Given an input image, a compact multi-exposure generation network is introduced to generate images with different exposure conditions for multi-dimensional regression and exposure correction in the next stage. An auxiliary module is designed to predict the region-wise exposure values, guiding the proposed Encoder-Decoder ANP (Attentive Neural Processes) to regress the final corrected image. The experimental results show that ExReg can generate well-exposed results and outperform the SOTA method in PSNR for extensive exposure problems. Furthermore, the processing speed, with 0.05 seconds per image on an RTX 3090, is efficient. When tested on the same image under various exposure levels, ExReg also yields results that are visually consistent and physically accurate.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral Pedestrian Detection</title>
<link>https://arxiv.org/abs/2308.01042</link>
<guid>https://arxiv.org/abs/2308.01042</guid>
<content:encoded><![CDATA[

arXiv:2308.01042v3 Announce Type: replace 
Abstract: Multispectral pedestrian detection is essential to various tasks especially autonomous driving, for which both the accuracy and computational cost are of paramount importance. Most existing approaches treat RGB and infrared modalities equally. They typically adopt two symmetrical backbones for multimodal feature extraction, which ignore the substantial differences between modalities and bring great difficulty for the reduction of the computational cost as well as effective crossmodal fusion. In this work, we propose a novel and efficient framework named Wavelet-context Cooperative Network (WCCNet), which differentially extracts complementary features across spectra with low computational cost and further fuses these diverse features based on their spatially relevant cross-modal semantics. WCCNet explores an asymmetric but cooperative dual-stream backbone, in which WCCNet utilizes generic neural layers for texture-rich feature extraction from RGB modality, while proposing Mixture of Wavelet Experts (MoWE) to capture complementary frequency patterns of infrared modality. By assessing multispectral environmental context, MoWE generates routing scores to selectively activate specific learnable Adaptive DWT (ADWT) layers, alongside shared static DWT, which are both considerible lightwight and efficient to significantly reduce computational overhead and facilitate subsequent fusion. To further fuse these multispectral features with significant semantic differences, we elaborately design the crossmodal rearranging fusion module (CMRF), which aims to mitigate misalignment and merge semantically complementary features in spatially-related local regions to amplify the crossmodal reciprocal information. Results from comprehensive evaluations on KAIST and FLIR benchmarks indicate that WCCNet outperforms state-of-the-art methods with considerable computational efficiency and competitive accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADS: Plug-and-Play 3D Human Pose Analysis via Diffusion Generative Modeling</title>
<link>https://arxiv.org/abs/2401.08930</link>
<guid>https://arxiv.org/abs/2401.08930</guid>
<content:encoded><![CDATA[

arXiv:2401.08930v2 Announce Type: replace 
Abstract: Diffusion models have demonstrated impressive capabilities in modeling complex data distributions and are increasingly applied in various generative tasks. In this work, we propose Pose Analysis by Diffusion Synthesis PADS, a unified generative modeling framework for 3D human pose analysis. PADS first learns a task-agnostic 3D pose prior via unconditional diffusion synthesis and then performs training-free adaptation to a wide range of pose analysis tasks, including 3D pose estimation, denoising, completion, etc., through a posterior sampling scheme. By formulating each task as an inverse problem with a known forward operator, PADS injects task-specific constraints during inference while keeping the pose prior fixed. This plug-and-play framework removes the need for task-specific supervision or retraining, offering flexibility and scalability across diverse conditions. Extensive experiments on different benchmarks showcase the superior performance against both learning-based and optimization-based baselines, demonstrating the effectiveness and generalization capability of our method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foveated Retinotopy Improves Classification and Localization in Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2402.15480</link>
<guid>https://arxiv.org/abs/2402.15480</guid>
<content:encoded><![CDATA[

arXiv:2402.15480v5 Announce Type: replace 
Abstract: From falcons spotting preys to humans recognizing faces, rapid visual abilities depend on a foveated retinal organization which delivers high-acuity central vision while preserving low-resolution periphery. This organization is conserved along early visual pathways but remains underexplored in machine learning. Here we examine how embedding a foveated retinotopic transformation as a preprocessing layer impacts convolutional neural networks (CNNs) for image classification. By applying a log-polar mapping to off-the-shelf models and retraining them, we retain comparable accuracy while improving robustness to scale and rotation. We show that this architecture becomes highly sensitive to fixation-point shifts, and that this sensitivity yields a proxy for defining saliency maps that effectively facilitates object localization. Our results show that foveated retinotopy encodes prior geometric knowledge, offering a solution to visual-search and enhancing both classification and localization. These findings connect biological vision principles with artificial networks, pointing to new, robust and efficient directions for computer-vision systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient and Harmonized Framework for Balanced Cross-Domain Feature Integration</title>
<link>https://arxiv.org/abs/2403.18461</link>
<guid>https://arxiv.org/abs/2403.18461</guid>
<content:encoded><![CDATA[

arXiv:2403.18461v3 Announce Type: replace 
Abstract: Despite significant advancements in image generation using advanced generative frameworks, cross-image integration of content and style remains a key challenge. Current generative models, while powerful, frequently depend on vague textual prompts to define styles--creating difficulties in balancing content semantics and style preservation. We propose a novel framework that utilizes customized models to learn style representations. It enhances content preservation through cross-model feature and attention modulation, leveraging the inherent semantic consistency across models. Additionally, we introduce fixed feature and adaptive attention fusion to achieve the desired balance between content and style. We further develop spatial (mask-guided localized) and temporal (multi-style compositional) multi-model combinations, enabling flexible fusion of models and styles. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in balancing content preservation and stylistic coherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Wrong-way Cycling Detection in CCTV Videos: Sparse Sampling is All You Need</title>
<link>https://arxiv.org/abs/2405.07293</link>
<guid>https://arxiv.org/abs/2405.07293</guid>
<content:encoded><![CDATA[

arXiv:2405.07293v2 Announce Type: replace 
Abstract: Effective monitoring of unusual transportation behaviors, such as wrong-way cycling (i.e., riding a bicycle or e-bike against designated traffic flow), is crucial for optimizing law enforcement deployment and traffic planning. However, accurately recording all wrong-way cycling events is both unnecessary and infeasible in resource-constrained environments, as it requires high-resolution cameras for evidence collection and event detection. To address this challenge, we propose WWC-Predictor, a novel method for efficiently estimating the wrong-way cycling ratio, defined as the proportion of wrong-way cycling events relative to the total number of cycling movements over a given time period. The core innovation of our method lies in accurately detecting wrong-way cycling events in sparsely sampled frames using a light-weight detector, then estimating the overall ratio using an autoregressive moving average model. To evaluate the effectiveness of our method, we construct a benchmark dataset consisting of 35 minutes of video sequences with minute-level annotations.Our method achieves an average error rate of a mere 1.475\% while consuming only 19.12\% GPU time required by conventional tracking methods, validating its effectiveness in estimating the wrong-way cycling ratio. Our source code is publicly available at: https://github.com/VICA-Lab-HKUST-GZ/WWC-Predictor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</title>
<link>https://arxiv.org/abs/2405.20336</link>
<guid>https://arxiv.org/abs/2405.20336</guid>
<content:encoded><![CDATA[

arXiv:2405.20336v3 Announce Type: replace 
Abstract: In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain</title>
<link>https://arxiv.org/abs/2411.19534</link>
<guid>https://arxiv.org/abs/2411.19534</guid>
<content:encoded><![CDATA[

arXiv:2411.19534v2 Announce Type: replace 
Abstract: We tackle the problem of quantifying the number of objects by a generative text-to-image model. Rather than retraining such a model for each new image domain of interest, which leads to high computational costs and limited scalability, we are the first to consider this problem from a domain-agnostic perspective. We propose QUOTA, an optimization framework for text-to-image models that enables effective object quantification across unseen domains without retraining. It leverages a dual-loop meta-learning strategy to optimize a domain-invariant prompt. Further, by integrating prompt learning with learnable counting and domain tokens, our method captures stylistic variations and maintains accuracy, even for object classes not encountered during training. For evaluation, we adopt a new benchmark specifically designed for object quantification in domain generalization, enabling rigorous assessment of object quantification accuracy and adaptability across unseen domains in text-to-image generation. Extensive experiments demonstrate that QUOTA outperforms conventional models in both object quantification accuracy and semantic consistency, setting a new benchmark for efficient and scalable text-to-image generation for any domain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeWalker: Personalized Neural Space for Lifelong Head Avatars</title>
<link>https://arxiv.org/abs/2412.02421</link>
<guid>https://arxiv.org/abs/2412.02421</guid>
<content:encoded><![CDATA[

arXiv:2412.02421v2 Announce Type: replace 
Abstract: We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CleanDIFT: Diffusion Features without Noise</title>
<link>https://arxiv.org/abs/2412.03439</link>
<guid>https://arxiv.org/abs/2412.03439</guid>
<content:encoded><![CDATA[

arXiv:2412.03439v3 Announce Type: replace 
Abstract: Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep priors for satellite image restoration with accurate uncertainties</title>
<link>https://arxiv.org/abs/2412.04130</link>
<guid>https://arxiv.org/abs/2412.04130</guid>
<content:encoded><![CDATA[

arXiv:2412.04130v2 Announce Type: replace 
Abstract: Satellite optical images, upon their on-ground receipt, offer a distorted view of the observed scene. Their restoration, including denoising, deblurring, and sometimes super-resolution, is required before their exploitation. Moreover, quantifying the uncertainties related to this restoration helps to reduce the risks of misinterpreting the image content. Deep learning methods are now state-of-the-art for satellite image restoration. Among them, direct inversion methods train a specific network for each sensor, and generally provide a point estimation of the restored image without the associated uncertainties. Alternatively, deep regularization (DR) methods learn a deep prior on target images before plugging it, as the regularization term, into a model-based optimization scheme. This allows for restoring images from several sensors with a single network and possibly for estimating associated uncertainties. In this paper, we introduce VBLE-xz, a DR method that solves the inverse problem in the latent space of a variational compressive autoencoder (CAE). We adapt the regularization strength by modulating the bitrate of the trained CAE with a training-free approach. Then, VBLE-xz estimates relevant uncertainties jointly in the latent and in the image spaces by sampling an explicit posterior estimated within variational inference. This enables fast posterior sampling, unlike state-of-the-art DR methods that use Markov chains or diffusion-based approaches. We conduct a comprehensive set of experiments on very high-resolution simulated and real Pl\'eiades images, asserting the performance, robustness and scalability of the proposed method. They demonstrate that VBLE-xz represents a compelling alternative to direct inversion methods when uncertainty quantification is required. The code associated to this paper is available in https://github.com/MaudBqrd/VBLExz.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment</title>
<link>https://arxiv.org/abs/2412.04783</link>
<guid>https://arxiv.org/abs/2412.04783</guid>
<content:encoded><![CDATA[

arXiv:2412.04783v5 Announce Type: replace 
Abstract: Wireless sensing has recently found widespread applications in diverse environments, including homes, offices, and public spaces. By analyzing patterns in channel state information (CSI), it is possible to infer human actions for tasks such as person identification, gesture recognition, and fall detection. However, CSI is highly sensitive to environmental changes, where even minor alterations can significantly distort the CSI patterns. This sensitivity often leads to performance degradation or outright failure when applying wireless sensing models trained in one environment to another. To address this challenge, Domain Alignment (DAL) has been widely adopted for cross-domain classification tasks, as it focuses on aligning the global distributions of the source and target domains in feature space. Despite its popularity, DAL often neglects inter-category relationships, which can lead to misalignment between categories across domains, even when global alignment is achieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless sensing. Our approach begins by constructing a help set using KNN from the target domain, enabling local alignment between the source and target domains within each category using MMD. Additionally, we address a key instability issue commonly observed in cross-domain methods, where model performance fluctuates sharply between epochs. Further, most existing methods struggle to determine an optimal stopping point during training due to the absence of labeled data from the target domain. Our method resolves this by excluding the support set from the target domain during training and employing it as a validation set to determine the stopping criterion. The dataset and code are publicly available at https://github.com/RS2002/KNN-MMD .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2412.16809</link>
<guid>https://arxiv.org/abs/2412.16809</guid>
<content:encoded><![CDATA[

arXiv:2412.16809v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has recently attracted wide attentions in various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation, due to its photorealistic and efficient rendering performance. High-quality reconstrution of 3DGS relies on sufficient splats and a reasonable distribution of these splats to fit real geometric surface and texture details, which turns out to be a challenging problem. We present GeoTexDensifier, a novel geometry-texture-aware densification strategy to reconstruct high-quality Gaussian splats which better comply with the geometric structure and texture richness of the scene. Specifically, our GeoTexDensifier framework carries out an auxiliary texture-aware densification method to produce a denser distribution of splats in fully textured areas, while keeping sparsity in low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile, a geometry-aware splitting strategy takes depth and normal priors to guide the splitting sampling and filter out the noisy splats whose initial positions are far from the actual geometric surfaces they aim to fit, under a Validation of Depth Ratio Change checking. With the help of relative monocular depth prior, such geometry-aware validation can effectively reduce the influence of scattered Gaussians to the final rendering quality, especially in regions with weak textures or without sufficient training views. The texture-aware densification and geometry-aware splitting strategies are fully combined to obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier framework on various datasets and compare our Novel View Synthesis results to other state-of-the-art 3DGS approaches, with detailed quantitative and qualitative evaluations to demonstrate the effectiveness of our method in producing more photorealistic 3DGS models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Reality-Virtuality Interconnections in Urban Digital Twins for Superior Intelligent Road Inspection and Simulation</title>
<link>https://arxiv.org/abs/2412.17699</link>
<guid>https://arxiv.org/abs/2412.17699</guid>
<content:encoded><![CDATA[

arXiv:2412.17699v2 Announce Type: replace 
Abstract: Road inspection is crucial for maintaining road serviceability and ensuring traffic safety, as road defects gradually develop and compromise functionality. Traditional inspection methods, which rely on manual evaluations, are labor-intensive, costly, and time-consuming. While data-driven approaches are gaining traction, the scarcity and spatial sparsity of real-world road defects present significant challenges in acquiring high-quality datasets. Existing simulators designed to generate detailed synthetic driving scenes, however, lack models for road defects. Moreover, advanced driving tasks that involve interactions with road surfaces, such as planning and control in defective areas, remain underexplored. To address these limitations, we propose a multi-modal sensor platform integrated with an urban digital twin (UDT) system for intelligent road inspection. First, hierarchical road models are constructed from real-world driving data collected using vehicle-mounted sensors, resulting in highly detailed representations of road defect structures and surface elevations. Next, digital road twins are generated to create simulation environments for comprehensive analysis and evaluation of algorithm performance. These scenarios are then imported into a simulator to facilitate both data acquisition and physical simulation. Experimental results demonstrate that driving tasks, including perception and decision-making, benefit significantly from the high-fidelity road defect scenes generated by our system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPBridge: Latent Diffusion Bridge for Dense Prediction</title>
<link>https://arxiv.org/abs/2412.20506</link>
<guid>https://arxiv.org/abs/2412.20506</guid>
<content:encoded><![CDATA[

arXiv:2412.20506v4 Announce Type: replace 
Abstract: Diffusion models demonstrate remarkable capabilities in capturing complex data distributions and have achieved compelling results in many generative tasks. While they have recently been extended to dense prediction tasks such as depth estimation and surface normal prediction, their full potential in this area remains underexplored. As target signal maps and input images are pixel-wise aligned, the conventional noise-to-data generation paradigm is inefficient, and input images can serve as a more informative prior compared to pure noise. Diffusion bridge models, which support data-to-data generation between two general data distributions, offer a promising alternative, but they typically fail to exploit the rich visual priors embedded in large pretrained foundation models. To address these limitations, we integrate diffusion bridge formulation with structured visual priors and introduce DPBridge, the first latent diffusion bridge framework for dense prediction tasks. To resolve the incompatibility between diffusion bridge models and pretrained diffusion backbones, we propose (1) a tractable reverse transition kernel for the diffusion bridge process, enabling maximum likelihood training scheme; (2) finetuning strategies including distribution-aligned normalization and image consistency loss. Experiments across extensive benchmarks validate that our method consistently achieves superior performance, demonstrating its effectiveness and generalization capability under different scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Anchored, Class Variance-Optimized Clustering for Robust Semi-Supervised Few-Shot Learning</title>
<link>https://arxiv.org/abs/2501.14401</link>
<guid>https://arxiv.org/abs/2501.14401</guid>
<content:encoded><![CDATA[

arXiv:2501.14401v3 Announce Type: replace 
Abstract: Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets. To further establish its robustness, we conduct extensive experiments under challenging conditions, showing that the model generalizes well to domain shifts and achieves new state-of-the-art performance in open-set settings with distractor classes, highlighting its effectiveness for real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Calibration of a Multi-Camera System with Limited Overlapping Fields of View for 3D Surgical Scene Reconstruction</title>
<link>https://arxiv.org/abs/2501.16221</link>
<guid>https://arxiv.org/abs/2501.16221</guid>
<content:encoded><![CDATA[

arXiv:2501.16221v3 Announce Type: replace 
Abstract: The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise. The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations. We contribute a novel, fast, and fully automatic calibration method based on the projection of multi-scale markers (MSMs) using a ceiling-mounted projector. MSMs consist of 2D patterns projected at varying scales, ensuring accurate extraction of well distributed point correspondences across significantly different viewpoints and zoom levels. Validation is performed using both synthetic and real data captured in a mock-up OR, with comparisons to traditional manual marker-based methods as well as markerless calibration methods. The method achieves accuracy comparable to manual, operator-dependent calibration methods while exhibiting higher robustness under conditions of significant differences in zoom levels. Additionally, we show that state-of-the-art Structure-from-Motion (SfM) pipelines are ineffective in 3D-SSR settings, even when additional texture is projected onto the OR floor. The use of a ceiling-mounted entry-level projector proves to be an effective alternative to operator-dependent, traditional marker-based methods, paving the way for fully automated 3D-SSR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</title>
<link>https://arxiv.org/abs/2502.13764</link>
<guid>https://arxiv.org/abs/2502.13764</guid>
<content:encoded><![CDATA[

arXiv:2502.13764v4 Announce Type: replace 
Abstract: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved Pure Fully Connected Neural Network for Rice Grain Classification</title>
<link>https://arxiv.org/abs/2503.03111</link>
<guid>https://arxiv.org/abs/2503.03111</guid>
<content:encoded><![CDATA[

arXiv:2503.03111v4 Announce Type: replace 
Abstract: Rice is a staple food for a significant portion of the world's population, providing essential nutrients and serving as a versatile in-gredient in a wide range of culinary traditions. Recently, the use of deep learning has enabled automated classification of rice, im-proving accuracy and efficiency. However, classical models based on first-stage training may face difficulties in distinguishing between rice varieties with similar external characteristics, thus leading to misclassifications. Considering the transparency and feasibility of model, we selected and gradually improved pure fully connected neural network to achieve classification of rice grain. The dataset we used contains both global and domestic rice images obtained from websites and laboratories respectively. First, the training mode was changed from one-stage training to two-stage training, which significantly contributes to distinguishing two similar types of rice. Secondly, the preprocessing method was changed from random tilting to horizontal or vertical position cor-rection. After those two enhancements, the accuracy of our model increased notably from 97% to 99%. In summary, two subtle methods proposed in this study can remarkably enhance the classification ability of deep learning models in terms of the classification of rice grain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</title>
<link>https://arxiv.org/abs/2503.03726</link>
<guid>https://arxiv.org/abs/2503.03726</guid>
<content:encoded><![CDATA[

arXiv:2503.03726v2 Announce Type: replace 
Abstract: Estimating the 6D pose of textureless objects from RGB images is an important problem in robotics. Due to appearance ambiguities, rotational symmetries, and severe occlusions, single-view based 6D pose estimators are still unable to handle a wide range of objects, motivating research towards multi-view pose estimation and next-best-view prediction that addresses these limitations. In this work, we propose a comprehensive active perception framework for estimating the 6D poses of textureless objects using only RGB images. Our approach is built upon a key idea: decoupling the 6D pose estimation into a two-step sequential process can greatly improve both accuracy and efficiency. First, we estimate the 3D translation of each object, resolving scale and depth ambiguities inherent to RGB images. These estimates are then used to simplify the subsequent task of determining the 3D orientation, which we achieve through canonical scale template matching. Building on this formulation, we then introduce an active perception strategy that predicts the next best camera viewpoint to capture an RGB image, effectively reducing object pose uncertainty and enhancing pose accuracy. We evaluate our method on the public ROBI and TOD datasets, as well as on our reconstructed transparent object dataset, T-ROBI. Under the same camera viewpoints, our multi-view pose estimation significantly outperforms state-of-the-art approaches. Furthermore, by leveraging our next-best-view strategy, our approach achieves high pose accuracy with fewer viewpoints than heuristic-based policies across all evaluated datasets. The accompanying video and T-ROBI dataset will be released on our project page: https://trailab.github.io/ActiveODPE.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic Audiovisual Narrative Processing</title>
<link>https://arxiv.org/abs/2503.06940</link>
<guid>https://arxiv.org/abs/2503.06940</guid>
<content:encoded><![CDATA[

arXiv:2503.06940v2 Announce Type: replace 
Abstract: Most research decoding brain signals into images, often using them as priors for generative models, has focused only on visual content. This overlooks the brain's natural ability to integrate auditory and visual information, for instance, sound strongly influences how we perceive visual scenes. To investigate this, we propose a new task of reconstructing continuous video stimuli from multimodal brain signals recorded during audiovisual stimulation. To enable this, we introduce CineBrain, the first large-scale dataset that synchronizes fMRI and EEG during audiovisual viewing, featuring six hours of \textit{The Big Bang Theory} episodes for cross-modal alignment. We also conduct the first systematic exploration of combining fMRI and EEG for video reconstruction and present CineSync, a framework for reconstructing dynamic video using a Multi-Modal Fusion Encoder and a Neural Latent Decoder. CineSync achieves state-of-the-art performance in dynamic reconstruction, leveraging the complementary strengths of fMRI and EEG to improve visual fidelity. Our analysis shows that auditory cortical activations enhance decoding accuracy, highlighting the role of auditory input in visual perception. Project Page: https://jianxgao.github.io/CineBrain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2503.07874</link>
<guid>https://arxiv.org/abs/2503.07874</guid>
<content:encoded><![CDATA[

arXiv:2503.07874v2 Announce Type: replace 
Abstract: Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2503.08884</link>
<guid>https://arxiv.org/abs/2503.08884</guid>
<content:encoded><![CDATA[

arXiv:2503.08884v2 Announce Type: replace 
Abstract: Unimodal vision models are known to rely on spurious correlations, but it remains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit similar biases despite language supervision. In this paper, we investigate spurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4 and open-set object detectors to automatically identify spurious visual cues without human supervision. Our findings reveal that spurious correlations cause two major failure modes in MLLMs: (1) over-reliance on spurious cues for object recognition, where removing these cues reduces accuracy, and (2) object hallucination, where spurious cues amplify the hallucination by over 10x. We validate our findings in various MLLMs and datasets. Beyond diagnosing these failures, we explore potential mitigation strategies, such as prompt ensembling and reasoning-based prompting, and conduct ablation studies to examine the root causes of spurious bias in MLLMs. By exposing the persistence of spurious correlations, our study calls for more rigorous evaluation methods and mitigation strategies to enhance the reliability of MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FourierSR: A Fourier Token-based Plugin for Efficient Image Super-Resolution</title>
<link>https://arxiv.org/abs/2503.10043</link>
<guid>https://arxiv.org/abs/2503.10043</guid>
<content:encoded><![CDATA[

arXiv:2503.10043v2 Announce Type: replace 
Abstract: Image super-resolution (SR) aims to recover low-resolution images to high-resolution images, where improving SR efficiency is a high-profile challenge. However, commonly used units in SR, like convolutions and window-based Transformers, have limited receptive fields, making it challenging to apply them to improve SR under extremely limited computational cost. To address this issue, inspired by modeling convolution theorem through token mix, we propose a Fourier token-based plugin called FourierSR to improve SR uniformly, which avoids the instability or inefficiency of existing token mix technologies when applied as plug-ins. Furthermore, compared to convolutions and windows-based Transformers, our FourierSR only utilizes Fourier transform and multiplication operations, greatly reducing complexity while having global receptive fields. Experimental results show that our FourierSR as a plug-and-play unit brings an average PSNR gain of 0.34dB for existing efficient SR methods on Manga109 test set at the scale of x4, while the average increase in the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. We will release our codes upon acceptance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PanDx: AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on Contrast-enhanced CT</title>
<link>https://arxiv.org/abs/2503.10068</link>
<guid>https://arxiv.org/abs/2503.10068</guid>
<content:encoded><![CDATA[

arXiv:2503.10068v3 Announce Type: replace 
Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the most aggressive forms of pancreatic cancer and is often diagnosed at an advanced stage due to subtle early imaging signs. To enable earlier detection and improve clinical decision-making, we propose a coarse-to-fine AI-assisted framework named PanDx for identifying PDAC on contrast-enhanced CT scans. Our approach integrates two novel techniques: (1) distribution-aware stratified ensembling to improve generalization across lesion variations, and (2) peak-scaled lesion candidate extraction to enhance lesion localization precision. PanDx is developed and evaluated as part of the PANORAMA challenge, where it ranked 1st place on the official test set with an AUROC of 0.9263 and an AP of 0.7243. Furthermore, we analyzed failure cases with a radiologist to identify the limitations of AI models on this task and discussed potential future directions for model improvement. Our code and models are publicly available at https://github.com/han-liu/PanDx.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastVID: Dynamic Density Pruning for Fast Video Large Language Models</title>
<link>https://arxiv.org/abs/2503.11187</link>
<guid>https://arxiv.org/abs/2503.11187</guid>
<content:encoded><![CDATA[

arXiv:2503.11187v3 Announce Type: replace 
Abstract: Video Large Language Models have demonstrated strong video understanding capabilities, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. Existing pruning techniques fail to effectively exploit the spatiotemporal redundancy present in video data. To bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. Leveraging these insights, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID. Specifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential spatial and temporal information. Our method significantly reduces computational overhead while maintaining temporal and visual integrity. Extensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision, LLaVA-Video, Qwen2-VL, and Qwen2.5-VL. Notably, on LLaVA-OneVision-7B, FastVID effectively prunes $\textbf{90.3%}$ of video tokens, reduces FLOPs to $\textbf{8.3%}$, and accelerates the LLM prefill stage by $\textbf{7.1}\times$, while maintaining $\textbf{98.0%}$ of the original accuracy. The code is available at https://github.com/LunarShen/FastVID.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[

arXiv:2503.14505v2 Announce Type: replace 
Abstract: We introduce MusicInfuser, an approach that aligns pre-trained text-to-video diffusion models to generate high-quality dance videos synchronized with specified music tracks. Rather than training a multimodal audio-video or audio-motion model from scratch, our method demonstrates how existing video diffusion models can be efficiently adapted to align with musical inputs. We propose a novel layer-wise adaptability criterion based on a guidance-inspired constructive influence function to select adaptable layers, significantly reducing training costs while preserving rich prior knowledge, even with limited, specialized datasets. Experiments show that MusicInfuser effectively bridges the gap between music and video, generating novel and diverse dance movements that respond dynamically to music. Furthermore, our framework generalizes well to unseen music tracks, longer video sequences, and unconventional subjects, outperforming baseline models in consistency and synchronization. All of this is achieved without requiring motion data, with training completed on a single GPU within a day.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Distillation of 3D Point Clouds via Distribution Matching</title>
<link>https://arxiv.org/abs/2503.22154</link>
<guid>https://arxiv.org/abs/2503.22154</guid>
<content:encoded><![CDATA[

arXiv:2503.22154v3 Announce Type: replace 
Abstract: Large-scale datasets are usually required to train deep neural networks, but it increases the computational complexity hindering the practical applications. Recently, dataset distillation for images and texts has been attracting a lot of attention, that reduces the original dataset to a synthetic dataset to alleviate the computational burden of training while preserving essential task-relevant information. However, the dataset distillation for 3D point clouds remains largely unexplored, as the point clouds exhibit fundamentally different characteristics from that of images, making the dataset distillation more challenging. In this paper, we propose a distribution matching-based distillation framework for 3D point clouds that jointly optimizes the geometric structures as well as the orientations of the synthetic 3D objects. To address the semantic misalignment caused by unordered indexing of points, we introduce a Semantically Aligned Distribution Matching loss computed on the sorted features in each channel. Moreover, to address the rotation variation, we jointly learn the optimal rotation angles while updating the synthetic dataset to better align with the original feature distribution. Extensive experiments on widely used benchmark datasets demonstrate that the proposed method consistently outperforms existing dataset distillation methods, achieving superior accuracy and strong cross-architecture generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.07960</link>
<guid>https://arxiv.org/abs/2504.07960</guid>
<content:encoded><![CDATA[

arXiv:2504.07960v3 Announce Type: replace 
Abstract: Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relation-R1: Progressively Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relation Comprehension</title>
<link>https://arxiv.org/abs/2504.14642</link>
<guid>https://arxiv.org/abs/2504.14642</guid>
<content:encoded><![CDATA[

arXiv:2504.14642v3 Announce Type: replace 
Abstract: Recent advances in multi-modal large language models (MLLMs) have significantly improved object-level grounding and region captioning. However, they remain limited in visual relation understanding, struggling even with binary relation detection, let alone \textit{N}-ary relations involving multiple semantic roles. The core reason is the lack of modeling for \textit{structural semantic dependencies} among multi-entities, leading to unreliable outputs, hallucinations, and over-reliance on language priors (\eg, defaulting to ``person drinks a milk'' if a person is merely holding it). To this end, we propose Relation-R1, the \textit{first unified} relation comprehension framework that explicitly integrates cognitive chain-of-thought (CoT)-guided supervised fine-tuning (SFT) and group relative policy optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we first establish foundational reasoning capabilities via SFT, enforcing structured outputs with thinking processes. Then, GRPO is utilized to refine these outputs via multi-rewards optimization, prioritizing visual-semantic grounding over language-induced biases, thereby improving generalization capability. Furthermore, we investigate the impact of various CoT strategies within this framework, demonstrating that a specific-to-general progressive approach in CoT guidance further improves generalization, especially in capturing synonymous \textit{N}-ary relations. Extensive experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1 achieves state-of-the-art performance in both binary and \textit{N}-ary relation understanding.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation</title>
<link>https://arxiv.org/abs/2504.15134</link>
<guid>https://arxiv.org/abs/2504.15134</guid>
<content:encoded><![CDATA[

arXiv:2504.15134v4 Announce Type: replace 
Abstract: Category-level object pose estimation aims to predict the 6D pose and size of previously unseen instances from predefined categories, requiring strong generalization across diverse object instances. Although many previous methods attempt to mitigate intra-class variations, they often struggle with instances exhibiting complex geometries or significant deviations from canonical shapes. To address this issue, we propose INKL-Pose, a novel category-level object pose estimation framework that enables INstance-adaptive Keypoint Learning with local-to-global geometric aggregation. Specifically, our method first predicts semantically consistent and geometrically informative keypoints using an Instance-Adaptive Keypoint Detector, then refines them: (1) a Local Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global Keypoint Feature Aggregator using bidirectional Mamba for structural consistency. To enable bidirectional modeling in Mamba, we introduce a simple yet effective Feature Sequence Flipping strategy that preserves spatial coherence while constructing backward feature sequence. Additionally, we design a surface loss and a separation loss to encourage uniform coverage and spatial diversity in keypoint distribution. The resulting keypoints are mapped to a canonical space for 6D pose and size regression. Extensive experiments on CAMERA25, REAL275, and HouseCat6D show that INKL-Pose achieves state-of-the-art performance with 16.7M parameters and runs at 36 FPS on an NVIDIA RTX 4090D GPU.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering</title>
<link>https://arxiv.org/abs/2504.17545</link>
<guid>https://arxiv.org/abs/2504.17545</guid>
<content:encoded><![CDATA[

arXiv:2504.17545v2 Announce Type: replace 
Abstract: We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for radiance field rendering, wherein a set of 2D opaque surfels with view-dependent colors represent the coarse-scale geometry and appearance of scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale appearance details. The rendering with GESs consists of two passes -- surfels are first rasterized through a standard graphics pipeline to produce depth and color maps, and then Gaussians are splatted with depth testing and color accumulation on each pixel order independently. The optimization of GESs from multi-view images is performed through an elaborate coarse-to-fine procedure, faithfully capturing rich scene appearance. The entirely sorting-free rendering of GESs not only achieves very fast rates, but also produces view-consistent images, successfully avoiding popping artifacts under view changes. The basic GES representation can be easily extended to achieve anti-aliasing in rendering (Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage (Compact-GES), and reconstruct better scene geometries by replacing 3D Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs advance the state-of-the-arts as a compelling representation for ultra-fast high-fidelity radiance field rendering.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.13261</link>
<guid>https://arxiv.org/abs/2505.13261</guid>
<content:encoded><![CDATA[

arXiv:2505.13261v2 Announce Type: replace 
Abstract: In this work, we investigate how explicitly modeling problem's difficulty prior information shapes the effectiveness of reinforcement learning based fine-tuning for multimodal reasoning. Our exploration mainly comprises of following three perspective: First, through offline data curation, we analyze the U-shaped difficulty distribution of two given datasets using the base model by multi-round sampling, and then filter out prompts that are either too simple or extremely difficult to provide meaningful gradients and perform subsequent two-stage training. Second, we implement an online advantage differentiation, computing group-wise empirical accuracy as a difficulty proxy to adaptively reweight advantages estimation, providing stronger learning signals for more challenging problems. Finally, we introduce difficulty hints as explicit prompts for more complex samples in the second training stage, encouraging the model to calibrate its reasoning depth and perform reflective validation checks. Our comprehensive approach demonstrates significant performances across various multi-modal mathematical reasoning benchmarks with only 2K+0.6K two-stage training data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[

arXiv:2506.00227v2 Announce Type: replace 
Abstract: Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How PARTs assemble into wholes: Learning the relative composition of images</title>
<link>https://arxiv.org/abs/2506.03682</link>
<guid>https://arxiv.org/abs/2506.03682</guid>
<content:encoded><![CDATA[

arXiv:2506.03682v2 Announce Type: replace 
Abstract: The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normalize Filters! Classical Wisdom for Deep Vision</title>
<link>https://arxiv.org/abs/2506.04401</link>
<guid>https://arxiv.org/abs/2506.04401</guid>
<content:encoded><![CDATA[

arXiv:2506.04401v4 Announce Type: replace 
Abstract: Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation</title>
<link>https://arxiv.org/abs/2506.05444</link>
<guid>https://arxiv.org/abs/2506.05444</guid>
<content:encoded><![CDATA[

arXiv:2506.05444v2 Announce Type: replace 
Abstract: Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote sensing applications, particularly water body detection. However, deep learning-based segmentation models often face challenges related to convergence speed and stability, mainly due to the complex statistical distribution of this type of data. In this study, we evaluate the impact of mode normalization on two widely used semantic segmentation models, U-Net and SegNet. Specifically, we integrate mode normalization, to reduce convergence time while maintaining the performance of the baseline models. Experimental results demonstrate that mode normalization significantly accelerates convergence. Furthermore, cross-validation results indicate that normalized models exhibit increased stability in different zones. These findings highlight the effectiveness of normalization in improving computational efficiency and generalization in SAR image segmentation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WakeupUrban: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imagery</title>
<link>https://arxiv.org/abs/2506.09476</link>
<guid>https://arxiv.org/abs/2506.09476</guid>
<content:encoded><![CDATA[

arXiv:2506.09476v3 Announce Type: replace 
Abstract: Historical satellite imagery archive, such as Keyhole satellite data, offers rare insights into understanding early urban development and long-term transformation. However, severe quality degradation ($\textit{e.g.}$, distortion, misalignment, and spectral scarcity) and the absence of annotations have long hindered its analysis. To bridge this gap and enhance understanding of urban development, we introduce $\textbf{WakeupUrbanBench}$, an annotated segmentation dataset based on historical satellite imagery with the earliest observation time among all existing remote sensing (RS) datasets, along with a framework for unsupervised segmentation tasks, $\textbf{WakeupUSM}$. First, WakeupUrbanBench serves as a pioneer, expertly annotated dataset built on mid-$20^{\text{th}}$ century RS imagery, involving four key urban classes and spanning 4 cities across 2 continents with nearly 1000 km$^2$ area of diverse urban morphologies, and additionally introducing one present-day city. Second, WakeupUSM is a novel unsupervised semantic segmentation framework for historical RS imagery. It employs a confidence-aware alignment mechanism and focal-confidence loss based on a self-supervised learning architecture, which generates robust pseudo-labels and adaptively prioritizes prediction difficulty and label reliability to improve unsupervised segmentation on noisy historical data without manual supervision. Comprehensive experiments demonstrate WakeupUSM significantly outperforms existing unsupervised segmentation methods $\textbf{both WakeupUrbanBench and public dataset}$, promising to pave the way for quantitative studies of long-term urban change using modern computer vision. Our benchmark and codes will be released at https://github.com/Tianxiang-Hao/WakeupUrban.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmenting Visuals With Querying Words: Language Anchors For Semi-Supervised Image Segmentation</title>
<link>https://arxiv.org/abs/2506.13925</link>
<guid>https://arxiv.org/abs/2506.13925</guid>
<content:encoded><![CDATA[

arXiv:2506.13925v3 Announce Type: replace 
Abstract: Vision Language Models (VLMs) provide rich semantic priors but are underexplored in Semi supervised Semantic Segmentation. Recent attempts to integrate VLMs to inject high level semantics overlook the semantic misalignment between visual and textual representations that arises from using domain invariant text embeddings without adapting them to dataset and image specific contexts. This lack of domain awareness, coupled with limited annotations, weakens the model semantic understanding by preventing effective vision language alignment. As a result, the model struggles with contextual reasoning, shows weak intra class discrimination, and confuses similar classes. To address these challenges, we propose Hierarchical Vision Language transFormer (HVLFormer), which achieves domain aware and domain robust alignment between visual and textual representations within a mask transformer architecture. Firstly, we transform text embeddings from pretrained VLMs into textual object queries, enabling the generation of multi scale, dataset aware queries that capture class semantics from coarse to fine granularity and enhance contextual reasoning. Next, we refine these queries by injecting image specific visual context to align textual semantics with local scene structures and enhance class discrimination. Finally, to achieve domain robustness, we introduce cross view and modal consistency regularization, which enforces prediction consistency within mask-transformer architecture across augmented views. Moreover, it ensures stable vision language alignment during decoding. With less than 1% training data, HVLFormer outperforms state of the art methods on Pascal VOC, COCO, ADE20K, and Cityscapes. Our code and results will be available on GitHub.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual symbolic mechanisms: Emergent symbol processing in vision language models</title>
<link>https://arxiv.org/abs/2506.15871</link>
<guid>https://arxiv.org/abs/2506.15871</guid>
<content:encoded><![CDATA[

arXiv:2506.15871v2 Announce Type: replace 
Abstract: To accurately process a visual scene, observers must bind features together to represent individual objects. This capacity is necessary, for instance, to distinguish an image containing a red square and a blue circle from an image containing a blue square and a red circle. Recent work has found that language models solve this 'binding problem' via a set of symbol-like, content-independent indices, but it is unclear whether similar mechanisms are employed by Vision Language Models (VLMs). This question is especially relevant, given the persistent failures of VLMs on tasks that require binding. Here, we identify a previously unknown set of emergent symbolic mechanisms that support binding specifically in VLMs, via a content-independent, spatial indexing scheme. Moreover, we find that binding errors, when they occur, can be traced directly to failures in these mechanisms. Taken together, these results shed light on the mechanisms that support symbol-like processing in VLMs, and suggest possible avenues for reducing the number of binding failures exhibited by these models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Camera Calibration via Circular Patterns: A Comprehensive Framework with Detection Uncertainty and Unbiased Projection Model</title>
<link>https://arxiv.org/abs/2506.16842</link>
<guid>https://arxiv.org/abs/2506.16842</guid>
<content:encoded><![CDATA[

arXiv:2506.16842v2 Announce Type: replace 
Abstract: Camera calibration using planar targets has been widely favored, and two types of control points have been mainly considered as measurements: the corners of the checkerboard and the centroid of circles. Since a centroid is derived from numerous pixels, the circular pattern provides more precise measurements than the checkerboard. However, the existing projection model of circle centroids is biased under lens distortion, resulting in low performance. To surmount this limitation, we propose an unbiased projection model of the circular pattern and demonstrate its superior accuracy compared to the checkerboard. Complementing this, we introduce uncertainty into circular patterns to enhance calibration robustness and completeness. Defining centroid uncertainty improves the performance of calibration components, including pattern detection, optimization, and evaluation metrics. We also provide guidelines for performing good camera calibration based on the evaluation metric. The core concept of this approach is to model the boundary points of a two-dimensional shape as a Markov random field, considering its connectivity. The shape distribution is propagated to the centroid uncertainty through an appropriate shape representation based on the Green theorem. Consequently, the resulting framework achieves marked gains in calibration accuracy and robustness. The complete source code and demonstration video are available at https://github.com/chaehyeonsong/discocal.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MR-COSMO: Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation</title>
<link>https://arxiv.org/abs/2506.20991</link>
<guid>https://arxiv.org/abs/2506.20991</guid>
<content:encoded><![CDATA[

arXiv:2506.20991v3 Announce Type: replace 
Abstract: The rapid advancement of vision-language models (VLMs) in 3D domains has accelerated research in text-query-guided point cloud processing, though existing methods underperform in point-level segmentation due to inadequate 3D-text alignment that limits local feature-text context linking. To address this limitation, we propose MR-COSMO, a Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation, establishing explicit alignment between 3D point clouds and text/2D image data through a dedicated direct cross-modal alignment module while implementing a visual-text memory module with specialized feature banks. This direct alignment mechanism enables precise fusion of geometric and semantic features, while the memory module employs specialized banks storing text features, visual features, and their correspondence mappings to dynamically enhance scene-specific representations via attention-based knowledge recall. Comprehensive experiments across 3D instruction, reference, and semantic segmentation benchmarks confirm state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FACM: Flow-Anchored Consistency Models</title>
<link>https://arxiv.org/abs/2507.03738</link>
<guid>https://arxiv.org/abs/2507.03738</guid>
<content:encoded><![CDATA[

arXiv:2507.03738v2 Announce Type: replace 
Abstract: Continuous-time Consistency Models (CMs) promise efficient few-step generation but face significant challenges with training instability. We argue this instability stems from a fundamental conflict: Training the network exclusively on a shortcut objective leads to the catastrophic forgetting of the instantaneous velocity field that defines the flow. Our solution is to explicitly anchor the model in the underlying flow, ensuring high trajectory fidelity during training. We introduce the Flow-Anchored Consistency Model (FACM), where a Flow Matching (FM) task serves as a dynamic anchor for the primary CM shortcut objective. Key to this Flow-Anchoring approach is a novel expanded time interval strategy that unifies optimization for a single model while decoupling the two tasks to ensure stable, architecturally-agnostic training. By distilling a pre-trained LightningDiT model, our method achieves a state-of-the-art FID of 1.32 with two steps (NFE=2) and 1.70 with just one step (NFE=1) on ImageNet 256x256. To address the challenge of scalability, we develop a memory-efficient Chain-JVP that resolves key incompatibilities with FSDP. This method allows us to scale FACM training on a 14B parameter model (Wan 2.2), accelerating its Text-to-Image inference from 2x40 to 2-8 steps. Our code and pretrained models: https://github.com/ali-vilab/FACM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAST-Phys: Contactless Affective States Through Physiological signals Database</title>
<link>https://arxiv.org/abs/2507.06080</link>
<guid>https://arxiv.org/abs/2507.06080</guid>
<content:encoded><![CDATA[

arXiv:2507.06080v2 Announce Type: replace 
Abstract: In recent years, affective computing and its applications have become a fast-growing research topic. Despite significant advancements, the lack of affective multi-modal datasets remains a major bottleneck in developing accurate emotion recognition systems. Furthermore, the use of contact-based devices during emotion elicitation often unintentionally influences the emotional experience, reducing or altering the genuine spontaneous emotional response. This limitation highlights the need for methods capable of extracting affective cues from multiple modalities without physical contact, such as remote physiological emotion recognition. To address this, we present the Contactless Affective States Through Physiological Signals Database (CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal remote physiological emotion recognition using facial and physiological cues. The dataset includes diverse physiological signals, such as photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate (RR), alongside high-resolution uncompressed facial video recordings, enabling the potential for remote signal recovery. Our analysis highlights the crucial role of physiological signals in realistic scenarios where facial expressions alone may not provide sufficient emotional information. Furthermore, we demonstrate the potential of remote multi-modal emotion recognition by evaluating the impact of individual and fused modalities, showcasing its effectiveness in advancing contactless emotion recognition technologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnthroTAP: Learning Point Tracking with Real-World Motion</title>
<link>https://arxiv.org/abs/2507.06233</link>
<guid>https://arxiv.org/abs/2507.06233</guid>
<content:encoded><![CDATA[

arXiv:2507.06233v2 Announce Type: replace 
Abstract: Point tracking models often struggle to generalize to real-world videos because large-scale training data is predominantly synthetic--the only source currently feasible to produce at scale. Collecting real-world annotations, however, is prohibitively expensive, as it requires tracking hundreds of points across frames. We introduce AnthroTAP, an automated pipeline that generates large-scale pseudo-labeled point tracking data from real human motion videos. Leveraging the structured complexity of human movement-non-rigid deformations, articulated motion, and frequent occlusions-AnthroTAP fits Skinned Multi-Person Linear (SMPL) models to detected humans, projects mesh vertices onto image planes, resolves occlusions via ray-casting, and filters unreliable tracks using optical flow consistency. A model trained on the AnthroTAP dataset achieves state-of-the-art performance on TAP-Vid, a challenging general-domain benchmark for tracking any point on diverse rigid and non-rigid objects (e.g., humans, animals, robots, and vehicles). Our approach outperforms recent self-supervised teacher-student models trained on vastly larger real datasets, while requiring only one day of training on 4 GPUs. AnthroTAP shows that structured human motion offers a scalable and effective source of real-world supervision for point tracking. Code and datasets will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Linear Separability Ceiling: Aligning Representations in VLMs</title>
<link>https://arxiv.org/abs/2507.07574</link>
<guid>https://arxiv.org/abs/2507.07574</guid>
<content:encoded><![CDATA[

arXiv:2507.07574v3 Announce Type: replace 
Abstract: A challenge in advancing Visual-Language Models (VLMs) is determining whether their failures on abstract reasoning tasks, such as Bongard problems, stem from flawed perception or faulty top-down reasoning. To disentangle these factors, we introduce a diagnostic framework centered on the Linear Separability Ceiling (LSC), the performance achievable by a linear classifier on a VLM's raw visual embeddings. Applying this framework to state-of-the-art VLMs, we uncover a pervasive ``alignment gap'', where most models fail to generatively outperform the linear separability of their representations. We find that the few models surpassing this ceiling do so via two mechanisms: by further refining visual representations into a more linearly separable format or by executing non-linear decision logic. We demonstrate that this bottleneck is not a fundamental limitation but a solvable visual alignment issue. Our method augments standard next-token prediction with a contrastive objective to restructure the visual manifold into a more one-dimensionally linear geometry, improving image-to-image comparison and enabling models to significantly surpass the LSC on abstract binary classification tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</title>
<link>https://arxiv.org/abs/2508.08783</link>
<guid>https://arxiv.org/abs/2508.08783</guid>
<content:encoded><![CDATA[

arXiv:2508.08783v2 Announce Type: replace 
Abstract: Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models (LLMs) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint decoder is designed to progressively refine pose predictions, improving robustness to occlusion and annotation sparsity. Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</title>
<link>https://arxiv.org/abs/2508.10576</link>
<guid>https://arxiv.org/abs/2508.10576</guid>
<content:encoded><![CDATA[

arXiv:2508.10576v4 Announce Type: replace 
Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks.Furthermore, grounded in the observation that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, we posit that reasoning ability serves as the key to unlocking it. We devise a multi-stage, modality-progressive reinforcement learning approach, resulting in HumanSense-Omni-Reasoning, which substantially enhances performance on higher-level understanding and interactive tasks. Additionally, we observe that successful reasoning processes appear to exhibit consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner.Project page: \textcolor{brightpink}{https://digital-avatar.github.io/ai/HumanSense/}
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2508.17364</link>
<guid>https://arxiv.org/abs/2508.17364</guid>
<content:encoded><![CDATA[

arXiv:2508.17364v3 Announce Type: replace 
Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Color Bind: Exploring Color Perception in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2508.19791</link>
<guid>https://arxiv.org/abs/2508.19791</guid>
<content:encoded><![CDATA[

arXiv:2508.19791v3 Announce Type: replace 
Abstract: Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps</title>
<link>https://arxiv.org/abs/2509.11574</link>
<guid>https://arxiv.org/abs/2509.11574</guid>
<content:encoded><![CDATA[

arXiv:2509.11574v2 Announce Type: replace 
Abstract: While recent Gaussian-based SLAM methods achieve photorealistic reconstruction from RGB-D data, their computational performance remains a critical bottleneck. State-of-the-art techniques operate at less than 20 fps, significantly lagging behind geometry-based approaches like KinectFusion (hundreds of fps). This limitation stems from the heavy computational burden: modeling scenes requires numerous Gaussians and complex iterative optimization to fit RGB-D data; insufficient Gaussian counts or optimization iterations cause severe quality degradation. To address this, we propose a Gaussian-SDF hybrid representation, combining a colorized signed distance field (SDF) for smooth geometry and appearance with 3D Gaussians to capture underrepresented details. The SDF is efficiently constructed via RGB-D fusion (as in geometry-based methods), while Gaussians undergo iterative optimization. Our representation enables significant Gaussian reduction (50% fewer) by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization (75% fewer iterations) through targeted appearance refinement. Building upon this representation, we develop GPS-SLAM (Gaussian-plus-SDF SLAM), a real-time 3D reconstruction system achieving over 150 fps on real-world Azure Kinect sequences, faster by an order-of-magnitude than state-of-the-art techniques while maintaining comparable reconstruction quality. The source code and data are available at https://gapszju.github.io/GPS-SLAM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing</title>
<link>https://arxiv.org/abs/2509.23103</link>
<guid>https://arxiv.org/abs/2509.23103</guid>
<content:encoded><![CDATA[

arXiv:2509.23103v2 Announce Type: replace 
Abstract: Reducing the cost of multiplications is critical for efficient deep neural network deployment, especially in energy-constrained edge devices. In this work, we introduce HTMA-Net, a novel framework that integrates the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to reduce arithmetic complexity while maintaining accuracy. Unlike prior methods that only target multiplications in convolutional layers or focus solely on in-memory acceleration, HTMA-Net selectively replaces intermediate convolutions with Hybrid Hadamard-based transform layers whose internal convolutions are implemented via multiplication-avoiding in-memory operations. We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet, and provide a detailed comparison against regular, MF-only, and HT-only variants. Results show that HTMA-Net eliminates up to 52\% of multiplications compared to baseline ResNet-18, ResNet-20, and ResNet-32 models, while achieving comparable accuracy in evaluation and significantly reducing computational complexity and the number of parameters. Our results demonstrate that combining structured Hadamard transform layers with SRAM-based in-memory computing multiplication-avoiding operators is a promising path towards efficient deep learning architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection</title>
<link>https://arxiv.org/abs/2509.23316</link>
<guid>https://arxiv.org/abs/2509.23316</guid>
<content:encoded><![CDATA[

arXiv:2509.23316v2 Announce Type: replace 
Abstract: Object detection has advanced significantly in the closed-set setting, but real-world deployment remains limited by two challenges: poor generalization to unseen categories and insufficient robustness under adverse conditions. Prior research has explored these issues separately: visible-infrared detection improves robustness but lacks generalization, while open-world detection leverages vision-language alignment strategy for category diversity but struggles under extreme environments. This trade-off leaves robustness and diversity difficult to achieve simultaneously. To mitigate these issues, we propose \textbf{C3-OWD}, a curriculum cross-modal contrastive learning framework that unifies both strengths. Stage~1 enhances robustness by pretraining with RGBT data, while Stage~2 improves generalization via vision-language alignment. To prevent catastrophic forgetting between two stages, we introduce an Exponential Moving Average (EMA) mechanism that theoretically guarantees preservation of pre-stage performance with bounded parameter lag and function consistency. Experiments on FLIR, OV-COCO, and OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$ AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\text{Novel}}$ on OV-COCO, and $35.7$ mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness and diversity evaluations. Code available at: https://github.com/justin-herry/C3-OWD.git.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training</title>
<link>https://arxiv.org/abs/2509.23661</link>
<guid>https://arxiv.org/abs/2509.23661</guid>
<content:encoded><![CDATA[

arXiv:2509.23661v3 Announce Type: replace 
Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 22M instruction dataset LLaVA-OneVision-1.5-Instruct. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision-1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. (4) RL-based Post-training: We unlock the model's latent potential through a lightweight RL stage, effectively eliciting robust chain-of-thought reasoning to significantly boost performance on complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease</title>
<link>https://arxiv.org/abs/2509.23719</link>
<guid>https://arxiv.org/abs/2509.23719</guid>
<content:encoded><![CDATA[

arXiv:2509.23719v3 Announce Type: replace 
Abstract: Parkinson's disease (PD) is a common neurodegenerative disorder that severely diminishes patients' quality of life. Its global prevalence has increased markedly in recent decades. Current diagnostic workflows are complex and heavily reliant on neurologists' expertise, often resulting in delays in early detection and missed opportunities for timely intervention. To address these issues, we propose an end-to-end automated diagnostic method for PD, termed PD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly from raw MRI scans. This framework first introduces an MRI Pre-processing Module (MRI-Processor) to mitigate inter-subject and inter-scanner variability by flexibly integrating established medical imaging preprocessing tools. It then incorporates two forms of clinical prior knowledge: (1) Brain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions strongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior), which reflects the accelerated aging typically observed in PD-associated regions. Building on these priors, we design two dedicated modules: the Relevance-Prior Guided Feature Aggregation Module (Aggregator), which guides the model to focus on PD-associated regions at the inter-subject level, and the Age-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps as auxiliary constraints at the intra-subject level to enhance diagnostic accuracy and clinical interpretability. Furthermore, we collected external test data from our collaborating hospital. Experimental results show that PD-Diag-Net achieves 86\% accuracy on external tests and over 96% accuracy in early-stage diagnosis, outperforming existing advanced methods by more than 20%.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing</title>
<link>https://arxiv.org/abs/2509.23927</link>
<guid>https://arxiv.org/abs/2509.23927</guid>
<content:encoded><![CDATA[

arXiv:2509.23927v3 Announce Type: replace 
Abstract: Cross-modal artificial intelligence, represented by visual language models, has achieved significant success in general image understanding. However, a fundamental cognitive inconsistency exists between general visual representation and remote sensing image interpretation: remote sensing images couple topography, terrain, and spatial structure, thereby inherently requiring models to possess deep geoscientific understanding. This cognitive difference is further amplified in synthetic aperture radar (SAR) imagery: while SAR possesses irreplaceable all-weather, all-day observation capabilities, it is constrained by coherent imaging mechanisms, exhibiting significant modal heterogeneity with general images. To address this inconsistency, we propose FUSAR-KLIP, the first knowledge-guided general multimodal foundational model for SAR, along with reusable data and evaluation baselines. Specifically: (1) FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection attributes) was constructed, covering multiple satellite platforms, 120,000 images, and 135 cities; (2) Aligned structured text was generated through hierarchical cognitive thought chains, accurately encoding more than 1 million multidimensional semantic information from geomorphological environment and regional attributes to spatial relationships; (3) A self-consistent iterative optimization mechanism was designed to guide cross-modal learning with this knowledge information consistent with human cognition and physical laws in a self-supervised closed loop consisting of contrast, matching, and reconstruction; (4) A unified evaluation benchmark was established in 11 typical downstream tasks in the two major categories of vision and language, and compared with 15 mainstream foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Splitting: Self-supervised learning from incomplete data</title>
<link>https://arxiv.org/abs/2510.00929</link>
<guid>https://arxiv.org/abs/2510.00929</guid>
<content:encoded><![CDATA[

arXiv:2510.00929v4 Announce Type: replace 
Abstract: Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, sparse-view computed tomography, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models. The code is available at https://github.com/vsechaud/Equivariant-Splitting
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Representation Learning for Customized Tasks</title>
<link>https://arxiv.org/abs/2510.04564</link>
<guid>https://arxiv.org/abs/2510.04564</guid>
<content:encoded><![CDATA[

arXiv:2510.04564v2 Announce Type: replace 
Abstract: Conventional representation learning methods learn a universal representation that primarily captures dominant semantics, which may not always align with customized downstream tasks. For instance, in animal habitat analysis, researchers prioritize scene-related features, whereas universal embeddings emphasize categorical semantics, leading to suboptimal results. As a solution, existing approaches resort to supervised fine-tuning, which however incurs high computational and annotation costs. In this paper, we propose Conditional Representation Learning (CRL), aiming to extract representations tailored to arbitrary user-specified criteria. Specifically, we reveal that the semantics of a space are determined by its basis, thereby enabling a set of descriptive words to approximate the basis for a customized feature space. Building upon this insight, given a user-specified criterion, CRL first employs a large language model (LLM) to generate descriptive texts to construct the semantic basis, then projects the image representation into this conditional feature space leveraging a vision-language model (VLM). The conditional representation better captures semantics for the specific criterion, which could be utilized for multiple customized tasks. Extensive experiments on classification and retrieval tasks demonstrate the superiority and generality of the proposed CRL. The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control</title>
<link>https://arxiv.org/abs/2510.09561</link>
<guid>https://arxiv.org/abs/2510.09561</guid>
<content:encoded><![CDATA[

arXiv:2510.09561v2 Announce Type: replace 
Abstract: Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering</title>
<link>https://arxiv.org/abs/2510.09995</link>
<guid>https://arxiv.org/abs/2510.09995</guid>
<content:encoded><![CDATA[

arXiv:2510.09995v2 Announce Type: replace 
Abstract: Lens flare occurs when shooting towards strong light sources, significantly degrading the visual quality of images. Due to the difficulty in capturing flare-corrupted and flare-free image pairs in the real world, existing datasets are typically synthesized in 2D by overlaying artificial flare templates onto background images. However, the lack of flare diversity in templates and the neglect of physical principles in the synthesis process hinder models trained on these datasets from generalizing well to real-world scenarios. To address these challenges, we propose a new physics-informed method for flare data generation, which consists of three stages: parameterized template creation, the laws of illumination-aware 2D synthesis, and physical engine-based 3D rendering, which finally gives us a mixed flare dataset that incorporates both 2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D scenes. Furthermore, we design a masking approach to obtain real-world flare-free images from their corrupted counterparts to measure the performance of the model on real-world images. Extensive experiments demonstrate the effectiveness of our method and dataset.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</title>
<link>https://arxiv.org/abs/2510.12793</link>
<guid>https://arxiv.org/abs/2510.12793</guid>
<content:encoded><![CDATA[

arXiv:2510.12793v2 Announce Type: replace 
Abstract: Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Hallucinations in Diffusion Models</title>
<link>https://arxiv.org/abs/2510.13080</link>
<guid>https://arxiv.org/abs/2510.13080</guid>
<content:encoded><![CDATA[

arXiv:2510.13080v2 Announce Type: replace 
Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress in generative tasks, such as image and video synthesis. However, they still often produce hallucinated samples (hallucinations) that conflict with real-world knowledge, such as generating an implausible duplicate cup floating beside another cup. Despite their prevalence, the lack of feasible methodologies for systematically quantifying such hallucinations hinders progress in addressing this challenge and obscures potential pathways for designing next-generation generative models under factual constraints. In this work, we bridge this gap by focusing on a specific form of hallucination, which we term counting hallucination, referring to the generation of an incorrect number of instances or structured objects, such as a hand image with six fingers, despite such patterns being absent from the training data. To this end, we construct a dataset suite CountHalluSet, with well-defined counting criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets, we develop a standardized evaluation protocol for quantifying counting hallucinations, and systematically examine how different sampling conditions in DPMs, including solver type, ODE solver order, sampling steps, and initial noise, affect counting hallucination levels. Furthermore, we analyze their correlation with common evaluation metrics such as FID, revealing that this widely used image quality metric fails to capture counting hallucinations consistently. This work aims to take the first step toward systematically quantifying hallucinations in diffusion models and offer new insights into the investigation of hallucination phenomena in image generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</title>
<link>https://arxiv.org/abs/2510.14945</link>
<guid>https://arxiv.org/abs/2510.14945</guid>
<content:encoded><![CDATA[

arXiv:2510.14945v2 Announce Type: replace 
Abstract: We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.20268</link>
<guid>https://arxiv.org/abs/2510.20268</guid>
<content:encoded><![CDATA[

arXiv:2510.20268v2 Announce Type: replace 
Abstract: Video anomaly detection (VAD) is a challenging task that detects anomalous frames in continuous surveillance videos. Most previous work utilizes the spatio-temporal correlation of visual features to distinguish whether there are abnormalities in video snippets. Recently, some works attempt to introduce multi-modal information, like text feature, to enhance the results of video anomaly detection. However, these works merely incorporate text features into video snippets in a coarse manner, overlooking the significant amount of redundant information that may exist within the video snippets. Therefore, we propose to leverage the diversity among multi-modal information to further refine the extracted features, reducing the redundancy in visual features, and we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD). Specifically, we generate more grained multi-modal feature based on the video snippet, which summarizes the main content, and text features based on the captions of original video will be introduced to further enhance the visual features of highlighted portions. Experiments show that the proposed GMFVAD achieves state-of-the-art performance on four mainly datasets. Ablation experiments also validate that the improvement of GMFVAD is due to the reduction of redundant information.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Physically Executable 3D Gaussian for Embodied Navigation</title>
<link>https://arxiv.org/abs/2510.21307</link>
<guid>https://arxiv.org/abs/2510.21307</guid>
<content:encoded><![CDATA[

arXiv:2510.21307v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Level Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2510.25387</link>
<guid>https://arxiv.org/abs/2510.25387</guid>
<content:encoded><![CDATA[

arXiv:2510.25387v2 Announce Type: replace 
Abstract: The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegionRAG: Region-level Retrieval-Augmented Generation for Visual Document Understanding</title>
<link>https://arxiv.org/abs/2510.27261</link>
<guid>https://arxiv.org/abs/2510.27261</guid>
<content:encoded><![CDATA[

arXiv:2510.27261v2 Announce Type: replace 
Abstract: Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method for empowering LLMs by leveraging candidate visual documents. However, current methods consider the entire document as the basic retrieval unit, introducing substantial irrelevant visual content in two ways: 1) Relevant documents often contain large regions unrelated to the query, diluting the focus on salient information; 2) Retrieving multiple documents to increase recall further introduces redundant and irrelevant documents. These redundant contexts distract the model's attention and further degrade the performance. To address this challenge, we propose RegionRAG, a novel framework that shifts the retrieval paradigm from the document level to the region level. During training, we design a hybrid supervision strategy from both labeled data and unlabeled data to pinpoint relevant patches. During inference, we propose a dynamic pipeline that intelligently groups salient patches into complete semantic regions. By delegating the task of identifying relevant regions to the retriever, RegionRAG enables the generator to focus solely on concise, query-relevant visual content, improving both efficiency and accuracy. Experiments on six benchmarks demonstrate that RegionRAG achieves state-of-the-art performance. It improves retrieval accuracy by 10.02% in R@1 on average, and boosts question answering accuracy by 3.56% while using only 71.42% visual tokens compared with prior methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation</title>
<link>https://arxiv.org/abs/2511.00511</link>
<guid>https://arxiv.org/abs/2511.00511</guid>
<content:encoded><![CDATA[

arXiv:2511.00511v4 Announce Type: replace 
Abstract: Significant progress has been achieved in high-fidelity video synthesis, yet current paradigms often fall short in effectively integrating identity information from multiple subjects. This leads to semantic conflicts and suboptimal performance in preserving identities and interactions, limiting controllability and applicability. To tackle this issue, we introduce ID-Crafter, a framework for multi-subject video generation that achieves superior identity preservation and semantic coherence. ID-Crafter integrates three key components: (i) a hierarchical identity-preserving attention mechanism that progressively aggregates features at intra-subject, inter-subject, and cross-modal levels; (ii) a semantic understanding module powered by a pretrained Vision-Language Model (VLM) to provide fine-grained guidance and capture complex inter-subject relationships; and (iii) an online reinforcement learning phase to further refine the model for critical concepts. Furthermore, we construct a new dataset to facilitate robust training and evaluation. Extensive experiments demonstrate that ID-Crafter establishes new state-of-the-art performance on multi-subject video generation benchmarks, excelling in identity preservation, temporal consistency, and overall video quality. Project page: https://angericky.github.io/ID-Crafter
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
<link>https://arxiv.org/abs/2511.03132</link>
<guid>https://arxiv.org/abs/2511.03132</guid>
<content:encoded><![CDATA[

arXiv:2511.03132v3 Announce Type: replace 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffRegCD: Integrated Registration and Change Detection with Diffusion Features</title>
<link>https://arxiv.org/abs/2511.07935</link>
<guid>https://arxiv.org/abs/2511.07935</guid>
<content:encoded><![CDATA[

arXiv:2511.07935v3 Announce Type: replace 
Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images</title>
<link>https://arxiv.org/abs/2511.08987</link>
<guid>https://arxiv.org/abs/2511.08987</guid>
<content:encoded><![CDATA[

arXiv:2511.08987v3 Announce Type: replace 
Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $\mu m$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS</title>
<link>https://arxiv.org/abs/2511.09397</link>
<guid>https://arxiv.org/abs/2511.09397</guid>
<content:encoded><![CDATA[

arXiv:2511.09397v2 Announce Type: replace 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PANDA -- Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning</title>
<link>https://arxiv.org/abs/2511.09791</link>
<guid>https://arxiv.org/abs/2511.09791</guid>
<content:encoded><![CDATA[

arXiv:2511.09791v3 Announce Type: replace 
Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</title>
<link>https://arxiv.org/abs/2511.12919</link>
<guid>https://arxiv.org/abs/2511.12919</guid>
<content:encoded><![CDATA[

arXiv:2511.12919v3 Announce Type: replace 
Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation</title>
<link>https://arxiv.org/abs/2511.13102</link>
<guid>https://arxiv.org/abs/2511.13102</guid>
<content:encoded><![CDATA[

arXiv:2511.13102v2 Announce Type: replace 
Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features</title>
<link>https://arxiv.org/abs/2511.13115</link>
<guid>https://arxiv.org/abs/2511.13115</guid>
<content:encoded><![CDATA[

arXiv:2511.13115v2 Announce Type: replace 
Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title>
<link>https://arxiv.org/abs/2511.16020</link>
<guid>https://arxiv.org/abs/2511.16020</guid>
<content:encoded><![CDATA[

arXiv:2511.16020v2 Announce Type: replace 
Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle</title>
<link>https://arxiv.org/abs/2511.17171</link>
<guid>https://arxiv.org/abs/2511.17171</guid>
<content:encoded><![CDATA[

arXiv:2511.17171v2 Announce Type: replace 
Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</title>
<link>https://arxiv.org/abs/2511.17362</link>
<guid>https://arxiv.org/abs/2511.17362</guid>
<content:encoded><![CDATA[

arXiv:2511.17362v2 Announce Type: replace 
Abstract: Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling</title>
<link>https://arxiv.org/abs/2511.18858</link>
<guid>https://arxiv.org/abs/2511.18858</guid>
<content:encoded><![CDATA[

arXiv:2511.18858v2 Announce Type: replace 
Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance. Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10. Codes are available at https://github.com/2018cx/RLDD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation</title>
<link>https://arxiv.org/abs/2511.19004</link>
<guid>https://arxiv.org/abs/2511.19004</guid>
<content:encoded><![CDATA[

arXiv:2511.19004v2 Announce Type: replace 
Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[

arXiv:2511.20853v2 Announce Type: replace 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video</title>
<link>https://arxiv.org/abs/2511.21946</link>
<guid>https://arxiv.org/abs/2511.21946</guid>
<content:encoded><![CDATA[

arXiv:2511.21946v2 Announce Type: replace 
Abstract: Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods. Project page: https://finlay-hudson.github.io/tapvid360
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</title>
<link>https://arxiv.org/abs/2512.02648</link>
<guid>https://arxiv.org/abs/2512.02648</guid>
<content:encoded><![CDATA[

arXiv:2512.02648v2 Announce Type: replace 
Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.02660</link>
<guid>https://arxiv.org/abs/2512.02660</guid>
<content:encoded><![CDATA[

arXiv:2512.02660v2 Announce Type: replace 
Abstract: Late-interaction multimodal retrieval models like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on area efficiency. We evaluate on BBox-DocVQA with ground-truth bounding boxes. For within-page localization (given correct page retrieval), ColQwen3-4B with percentile-50 thresholding achieves 59.7% hit rate at IoU@0.5 (84.4% at IoU@0.25, 35.8% at IoU@0.7), with mean IoU of 0.569, compared to ~6.7% for random region selection. Our approach reduces context tokens by 28.8% compared to returning all OCR regions and by 52.3% compared to full-page image tokens. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation at https://github.com/athrael-soju/Snappy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</title>
<link>https://arxiv.org/abs/2512.02700</link>
<guid>https://arxiv.org/abs/2512.02700</guid>
<content:encoded><![CDATA[

arXiv:2512.02700v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval</title>
<link>https://arxiv.org/abs/2512.02792</link>
<guid>https://arxiv.org/abs/2512.02792</guid>
<content:encoded><![CDATA[

arXiv:2512.02792v2 Announce Type: replace 
Abstract: Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difference Decomposition Networks for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2512.03470</link>
<guid>https://arxiv.org/abs/2512.03470</guid>
<content:encoded><![CDATA[

arXiv:2512.03470v2 Announce Type: replace 
Abstract: Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
<link>https://arxiv.org/abs/2512.03979</link>
<guid>https://arxiv.org/abs/2512.03979</guid>
<content:encoded><![CDATA[

arXiv:2512.03979v2 Announce Type: replace 
Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/Blur-Diffusion-Model/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Group Actions In Disentangled Latent Image Representations</title>
<link>https://arxiv.org/abs/2512.04015</link>
<guid>https://arxiv.org/abs/2512.04015</guid>
<content:encoded><![CDATA[

arXiv:2512.04015v2 Announce Type: replace 
Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title>
<link>https://arxiv.org/abs/2512.04810</link>
<guid>https://arxiv.org/abs/2512.04810</guid>
<content:encoded><![CDATA[

arXiv:2512.04810v5 Announce Type: replace 
Abstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-X: Generative 4D Video Rendering with Camera and Illumination Control</title>
<link>https://arxiv.org/abs/2512.05115</link>
<guid>https://arxiv.org/abs/2512.05115</guid>
<content:encoded><![CDATA[

arXiv:2512.05115v2 Announce Type: replace 
Abstract: Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEAR: Dataset for Evaluating the Aesthetics of Rendering</title>
<link>https://arxiv.org/abs/2512.05209</link>
<guid>https://arxiv.org/abs/2512.05209</guid>
<content:encoded><![CDATA[

arXiv:2512.05209v2 Announce Type: replace 
Abstract: Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tau Anomaly Detection in PET Imaging via Bilateral-Guided Deterministic Diffusion Model</title>
<link>https://arxiv.org/abs/2405.13199</link>
<guid>https://arxiv.org/abs/2405.13199</guid>
<content:encoded><![CDATA[

arXiv:2405.13199v2 Announce Type: replace-cross 
Abstract: The emergence of tau PET imaging over the last decade has enabled Alzheimer's disease (AD) researchers to examine tau pathology in vivo and more effectively characterize the disease trajectories of AD. Current tau PET analysis methods, however, typically perform inferences on large cortical ROIs and are limited in the detection of localized tau pathology that varies across subjects. In this work, we propose a novel bilateral-guided deterministic diffusion sampling method to perform anomaly detection from tau PET imaging data. By including individualized brain structure and cognitively normal (CN) template conditions, our model computes a voxel-level anomaly map based on the deterministically sampled pseudo-healthy reconstruction. We train our model on ADNI CN subjects (n=380) and evaluate anomaly localization performance on the left MCI/AD subjects (n=154) and the preclinical subjects of the A4 clinical trial (n=447). We further train a CNN classifier on the derived 3D anomaly maps from ADNI, including CN and MCI/AD, to classify subjects into two groups and test classification performance on A4. We demonstrate that our method outperforms baselines in anomaly localization. Additionally, we show that our method can successfully group preclinical subjects with significantly different cognitive functions, highlighting the potential of our approach for application in preclinical screening tests. The code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIC: Circular Image Compression</title>
<link>https://arxiv.org/abs/2407.15870</link>
<guid>https://arxiv.org/abs/2407.15870</guid>
<content:encoded><![CDATA[

arXiv:2407.15870v3 Announce Type: replace-cross 
Abstract: Learned image compression (LIC) is currently the cutting-edge method. However, the inherent difference between testing and training images of LIC results in performance degradation to some extent. Especially for out-of-sample, out-of-distribution, or out-of-domain testing images, the performance of LIC dramatically degraded. Classical LIC is a serial image compression (SIC) approach that utilizes an open-loop architecture with serial encoding and decoding units. Nevertheless, according to the theory of automatic control, a closed-loop architecture holds the potential to improve the dynamic and static performance of LIC. Therefore, a circular image compression (CIC) approach with closed-loop encoding and decoding elements is proposed to minimize the gap between testing and training images and upgrade the capability of LIC. The proposed CIC establishes a nonlinear loop equation and proves that steady-state error between reconstructed and original images is close to zero by Taylor series expansion. The proposed CIC method possesses the property of Post-Training and plug-and-play which can be built on any existing advanced SIC methods. Experimental results on five public image compression datasets demonstrate that the proposed CIC outperforms five competing state-of-the-art open-source SIC algorithms in reconstruction capacity. Experimental results further show that the proposed method is suitable for out-of-sample testing images with dark backgrounds, sharp edges, high contrast, grid shapes, or complex patterns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Simultaneous Multislice MRI Reconstruction Using Slice-Wise Learned Generative Diffusion Priors</title>
<link>https://arxiv.org/abs/2407.21600</link>
<guid>https://arxiv.org/abs/2407.21600</guid>
<content:encoded><![CDATA[

arXiv:2407.21600v3 Announce Type: replace-cross 
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAISI: Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2409.11169</link>
<guid>https://arxiv.org/abs/2409.11169</guid>
<content:encoded><![CDATA[

arXiv:2409.11169v3 Announce Type: replace-cross 
Abstract: Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WALINET: A water and lipid identification convolutional Neural Network for nuisance signal removal in 1H MR Spectroscopic Imaging</title>
<link>https://arxiv.org/abs/2410.00746</link>
<guid>https://arxiv.org/abs/2410.00746</guid>
<content:encoded><![CDATA[

arXiv:2410.00746v2 Announce Type: replace-cross 
Abstract: Purpose. Proton Magnetic Resonance Spectroscopic Imaging (1H-MRSI) provides non-invasive spectral-spatial mapping of metabolism. However, long-standing problems in whole-brain 1H-MRSI are spectral overlap of metabolite peaks with large lipid signal from scalp, and overwhelming water signal that distorts spectra. Fast and effective methods are needed for high-resolution 1H-MRSI to accurately remove lipid and water signals while preserving the metabolite signal. The potential of supervised neural networks for this task remains unexplored, despite their success for other MRSI processing.
  Methods. We introduce a deep-learning method based on a modified Y-NET network for water and lipid removal in whole-brain 1H-MRSI. The WALINET (WAter and LIpid neural NETwork) was compared to conventional methods such as the state-of-the-art lipid L2 regularization and Hankel-Lanczos singular value decomposition (HLSVD) water suppression. Methods were evaluated on simulated and in-vivo whole-brain MRSI using NMRSE, SNR, CRLB, and FWHM metrics.
  Results. WALINET is significantly faster and needs 8s for high-resolution whole-brain MRSI, compared to 42 minutes for conventional HLSVD+L2. Quantitative analysis shows WALINET has better performance than HLSVD+L2: 1) more lipid removal with 41% lower NRMSE, 2) better metabolite signal preservation with 71% lower NRMSE in simulated data, 155% higher SNR and 50% lower CRLB in in-vivo data. Metabolic maps obtained by WALINET in healthy subjects and patients show better gray/white-matter contrast with more visible structural details.
  Conclusions. WALINET has superior performance for nuisance signal removal and metabolite quantification on whole-brain 1H-MRSI compared to conventional state-of-the-art techniques. This represents a new application of deep-learning for MRSI processing, with potential for automated high-throughput workflow.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Quantum Machine Learning for Multispectral Images Segmentation: Case Study</title>
<link>https://arxiv.org/abs/2503.08962</link>
<guid>https://arxiv.org/abs/2503.08962</guid>
<content:encoded><![CDATA[

arXiv:2503.08962v3 Announce Type: replace-cross 
Abstract: The emergence of Big Data changed how we approach information systems engineering. Nowadays, when we can use remote sensing techniques for Big Data acquisition, the issues such data introduce are as important as ever. One of those concerns is the processing of the data. Classical methods often fail to address that problem or are incapable of processing the data in a reasonable time. With that in mind information system engineers are required to investigate different approaches to the data processing. The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to investigate their application to real-life computational problem. This field of study is called quantum (information) systems engineering and usually focuses on technical problems with the contemporary devices. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, less explored side of this medal. Using multispectral image segmentation as a task example, we investigated how difficult it is to run a hybrid quantum-classical model on a real, publicly available quantum device. To quantify how and explain why the performance of our model changed when ran on a real device, we propose new explainability metrics. These metrics introduce new meaning to the explainable quantum machine learning; the explanation of the performance issue comes from the quantum device behavior. We also analyzed the expected money costs of running similar experiment on contemporary quantum devices using standard market prices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reference-Free 3D Reconstruction of Brain Dissection Slabs via Learned Atlas Coordinates</title>
<link>https://arxiv.org/abs/2503.09963</link>
<guid>https://arxiv.org/abs/2503.09963</guid>
<content:encoded><![CDATA[

arXiv:2503.09963v2 Announce Type: replace-cross 
Abstract: Correlation of neuropathology with MRI has the potential to transfer microscopic signatures of pathology to in vivo scans. There is increasing interest in building these correlations from 3D reconstructed stacks of slab photographs, which are routinely taken during dissection at brain banks. These photographs bypass the need for ex vivo MRI, which is not widely accessible. However, existing methods either require a corresponding 3D reference (e.g., an ex vivo MRI scans, or a brain surface acquired with a structured light scanner) or a full stack of brain slabs, which severely limits applicability. Here we propose RefFree, a 3D reconstruction method for dissection photographs that does not require an external reference. RefFree coherently reconstructs a 3D volume for an arbitrary set of slabs (including a single slab) using predicted 3D coordinates in the standard atlas space (MNI) as guidance. To support RefFree's pipeline, we train an atlas coordinate prediction network that estimates the coordinate map from a 2D photograph, using synthetic photographs generated from digitally sliced 3D MRI data with randomized appearance for enhanced generalization. As a by-product, RefFree can propagate information (e.g., anatomical labels) from atlas space to one single photograph even without reconstruction. Experiments on simulated and real data show that, when all slabs are available, RefFree achieves performance comparable to existing classical methods but at substantially higher speed. Moreover, RefFree yields accurate reconstruction and registration for partial stacks or even a single slab. Our code is available at https://github.com/lintian-a/reffree.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[

arXiv:2503.19041v3 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score-Based Turbo Message Passing for Plug-and-Play Compressive Image Recovery</title>
<link>https://arxiv.org/abs/2503.22140</link>
<guid>https://arxiv.org/abs/2503.22140</guid>
<content:encoded><![CDATA[

arXiv:2503.22140v2 Announce Type: replace-cross 
Abstract: Message passing algorithms have been tailored for compressive imaging applications by plugging in different types of off-the-shelf image denoisers. These off-the-shelf denoisers mostly rely on some generic or hand-crafted priors for denoising. Due to their insufficient accuracy in capturing the true image prior, these methods often fail to produce satisfactory results, especially in highly underdetermined scenarios. On the other hand, score-based generative modeling offers a promising way to accurately characterize the sophisticated image distribution. In this paper, by exploiting the close relation between score-based modeling and empirical Bayes-optimal denoising, we devise a message passing framework that integrates a score-based minimum mean squared error (MMSE) denoiser for compressive image recovery. Experiments on the FFHQ dataset demonstrate that our method strikes a significantly better performance-complexity tradeoff than conventional message passing, regularized linear regression, and score-based posterior sampling baselines. Remarkably, our method typically converges in fewer than 20 neural function evaluations (NFEs).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2505.04050</link>
<guid>https://arxiv.org/abs/2505.04050</guid>
<content:encoded><![CDATA[

arXiv:2505.04050v2 Announce Type: replace-cross 
Abstract: 3D terrain models are essential in fields such as video game development and film production. Since surface color often correlates with terrain geometry, capturing this relationship is crucial to achieving realism. However, most existing methods generate either a heightmap or a texture, without sufficiently accounting for the inherent correlation. In this paper, we propose a method that jointly generates terrain heightmaps and textures using a latent diffusion model. First, we train the model in an unsupervised manner to randomly generate paired heightmaps and textures. Then, we perform supervised learning of an external adapter to enable user control via hand-drawn sketches. Experiments show that our approach allows intuitive terrain generation while preserving the correlation between heightmaps and textures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</title>
<link>https://arxiv.org/abs/2505.08919</link>
<guid>https://arxiv.org/abs/2505.08919</guid>
<content:encoded><![CDATA[

arXiv:2505.08919v2 Announce Type: replace-cross 
Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes</title>
<link>https://arxiv.org/abs/2506.01950</link>
<guid>https://arxiv.org/abs/2506.01950</guid>
<content:encoded><![CDATA[

arXiv:2506.01950v4 Announce Type: replace-cross 
Abstract: We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation. Project page: https://eku127.github.io/DualMap/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning</title>
<link>https://arxiv.org/abs/2506.12041</link>
<guid>https://arxiv.org/abs/2506.12041</guid>
<content:encoded><![CDATA[

arXiv:2506.12041v3 Announce Type: replace-cross 
Abstract: We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[

arXiv:2506.23046v3 Announce Type: replace-cross 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[

arXiv:2508.11849v3 Announce Type: replace-cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model</title>
<link>https://arxiv.org/abs/2508.14681</link>
<guid>https://arxiv.org/abs/2508.14681</guid>
<content:encoded><![CDATA[

arXiv:2508.14681v3 Announce Type: replace-cross 
Abstract: Multiplex imaging is revolutionizing pathology by enabling the simultaneous visualization of multiple biomarkers within tissue samples, providing molecular-level insights that traditional hematoxylin and eosin (H&amp;E) staining cannot provide. However, the complexity and cost of multiplex data acquisition have hindered its widespread adoption. Additionally, most existing large repositories of H&amp;E images lack corresponding multiplex images, limiting opportunities for multimodal analysis. To address these challenges, we leverage recent advances in latent diffusion models (LDMs), which excel at modeling complex data distributions by utilizing their powerful priors for fine-tuning to a target domain. In this paper, we introduce a novel framework for virtual multiplex staining that utilizes pretrained LDM parameters to generate multiplex images from H&amp;E images using a conditional diffusion model. Our approach enables marker-by-marker generation by conditioning the diffusion model on each marker, while sharing the same architecture across all markers. To tackle the challenge of varying pixel value distributions across different marker stains and to improve inference speed, we fine-tune the model for single-step sampling, enhancing both color contrast fidelity and inference efficiency through pixel-level loss functions. We validate our framework on two publicly available datasets, notably demonstrating its effectiveness in generating up to 18 different marker types with improved accuracy, a substantial increase over the 2-3 marker types achieved in previous approaches. This validation highlights the potential of our framework, pioneering virtual multiplex staining. Finally, this paper bridges the gap between H&amp;E and multiplex imaging, potentially enabling retrospective studies and large-scale analyses of existing H&amp;E image repositories.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI Reconstruction</title>
<link>https://arxiv.org/abs/2509.21531</link>
<guid>https://arxiv.org/abs/2509.21531</guid>
<content:encoded><![CDATA[

arXiv:2509.21531v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) requires long acquisition times, raising costs, reducing accessibility, and making scans more susceptible to motion artifacts. Diffusion probabilistic models that learn data-driven priors can potentially assist in reducing acquisition time. However, they typically require large training datasets that can be prohibitively expensive to collect. Patch-based diffusion models have shown promise in learning effective data-driven priors over small real-valued datasets, but have not yet demonstrated clinical value in MRI. We extend the Patch-based Diffusion Inverse Solver (PaDIS) to complex-valued, multi-coil MRI reconstruction, and compare it against a state-of-the-art whole-image diffusion baseline (FastMRI-EDM) for 7x undersampled MRI reconstruction on the FastMRI brain dataset. We show that PaDIS-MRI models trained on small datasets of as few as 25 k-space images outperform FastMRI-EDM on image quality metrics (PSNR, SSIM, NRMSE), pixel-level uncertainty, cross-contrast generalization, and robustness to severe k-space undersampling. In a blinded study with three radiologists, PaDIS-MRI reconstructions were chosen as diagnostically superior in 91.7% of cases, compared to baselines (i) FastMRI-EDM and (ii) classical convex reconstruction with wavelet sparsity. These findings highlight the potential of patch-based diffusion priors for high-fidelity MRI reconstruction in data-scarce clinical settings where diagnostic confidence matters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
<link>https://arxiv.org/abs/2510.14974</link>
<guid>https://arxiv.org/abs/2510.14974</guid>
<content:encoded><![CDATA[

arXiv:2510.14974v2 Announce Type: replace-cross 
Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[

arXiv:2511.15407v2 Announce Type: replace-cross 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj</title>
<link>https://arxiv.org/abs/2511.18248</link>
<guid>https://arxiv.org/abs/2511.18248</guid>
<content:encoded><![CDATA[

arXiv:2511.18248v2 Announce Type: replace-cross 
Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Histopathologic Assessment of Hirschsprung Disease Using a Multi-Stage Vision Transformer Framework</title>
<link>https://arxiv.org/abs/2511.20734</link>
<guid>https://arxiv.org/abs/2511.20734</guid>
<content:encoded><![CDATA[

arXiv:2511.20734v2 Announce Type: replace-cross 
Abstract: Hirschsprung Disease is characterized by the absence of ganglion cells in the myenteric plexus. Therefore, the correct identification of ganglion cells is crucial for diagnosing Hirschsprung disease. We introduce a three-stage analysis framework that mimics the pathologist's diagnostic approach. The framework, based on a Vision Transformer model (ViT-B/16), sequentially segments the muscularis propria, segments the myenteric plexus, and detects ganglion cells within anatomically valid regions. 30 whole-slide images of colon tissue were used, each containing manual annotations of muscularis, plexus, and ganglion cells. A 5-fold cross-validation scheme was applied to each stage, along with resolution-specific tiling strategies and tailored postprocessing to ensure anatomical consistency. The proposed method achieved a Dice coefficient of 89.9% and a Plexus Inclusion Rate of 100% for muscularis segmentation. Plexus segmentation reached a recall of 94.8%, a precision of 84.2% and a Ganglia Inclusion Rate of 99.7%. For ganglion cells annotated with high certainty, the model achieved 62.1\% precision and 89.1% recall. When considering all annotated ganglion cells, regardless of certainty level, the overall precision was 67.0%. These results indicate that ViT-based models are effective at leveraging global tissue context and capturing cellular morphology at small scales, even within complex histological tissue structures. This multi-stage methodology has great potential to support digital pathology workflows by reducing inter-observer variability and assisting in the evaluation of Hirschsprung disease. The clinical impact will be evaluated in future work with larger multi-center datasets and additional expert annotations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</title>
<link>https://arxiv.org/abs/2512.02020</link>
<guid>https://arxiv.org/abs/2512.02020</guid>
<content:encoded><![CDATA[

arXiv:2512.02020v2 Announce Type: replace-cross 
Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization</title>
<link>https://arxiv.org/abs/2512.03522</link>
<guid>https://arxiv.org/abs/2512.03522</guid>
<content:encoded><![CDATA[

arXiv:2512.03522v2 Announce Type: replace-cross 
Abstract: Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning</title>
<link>https://arxiv.org/abs/2508.21451</link>
<guid>https://arxiv.org/abs/2508.21451</guid>
<content:encoded><![CDATA[
<div> Streaming image captioning, multimodal language models, lightweight model, self-refinement, video QA<br /><br />Summary:<br /><br />1. The paper addresses the challenge of streaming image captioning, crucial for systems like video chatbots and navigation robots, which interpret continuous visual inputs. 2. Existing methods rely on large multimodal language models (MLLMs) that are computationally expensive and impractical for many applications. 3. The authors propose a lightweight captioning model by replacing the large language component of MLLMs with a compact 125M-parameter model, achieving similar performance despite a 93x reduction in size. 4. This outcome suggests that factual image captioning does not heavily depend on the complex reasoning capabilities of large language models. 5. To improve reliability, they introduce a multimodal self-refinement framework inspired by human vision, where the model first generates a coarse global caption and then refines it by focusing on salient regions referenced by the previous caption. 6. Experiments show that this approach outperforms existing models in both single-sentence and detailed image captioning tasks and extends effectively to long-range video question answering scenarios. <div>
arXiv:2508.21451v4 Announce Type: replace 
Abstract: Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification</title>
<link>https://arxiv.org/abs/2512.11015</link>
<guid>https://arxiv.org/abs/2512.11015</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, gender classification, image-text fusion, bias mitigation, multimodal representation<br /><br />Summary:<br /><br />This article addresses the challenge of fairness in AI with a focus on facial image-based gender classification. It introduces novel text-guided approaches to improve fairness by integrating semantic information extracted from image captions into the training process. Two primary strategies are proposed: Image Text Matching (ITM) guidance, which trains the model to identify fine-grained alignments between facial images and descriptive texts for enhanced multimodal understanding, and Image Text Fusion, which merges image and text modalities into unified representations that help reduce demographic biases. Experimental results on benchmark datasets demonstrate that these methods significantly improve accuracy and mitigate bias across different gender and racial groups, outperforming current approaches. A distinctive feature of this research is its reliance on textual guidance without requiring explicit demographic labels, making the technique applicable to a broad range of scenarios. The study also highlights the interpretability and intuitiveness of the training paradigm established through semantic information, linking improved fairness with better model generalization. Overall, the proposed methodologies offer meaningful advancements toward combating demographic bias in gender classification systems while maintaining application agnosticism and enhancing model transparency. <div>
arXiv:2512.11015v1 Announce Type: new 
Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoccerMaster: A Vision Foundation Model for Soccer Understanding</title>
<link>https://arxiv.org/abs/2512.11016</link>
<guid>https://arxiv.org/abs/2512.11016</guid>
<content:encoded><![CDATA[
<div> SoccerUnderstanding, VisionFoundationModel, MultiTaskPretraining, DataCuration, SoccerVideoDatasets<br /><br />Summary:<br /><br />This paper introduces SoccerMaster, the first soccer-specific vision foundation model designed to unify multiple soccer visual understanding tasks in a single framework through supervised multi-task pretraining. Unlike previous approaches that rely on isolated, task-specific expert models, SoccerMaster is built to handle a wide range of tasks from fine-grained perception such as athlete detection to high-level semantic reasoning like event classification. To support this, the authors developed an automated data curation pipeline that generates scalable spatial annotations, which are integrated with existing soccer video datasets to form SoccerFactory, a comprehensive pretraining data resource. This extensive and diverse dataset enables more effective model training across different understanding tasks. Evaluation experiments demonstrate that SoccerMaster consistently outperforms expert models tailored for individual tasks, confirming its superior generalization and versatility. The paper also emphasizes the openness of their work by committing to publicly releasing the data, code, and final model, encouraging further research and application in soccer visual analysis. <div>
arXiv:2512.11016v1 Announce Type: new 
Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.11057</link>
<guid>https://arxiv.org/abs/2512.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: Tuberculosis, Chest X-ray, Knowledge Distillation, CNN, TBX11k Dataset  

<br /><br />Summary:  
1. Tuberculosis (TB) remains a major global health issue, especially in resource-limited countries, with chest X-ray (CXR) imaging being a common diagnostic tool that requires expert interpretation.  
2. Machine learning models have demonstrated high accuracy in TB classification, but they frequently rely on spurious correlations, limiting their generalizability across different datasets and clinical environments.  
3. Creating large, well-annotated medical image datasets is costly and labor-intensive, involving domain experts and multiple annotators to achieve reliable labels.  
4. This study employs a knowledge distillation approach using a teacher-student framework based on the ResNet50 architecture to train convolutional neural networks (CNNs) that not only reduce spurious correlations but also localize TB-related abnormalities without needing bounding-box annotations.  
5. Experiments conducted on the TBX11k dataset show that the student model achieves a significant mean Intersection over Union (mIOU) score of 0.2428, outperforming the teacher model and exhibiting enhanced robustness and potential for practical clinical deployment in diverse healthcare settings. <div>
arXiv:2512.11057v1 Announce Type: new 
Abstract: Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.11060</link>
<guid>https://arxiv.org/abs/2512.11060</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Optical Coherence Tomography Angiography, Synthetic Vasculature Reasoning, Diabetic Retinopathy, Image-Text Dataset<br /><br />Summary:  
This paper addresses the challenge of training Vision-Language Models (VLMs) for detailed medical reasoning, emphasizing the scarcity of large-scale, precisely annotated image-text datasets in specialized domains such as Optical Coherence Tomography Angiography (OCTA). To overcome this, the authors introduce Synthetic Vasculature Reasoning (SVR), a framework that generates realistic synthetic retinal vasculature images incorporating key Diabetic Retinopathy (DR) features such as capillary dropout, microaneurysms, neovascularization, and tortuosity, alongside automatically produced granular explanatory texts. Leveraging SVR, they curate OCTA-100K-SVR, a large-scale dataset containing 100,000 OCTA image and reasoning text pairs. Training a general-purpose Vision-Language Model (Qwen3-VL-8b) on this dataset, the model achieves a zero-shot balanced classification accuracy of 89.67% on real-world OCTA images, significantly outperforming existing supervised baselines. Furthermore, human expert evaluation confirms that the model notably improves explanation quality and the localization of pathological features in clinical data. Overall, the work demonstrates that synthetic data generation can effectively bridge data scarcity in medical imaging and enhance interpretable diagnostic performance of Vision-Language Models. <div>
arXiv:2512.11060v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation</title>
<link>https://arxiv.org/abs/2512.11061</link>
<guid>https://arxiv.org/abs/2512.11061</guid>
<content:encoded><![CDATA[
<div> Keywords: generative video models, vision-language model, scene representation, physics simulation, world modeling  

<br /><br />Summary: Generative video models currently face major challenges such as violating physical and logical constraints, lacking interactivity, and operating as opaque black boxes, which limits their use for creating structured, queryable environments. To address these issues, the paper introduces a new paradigm that distills image-caption pairs into an abstract, tractable representation optimized for simulation. The proposed framework, VDAWorld, leverages a Vision-Language Model (VLM) as an intelligent agent to autonomously build grounded 2D or 3D scene representations by selecting appropriate vision tools. The VLM then chooses a compatible physics simulator, such as rigid body or fluid simulations, to operate on the scene representation. This approach allows VDAWorld to infer latent dynamics from static scenes and predict plausible future states. Experimental results demonstrate that combining intelligent abstraction with adaptive simulation enables the creation of a versatile world model capable of producing high-quality simulations across diverse dynamic scenarios. Overall, VDAWorld advances world modeling by integrating vision, language, and physics simulation into a coherent framework for interactive and physically consistent video generation. <div>
arXiv:2512.11061v1 Announce Type: new 
Abstract: Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring</title>
<link>https://arxiv.org/abs/2512.11076</link>
<guid>https://arxiv.org/abs/2512.11076</guid>
<content:encoded><![CDATA[
<div> Keywords: urban dynamics, event-based cameras, sensor fusion, privacy, machine learning<br /><br />Summary:<br /><br />1. The paper addresses the ongoing challenges in understanding human movement and urban dynamics, highlighting the evolution from traditional observation methods to advanced sensing technologies.<br />2. It provides a focused survey on event-based cameras, which differ from conventional RGB cameras by capturing changes in light intensity rather than color values.<br />3. Event-based cameras offer distinct advantages, such as effective operation in low-light conditions and enhanced privacy preservation, making them suitable for urban monitoring.<br />4. The paper discusses the applications, benefits, and challenges of event-based cameras and explores the integration of machine learning techniques to leverage their data effectively.<br />5. To overcome limitations inherent to event-based cameras, the authors propose multi-sensor fusion approaches, combining event-based cameras with infrared, event-LiDAR, or vibration sensors, aiming to improve accuracy and robustness in studying city dynamics. <div>
arXiv:2512.11076v1 Announce Type: new 
Abstract: Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description</title>
<link>https://arxiv.org/abs/2512.11098</link>
<guid>https://arxiv.org/abs/2512.11098</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared imaging, zero-shot learning, vision-language models, industrial sensing, thermal monitoring<br /><br />Summary:  
This paper presents VLM-IRIS, a novel zero-shot framework designed to adapt vision-language models (VLMs) to process infrared (IR) camera data, which traditionally poses challenges due to VLMs being trained on RGB images. The key innovation is preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible formats, specifically by converting them to a magma colormap representation. This preprocessing allows existing CLIP-based vision encoders, such as ViT-B/32, to effectively interpret infrared data without requiring any model retraining. The framework leverages centroid prompt ensembling to enhance zero-shot prediction accuracy. VLM-IRIS is demonstrated on the practical task of detecting the presence of workpieces on a 3D printer bed, where thermal contrasts between the build plate and objects enable precise monitoring. The results show that the approach achieves high accuracy in infrared imagery analysis, highlighting its potential for label-free, real-time industrial monitoring in low-light or enclosed machine environments where traditional vision systems fail. This introduces an important advancement for industrial applications by integrating thermal sensing with foundation VLMs, expanding their utility beyond RGB visual domains. <div>
arXiv:2512.11098v1 Announce Type: new 
Abstract: Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction</title>
<link>https://arxiv.org/abs/2512.11099</link>
<guid>https://arxiv.org/abs/2512.11099</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual grounding, Modular encoder-decoder, Multimodal Large Language Model, Object detection, Reinforcement learning<br /><br />Summary:<br /><br />The paper introduces VGent, a novel modular encoder-decoder architecture for visual grounding that separates high-level reasoning from low-level bounding box prediction. Unlike existing approaches that rely on auto-regressive decoding in Multimodal Large Language Models (MLLMs) or re-aligning LLMs with vision features—which are slow and prone to hallucinations or degrade reasoning ability—VGent uses a frozen MLLM encoder to maintain pretrained reasoning strength. A decoder then selects target boxes by cross-attending to the encoder’s hidden states using high-quality boxes from object detectors as queries, enabling fast and efficient inference. The design synergizes advances in both object detection and MLLMs while avoiding common pitfalls like hallucination and slow decoding. The system supports modular upgrades including QuadThinker, an RL-based method to improve multi-target reasoning; mask-aware labels to resolve detection-segmentation ambiguities; and a global target recognition component to better identify all targets, enhancing proposal selection. Experiments on multi-target visual grounding benchmarks demonstrate state-of-the-art performance with a +20.6% F1 score improvement over previous methods, along with +8.2% gIoU and +5.8% cIoU gains under visual reference challenges, all achieved without sacrificing inference speed. <div>
arXiv:2512.11099v1 Announce Type: new 
Abstract: Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-driven Fusion of Pathology Foundation Models for Enhanced Disease Characterization</title>
<link>https://arxiv.org/abs/2512.11104</link>
<guid>https://arxiv.org/abs/2512.11104</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, pathology, feature fusion, cancer grading, whole-slide images  

<br /><br />Summary:  
This study investigates the integration of multiple pathology foundation models (FMs) to improve cancer grading and staging performance across kidney, prostate, and rectal cancers. Researchers used diagnostic H&amp;E whole-slide images for three cancer types, classifying them into low or high grade/stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were employed to train classifiers. Three FM fusion strategies were compared: majority-vote ensembling, naive feature concatenation, and an intelligent, correlation-guided pruning method to remove redundant features. Under patient-stratified cross-validation with hold-out testing, the intelligent fusion of tile-level embeddings consistently outperformed single FMs and naive fusion methods across all cancers. Similarity metrics revealed that, while FM embedding spaces were globally aligned, their local neighborhood structures showed less agreement, suggesting complementary detailed information across models. Attention map visualizations showed that intelligent fusion focused more specifically on tumor regions and reduced attention to benign tissue, improving interpretability. Overall, the study demonstrates that correlation-guided fusion creates compact, task-specific representations, enhancing both predictive accuracy and biological interpretability in computational pathology applications. <div>
arXiv:2512.11104v1 Announce Type: new 
Abstract: Foundation models (FMs) have demonstrated strong performance across diverse pathology tasks. While there are similarities in the pre-training objectives of FMs, there is still limited understanding of their complementarity, redundancy in embedding spaces, or biological interpretation of features. In this study, we propose an information-driven, intelligent fusion strategy for integrating multiple pathology FMs into a unified representation and systematically evaluate its performance for cancer grading and staging across three distinct diseases. Diagnostic H&amp;E whole-slide images from kidney (519 slides), prostate (490 slides), and rectal (200 slides) cancers were dichotomized into low versus high grade or stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were considered to train downstream classifiers. We then evaluated three FM fusion schemes at both tile and slide levels: majority-vote ensembling, naive feature concatenation, and intelligent fusion based on correlation-guided pruning of redundant features. Under patient-stratified cross-validation with hold-out testing, intelligent fusion of tile-level embeddings yielded consistent gains in classification performance across all three cancers compared with the best single FMs and naive fusion. Global similarity metrics revealed substantial alignment of FM embedding spaces, contrasted by lower local neighborhood agreement, indicating complementary fine-grained information across FMs. Attention maps showed that intelligent fusion yielded concentrated attention on tumor regions while reducing spurious focus on benign regions. Our findings suggest that intelligent, correlation-guided fusion of pathology FMs can yield compact, task-tailored representations that enhance both predictive performance and interpretability in downstream computational pathology tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from a Generative Oracle: Domain Adaptation for Restoration</title>
<link>https://arxiv.org/abs/2512.11121</link>
<guid>https://arxiv.org/abs/2512.11121</guid>
<content:encoded><![CDATA[
<div> Keywords: image restoration, domain adaptation, generative oracle, pseudo-supervision, unsupervised learning  

<br /><br />Summary:  
This paper addresses the challenge of adapting pre-trained image restoration models to real-world, out-of-distribution degradations where ground truth data is unavailable. Traditional methods often struggle because they require architectural changes and paired data, which are impractical in many scenarios. The authors propose LEGO (Learning from a Generative Oracle), a novel three-stage framework designed for post-training domain adaptation without the need for paired data. First, LEGO generates initial restorations using the existing pre-trained model on the new domain. Second, a frozen, large-scale generative oracle refines these initial outputs to produce high-quality pseudo-ground-truth images. Third, the original model is fine-tuned with a mixed-supervision strategy that combines the original in-distribution data and the newly created pseudo-pairs. This process enables effective domain adaptation while maintaining the model’s original robustness and requires no changes to the network architecture. Experimental results demonstrate that LEGO significantly improves performance on diverse real-world benchmarks by bridging the domain gap in an unsupervised manner. Overall, LEGO provides a practical, scalable, and effective solution to adapt image restoration models to unseen domains without sacrificing existing capabilities. <div>
arXiv:2512.11121v1 Announce Type: new 
Abstract: Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching</title>
<link>https://arxiv.org/abs/2512.11130</link>
<guid>https://arxiv.org/abs/2512.11130</guid>
<content:encoded><![CDATA[
<div> Keywords: stereo vision, zero-shot generalization, real-time processing, knowledge distillation, neural architecture search<br /><br />Summary:<br /><br />This paper introduces Fast-FoundationStereo, a novel family of stereo vision architectures designed to deliver strong zero-shot generalization while maintaining real-time frame rates. The authors identify the existing challenge where stereo foundation models excel in generalization but are computationally expensive, whereas efficient stereo models are faster but require domain-specific fine-tuning and compromise robustness. To address this, Fast-FoundationStereo uses a divide-and-conquer acceleration approach consisting of three key components: (1) knowledge distillation to compress a complex hybrid backbone into a single efficient student network, enabling faster inference; (2) blockwise neural architecture search to automatically discover optimal cost filtering designs within strict latency constraints, significantly reducing search complexity; and (3) structured pruning to remove redundancies in the iterative refinement module, enhancing efficiency further. Additionally, the authors developed an automatic pseudo-labeling pipeline to generate a large-scale dataset of 1.4 million in-the-wild stereo pairs, supplementing synthetic data and aiding effective knowledge distillation. The resulting model attains over 10 times speed improvement compared to FoundationStereo while closely matching its zero-shot accuracy performance. This work sets a new state-of-the-art benchmark for real-time stereo vision methods, combining accuracy and speed effectively. <div>
arXiv:2512.11130v1 Announce Type: new 
Abstract: Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning complete and explainable visual representations from itemized text supervision</title>
<link>https://arxiv.org/abs/2512.11141</link>
<guid>https://arxiv.org/abs/2512.11141</guid>
<content:encoded><![CDATA[
<div> itemized text supervision, cross-attention, visual representations, medical imaging, zero-shot learning  

<br /><br />Summary:  
Training vision models using language supervision allows for more general and transferable visual representations. However, typical approaches struggle with itemized text supervision found in domains like medical imaging and remote sensing, where multiple independent text annotations correspond to distinct findings within a single image. Unlike conventional multi-caption setups that are often redundant or overlapping, itemized text provides separate, semantically distinct descriptions. To address this, the paper introduces ItemizedCLIP, a framework that uses a cross-attention module to generate visual embeddings conditioned on individual text items. ItemizedCLIP incorporates customized training objectives to ensure item independence, encouraging distinct image regions to correspond to different items, and representation completeness, ensuring all text items are covered. The method was evaluated across four real-world domains with natural itemized supervision (brain MRI, head CT, chest CT, remote sensing) and one synthetic dataset. Results show that ItemizedCLIP significantly improves zero-shot classification performance and fine-grained interpretability compared to baseline models. The learned visual representations are semantically grounded, differentiable by item, complete in coverage, and visually interpretable. The code implementation has been made publicly available for further research and application. <div>
arXiv:2512.11141v1 Announce Type: new 
Abstract: Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context</title>
<link>https://arxiv.org/abs/2512.11167</link>
<guid>https://arxiv.org/abs/2512.11167</guid>
<content:encoded><![CDATA[
<div> Monkey Vision-Language Model, image tiling, high-resolution, reproducibility, global context<br /><br />Summary:<br /><br />This study focuses on reproducing and critically analyzing the Monkey Vision-Language Model (VLM) introduced by Li et al. (2023b), which aims at improving high-resolution image understanding by using image tiling to balance fine-grained detail recovery with computational efficiency. The researchers successfully replicate the tile-based approach using publicly available checkpoints and reimplement the original training pipeline. They confirm the original finding that tiling significantly aids in recovering local visual details in large images. Beyond reproduction, the study investigates the role of global context inclusion, offering practical insights that can guide future multimodal models handling high-resolution images. Despite confirming key findings, the authors observe variations in results, with the impact of tiling and global context differing depending on the nature of the task and the granularity of image tiles used. These deviations underscore the complexity and task sensitivity in applying tiling strategies to multimodal learning. Overall, the work stresses the importance of transparency and accessible infrastructure in reproducing complex models and contributes valuable empirical knowledge regarding tiling and context effects in vision-language modeling. <div>
arXiv:2512.11167v1 Announce Type: new 
Abstract: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight 3D Gaussian Splatting Compression via Video Codec</title>
<link>https://arxiv.org/abs/2512.11186</link>
<guid>https://arxiv.org/abs/2512.11186</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, Video Compression, Morton Scan, Principal Component Analysis, Rate-Distortion Performance  

<br /><br />Summary:  
This paper addresses the inefficiency of current video-based Gaussian Splatting (GS) compression methods, which rely on the computationally heavy Parallel Linear Assignment Sorting (PLAS) for converting 3D GS data into smooth 2D maps. To overcome these limitations on lightweight devices, the authors propose a Lightweight 3D Gaussian Splatting Compression method based on video codec, termed LGSCV. The approach features a novel two-stage Morton scan process: a 3D Morton scan permutes the GS primitives, followed by a 2D Morton scan mapping these into blockwise 2D maps compatible with square block coding units (CU) of standard video codecs. Although blockwise 2D maps perform comparably to PLAS at high bitrates, quality deteriorates at medium-to-low bitrates. To address this, Principal Component Analysis (PCA) reduces the dimensionality of spherical harmonics (SH), and a flexible, faster MiniPLAS is introduced to permute primitives within certain block sizes. Combining SH PCA and MiniPLAS significantly improves rate-distortion (RD) performance, especially at lower bitrates, while guiding CU size configuration and halving encoding time. Experiments on the MPEG dataset demonstrate LGSCV achieves over 20% RD gain over state-of-the-art methods, reduces 2D map generation time to about 1 second, and cuts overall encoding time by 50%. The source code is publicly available on GitHub. <div>
arXiv:2512.11186v1 Announce Type: new 
Abstract: Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization</title>
<link>https://arxiv.org/abs/2512.11189</link>
<guid>https://arxiv.org/abs/2512.11189</guid>
<content:encoded><![CDATA[
<div> BinEgo-360, Temporal Action Localization, Multi-task Learning, Ensemble Learning, Temporal Shift Module<br /><br />Summary: This paper presents a winning solution to the BinEgo-360 Challenge at ICCV 2025, focusing on temporal action localization (TAL) within multi-perspective and multi-modal video inputs. The dataset includes panoramic, third-person, and egocentric videos annotated with detailed action classes, posing unique challenges. The authors extend the Temporal Shift Module (TSM) to TAL tasks by incorporating a background class and segmenting videos into fixed-length, non-overlapping intervals for classification. A multi-task learning framework is proposed that jointly optimizes scene classification and TAL, exploiting contextual relationships between actions and their environments to enhance prediction accuracy. To further improve robustness and consistency, multiple models are integrated through a weighted ensemble strategy. This combination of multi-task learning, an efficient backbone architecture, and ensemble learning led the method to achieve first place in both the initial and extended rounds of the competition. The results demonstrate the effectiveness of leveraging contextual cues and ensemble techniques in handling the complex nature of multi-perspective, multi-modal video action localization tasks. <div>
arXiv:2512.11189v1 Announce Type: new 
Abstract: We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADKnitter: Compositional CAD Generation from Text and Geometry Guidance</title>
<link>https://arxiv.org/abs/2512.11199</link>
<guid>https://arxiv.org/abs/2512.11199</guid>
<content:encoded><![CDATA[
<div> CAD generation, diffusion sampling, compositional design, CAD assembly, KnitCAD dataset<br /><br />Summary:<br /><br />1. Designing computer-aided design (CAD) models traditionally requires significant expertise and time, with a focus on precision and detailed functionality. 2. Recent advances in 3D generation have shifted the focus from merely visual quality to also enabling editable and functional CAD designs. 3. Existing methods primarily handle single-part CAD generation but fall short when applied to multi-part assemblies, which are essential in real-world applications due to semantic and geometric constraints among parts. 4. The paper introduces CADKnitter, a novel framework for compositional CAD generation that uses a geometry-guided diffusion sampling strategy to create complementary CAD parts that respect both geometric compatibility with the existing model and semantic alignment with user-provided text prompts. 5. To support training and evaluation, the authors curated KnitCAD, a comprehensive dataset of over 310,000 CAD models enriched with textual prompts and detailed assembly metadata, enabling semantic and geometric constraint consideration. Experimental results demonstrate that CADKnitter outperforms state-of-the-art baselines in generating coherent and semantically meaningful multi-part CAD assemblies, highlighting its effectiveness for practical CAD design workflows. <div>
arXiv:2512.11199v1 Announce Type: new 
Abstract: Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path</title>
<link>https://arxiv.org/abs/2512.11203</link>
<guid>https://arxiv.org/abs/2512.11203</guid>
<content:encoded><![CDATA[
<div> Autoregressive video diffusion models, inference-time alignment, noise refiner, AutoRefiner, sample fidelity<br /><br />Summary:<br /><br />Autoregressive video diffusion models (AR-VDMs) offer scalable and efficient alternatives to bidirectional video diffusion models, enabling real-time and interactive video generation applications. However, challenges persist in enhancing the sample fidelity produced by these models. One approach to improve fidelity is inference-time alignment, which optimizes the noise space for better outputs without updating the model parameters. Traditional optimization or search-based methods for inference-time alignment, however, are computationally expensive and impractical for AR-VDMs. Inspired by advancements in text-to-image (T2I) diffusion models, where feedforward noise refiners modulate sampled noises in a single forward pass, the authors explore extending this idea to AR-VDMs. They find that naïvely applying T2I noise refiners to AR-VDMs fails to improve performance. To address this, they propose AutoRefiner, a noise refiner specifically designed for AR-VDMs. AutoRefiner incorporates two key innovations: pathwise noise refinement, which refines noise along stochastic denoising paths, and a reflective key-value cache (KV-cache) to maintain consistency across steps. Experimental results demonstrate that AutoRefiner effectively enhances sample fidelity while serving as an efficient, plug-in component compatible with existing AR-VDMs. <div>
arXiv:2512.11203v1 Announce Type: new 
Abstract: Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection</title>
<link>https://arxiv.org/abs/2512.11215</link>
<guid>https://arxiv.org/abs/2512.11215</guid>
<content:encoded><![CDATA[
<div> Wildfire smoke, multimodal large language models, smoke detection, smoke localization, early-stage detection<br /><br />Summary:<br /><br />This paper presents SmokeBench, a benchmark designed to evaluate the ability of multimodal large language models (MLLMs) to identify and localize wildfire smoke in images. The benchmark includes four specific tasks: smoke classification, tile-based smoke localization, grid-based smoke localization, and overall smoke detection. Several state-of-the-art MLLMs such as Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro were tested using this benchmark. The results indicate that although some models can successfully classify the presence of smoke when it occupies a large area, all models face significant challenges in precisely localizing smoke, particularly in the early stages of a wildfire. Further analysis suggests that the volume of smoke is a strong predictor of model performance, while visual contrast has a lesser impact. These findings emphasize the limitations of current MLLMs in safety-critical tasks like wildfire monitoring and spotlight the need for advanced techniques to enhance early-stage smoke detection and localization accuracy. <div>
arXiv:2512.11215v1 Announce Type: new 
Abstract: Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFMF: World Modeling by Forecasting Vision Foundation Model Features</title>
<link>https://arxiv.org/abs/2512.11225</link>
<guid>https://arxiv.org/abs/2512.11225</guid>
<content:encoded><![CDATA[
<div> Keywords: world modeling, vision foundation models, generative forecasting, autoregressive flow matching, latent space<br /><br />Summary:<br /><br />1. The paper addresses the challenge of forecasting future world states from partial observations, which is a fundamental problem in world modeling. 2. Traditional methods focus on stochastic video generation by predicting future pixels, which is computationally expensive and not directly useful for decision-making tasks because it requires interpreting RGB images into actionable signals. 3. An alternative approach leverages features from vision foundation models (VFMs) to represent the world state and performs deterministic regression to predict these features, allowing direct translation into useful outputs like semantic segmentation and depth, while being computationally more efficient. 4. However, deterministic regression suffers from averaging over multiple plausible futures, thus failing to properly capture uncertainty and reducing forecast accuracy. 5. To overcome this limitation, the authors propose a generative forecaster that uses autoregressive flow matching in the VFM feature space, encoding the features into a compact latent space optimized for diffusion-based generative modeling. 6. This latent space encoding preserves information better than PCA-based approaches and can decode predictions into multiple interpretable modalities including semantic segmentation, depth, surface normals, and RGB images. 7. Experimental results show that the proposed method produces sharper and more accurate multimodal predictions than regression under matched computational budgets and architectures. 8. The study concludes that stochastic conditional generation in VFM feature space is a promising and scalable direction for developing future world models. <div>
arXiv:2512.11225v1 Announce Type: new 
Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model</title>
<link>https://arxiv.org/abs/2512.11226</link>
<guid>https://arxiv.org/abs/2512.11226</guid>
<content:encoded><![CDATA[
<div> Future scene prediction, Chain of Thought, end-to-end planner, autonomous driving, motion planning<br /><br />Summary:<br /><br />1. Autonomous driving systems typically use end-to-end planners that process raw sensor data to generate motion plans or control actions based solely on the current scene. 2. This approach can be suboptimal in dynamic traffic environments where the ego vehicle’s actions affect future scenes, necessitating predictive reasoning about scene evolution. 3. The authors propose FutureX, a Chain of Thought (CoT)-driven pipeline that enhances motion planning by modeling future scenes through a Latent World Model and refining trajectories accordingly. 4. FutureX includes an Auto-think Switch that evaluates the scene complexity and decides whether to engage in additional reasoning (Thinking mode) or produce an immediate plan (Instant mode). 5. In Thinking mode, the system predicts future scene states using CoT-guided rollouts, allowing a Summarizer Module to improve the motion plan iteratively. 6. Experiments show that FutureX improves the rationality of motion plans and reduces collision rates compared to existing methods without sacrificing efficiency. 7. Notably, it achieves a 6.2-point improvement in PDMS (Performance Driving Metric Score) for the TransFuser model evaluated on NAVSIM. 8. The authors plan to release the code, facilitating adoption and further research in this area. <div>
arXiv:2512.11226v1 Announce Type: new 
Abstract: In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation</title>
<link>https://arxiv.org/abs/2512.11229</link>
<guid>https://arxiv.org/abs/2512.11229</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, talking head generation, real-time streaming, video latent space, autoregressive generation<br /><br />Summary:<br /><br />1. The paper introduces REST, a novel diffusion-based framework designed for real-time, end-to-end streaming audio-driven talking head generation.<br />2. To facilitate real-time performance, REST employs a high spatiotemporal Variational Autoencoder (VAE) to compress video data into a compact latent space.<br />3. An ID-Context Cache mechanism is developed to support autoregressive streaming in the latent space by combining ID-Sink and Context-Cache techniques, which ensures temporal consistency and identity preservation over long streams.<br />4. The Asynchronous Streaming Distillation (ASD) training strategy is proposed to reduce error accumulation common in autoregressive models, by using a non-streaming teacher model with an asynchronous noise schedule to guide the streaming student model.<br />5. Experimental results demonstrate that REST significantly outperforms existing state-of-the-art methods with faster generation speeds and improved overall talking head quality, bridging the gap between diffusion-based and autoregressive paradigms for real-time applications. <div>
arXiv:2512.11229v1 Announce Type: new 
Abstract: Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing</title>
<link>https://arxiv.org/abs/2512.11234</link>
<guid>https://arxiv.org/abs/2512.11234</guid>
<content:encoded><![CDATA[
<div> Keywords: controllable scene generation, indoor scenes, multi-modal inputs, Indoor Domain-Specific Language, interaction-annotated assets  

<br /><br />Summary:  
1. The paper presents RoomPilot, a unified framework designed to generate controllable and interactive indoor scenes suitable for applications such as game development, architectural visualization, and embodied AI training.  
2. RoomPilot supports diverse multi-modal inputs including textual descriptions and CAD floor plans, parsing them into an Indoor Domain-Specific Language (IDSL) that serves as a shared semantic representation for scene synthesis.  
3. The IDSL enables the generation of coherent, high-quality indoor scenes from any single input modality while preserving interaction semantics, overcoming limitations of prior methods constrained by narrow input types or stochastic processes.  
4. Unlike traditional procedural approaches which often yield visually plausible but functionally inert layouts, RoomPilot integrates a curated dataset of interaction-annotated assets to produce environments with realistic object behaviors and physical consistency.  
5. Extensive experiments demonstrate RoomPilot’s strong multi-modal understanding, fine-grained controllability in scene generation, and superior visual fidelity, marking a significant advancement toward general-purpose controllable 3D indoor scene generation. <div>
arXiv:2512.11234v1 Announce Type: new 
Abstract: Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildCap: Facial Appearance Capture in the Wild via Hybrid Inverse Rendering</title>
<link>https://arxiv.org/abs/2512.11237</link>
<guid>https://arxiv.org/abs/2512.11237</guid>
<content:encoded><![CDATA[
<div> Keywords: facial appearance capture, inverse rendering, in-the-wild, texel grid lighting, reflectance optimization  

<br /><br />Summary:  
1. The paper introduces WildCap, a novel method for capturing high-quality facial appearance from smartphone videos recorded in the wild, addressing cost and usability limitations of traditional controlled lighting setups.  
2. To disentangle reflectance properties from complex lighting in unconstrained conditions, the method uses a hybrid inverse rendering framework combining a data-driven approach called SwitchLight with model-based inverse rendering techniques.  
3. The authors identify that local artifacts generated by neural network predictions, such as shadow-baking, are non-physical and interfere with accurate modeling of lighting and material properties.  
4. To overcome this, a new texel grid lighting model is proposed, which explains these non-physical effects as clean albedo maps illuminated by local physical lighting sources, enabling a more physically consistent interpretation.  
5. During optimization, the method jointly samples a diffusion prior for reflectance maps while optimizing lighting parameters, effectively resolving the scale ambiguity between lighting intensity and albedo reflectance.  
6. Experimental results demonstrate that WildCap significantly outperforms prior methods under similar capture conditions, substantially narrowing the quality gap between in-the-wild and controlled facial appearance capture scenarios.  
7. The authors will make their code publicly available, supporting further research and practical applications in the area. <div>
arXiv:2512.11237v1 Announce Type: new 
Abstract: Existing methods achieve high-quality facial appearance capture under controllable lighting, which increases capture cost and limits usability. We propose WildCap, a novel method for high-quality facial appearance capture from a smartphone video recorded in the wild. To disentangle high-quality reflectance from complex lighting effects in in-the-wild captures, we propose a novel hybrid inverse rendering framework. Specifically, we first apply a data-driven method, i.e., SwitchLight, to convert the captured images into more constrained conditions and then adopt model-based inverse rendering. However, unavoidable local artifacts in network predictions, such as shadow-baking, are non-physical and thus hinder accurate inverse rendering of lighting and material. To address this, we propose a novel texel grid lighting model to explain non-physical effects as clean albedo illuminated by local physical lighting. During optimization, we jointly sample a diffusion prior for reflectance maps and optimize the lighting, effectively resolving scale ambiguity between local lights and albedo. Our method achieves significantly better results than prior arts in the same capture setup, closing the quality gap between in-the-wild and controllable recordings by a large margin. Our code will be released \href{https://yxuhan.github.io/WildCap/index.html}{\textcolor{magenta}{here}}.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition</title>
<link>https://arxiv.org/abs/2512.11239</link>
<guid>https://arxiv.org/abs/2512.11239</guid>
<content:encoded><![CDATA[
<div> Keywords: Incomplete multi-modal emotion recognition, cross-modal prompting, modality-specific features, knowledge propagation, dynamic re-weighting

<br /><br />Summary:  
Incomplete multi-modal emotion recognition (IMER) focuses on interpreting human intentions and emotions using partially observed multi-source data. Although multi-modal data offers richer information, challenges like performance gaps and under-optimized modalities limit its effectiveness, especially when confronted with missing data. To tackle these issues, the authors propose a novel Cross-modal Prompting (ComP) method designed to enhance coherent information by improving modality-specific features and boosting each modality's accuracy. The method introduces a progressive prompt generation module equipped with a dynamic gradient modulator to create concise, consistent semantic cues for each modality. Additionally, cross-modal knowledge propagation is employed to selectively amplify shared information across modalities through the generated prompts, strengthening the discrimination power of modality-specific outputs. A coordinator component dynamically re-weights modality outputs to complement existing balance strategies, further enhancing model performance. The effectiveness of the ComP method is validated through extensive experiments conducted on four diverse datasets, compared against seven state-of-the-art approaches under various missing data rates. The results demonstrate significant improvements in multi-modal emotion recognition accuracy and robustness, confirming the superiority of the proposed framework in handling incomplete multi-modal inputs. <div>
arXiv:2512.11239v1 Announce Type: new 
Abstract: Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PersonaLive! Expressive Portrait Image Animation for Live Streaming</title>
<link>https://arxiv.org/abs/2512.11253</link>
<guid>https://arxiv.org/abs/2512.11253</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion-based portrait animation, real-time performance, hybrid implicit signals, appearance distillation, autoregressive streaming generation

<br /><br />Summary:  
This paper introduces PersonaLive, a novel diffusion-based framework designed for streaming real-time portrait animation, addressing limitations in generation latency and real-time usability seen in previous models. The method utilizes hybrid implicit signals—combining implicit facial representations and 3D implicit keypoints—to enable expressive and precise image-level motion control. To improve inference speed, the authors propose a fewer-step appearance distillation strategy that reduces redundancy during the denoising process, significantly enhancing efficiency. PersonaLive also features an autoregressive micro-chunk streaming generation approach, integrating a sliding training strategy and a historical keyframe mechanism to achieve low-latency and stable long-term video generation. Extensive experimental results demonstrate that PersonaLive considerably outperforms prior diffusion-based portrait animation models, offering speedups ranging from 7 to 22 times while maintaining state-of-the-art performance. This work significantly advances the practical application of diffusion-based portrait animation in live streaming scenarios by balancing quality, realism, and real-time efficiency. <div>
arXiv:2512.11253v1 Announce Type: new 
Abstract: Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers</title>
<link>https://arxiv.org/abs/2512.11260</link>
<guid>https://arxiv.org/abs/2512.11260</guid>
<content:encoded><![CDATA[
<div> Transformers, Vision Transformers, Reformer, Locality-Sensitive Hashing, Computational Efficiency  

<br /><br />Summary:  
1. The paper explores the use of the Reformer architecture as an alternative backbone for vision tasks, aiming to address the high computational cost of standard Vision Transformers (ViTs) due to quadratic scaling of global self-attention.  
2. The Reformer model integrates patch-based tokenization with locality-sensitive hashing (LSH) attention to approximate global self-attention, reducing theoretical time complexity from 𝒪(n²) to 𝒪(n log n), where n is the sequence length.  
3. The model is evaluated on three datasets: CIFAR-10 for small-scale experimentation, ImageNet-100 for assessing accuracy-efficiency trade-offs under more typical settings, and a high-resolution medical imaging dataset to test performance with longer token sequences.  
4. Results show the Reformer achieves better accuracy than a ViT-style baseline on CIFAR-10, indicating promise on smaller datasets.  
5. However, in larger and higher-resolution experiments, the standard ViT outperforms the Reformer in practical efficiency and overall computation time. This suggests that the theoretical benefits of LSH-based attention methods like the Reformer are only realized when dealing with significantly longer sequences than those encountered in most high-resolution images. <div>
arXiv:2512.11260v1 Announce Type: new 
Abstract: Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Efficacy of Sentinel-2 versus Aerial Imagery in Serrated Tussock Classification</title>
<link>https://arxiv.org/abs/2512.11267</link>
<guid>https://arxiv.org/abs/2512.11267</guid>
<content:encoded><![CDATA[
<div> Invasive species, Serrated tussock, Sentinel-2 imagery, Remote sensing, Random forest<br /><br />Summary:<br /><br />1. Invasive species like Serrated tussock (Nassella trichotoma) severely threaten ecosystems and agriculture by disrupting native grasslands and reducing pasture productivity, with notable impacts in Victoria, Australia.<br />2. Traditional ground surveys help manage infestations at small scales but are inadequate for landscape-scale monitoring due to feasibility constraints.<br />3. Aerial imagery, while offering high spatial resolution necessary for detailed classification, poses cost and scalability challenges.<br />4. Satellite-based remote sensing, especially using Sentinel-2 images with higher spectral resolution and seasonal phenological data, offers a more cost-effective and scalable alternative despite lower spatial resolution.<br />5. The study develops eleven models combining spectral bands, texture features, vegetation indices, and seasonal data to classify Serrated tussock.<br />6. Using a random forest classifier, the best Sentinel-2 model (M76*) achieved 68% Overall Accuracy and 0.55 Overall Kappa, surpassing the best aerial imaging model’s 67% OA and 0.52 OK on the same data.<br />7. Results demonstrate the effectiveness of multi-temporal, feature-enhanced satellite imagery for scalable, landscape-scale invasive species classification, presenting a promising tool for ecological monitoring and management. <div>
arXiv:2512.11267v1 Announce Type: new 
Abstract: Invasive species pose major global threats to ecosystems and agriculture. Serrated tussock (\textit{Nassella trichotoma}) is a highly competitive invasive grass species that disrupts native grasslands, reduces pasture productivity, and increases land management costs. In Victoria, Australia, it presents a major challenge due to its aggressive spread and ecological impact. While current ground surveys and subsequent management practices are effective at small scales, they are not feasible for landscape-scale monitoring. Although aerial imagery offers high spatial resolution suitable for detailed classification, its high cost limits scalability. Satellite-based remote sensing provides a more cost-effective and scalable alternative, though often with lower spatial resolution. This study evaluates whether multi-temporal Sentinel-2 imagery, despite its lower spatial resolution, can provide a comparable and cost-effective alternative for landscape-scale monitoring of serrated tussock by leveraging its higher spectral resolution and seasonal phenological information. A total of eleven models have been developed using various combinations of spectral bands, texture features, vegetation indices, and seasonal data. Using a random forest classifier, the best-performing Sentinel-2 model (M76*) has achieved an Overall Accuracy (OA) of 68\% and an Overall Kappa (OK) of 0.55, slightly outperforming the best-performing aerial imaging model's OA of 67\% and OK of 0.52 on the same dataset. These findings highlight the potential of multi-seasonal feature-enhanced satellite-based models for scalable invasive species classification.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion</title>
<link>https://arxiv.org/abs/2512.11274</link>
<guid>https://arxiv.org/abs/2512.11274</guid>
<content:encoded><![CDATA[
<div> FilmWeaver, multi-shot video generation, inter-shot consistency, autoregressive diffusion, dual-level cache<br /><br />Summary:  
The paper presents FilmWeaver, a new framework addressing the challenges of generating multi-shot videos with consistent characters and backgrounds over arbitrary lengths and shot counts. The approach uses an autoregressive diffusion paradigm to enable the generation of videos of any length flexibly. A crucial innovation is the decoupling of the video consistency problem into inter-shot consistency and intra-shot coherence. This is achieved by a dual-level cache system: the shot memory stores keyframes from previous shots to maintain scene and character identity, while the temporal memory holds frame history from the current shot to ensure smooth and continuous motion. FilmWeaver supports multi-round user interaction, allowing flexible creation of multi-shot videos. Its design also allows for additional capabilities such as multi-concept injection and video extension, demonstrating high versatility. To support training, the authors developed a comprehensive pipeline to create a high-quality multi-shot video dataset. Experimental results show that FilmWeaver outperforms existing methods in both consistency and aesthetic quality metrics, enabling more controllable, consistent, and narrative-driven video content creation. The project page is available at https://filmweaver.github.io. <div>
arXiv:2512.11274v1 Announce Type: new 
Abstract: Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RcAE: Recursive Reconstruction Framework for Unsupervised Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.11284</link>
<guid>https://arxiv.org/abs/2512.11284</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised anomaly detection, recursive autoencoder, cross recursion detection, detail preservation, industrial defects<br /><br />Summary:<br /><br />This paper addresses the challenge of unsupervised industrial anomaly detection, focusing on identifying defects without relying on labeled data. Traditional autoencoder-based approaches fall short due to their single-pass decoding, which often leads to incomplete anomaly suppression and loss of fine details. To overcome this, the authors propose a Recursive Autoencoder (RcAE) architecture that iteratively reconstructs input images, progressively suppressing anomalies while refining normal patterns. This recursive process generates a sequence of reconstructions that reveal abnormalities obscured in earlier passes. To exploit this dynamic, a Cross Recursion Detection (CRD) module is introduced, which monitors inconsistencies across recursive steps to improve detection of both subtle and large-scale defects. Furthermore, a Detail Preservation Network (DPN) is integrated to recover fine textures typically lost during reconstruction. Experimental results show that the proposed method significantly outperforms existing non-diffusion based models and matches the performance of state-of-the-art diffusion models, but with only 10% of their parameters and much faster inference speeds. These advantages demonstrate the approach's practical efficiency and effectiveness for real-world unsupervised industrial anomaly detection tasks. <div>
arXiv:2512.11284v1 Announce Type: new 
Abstract: Unsupervised industrial anomaly detection requires accurately identifying defects without labeled data. Traditional autoencoder-based methods often struggle with incomplete anomaly suppression and loss of fine details, as their single-pass decoding fails to effectively handle anomalies with varying severity and scale. We propose a recursive architecture for autoencoder (RcAE), which performs reconstruction iteratively to progressively suppress anomalies while refining normal structures. Unlike traditional single-pass models, this recursive design naturally produces a sequence of reconstructions, progressively exposing suppressed abnormal patterns. To leverage this reconstruction dynamics, we introduce a Cross Recursion Detection (CRD) module that tracks inconsistencies across recursion steps, enhancing detection of both subtle and large-scale anomalies. Additionally, we incorporate a Detail Preservation Network (DPN) to recover high-frequency textures typically lost during reconstruction. Extensive experiments demonstrate that our method significantly outperforms existing non-diffusion methods, and achieves performance on par with recent diffusion models with only 10% of their parameters and offering substantially faster inference. These results highlight the practicality and efficiency of our approach for real-world applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context</title>
<link>https://arxiv.org/abs/2512.11293</link>
<guid>https://arxiv.org/abs/2512.11293</guid>
<content:encoded><![CDATA[
<div> Keywords: video autoencoder, temporal-spatial decoupling, autoregressive model, video compression, video generation<br /><br />Summary: The paper proposes a novel Autoregressive Video Autoencoder (ARVAE) aimed at improving video compression and reconstruction by addressing the limitations of existing video autoencoders that entangle spatial and temporal information. ARVAE compresses and reconstructs each video frame conditioned on its predecessor in an autoregressive manner, enabling flexible handling of videos with arbitrary lengths. The model introduces a temporal-spatial decoupled representation that combines a downsampled flow field to maintain temporal coherence with spatial relative compensation to capture newly emerged content, enhancing compression efficiency without information loss. The encoder compresses the current and previous frames into a temporal motion component and a spatial supplement, respectively, while the decoder uses these latent representations along with the preceding frame to reconstruct the original frame. A multi-stage training strategy is employed to progressively improve model performance. Extensive experiments demonstrate that ARVAE surpasses previous methods in reconstruction quality while relying on lightweight models and small-scale training data. Furthermore, evaluations on downstream video generation tasks illustrate the strong potential of ARVAE for practical applications in video synthesis and processing. <div>
arXiv:2512.11293v1 Announce Type: new 
Abstract: Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</title>
<link>https://arxiv.org/abs/2512.11296</link>
<guid>https://arxiv.org/abs/2512.11296</guid>
<content:encoded><![CDATA[
<div> Keywords: G-code verification, CNC machining, Human-Machine Interface, Vision-Language Models, few-shot learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of verifying manually generated G-code used in CNC machines, recognizing that prior verification methods rely heavily on Large Language Models that only analyze written code.<br /><br />2. It highlights the limitation of LLMs in incorporating information from the Human-Machine Interface (HMI), which visually displays machine status and errors crucial for CNC operations.<br /><br />3. The authors propose a novel few-shot learning approach using Vision-Language Models (VLMs) that can simultaneously analyze the G-code text and paired HMI screenshots from a 15-slant-PRO lathe to detect errors and safety issues.<br /><br />4. Their input dataset consists of both error-free and error-prone paired samples, and the VLM is guided via a structured JSON schema derived from heuristic knowledge to improve understanding.<br /><br />5. Experimental results demonstrate that few-shot prompting of the VLM improves detection accuracy of discrepancies between G-code and HMI errors over zero-shot methods, making their framework effective for comprehensive debugging of CNC training-generated G-code. <div>
arXiv:2512.11296v1 Announce Type: new 
Abstract: Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2512.11301</link>
<guid>https://arxiv.org/abs/2512.11301</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view egocentric, dynamic scene reconstruction, AR glasses, temporal synchronization, free-viewpoint video  

<br /><br />Summary:  
This paper introduces MultiEgo, the first dataset designed specifically for multi-view egocentric dynamic scene reconstruction, addressing a gap in existing datasets that mainly focus on static multi-view or single-egocentric setups. The dataset captures five canonical social interaction scenes, including meetings, performances, and presentations, each recorded from five participants wearing augmented reality (AR) glasses. A custom hardware-based data acquisition system and processing pipeline ensure sub-millisecond temporal synchronization across multiple viewpoints while providing accurate pose annotations. The dataset's design supports high-fidelity reconstruction and analysis of 4D dynamic social scenes from multiple egocentric perspectives. Experimentation validates MultiEgo’s practical utility and effectiveness for free-viewpoint video (FVV) applications, highlighting its potential to advance research in holographic documentation and realistic scene rendering. MultiEgo thus establishes itself as a foundational resource for the research community focused on multi-view egocentric dynamic scene reconstruction, enabling novel explorations in social interaction holography and immersive media technologies. <div>
arXiv:2512.11301v1 Announce Type: new 
Abstract: Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATMapTR: Satellite Image Enhanced Online HD Map Construction</title>
<link>https://arxiv.org/abs/2512.11319</link>
<guid>https://arxiv.org/abs/2512.11319</guid>
<content:encoded><![CDATA[
<div> Keywords: HD maps, satellite images, feature fusion, autonomous driving, nuScenes dataset<br /><br />Summary:<br /><br />1. The paper addresses challenges in real-time HD map construction for autonomous driving, specifically the issues caused by low-quality input data from onboard sensors, such as incomplete, noisy, or missing information due to limited capabilities and occlusions.<br /><br />2. Satellite images provide a complementary, stable, wide-area perspective but suffer from degradation due to shadows and occlusions from vegetation and buildings, limiting their direct use in Bird’s Eye View (BEV) mapping.<br /><br />3. The authors propose SATMapTR, a novel online map construction model that effectively incorporates satellite images by introducing two main components: a gated feature refinement module that adaptively filters satellite features using high-level semantic and low-level structural information to enhance signal-to-noise ratio of map-relevant features.<br /><br />4. The second component is a geometry-aware fusion module that fuses satellite and BEV features at a precise grid-to-grid level, reducing interference from irrelevant or degraded regions, improving map accuracy and robustness.<br /><br />5. Experimental results on the nuScenes dataset demonstrate SATMapTR’s superior performance with a mean average precision (mAP) of 73.8, outperforming previous satellite-enhanced models by up to 14.2 mAP, showing robustness under adverse weather and sensor failure conditions, and achieving nearly threefold higher mAP at extended perception ranges. <div>
arXiv:2512.11319v1 Announce Type: new 
Abstract: High-definition (HD) maps are evolving from pre-annotated to real-time construction to better support autonomous driving in diverse scenarios. However, this process is hindered by low-quality input data caused by onboard sensors limited capability and frequent occlusions, leading to incomplete, noisy, or missing data, and thus reduced mapping accuracy and robustness. Recent efforts have introduced satellite images as auxiliary input, offering a stable, wide-area view to complement the limited ego perspective. However, satellite images in Bird's Eye View are often degraded by shadows and occlusions from vegetation and buildings. Prior methods using basic feature extraction and fusion remain ineffective. To address these challenges, we propose SATMapTR, a novel online map construction model that effectively fuses satellite image through two key components: (1) a gated feature refinement module that adaptively filters satellite image features by integrating high-level semantics with low-level structural cues to extract high signal-to-noise ratio map-relevant representations; and (2) a geometry-aware fusion module that consistently fuse satellite and BEV features at a grid-to-grid level, minimizing interference from irrelevant regions and low-quality inputs. Experimental results on the nuScenes dataset show that SATMapTR achieves the highest mean average precision (mAP) of 73.8, outperforming state-of-the-art satellite-enhanced models by up to 14.2 mAP. It also shows lower mAP degradation under adverse weather and sensor failures, and achieves nearly 3 times higher mAP at extended perception ranges.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeyframeFace: From Text to Expressive Facial Keyframes</title>
<link>https://arxiv.org/abs/2512.11321</link>
<guid>https://arxiv.org/abs/2512.11321</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic 3D facial animation, text-to-animation, KeyframeFace dataset, Large Language Models, ARKit coefficients<br /><br />Summary:  
Generating dynamic 3D facial animations from natural language input requires understanding both the temporal semantics and fine-grained facial expression changes. Current datasets and approaches generally focus on speech-driven animation or lack structured semantic and temporal guidance, limiting expressive and human-like animation outputs. To address these limitations, the authors introduce KeyframeFace, a large-scale multimodal dataset tailored for text-to-animation tasks with keyframe-level supervision. KeyframeFace includes 2,100 expressive scripts paired with monocular videos, frame-by-frame ARKit facial coefficients, contextual backgrounds, complex emotional expressions, manually annotated keyframes, and multi-perspective annotations generated using Large Language Models (LLMs) and Multimodal LLMs. Building upon this dataset, the authors propose the first text-to-animation framework that explicitly integrates prior knowledge from LLMs to guide interpretable facial motion synthesis. This approach exploits the semantic understanding capability of LLMs combined with the interpretability and temporal structure embedded in ARKit coefficients, enabling the generation of high-fidelity, expressive facial animations. Together, the KeyframeFace dataset and the LLM-based framework establish a new foundation for interpretable, keyframe-guided, and context-aware text-driven 3D facial animation. The code and dataset have been made publicly available for further research and development. <div>
arXiv:2512.11321v1 Announce Type: new 
Abstract: Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLLM Machine Unlearning via Visual Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.11325</link>
<guid>https://arxiv.org/abs/2512.11325</guid>
<content:encoded><![CDATA[
<div> Keywords: machine unlearning, multi-modal large models (MLLM), visual knowledge distillation, selective unlearning, model robustness  

<br /><br />Summary:  
This paper addresses the challenge of machine unlearning specifically for multi-modal large language models (MLLMs), which combine both visual and textual knowledge. Unlike most existing unlearning methods designed for large language models (LLMs), this work focuses on the underexplored area of MLLM unlearning. The authors propose a novel approach that disentangles visual and textual knowledge within MLLMs, allowing selective erasure of target visual knowledge while preserving textual information. Their method introduces a Visual Knowledge Distillation (VKD) scheme, which uses intermediate visual representations in the MLLM as supervision signals, contrasting with previous output-level supervision approaches. This leads to improved unlearning effectiveness and better utility of the model after unlearning. Additionally, the approach only fine-tunes visual components of the MLLM, resulting in notable efficiency gains during the unlearning process. Extensive experiments demonstrate that this method outperforms current state-of-the-art unlearning techniques on both effectiveness and efficiency metrics. Lastly, the paper is the first to evaluate the robustness of MLLM unlearning methods against relearning attacks, highlighting the security and reliability of their approach in practical settings. <div>
arXiv:2512.11325v1 Announce Type: new 
Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene</title>
<link>https://arxiv.org/abs/2512.11327</link>
<guid>https://arxiv.org/abs/2512.11327</guid>
<content:encoded><![CDATA[
<div> Lens flare, video restoration, spatiotemporal modeling, dynamic synthesis, attention mechanism<br /><br />Summary:<br /><br />1. The paper addresses the challenge of removing lens flare in videos, a degradation caused by strong light sources, which is more complex than in images due to independent motion between flare, light sources, and scene content. 2. It introduces a physics-informed dynamic flare synthesis pipeline that simulates light source motion with optical flow and models temporal behaviors of scattering and reflective flares, capturing their dynamic nature accurately. 3. A novel video flare removal network is designed, incorporating an attention module to spatially suppress flare regions and a Mamba-based temporal modeling component that captures long-range spatiotemporal dependencies without requiring multi-frame alignment. 4. To evaluate their method, the authors build the first comprehensive video flare dataset, including synthetic paired videos and real-world videos sourced from the internet to test generalization capabilities. 5. Extensive experiments demonstrate that the proposed approach significantly outperforms existing video-based and image-based flare removal techniques by effectively removing dynamic flares while preserving the integrity of light sources and maintaining spatiotemporal consistency across frames. <div>
arXiv:2512.11327v1 Announce Type: new 
Abstract: Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreqDINO: Frequency-Guided Adaptation for Generalized Boundary-Aware Ultrasound Image Segmentation</title>
<link>https://arxiv.org/abs/2512.11335</link>
<guid>https://arxiv.org/abs/2512.11335</guid>
<content:encoded><![CDATA[
<div> Keywords: Ultrasound segmentation, frequency-guided, boundary refinement, multi-scale extraction, deep learning<br /><br />Summary:<br /><br />1. Ultrasound image segmentation is crucial for clinical diagnosis but is challenged by speckle noise and imaging artifacts that degrade boundary clarity.<br />2. DINOv3, a powerful medical image segmentation model pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation.<br />3. The proposed FreqDINO framework addresses this limitation by incorporating frequency-guided strategies to enhance boundary perception and structural consistency.<br />4. FreqDINO introduces a Multi-scale Frequency Extraction and Alignment (MFEA) module that separates low-frequency structural information from high-frequency boundary details and aligns them through learnable attention mechanisms.<br />5. A Frequency-Guided Boundary Refinement (FGBR) module is designed to extract boundary prototypes from high-frequency components, refining spatial features.<br />6. Furthermore, a Multi-task Boundary-Guided Decoder (MBGD) ensures spatial coherence between boundary and semantic segmentation predictions.<br />7. Extensive experiments demonstrate that FreqDINO outperforms state-of-the-art ultrasound segmentation methods, showing superior accuracy and generalization.<br />8. The code for FreqDINO is publicly available at https://github.com/MingLang-FD/FreqDINO. <div>
arXiv:2512.11335v1 Announce Type: new 
Abstract: Ultrasound image segmentation is pivotal for clinical diagnosis, yet challenged by speckle noise and imaging artifacts. Recently, DINOv3 has shown remarkable promise in medical image segmentation with its powerful representation capabilities. However, DINOv3, pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation. To address this limitation, we propose FreqDINO, a frequency-guided segmentation framework that enhances boundary perception and structural consistency. Specifically, we devise a Multi-scale Frequency Extraction and Alignment (MFEA) strategy to separate low-frequency structures and multi-scale high-frequency boundary details, and align them via learnable attention. We also introduce a Frequency-Guided Boundary Refinement (FGBR) module that extracts boundary prototypes from high-frequency components and refines spatial features. Furthermore, we design a Multi-task Boundary-Guided Decoder (MBGD) to ensure spatial coherence between boundary and semantic predictions. Extensive experiments demonstrate that FreqDINO surpasses state-of-the-art methods with superior achieves remarkable generalization capability. The code is at https://github.com/MingLang-FD/FreqDINO.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models</title>
<link>https://arxiv.org/abs/2512.11336</link>
<guid>https://arxiv.org/abs/2512.11336</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-grained video understanding, Video LLM, Unified visual-language alignment, UFVideo-Bench, Temporal localization  

<br /><br />Summary: With advancements in multi-modal Large Language Models (LLMs), video-specific LLMs have emerged but remain limited to specialized tasks without achieving comprehensive, multi-grained video understanding. To address this, the paper introduces UFVideo, the first Video LLM designed for unified multi-grained cooperative understanding across global, pixel, and temporal scales within a single model. UFVideo employs a unified visual-language guided alignment approach, enabling flexible handling of video inputs and tasks such as textual response generation, temporal localization, and grounded mask prediction. The model dynamically encodes visual and text inputs depending on the task requirements. To rigorously assess UFVideo’s multi-grained understanding capabilities, the authors construct UFVideo-Bench, a benchmark comprising three distinct collaborative tasks spanning different scales, demonstrating the model’s adaptability and superior performance compared to GPT-4o. Furthermore, the effectiveness of UFVideo is validated across nine public benchmarks covering a variety of common video understanding tasks. The results provide insights and set the groundwork for the development of future Video LLMs, highlighting the importance of unified multi-scale visual-language integration for holistic video perception. <div>
arXiv:2512.11336v1 Announce Type: new 
Abstract: With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Specific Distance Correlation Matching for Few-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11340</link>
<guid>https://arxiv.org/abs/2512.11340</guid>
<content:encoded><![CDATA[
<div> Few-shot action recognition, CLIP fine-tuning, distance correlation, task-specific matching, Ladder Side Network  

<br /><br />Summary:  
This paper addresses key limitations in few-shot action recognition (FSAR). First, it critiques existing set matching methods that rely on cosine similarity, which capture only linear inter-frame relationships and use instance-level information, thus missing nonlinear dependencies and task-specific cues. Second, it notes that while fine-tuning CLIP with skip-fusion (side) layers reduces memory usage, these layers are difficult to optimize with limited training data. To overcome these issues, the authors propose TS-FSAR, a novel framework with three key components. The Visual Ladder Side Network (LSN) enables efficient fine-tuning of CLIP, improving adaptation under few-shot conditions. The Task-Specific Distance Correlation Matching (TS-DCM) metric uses α-distance correlation to model both linear and nonlinear dependencies between video frames and incorporates task prototypes for tailored matching. The Guiding LSN with Adapted CLIP (GLAC) module regularizes LSN training by leveraging adapted frozen CLIP features to improve α-distance correlation estimation despite limited supervision. Experiments on five standard benchmarks demonstrate that TS-FSAR surpasses prior state-of-the-art methods, showing the effectiveness of the proposed components in improving few-shot action recognition performance. <div>
arXiv:2512.11340v1 Announce Type: new 
Abstract: Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $\alpha$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $\alpha$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture</title>
<link>https://arxiv.org/abs/2512.11350</link>
<guid>https://arxiv.org/abs/2512.11350</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic accident detection, transformer architecture, motion cues, optical flow, video features<br /><br />Summary:<br />Road traffic accidents are a major cause of global mortality, with increasing rates driven by population growth, urbanization, and motorization, raising concerns about current traffic surveillance systems. Traditional computer vision methods face challenges due to their limited understanding of spatiotemporal dynamics and poor ability to generalize across different domains. Transformer architectures have shown promise in modeling complex spatial-temporal dependencies and support efficient parallel processing, but their application in automated traffic accident detection is hindered by the lack of large, diverse datasets. To address this, the authors curated a comprehensive and balanced dataset encompassing various traffic environments, accident types, and contextual situations. Using this dataset, a novel transformer-based accident detection model was developed, combining convolutional layers for extracting local spatial correlations within video frames and transformers to capture sequential-temporal relationships across frames. Importantly, the study highlights the critical role of motion cues in understanding dynamic scenes, especially during accidents, which are often overlooked in previous work relying on static or coarse temporal features. Multiple methods for integrating motion information were tested, with the best results achieved by concatenating RGB video features with optical flow data, yielding an accuracy of 88.3%. The proposed model's performance was also benchmarked against vision-language models like GPT, Gemini, and LLaVA-NeXT-Video, demonstrating its effectiveness in accident detection tasks. <div>
arXiv:2512.11350v1 Announce Type: new 
Abstract: Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</title>
<link>https://arxiv.org/abs/2512.11354</link>
<guid>https://arxiv.org/abs/2512.11354</guid>
<content:encoded><![CDATA[
<div> Underwater pipelines, structured light 3D imaging, multi-source information fusion, pose estimation, defect detection<br /><br />Summary:<br /><br />This paper addresses the critical issue of corrosion in underwater pipelines by developing an intelligent real-time imaging system for underwater pipeline detection called the UW-SLD system. The system leverages structured light 3D imaging to capture detailed spatial information necessary for precise defect characterization. To process underwater images efficiently, a fast distortion correction (FDC) method is introduced for image rectification. The authors tackle the difficulty of extrinsic calibration between underwater sensors by proposing a factor graph-based parameter optimization to accurately estimate the transformation between structured light and acoustic sensors. A multi-mode 3D imaging strategy is designed to handle variations in pipeline geometry. To counter disturbances common in underwater environments, a multi-source information fusion strategy combined with an adaptive extended Kalman filter (AEKF) is employed, ensuring stable pose estimation and accurate measurements. The paper also introduces an edge detection-based ICP (ED-ICP) algorithm that integrates a pipeline edge detection network with enhanced point cloud registration, enabling robust reconstruction of defect structures under variable motion. Extensive experiments demonstrate the system’s superior accuracy, adaptability, and robustness across different operation modes, speeds, and depths, proving its strong potential for autonomous underwater pipeline inspection. <div>
arXiv:2512.11354v1 Announce Type: new 
Abstract: Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video</title>
<link>https://arxiv.org/abs/2512.11356</link>
<guid>https://arxiv.org/abs/2512.11356</guid>
<content:encoded><![CDATA[
<div> dynamic scene reconstruction, monocular RGB video, Dynamic Gaussian Splatting, video segmentation, skeleton-based sampling  

<br /><br />Summary:  
This paper proposes a fully automatic pipeline aimed at reconstructing dynamic scenes from casually captured monocular RGB videos. Instead of creating a new scene representation, the approach enhances existing priors used in Dynamic Gaussian Splatting. The method begins by combining video segmentation with epipolar-error maps to generate detailed object-level masks that accurately capture thin structures. These masks serve two key roles: guiding an object-depth loss to refine consistent video depth and enabling skeleton-based sampling along with mask-guided re-identification for producing reliable and comprehensive 2-D object tracks. Additionally, the system incorporates two objectives within the reconstruction stage—a virtual-view depth loss designed to eliminate floating artifacts and a scaffold-projection loss that links motion nodes to the 2-D tracks to maintain fine geometric details and coherent object motion. The integration of these refined priors and losses enables the pipeline to outperform previous monocular dynamic scene reconstruction methods. Notably, it achieves visibly superior rendering results, providing enhanced fidelity and more accurate dynamic scene representations from single-camera RGB video input. <div>
arXiv:2512.11356v1 Announce Type: new 
Abstract: We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts</title>
<link>https://arxiv.org/abs/2512.11360</link>
<guid>https://arxiv.org/abs/2512.11360</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV, crop detection, rice seedlings, Faster R-CNN, transfer learning<br /><br />Summary:<br /><br />1. The paper focuses on efficient crop detection using Unmanned Aerial Vehicles (UAVs), which is crucial for advancing precision agriculture but is challenged by the small size of targets and variable environmental conditions.<br /><br />2. Specifically, it targets the detection of rice seedlings in paddy fields by utilizing a Faster R-CNN architecture enhanced through transfer learning to improve detection accuracy and speed.<br /><br />3. To address the difficulty of detecting small objects in high-resolution aerial images, the authors compiled a large UAV image dataset specially curated for training the model.<br /><br />4. The model’s robustness and generalization capability were rigorously tested across three distinct test sets collected at different times, allowing evaluation of performance under varying imaging conditions.<br /><br />5. Experimental results showed that transfer learning significantly accelerates model convergence in agricultural object detection tasks and provides consistent performance despite domain shifts caused by different image acquisition conditions, demonstrating the method's effectiveness and reliability. <div>
arXiv:2512.11360v1 Announce Type: new 
Abstract: Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection</title>
<link>https://arxiv.org/abs/2512.11369</link>
<guid>https://arxiv.org/abs/2512.11369</guid>
<content:encoded><![CDATA[
<div> Camouflaged Object Detection, Channel Information Interaction, Boundary Extraction, Region Extraction, Multi-scale Enhancement<br /><br />Summary:<br /><br />1. The paper addresses challenges in Camouflaged Object Detection (COD), focusing on identifying and segmenting objects that visually blend into backgrounds. <br /><br />2. It identifies two main decoding stage issues: insufficient cross-channel information interaction within same-layer features and ineffective co-modeling of boundary and region information.<br /><br />3. To improve cross-channel interaction, the authors propose the Channel Information Interaction Module (CIIM), which reorganizes and integrates features across channels both horizontally and vertically to capture complementary information.<br /><br />4. To jointly model boundary and region information, they design a collaborative decoding architecture using Boundary Extraction (BE) and Region Extraction (RE) modules to generate boundary priors and object localization maps. This process incorporates hybrid attention for feature calibration to resolve semantic ambiguity and produce sharp boundaries.<br /><br />5. The Multi-scale Enhancement (MSE) module is introduced to enrich contextual feature representations.<br /><br />6. Extensive experiments on four benchmark COD datasets demonstrate state-of-the-art performance and validate the model's effectiveness.<br /><br />7. The model is further applied successfully to related tasks such as Salient Object Detection (SOD), polyp segmentation, transparent object detection, and industrial/road defect detection, proving its adaptability.<br /><br />8. The code and experimental results are publicly accessible at https://github.com/akuan1234/ARNet-v2. <div>
arXiv:2512.11369v1 Announce Type: new 
Abstract: Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty</title>
<link>https://arxiv.org/abs/2512.11373</link>
<guid>https://arxiv.org/abs/2512.11373</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic segmentation, out-of-distribution, Wasserstein loss, Kullback-Leibler regularization, Dice consistency<br /><br />Summary:<br /><br />1. Deep neural networks (DNNs) have achieved state-of-the-art results in semantic segmentation tasks but face limitations when handling objects not seen during training, referred to as out-of-distribution (OOD) objects. <br />2. The detection and segmentation of OOD objects are critical in applications where safety is paramount, such as autonomous driving, where unknown objects can pose serious risks.<br />3. The paper proposes a novel evidence segmentation framework leveraging a Wasserstein loss function. This loss effectively measures distributional differences by considering the geometry of the probability simplex, offering a more natural and informative way to quantify uncertainty.<br />4. To enhance the robustness and structural accuracy of the segmentation, the framework integrates Kullback-Leibler divergence as a regularization term and incorporates Dice loss-based structural consistency constraints.<br />5. Experimental results demonstrate that the proposed approach surpasses traditional uncertainty-based methods in OOD segmentation performance, indicating better identification and delineation of unknown objects.<br /><br />This work thus contributes a theoretically motivated and practically effective method for improving open-world semantic segmentation by combining advanced distributional metrics and structural regularization techniques. <div>
arXiv:2512.11373v1 Announce Type: new 
Abstract: Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The N-Body Problem: Parallel Execution from Single-Person Egocentric Video</title>
<link>https://arxiv.org/abs/2512.11393</link>
<guid>https://arxiv.org/abs/2512.11393</guid>
<content:encoded><![CDATA[
<div> N-Body Problem, parallelisation, Vision-Language Model, EPIC-Kitchens, task scheduling<br /><br />Summary:<br /><br />This paper introduces the "N-Body Problem" in the context of egocentric video analysis, where the task is to determine how N individuals can collectively perform the same activities observed from a single person's video. The main objective is to maximize speed-up via parallelisation but ensuring the feasibility of such task assignments is challenging due to real-world constraints such as spatial collisions and object usage conflicts. To tackle this, the authors formalise the N-Body Problem and develop a comprehensive set of evaluation metrics that assess both the performance (speed-up and task coverage) and feasibility (spatial collisions, object conflicts, causal constraints) of the parallel execution. They propose a structured prompting strategy to guide a Vision-Language Model (VLM) to comprehend the 3D environment, object interactions, and temporal dependencies to generate physically plausible and efficient multi-agent task schedules. Empirically, on 100 videos from EPIC-Kitchens and HD-EPIC datasets with N=2 agents, their method significantly outperforms baseline approaches based on Gemini 2.5 Pro. It achieves a 45% increase in action coverage while simultaneously reducing collision rates by 55%, object conflicts by 45%, and causal conflicts by 55%, demonstrating improved effectiveness in feasible task parallelisation from a single egocentric video. <div>
arXiv:2512.11393v1 Announce Type: new 
Abstract: Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing</title>
<link>https://arxiv.org/abs/2512.11395</link>
<guid>https://arxiv.org/abs/2512.11395</guid>
<content:encoded><![CDATA[
<div> flow matching, text-to-image editing, complex editing, velocity decomposition, source consistency<br /><br />Summary:<br /><br />This paper addresses the challenge of complex text-based image editing, which involves multiple editing targets. Current solutions fall short: single-round editing struggles with long text input while multi-round editing suffers from cumulative inconsistency, making it difficult to balance semantic alignment and source structure preservation. The authors propose FlowDC, a novel approach that decouples complex edits into multiple sub-editing effects applied in parallel, improving efficiency and consistency. Additionally, they identify that the velocity component orthogonal to the editing displacement negatively impacts source structure preservation. To counter this, FlowDC decomposes the velocity and attenuates the orthogonal part to maintain better source consistency. To effectively evaluate complex editing methods, the authors create a new benchmark named Complex-PIE-Bench. Experimental results on Complex-PIE-Bench and an additional benchmark demonstrate that FlowDC outperforms existing methods in both semantic alignment and source consistency. Furthermore, ablation studies are conducted to analyze and validate the contributions of different modules within FlowDC. The study highlights FlowDC’s ability to meet the escalating demands in complex text-based image editing, advancing toward more accurate and consistent multi-target image modifications. <div>
arXiv:2512.11395v1 Announce Type: new 
Abstract: With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Reconstruction and Repair for Multi-class Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.11401</link>
<guid>https://arxiv.org/abs/2512.11401</guid>
<content:encoded><![CDATA[
<div> Industrial anomaly detection, multi-class anomaly detection, identity mapping problem, Collaborative Reconstruction and Repair (CRR), feature-level random masking<br /><br />Summary:<br /><br />Industrial anomaly detection aims to identify unknown anomalous patterns that deviate from normal data distributions. This task is challenging, especially in an open-set setting where anomalies are unknown and diverse. Traditional approaches often build separate models for each class, leading to high memory consumption and limited generalizability. To overcome this, the study proposes a unified framework called Collaborative Reconstruction and Repair (CRR) for multi-class anomaly detection. Conventional reconstruction-based networks tend to suffer from an identity mapping problem, where the network simply replicates input features regardless of anomaly presence, resulting in detection failures. CRR addresses this by transforming the reconstruction task into repairation: the decoder is optimized to reconstruct normal samples while repairing synthesized anomalies, generating distinct representations for anomalous regions and similar ones for normal regions relative to the encoder output. Additionally, the method employs feature-level random masking to ensure decoder representations maintain sufficient local information. To further enhance anomaly localization and reduce detection errors from encoder-decoder feature discrepancies, a segmentation network is trained with synthetic anomaly masks as supervision. Extensive experiments on industrial datasets demonstrate that CRR effectively alleviates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection. <div>
arXiv:2512.11401v1 Announce Type: new 
Abstract: Industrial anomaly detection is a challenging open-set task that aims to identify unknown anomalous patterns deviating from normal data distribution. To avoid the significant memory consumption and limited generalizability brought by building separate models per class, we focus on developing a unified framework for multi-class anomaly detection. However, under this challenging setting, conventional reconstruction-based networks often suffer from an identity mapping problem, where they directly replicate input features regardless of whether they are normal or anomalous, resulting in detection failures. To address this issue, this study proposes a novel framework termed Collaborative Reconstruction and Repair (CRR), which transforms the reconstruction to repairation. First, we optimize the decoder to reconstruct normal samples while repairing synthesized anomalies. Consequently, it generates distinct representations for anomalous regions and similar representations for normal areas compared to the encoder's output. Second, we implement feature-level random masking to ensure that the representations from decoder contain sufficient local information. Finally, to minimize detection errors arising from the discrepancies between feature representations from the encoder and decoder, we train a segmentation network supervised by synthetic anomaly masks, thereby enhancing localization performance. Extensive experiments on industrial datasets that CRR effectively mitigates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion</title>
<link>https://arxiv.org/abs/2512.11423</link>
<guid>https://arxiv.org/abs/2512.11423</guid>
<content:encoded><![CDATA[
<div> Keywords: audio-driven avatar, autoregressive diffusion, Progressive Step Bootstrapping, Motion Condition Injection, Unbounded RoPE  

<br /><br />Summary: This paper introduces JoyAvatar, an advanced audio-driven autoregressive model designed for real-time inference and infinite-length video generation in avatar synthesis. The first key contribution is Progressive Step Bootstrapping (PSB), which strategically assigns more denoising steps to initial frames to stabilize video generation and minimize error accumulation over time. The second innovation, Motion Condition Injection (MCI), improves temporal coherence by incorporating noise-corrupted previous frames as motion conditions, ensuring smoother and more consistent motion transitions. Third, Unbounded RoPE via Cache-Resetting (URCR) enables the system to generate videos of unlimited length by dynamically managing positional encoding, overcoming traditional fixed-length limitations. The model consists of 1.3 billion parameters and achieves an efficient generation speed of 16 frames per second on a single GPU, making it suitable for practical applications. In addition to real-time performance and infinite video duration, JoyAvatar delivers competitive results in visual quality, temporal consistency, and lip synchronization, which are critical for high-fidelity avatar generation. Overall, JoyAvatar advances the state of the art in audio-driven avatar generation by addressing key challenges of computational overhead, long-duration video synthesis, and quality degradation in autoregressive diffusion methods. <div>
arXiv:2512.11423v1 Announce Type: new 
Abstract: Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowception: Temporally Expansive Flow Matching for Video Generation</title>
<link>https://arxiv.org/abs/2512.11438</link>
<guid>https://arxiv.org/abs/2512.11438</guid>
<content:encoded><![CDATA[
<div> Flowception, video generation, non-autoregressive, frame insertion, denoising<br /><br />Summary:<br /><br />We introduce Flowception, a cutting-edge video generation framework that operates in a non-autoregressive and variable-length manner. Unlike traditional autoregressive models, Flowception mitigates error accumulation and drift by employing a novel frame insertion mechanism that compresses long-term context efficiently. This mechanism interleaves discrete frame insertions with continuous frame denoising, enabling effective handling of temporal dependencies. Compared to full-sequence flow models, Flowception achieves a three-fold reduction in floating-point operations (FLOPs) during training, making it computationally more efficient. Additionally, the framework supports local attention variations, enhancing its flexibility. A key innovation is the joint learning of video length alongside content, allowing adaptive video generation. Experimental evaluation demonstrates superior performance through improved Fréchet Video Distance (FVD) and VBench metrics relative to both autoregressive and full-sequence baselines. Qualitative results further confirm these quantitative benefits. Finally, Flowception’s design naturally integrates multiple tasks such as image-to-video generation and video interpolation by learning to insert and denoise frames within a sequence, illustrating its versatility across video-related applications. <div>
arXiv:2512.11438v1 Announce Type: new 
Abstract: We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YawDD+: Frame-level Annotations for Accurate Yawn Prediction</title>
<link>https://arxiv.org/abs/2512.11446</link>
<guid>https://arxiv.org/abs/2512.11446</guid>
<content:encoded><![CDATA[
arXiv:2512.11446v1 Announce Type: new 
Abstract: Driver fatigue remains a leading cause of road accidents, with 24\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\% and mAP by 5\% over video-level supervision, achieving 99.34\% classification accuracy and 95.69\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.11458</link>
<guid>https://arxiv.org/abs/2512.11458</guid>
<content:encoded><![CDATA[
arXiv:2512.11458v1 Announce Type: new 
Abstract: We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring MLLM-Diffusion Information Transfer with MetaCanvas</title>
<link>https://arxiv.org/abs/2512.11464</link>
<guid>https://arxiv.org/abs/2512.11464</guid>
<content:encoded><![CDATA[
arXiv:2512.11464v1 Announce Type: new 
Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation</title>
<link>https://arxiv.org/abs/2512.11465</link>
<guid>https://arxiv.org/abs/2512.11465</guid>
<content:encoded><![CDATA[
arXiv:2512.11465v1 Announce Type: new 
Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop</title>
<link>https://arxiv.org/abs/2512.11480</link>
<guid>https://arxiv.org/abs/2512.11480</guid>
<content:encoded><![CDATA[
arXiv:2512.11480v1 Announce Type: new 
Abstract: A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing</title>
<link>https://arxiv.org/abs/2512.11490</link>
<guid>https://arxiv.org/abs/2512.11490</guid>
<content:encoded><![CDATA[
arXiv:2512.11490v1 Announce Type: new 
Abstract: Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition</title>
<link>https://arxiv.org/abs/2512.11503</link>
<guid>https://arxiv.org/abs/2512.11503</guid>
<content:encoded><![CDATA[
arXiv:2512.11503v1 Announce Type: new 
Abstract: Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design</title>
<link>https://arxiv.org/abs/2512.11507</link>
<guid>https://arxiv.org/abs/2512.11507</guid>
<content:encoded><![CDATA[
arXiv:2512.11507v1 Announce Type: new 
Abstract: Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Geometric Understanding and Learned Data Priors in VGGT</title>
<link>https://arxiv.org/abs/2512.11508</link>
<guid>https://arxiv.org/abs/2512.11508</guid>
<content:encoded><![CDATA[
arXiv:2512.11508v1 Announce Type: new 
Abstract: The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction as a Bridge for Event-Based Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.11510</link>
<guid>https://arxiv.org/abs/2512.11510</guid>
<content:encoded><![CDATA[
arXiv:2512.11510v1 Announce Type: new 
Abstract: Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&amp;A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France</title>
<link>https://arxiv.org/abs/2512.11524</link>
<guid>https://arxiv.org/abs/2512.11524</guid>
<content:encoded><![CDATA[
arXiv:2512.11524v1 Announce Type: new 
Abstract: Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning</title>
<link>https://arxiv.org/abs/2512.11534</link>
<guid>https://arxiv.org/abs/2512.11534</guid>
<content:encoded><![CDATA[
arXiv:2512.11534v1 Announce Type: new 
Abstract: Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models</title>
<link>https://arxiv.org/abs/2512.11542</link>
<guid>https://arxiv.org/abs/2512.11542</guid>
<content:encoded><![CDATA[
arXiv:2512.11542v1 Announce Type: new 
Abstract: Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$\alpha$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$\alpha$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2</title>
<link>https://arxiv.org/abs/2512.11548</link>
<guid>https://arxiv.org/abs/2512.11548</guid>
<content:encoded><![CDATA[
arXiv:2512.11548v1 Announce Type: new 
Abstract: Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation</title>
<link>https://arxiv.org/abs/2512.11557</link>
<guid>https://arxiv.org/abs/2512.11557</guid>
<content:encoded><![CDATA[
arXiv:2512.11557v1 Announce Type: new 
Abstract: 3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</title>
<link>https://arxiv.org/abs/2512.11558</link>
<guid>https://arxiv.org/abs/2512.11558</guid>
<content:encoded><![CDATA[
arXiv:2512.11558v1 Announce Type: new 
Abstract: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-temporal Calving Front Segmentation</title>
<link>https://arxiv.org/abs/2512.11560</link>
<guid>https://arxiv.org/abs/2512.11560</guid>
<content:encoded><![CDATA[
arXiv:2512.11560v1 Announce Type: new 
Abstract: The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis</title>
<link>https://arxiv.org/abs/2512.11574</link>
<guid>https://arxiv.org/abs/2512.11574</guid>
<content:encoded><![CDATA[
arXiv:2512.11574v1 Announce Type: new 
Abstract: Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning for Seismic Data Processing</title>
<link>https://arxiv.org/abs/2512.11575</link>
<guid>https://arxiv.org/abs/2512.11575</guid>
<content:encoded><![CDATA[
arXiv:2512.11575v1 Announce Type: new 
Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using GUI Agent for Electronic Design Automation</title>
<link>https://arxiv.org/abs/2512.11611</link>
<guid>https://arxiv.org/abs/2512.11611</guid>
<content:encoded><![CDATA[
arXiv:2512.11611v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Image Compression</title>
<link>https://arxiv.org/abs/2512.11612</link>
<guid>https://arxiv.org/abs/2512.11612</guid>
<content:encoded><![CDATA[
arXiv:2512.11612v1 Announce Type: new 
Abstract: Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling</title>
<link>https://arxiv.org/abs/2512.11624</link>
<guid>https://arxiv.org/abs/2512.11624</guid>
<content:encoded><![CDATA[
arXiv:2512.11624v1 Announce Type: new 
Abstract: Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\mathbf{\Sigma}_{obs} = \mathbf{\Sigma}_{HR} + \mathbf{\Sigma}_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\times$--10$\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint</title>
<link>https://arxiv.org/abs/2512.11645</link>
<guid>https://arxiv.org/abs/2512.11645</guid>
<content:encoded><![CDATA[
arXiv:2512.11645v1 Announce Type: new 
Abstract: We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Pl\"ucker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation</title>
<link>https://arxiv.org/abs/2512.11654</link>
<guid>https://arxiv.org/abs/2512.11654</guid>
<content:encoded><![CDATA[
arXiv:2512.11654v1 Announce Type: new 
Abstract: The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing</title>
<link>https://arxiv.org/abs/2512.11680</link>
<guid>https://arxiv.org/abs/2512.11680</guid>
<content:encoded><![CDATA[
arXiv:2512.11680v1 Announce Type: new 
Abstract: Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection</title>
<link>https://arxiv.org/abs/2512.11683</link>
<guid>https://arxiv.org/abs/2512.11683</guid>
<content:encoded><![CDATA[
arXiv:2512.11683v1 Announce Type: new 
Abstract: Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text images processing system using artificial intelligence models</title>
<link>https://arxiv.org/abs/2512.11691</link>
<guid>https://arxiv.org/abs/2512.11691</guid>
<content:encoded><![CDATA[
arXiv:2512.11691v1 Announce Type: new 
Abstract: This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing</title>
<link>https://arxiv.org/abs/2512.11715</link>
<guid>https://arxiv.org/abs/2512.11715</guid>
<content:encoded><![CDATA[
arXiv:2512.11715v1 Announce Type: new 
Abstract: Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Change Detection in Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.11719</link>
<guid>https://arxiv.org/abs/2512.11719</guid>
<content:encoded><![CDATA[
arXiv:2512.11719v1 Announce Type: new 
Abstract: Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation</title>
<link>https://arxiv.org/abs/2512.11720</link>
<guid>https://arxiv.org/abs/2512.11720</guid>
<content:encoded><![CDATA[
arXiv:2512.11720v1 Announce Type: new 
Abstract: Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images</title>
<link>https://arxiv.org/abs/2512.11722</link>
<guid>https://arxiv.org/abs/2512.11722</guid>
<content:encoded><![CDATA[
arXiv:2512.11722v1 Announce Type: new 
Abstract: We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.11749</link>
<guid>https://arxiv.org/abs/2512.11749</guid>
<content:encoded><![CDATA[
arXiv:2512.11749v1 Announce Type: new 
Abstract: Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting</title>
<link>https://arxiv.org/abs/2512.11763</link>
<guid>https://arxiv.org/abs/2512.11763</guid>
<content:encoded><![CDATA[
arXiv:2512.11763v1 Announce Type: new 
Abstract: Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.
  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title>
<link>https://arxiv.org/abs/2512.11771</link>
<guid>https://arxiv.org/abs/2512.11771</guid>
<content:encoded><![CDATA[
arXiv:2512.11771v1 Announce Type: new 
Abstract: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator</title>
<link>https://arxiv.org/abs/2512.11782</link>
<guid>https://arxiv.org/abs/2512.11782</guid>
<content:encoded><![CDATA[
arXiv:2512.11782v1 Announce Type: new 
Abstract: Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs</title>
<link>https://arxiv.org/abs/2512.11791</link>
<guid>https://arxiv.org/abs/2512.11791</guid>
<content:encoded><![CDATA[
arXiv:2512.11791v1 Announce Type: new 
Abstract: Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</title>
<link>https://arxiv.org/abs/2512.11792</link>
<guid>https://arxiv.org/abs/2512.11792</guid>
<content:encoded><![CDATA[
arXiv:2512.11792v1 Announce Type: new 
Abstract: Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particulate: Feed-Forward 3D Object Articulation</title>
<link>https://arxiv.org/abs/2512.11798</link>
<guid>https://arxiv.org/abs/2512.11798</guid>
<content:encoded><![CDATA[
arXiv:2512.11798v1 Announce Type: new 
Abstract: We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</title>
<link>https://arxiv.org/abs/2512.11799</link>
<guid>https://arxiv.org/abs/2512.11799</guid>
<content:encoded><![CDATA[
arXiv:2512.11799v1 Announce Type: new 
Abstract: Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</title>
<link>https://arxiv.org/abs/2512.11800</link>
<guid>https://arxiv.org/abs/2512.11800</guid>
<content:encoded><![CDATA[
arXiv:2512.11800v1 Announce Type: new 
Abstract: The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.10966</link>
<guid>https://arxiv.org/abs/2512.10966</guid>
<content:encoded><![CDATA[
arXiv:2512.10966v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</title>
<link>https://arxiv.org/abs/2512.11047</link>
<guid>https://arxiv.org/abs/2512.11047</guid>
<content:encoded><![CDATA[
arXiv:2512.11047v1 Announce Type: cross 
Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles</title>
<link>https://arxiv.org/abs/2512.11145</link>
<guid>https://arxiv.org/abs/2512.11145</guid>
<content:encoded><![CDATA[
arXiv:2512.11145v1 Announce Type: cross 
Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.11194</link>
<guid>https://arxiv.org/abs/2512.11194</guid>
<content:encoded><![CDATA[
arXiv:2512.11194v1 Announce Type: cross 
Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy</title>
<link>https://arxiv.org/abs/2512.11218</link>
<guid>https://arxiv.org/abs/2512.11218</guid>
<content:encoded><![CDATA[
arXiv:2512.11218v1 Announce Type: cross 
Abstract: The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Aware Multi-Expert Architecture For Lifelong Deep Learning</title>
<link>https://arxiv.org/abs/2512.11243</link>
<guid>https://arxiv.org/abs/2512.11243</guid>
<content:encoded><![CDATA[
arXiv:2512.11243v1 Announce Type: cross 
Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</title>
<link>https://arxiv.org/abs/2512.11399</link>
<guid>https://arxiv.org/abs/2512.11399</guid>
<content:encoded><![CDATA[
arXiv:2512.11399v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Baseline: Examining Baseline Effects on Explainability Metrics</title>
<link>https://arxiv.org/abs/2512.11433</link>
<guid>https://arxiv.org/abs/2512.11433</guid>
<content:encoded><![CDATA[
arXiv:2512.11433v1 Announce Type: cross 
Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</title>
<link>https://arxiv.org/abs/2512.11532</link>
<guid>https://arxiv.org/abs/2512.11532</guid>
<content:encoded><![CDATA[
arXiv:2512.11532v1 Announce Type: cross 
Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model</title>
<link>https://arxiv.org/abs/2512.11582</link>
<guid>https://arxiv.org/abs/2512.11582</guid>
<content:encoded><![CDATA[
arXiv:2512.11582v1 Announce Type: cross 
Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastics of shapes and Kunita flows</title>
<link>https://arxiv.org/abs/2512.11676</link>
<guid>https://arxiv.org/abs/2512.11676</guid>
<content:encoded><![CDATA[
arXiv:2512.11676v1 Announce Type: cross 
Abstract: Stochastic processes of evolving shapes are used in applications including evolutionary biology, where morphology changes stochastically as a function of evolutionary processes. Due to the non-linear and often infinite-dimensional nature of shape spaces, the mathematical construction of suitable stochastic shape processes is far from immediate. We define and formalize properties that stochastic shape processes should ideally satisfy to be compatible with the shape structure, and we link this to Kunita flows that, when acting on shape spaces, induce stochastic processes that satisfy these criteria by their construction. We couple this with a survey of other relevant shape stochastic processes and show how bridge sampling techniques can be used to condition shape stochastic processes on observed data thereby allowing for statistical inference of parameters of the stochastic dynamics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particle Image Velocimetry Refinement via Consensus ADMM</title>
<link>https://arxiv.org/abs/2512.11695</link>
<guid>https://arxiv.org/abs/2512.11695</guid>
<content:encoded><![CDATA[
arXiv:2512.11695v1 Announce Type: cross 
Abstract: Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images</title>
<link>https://arxiv.org/abs/2512.11745</link>
<guid>https://arxiv.org/abs/2512.11745</guid>
<content:encoded><![CDATA[
arXiv:2512.11745v1 Announce Type: cross 
Abstract: Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</title>
<link>https://arxiv.org/abs/2512.11797</link>
<guid>https://arxiv.org/abs/2512.11797</guid>
<content:encoded><![CDATA[
arXiv:2512.11797v1 Announce Type: cross 
Abstract: The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Action Counting with Dynamic Queries</title>
<link>https://arxiv.org/abs/2403.01543</link>
<guid>https://arxiv.org/abs/2403.01543</guid>
<content:encoded><![CDATA[
arXiv:2403.01543v4 Announce Type: replace 
Abstract: Temporal repetition counting aims to quantify the repeated action cycles within a video. The majority of existing methods rely on the similarity correlation matrix to characterize the repetitiveness of actions, but their scalability is hindered due to the quadratic computational complexity. In this work, we introduce a novel approach that employs an action query representation to localize repeated action cycles with linear computational complexity. Based on this representation, we further develop two key components to tackle the essential challenges of temporal repetition counting. Firstly, to facilitate open-set action counting, we propose the dynamic update scheme on action queries. Unlike static action queries, this approach dynamically embeds video features into action queries, offering a more flexible and generalizable representation. Secondly, to distinguish between actions of interest and background noise actions, we incorporate inter-query contrastive learning to regularize the video representations corresponding to different action queries. As a result, our method significantly outperforms previous works, particularly in terms of long video sequences, unseen actions, and actions at various speeds. On the challenging RepCountA benchmark, we outperform the state-of-the-art method TransRAC by 26.5% in OBO accuracy, with a 22.7% mean error decrease and 94.1% computational burden reduction. Code is available at https://github.com/lizishi/DeTRC.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecDETR: A transformer-based hyperspectral point object detection network</title>
<link>https://arxiv.org/abs/2405.10148</link>
<guid>https://arxiv.org/abs/2405.10148</guid>
<content:encoded><![CDATA[
arXiv:2405.10148v4 Announce Type: replace 
Abstract: Hyperspectral target detection (HTD) aims to identify specific materials based on spectral information in hyperspectral imagery and can detect extremely small-sized objects, some of which occupy a smaller than one-pixel area. However, existing HTD methods are developed based on per-pixel binary classification, neglecting the three-dimensional cube structure of hyperspectral images (HSIs) that integrates both spatial and spectral dimensions. The synergistic existence of spatial and spectral features in HSIs enable objects to simultaneously exhibit both, yet the per-pixel HTD framework limits the joint expression of these features. In this paper, we rethink HTD from the perspective of spatial-spectral synergistic representation and propose hyperspectral point object detection as an innovative task framework. We introduce SpecDETR, the first specialized network for hyperspectral multi-class point object detection, which eliminates dependence on pre-trained backbone networks commonly required by vision-based object detectors. SpecDETR uses a multi-layer Transformer encoder with self-excited subpixel-scale attention modules to directly extract deep spatial-spectral joint features from hyperspectral cubes. We develop a simulated hyperspectral point object detection benchmark termed SPOD, and for the first time, evaluate and compare the performance of visual object detection networks and HTD methods on hyperspectral point object detection. Extensive experiments demonstrate that our proposed SpecDETR outperforms SOTA visual object detection networks and HTD methods. Our code and dataset are available at https://github.com/ZhaoxuLi123/SpecDETR.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual-Friendly Concept Protection via Selective Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2408.08518</link>
<guid>https://arxiv.org/abs/2408.08518</guid>
<content:encoded><![CDATA[
arXiv:2408.08518v3 Announce Type: replace 
Abstract: Personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. Researchers attempt to prevent malicious personalization using adversarial perturbations. However, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. They utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. In this work, we propose the Visual-Friendly Concept Protection (VCPro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. To ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the Lagrangian multiplier method. Qualitative and quantitative experiments validate that VCPro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Text-to-Image Generation with Reference Guidance</title>
<link>https://arxiv.org/abs/2411.16713</link>
<guid>https://arxiv.org/abs/2411.16713</guid>
<content:encoded><![CDATA[
arXiv:2411.16713v2 Announce Type: replace 
Abstract: Text-to-image diffusion models have demonstrated tremendous success in synthesizing visually stunning images given textual instructions. Despite remarkable progress in creating high-fidelity visuals, text-to-image models can still struggle with precisely rendering subjects, such as text spelling. To address this challenge, this paper explores using additional conditions of an image that provides visual guidance of the particular subjects for diffusion models to generate. In addition, this reference condition empowers the model to be conditioned in ways that the vocabularies of the text tokenizer cannot adequately represent, and further extends the model's generalization to novel capabilities such as generating non-English text spellings. We develop several small-scale expert plugins that efficiently endow a Stable Diffusion model with the capability to take different references. Each plugin is trained with auxiliary networks and loss functions customized for applications such as English scene-text generation, multi-lingual scene-text generation, and logo-image generation. Our expert plugins demonstrate superior results than the existing methods on all tasks, each containing only 28.55M trainable parameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion</title>
<link>https://arxiv.org/abs/2503.01220</link>
<guid>https://arxiv.org/abs/2503.01220</guid>
<content:encoded><![CDATA[
arXiv:2503.01220v3 Announce Type: replace 
Abstract: Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Using emerging tissue profiling technologies, researchers charted comprehensive atlases of mammalian brain with sub-cellular resolution and spatially resolved transcriptomic data. However, these tera-scale volumetric atlases pose computational challenges for modeling intricate brain structures within the native spatial context. We propose \textbf{Tera-MIND}, a novel generative framework capable of simulating \textbf{Tera}-scale \textbf{M}ouse bra\textbf{IN}s in 3D using a patch-based and boundary-aware \textbf{D}iffusion model. Taking spatial gene expression as conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D \textit{gene}-\textit{gene} self-attention, we identify spatial molecular interactions for key transcriptomic pathways, including glutamatergic and dopaminergic neuronal systems. Lastly, we showcase the translational applicability of Tera-MIND on previously unseen human brain samples. Tera-MIND offers an efficient generative modeling of whole virtual organisms, paving the way for integrative applications in biomedical research. Project website: https://musikisomorphie.github.io/Tera-MIND.html
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis</title>
<link>https://arxiv.org/abs/2503.11893</link>
<guid>https://arxiv.org/abs/2503.11893</guid>
<content:encoded><![CDATA[
arXiv:2503.11893v3 Announce Type: replace 
Abstract: The concept of waterbody style transfer remains largely unexplored in the underwater imaging and vision literature. Traditional image style transfer (STx) methods primarily focus on artistic and photorealistic blending, often failing to preserve object and scene geometry in images captured in high-scattering mediums such as underwater. The wavelength-dependent nonlinear attenuation and depth-dependent backscattering artifacts further complicate learning underwater image STx from unpaired data. This paper introduces UStyle, the first data-driven learning framework for transferring waterbody styles across underwater images without requiring prior reference images or scene information. We propose a novel depth-aware whitening and coloring transform (DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure perceptually consistent stylization while preserving scene structure. To enhance style transfer quality, we incorporate carefully designed loss functions that guide UStyle to maintain colorfulness, lightness, structural integrity, and frequency-domain characteristics, as well as high-level content in VGG and CLIP (contrastive language-image pretraining) feature spaces. By addressing domain-specific challenges, UStyle provides a robust framework for no-reference underwater image STx, surpassing state-of-the-art (SOTA) methods that rely solely on end-to-end reconstruction loss. Furthermore, we introduce the UF7D dataset, a curated collection of high-resolution underwater images spanning seven distinct waterbody styles, establishing a benchmark to support future research in underwater image STx. The UStyle inference pipeline and UF7D dataset are released at: https://github.com/uf-robopi/UStyle.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Free-Lunch Color-Texture Disentanglement for Stylized Image Generation</title>
<link>https://arxiv.org/abs/2503.14275</link>
<guid>https://arxiv.org/abs/2503.14275</guid>
<content:encoded><![CDATA[
arXiv:2503.14275v4 Announce Type: replace 
Abstract: Recent advances in Text-to-Image (T2I) diffusion models have transformed image generation, enabling significant progress in stylized generation using only a few style reference images. However, current diffusion-based methods struggle with fine-grained style customization due to challenges in controlling multiple style attributes, such as color and texture. This paper introduces the first tuning-free approach to achieve free-lunch color-texture disentanglement in stylized T2I generation, addressing the need for independently controlled style elements for the Disentangled Stylized Image Generation (DisIG) problem. Our approach leverages the Image-Prompt Additivity property in the CLIP image embedding space to develop techniques for separating and extracting Color-Texture Embeddings (CTE) from individual color and texture reference images. To ensure that the color palette of the generated image aligns closely with the color reference, we apply a whitening and coloring transformation to enhance color consistency. Additionally, to prevent texture loss due to the signal-leak bias inherent in diffusion training, we introduce a noise term that preserves textural fidelity during the Regularized Whitening and Coloring Transformation (RegWCT). Through these methods, our Style Attributes Disentanglement approach (SADis) delivers a more precise and customizable solution for stylized image generation. Experiments on images from the WikiArt and StyleDrop datasets demonstrate that, both qualitatively and quantitatively, SADis surpasses state-of-the-art stylization methods in the DisIG task.Code is released at https://deepffff.github.io/sadis.github.io/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2503.24379</link>
<guid>https://arxiv.org/abs/2503.24379</guid>
<content:encoded><![CDATA[
arXiv:2503.24379v2 Announce Type: replace 
Abstract: To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COSMO-INR: Complex Sinusoidal Modulation for Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2505.11640</link>
<guid>https://arxiv.org/abs/2505.11640</guid>
<content:encoded><![CDATA[
arXiv:2505.11640v3 Announce Type: replace 
Abstract: Implicit neural representations (INRs) are a powerful paradigm for modeling data, offering a continuous alternative to discrete signal representations. Their ability to compactly encode complex signals has led to strong performance in many vision tasks. Prior work shows INR performance is highly sensitive to the choice of activation function in the underlying multilayer perceptron, yet the theoretical reasons remain unclear. Key limitations also persist, including spectral bias (reduced sensitivity to high-frequency content), limited robustness to noise, and difficulty capturing local and global structure jointly. We analyze INR signal representation using harmonic analysis and Chebyshev polynomials. We prove that modulating activation functions with a complex sinusoidal term yields richer and more complete spectral support throughout the network. Building on this, we introduce a new activation function tailored to INRs and validate our theory using Chebyshev analysis and extensive experiments. We additionally use a regularized deep prior, extracted from a task-specific model, to adapt the activations, further improving convergence speed and stability. Across image reconstruction (average PSNR gain of +5.67 dB over the nearest counterpart on a diverse dataset), denoising (+0.46 dB PSNR), super-resolution (+0.64 dB over the nearest SOTA method for 6X upscaling), inpainting, and 3D shape reconstruction, our activation consistently outperforms existing state-of-the-art alternatives.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling</title>
<link>https://arxiv.org/abs/2505.17982</link>
<guid>https://arxiv.org/abs/2505.17982</guid>
<content:encoded><![CDATA[
arXiv:2505.17982v5 Announce Type: replace 
Abstract: Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual/textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at https://github.com/bryanwong17/HiVE-MIL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal In-Context Fine-Tuning with Temporal Reasoning for Versatile Control of Video Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00996</link>
<guid>https://arxiv.org/abs/2506.00996</guid>
<content:encoded><![CDATA[
arXiv:2506.00996v2 Announce Type: replace 
Abstract: Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCA-Video: Motion-Aware Concept Alignment for Consistent Video Editing</title>
<link>https://arxiv.org/abs/2506.01004</link>
<guid>https://arxiv.org/abs/2506.01004</guid>
<content:encoded><![CDATA[
arXiv:2506.01004v2 Announce Type: replace 
Abstract: We present MoCA-Video, a training-free framework for semantic mixing in videos. Operating in the latent space of a frozen video diffusion model, MoCA-Video utilizes class-agnostic segmentation with diagonal denoising scheduler to localize and track the target object across frames. To ensure temporal stability under semantic shifts, we introduce momentum-based correction to approximate novel hybrid distributions beyond trained data distribution, alongside a light gamma residual module that smooths out visual artifacts. We evaluate model's performance using SSIM, LPIPS, and a proposed metric, \metricnameabbr, which quantifies semantic alignment between reference and output. Extensive evaluation demonstrates that our model consistently outperforms both training-free and trained baselines, achieving superior semantic mixing and temporal coherence without retraining. Results establish that structured manipulation of diffusion noise trajectories enables controllable and high-quality video editing under semantic shifts.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing</title>
<link>https://arxiv.org/abs/2506.05046</link>
<guid>https://arxiv.org/abs/2506.05046</guid>
<content:encoded><![CDATA[
arXiv:2506.05046v2 Announce Type: replace 
Abstract: Text-driven video editing aims to modify video content based on natural language instructions. While recent training-free methods have leveraged pretrained diffusion models, they often rely on an inversion-editing paradigm. This paradigm maps the video to a latent space before editing. However, the inversion process is not perfectly accurate, often compromising appearance fidelity and motion consistency. To address this, we introduce FlowDirector, a novel training-free and inversion-free video editing framework. Our framework models the editing process as a direct evolution in the data space. It guides the video to transition smoothly along its inherent spatio-temporal manifold using an ordinary differential equation (ODE), thereby avoiding the inaccurate inversion step. From this foundation, we introduce three flow correction strategies for appearance, motion, and stability: 1) Direction-aware flow correction amplifies components that oppose the source direction and removes irrelevant terms, breaking conservative streamlines and enabling stronger structural and textural changes. 2) Motion-appearance decoupling optimizes motion agreement as an energy term at each timestep, significantly improving consistency and motion transfer. 3) Differential averaging guidance strategy leverages differences among multiple candidate flows to approximate a low variance regime at low cost, suppressing artifacts and stabilizing the trajectory. Extensive experiments across various editing tasks and benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction following, temporal consistency, and background preservation, establishing an efficient new paradigm for coherent video editing without inversion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.08710</link>
<guid>https://arxiv.org/abs/2506.08710</guid>
<content:encoded><![CDATA[
arXiv:2506.08710v3 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets are released at https://scenesplatpp.gaussianworld.ai/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Diffusion with Test-Time Training on Efficient Image Restoration</title>
<link>https://arxiv.org/abs/2506.14541</link>
<guid>https://arxiv.org/abs/2506.14541</guid>
<content:encoded><![CDATA[
arXiv:2506.14541v3 Announce Type: replace 
Abstract: Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-World Object Counting in Videos</title>
<link>https://arxiv.org/abs/2506.15368</link>
<guid>https://arxiv.org/abs/2506.15368</guid>
<content:encoded><![CDATA[
arXiv:2506.15368v2 Announce Type: replace 
Abstract: We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and objects of similar appearance, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model, to enable automated open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for this novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at https://www.robots.ox.ac.uk/~vgg/research/countvid/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MADrive: Memory-Augmented Driving Scene Modeling</title>
<link>https://arxiv.org/abs/2506.21520</link>
<guid>https://arxiv.org/abs/2506.21520</guid>
<content:encoded><![CDATA[
arXiv:2506.21520v2 Announce Type: replace 
Abstract: Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\sim}70$K 360{\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
arXiv:2506.21546v3 Announce Type: replace 
Abstract: Segmentation Vision-Language Models (VLMs) have significantly advanced grounded visual understanding, yet they remain prone to pixel-grounding hallucinations, producing masks for incorrect objects or for objects that are entirely absent. Existing evaluations rely almost entirely on text- or label-based perturbations, which check only whether the predicted mask matches the queried label. Such evaluations overlook the spatial footprint and severity of hallucination and therefore fail to reveal vision-driven hallucinations, which are more challenging and more prevalent. To address this gap, we formalize the task of Counterfactual Segmentation Reasoning (CSR), where a model must segment the referenced object in the factual image and abstain in its counterfactual counterpart. To support this task, we curate HalluSegBench, the first large-scale benchmark to diagnose referring and reasoning expression segmentation hallucinations using controlled visual counterfactuals, alongside new evaluation metrics that measure hallucination severity and disentangle vision- and language-driven failure modes. We further introduce RobustSeg, a segmentation VLM trained with counterfactual fine-tuning (CFT) to learn when to segment and when to abstain. Experimental results confirm RobustSeg reduces hallucinations by 30%, while improving segmentation performance on FP-RefCOCO(+/g).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing</title>
<link>https://arxiv.org/abs/2507.04678</link>
<guid>https://arxiv.org/abs/2507.04678</guid>
<content:encoded><![CDATA[
arXiv:2507.04678v2 Announce Type: replace 
Abstract: Spatiotemporal image generation is a highly meaningful task, which can generate future scenes conditioned on given observations. However, existing change generation methods can only handle event-driven changes (e.g., new buildings) and fail to model cross-temporal variations (e.g., seasonal shifts). In this work, we propose ChangeBridge, a conditional spatiotemporal image generation model for remote sensing. Given pre-event images and multimodal event controls, ChangeBridge generates post-event scenes that are both spatially and temporally coherent. The core idea is a drift-asynchronous diffusion bridge. Specifically, it consists of three main modules: a) Composed bridge initialization, which replaces noise initialization. It starts the diffusion from a composed pre-event state, modeling a diffusion bridge process. b) Asynchronous Drift Diffusion, which uses a pixel-wise drift map, assigning different drift magnitudes to event and temporal evolution. This enables differentiated generation during the pre-to-post transition. c) Drift-Aware Denoising, which embeds the drift map into the denoising network, guiding drift-aware reconstruction. Experiments show that ChangeBridge can generate better cross-spatiotemporal aligned scenarios compared to state-of-the-art methods. Additionally, ChangeBridge shows great potential for land-use planning and as a data generation engine for a series of change detection tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning</title>
<link>https://arxiv.org/abs/2507.05029</link>
<guid>https://arxiv.org/abs/2507.05029</guid>
<content:encoded><![CDATA[
arXiv:2507.05029v2 Announce Type: replace 
Abstract: Inertial mass plays a crucial role in robotic applications such as object grasping, manipulation, and simulation, providing a strong prior for planning and control. Accurately estimating an object's mass before interaction can significantly enhance the performance of various robotic tasks. However, mass estimation using only vision sensors is a relatively underexplored area. This paper proposes a novel approach combining sparse point-cloud data from depth images with RGB images to estimate the mass of objects. We evaluate a range of point-cloud processing architectures, alongside RGB-only methods. To overcome the limited availability of training data, we create a synthetic dataset using ShapeNetSem 3D models, simulating RGBD images via a Kinect camera. This synthetic data is used to train an image generation model for estimating dense depth maps, which we then use to augment an existing dataset of images paired with mass values. Our approach significantly outperforms existing benchmarks across all evaluated metrics. The data generation (https://github.com/RavineWindteer/ShapenetSem-to-RGBD) as well as the training of the depth estimator (https://github.com/RavineWindteer/GLPDepth-Edited) and the mass estimator (https://github.com/RavineWindteer/Depth-mass-estimator) are available online.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Annotation-Free Reinforcement Learning Query Rewriting via Verifiable Search Reward</title>
<link>https://arxiv.org/abs/2507.23242</link>
<guid>https://arxiv.org/abs/2507.23242</guid>
<content:encoded><![CDATA[
arXiv:2507.23242v2 Announce Type: replace 
Abstract: Optimizing queries for Retrieval-Augmented Generation (RAG) systems poses a significant challenge, particularly across diverse modal indices. We introduce RL-QR, a novel annotation-free reinforcement learning framework for query rewriting that eliminates the need for costly human-annotated data. By leveraging verifiable search rewards derived from index-aligned synthetic queries, RL-QR overcomes human-annotation dependencies, extending its applicability to various modalities and index domains. Experimental results demonstrate the framework's robustness, achieving substantial retrieval performance gains of up to 3.9$\times$ on lexical retrievers and 3.5$\times$ on semantic retrievers on the MTEB VIDORE V2 benchmark for unstructured visual documents, along with consistent 5\% to 10\% improvements on MS MARCO v2.1 and internal industrial datasets.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection</title>
<link>https://arxiv.org/abs/2508.02386</link>
<guid>https://arxiv.org/abs/2508.02386</guid>
<content:encoded><![CDATA[
arXiv:2508.02386v2 Announce Type: replace 
Abstract: We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised instance segmentation and object detection. COLER first uses our developed CutOnce to generate coarse pseudo labels, then enables the detector to learn from these masks. CutOnce applies Normalized Cut (NCut) only once and does not rely on any clustering methods (e.g., K-Means), but it can generate multiple object masks in an image. Our work opens a new direction for NCut algorithm in multi-object segmentation. We have designed several novel yet simple modules that not only allow CutOnce to fully leverage the object discovery capabilities of self-supervised model, but also free it from reliance on mask post-processing. During training, COLER achieves strong performance without requiring specially designed loss functions for pseudo labels, and its performance is further improved through self-training. COLER is a zero-shot unsupervised model that outperforms previous state-of-the-art methods on multiple benchmarks. We believe our method can help advance the field of unsupervised object localization. Code is available at: https://github.com/Quantumcraft616/COLER.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering</title>
<link>https://arxiv.org/abs/2508.11272</link>
<guid>https://arxiv.org/abs/2508.11272</guid>
<content:encoded><![CDATA[
arXiv:2508.11272v2 Announce Type: replace 
Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited -- compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Model's understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise Matters: Optimizing Matching Noise for Diffusion Classifiers</title>
<link>https://arxiv.org/abs/2508.11330</link>
<guid>https://arxiv.org/abs/2508.11330</guid>
<content:encoded><![CDATA[
arXiv:2508.11330v2 Announce Type: replace 
Abstract: Although today's pretrained discriminative vision-language models (e.g., CLIP) have demonstrated strong perception abilities, such as zero-shot image classification, they also suffer from the bag-of-words problem and spurious bias. To mitigate these problems, some pioneering studies leverage powerful generative models (e.g., pretrained diffusion models) to realize generalizable image classification, dubbed Diffusion Classifier (DC). Specifically, by randomly sampling a Gaussian noise, DC utilizes the differences of denoising effects with different category conditions to classify categories. Unfortunately, an inherent and notorious weakness of existing DCs is noise instability: different random sampled noises lead to significant performance changes. To achieve stable classification performance, existing DCs always ensemble the results of hundreds of sampled noises, which significantly reduces the classification speed. To this end, we firstly explore the role of noise in DC, and conclude that: there are some ``good noises'' that can relieve the instability. Meanwhile, we argue that these good noises should meet two principles: Frequency Matching and Spatial Matching. Regarding both principles, we propose a novel Noise Optimization method to learn matching (i.e., good) noise for DCs: NoOp. For frequency matching, NoOp first optimizes a dataset-specific noise: Given a dataset and a timestep t, optimize one randomly initialized parameterized noise. For Spatial Matching, NoOp trains a Meta-Network that adopts an image as input and outputs image-specific noise offset. The sum of optimized noise and noise offset will be used in DC to replace random noise. Extensive ablations on various datasets demonstrated the effectiveness of NoOp.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</title>
<link>https://arxiv.org/abs/2508.17186</link>
<guid>https://arxiv.org/abs/2508.17186</guid>
<content:encoded><![CDATA[
arXiv:2508.17186v2 Announce Type: replace 
Abstract: Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object changes (e.g., objects appearing or disappearing) from background variations (e.g., environmental changes due to light, weather, or seasonal shifts) in paired satellite images, relying only on paired image (i.e., image-level) classification labels. This technique significantly reduces the need for dense annotations required in fully-supervised change detection. However, as image-level supervision only indicates whether objects have changed in a scene, WSCD methods often misclassify background variations as object changes, especially in complex remote-sensing scenarios. In this work, we propose an Adversarial Class Prompting (AdvCP) method to address this co-occurring noise problem, including two phases: a) Adversarial Prompt Mining: After each training iteration, we introduce adversarial prompting perturbations, using incorrect one-hot image-level labels to activate erroneous feature mappings. This process reveals co-occurring adversarial samples under weak supervision, namely background variation features that are likely to be misclassified as object changes. b) Adversarial Sample Rectification: We integrate these adversarially prompt-activated pixel samples into training by constructing an online global prototype. This prototype is built from an exponentially weighted moving average of the current batch and all historical training data. Our AdvCP can be seamlessly integrated into current WSCD methods without adding additional inference cost. Experiments on ConvNet, Transformer, and Segment Anything Model (SAM)-based baselines demonstrate significant performance enhancements. Furthermore, we demonstrate the generalizability of AdvCP to other multi-class weakly-supervised dense prediction scenarios. Code is available at https://github.com/zhenghuizhao/AdvCP
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v4 Announce Type: replace 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Defocus Blur Control for Generative Image Models</title>
<link>https://arxiv.org/abs/2510.06215</link>
<guid>https://arxiv.org/abs/2510.06215</guid>
<content:encoded><![CDATA[
arXiv:2510.06215v2 Announce Type: replace 
Abstract: Current text-to-image diffusion models excel at generating diverse, high-quality images, yet they struggle to incorporate fine-grained camera metadata such as precise aperture settings. In this work, we introduce a novel text-to-image diffusion framework that leverages camera metadata, or EXIF data, which is often embedded in image files, with an emphasis on generating controllable lens blur. Our method mimics the physical image formation process by first generating an all-in-focus image, estimating its monocular depth, predicting a plausible focus distance with a novel focus distance transformer, and then forming a defocused image with an existing differentiable lens blur model. Gradients flow backwards through this whole process, allowing us to learn without explicit supervision to generate defocus effects based on content elements and the provided EXIF data. At inference time, this enables precise interactive user control over defocus effects while preserving scene contents, which is not achievable with existing diffusion models. Experimental results demonstrate that our model enables superior fine-grained control without altering the depicted scene.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.08442</link>
<guid>https://arxiv.org/abs/2510.08442</guid>
<content:encoded><![CDATA[
arXiv:2510.08442v2 Announce Type: replace 
Abstract: Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.52x improvement in sample efficiency and can solve challenging tasks from the ManiSkill3 benchmark that the baseline fails to learn, without modifying the underlying algorithm or hyperparameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression</title>
<link>https://arxiv.org/abs/2510.11344</link>
<guid>https://arxiv.org/abs/2510.11344</guid>
<content:encoded><![CDATA[
arXiv:2510.11344v2 Announce Type: replace 
Abstract: Spatial Transcriptomics (ST) enables the measurement of gene expression while preserving spatial information, offering critical insights into tissue architecture and disease pathology. Recent developments have explored the use of hematoxylin and eosin (H&amp;E)-stained whole-slide images (WSIs) to predict transcriptome-wide gene expression profiles through deep neural networks. This task is commonly framed as a regression problem, where each input corresponds to a localized image patch extracted from the WSI. However, predicting spatial gene expression from histological images remains a challenging problem due to the significant modality gap between visual features and molecular signals. Recent studies have attempted to incorporate both local and global information into predictive models. Nevertheless, existing methods still suffer from two key limitations: (1) insufficient granularity in local feature extraction, and (2) inadequate coverage of global spatial context. In this work, we propose a novel framework, MMAP (Multi-MAgnification and Prototype-enhanced architecture), that addresses both challenges simultaneously. To enhance local feature granularity, MMAP leverages multi-magnification patch representations that capture fine-grained histological details. To improve global contextual understanding, it learns a set of latent prototype embeddings that serve as compact representations of slide-level information. Extensive experimental results demonstrate that MMAP consistently outperforms all existing state-of-the-art methods across multiple evaluation metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation Coefficient (PCC).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net</title>
<link>https://arxiv.org/abs/2510.18326</link>
<guid>https://arxiv.org/abs/2510.18326</guid>
<content:encoded><![CDATA[
arXiv:2510.18326v2 Announce Type: replace 
Abstract: The increasing frequency of natural and human-induced disasters necessitates advanced visual recognition techniques capable of analyzing critical photographic data. With progress in artificial intelligence and resilient computational systems, rapid and accurate disaster classification has become crucial for efficient rescue operations. However, visual recognition in disaster contexts faces significant challenges due to limited and diverse data from the difficulties in collecting and curating comprehensive, high-quality disaster imagery. Few-Shot Learning (FSL) provides a promising approach to data scarcity, yet current FSL research mainly relies on generic benchmark datasets lacking remote-sensing disaster imagery, limiting its practical effectiveness. Moreover, disaster images exhibit high intra-class variation and inter-class similarity, hindering the performance of conventional metric-based FSL methods. To address these issues, this paper introduces the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), which linearly combines the Bhattacharyya coefficient and Hellinger distances to compare and aggregate feature probability distributions for robust prototype formation. The Bhattacharyya coefficient serves as a contrastive margin that enhances inter-class separability, while the Hellinger distance regularizes same-class alignment. This framework parallels contrastive learning but operates over probability distributions rather than embedded feature points. Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss is proposed as a distributional counterpart to cosine similarity loss, used jointly with categorical cross-entropy to significantly improve FSL performance. Experiments on four FSL benchmarks and two disaster image datasets demonstrate the superior effectiveness and generalization of ATTBHFA-Net compared to existing approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models</title>
<link>https://arxiv.org/abs/2511.07299</link>
<guid>https://arxiv.org/abs/2511.07299</guid>
<content:encoded><![CDATA[
arXiv:2511.07299v2 Announce Type: replace 
Abstract: Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM</title>
<link>https://arxiv.org/abs/2511.07438</link>
<guid>https://arxiv.org/abs/2511.07438</guid>
<content:encoded><![CDATA[
arXiv:2511.07438v2 Announce Type: replace 
Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions: one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</title>
<link>https://arxiv.org/abs/2511.13387</link>
<guid>https://arxiv.org/abs/2511.13387</guid>
<content:encoded><![CDATA[
arXiv:2511.13387v3 Announce Type: replace 
Abstract: Denoising diffusion models have emerged as a dominant paradigm in image generation. Discretizing image data into tokens is a critical step for effectively integrating images with Transformer and other architectures. Although the Denoising Diffusion Codebook Models (DDCM) pioneered the use of pre-trained diffusion models for image tokenization, it strictly relies on the traditional discrete-time DDPM architecture. Consequently, it fails to adapt to modern continuous-time variants-such as Flow Matching and Consistency Models-and suffers from inefficient sampling in high-noise regions. To address these limitations, this paper proposes the Generalized Denoising Diffusion Codebook Models (gDDCM). We establish a unified theoretical framework and introduce a generic "De-noise and Back-trace" sampling strategy. By integrating a deterministic ODE denoising step with a residual-aligned noise injection step, our method resolves the challenge of adaptation. Furthermore, we introduce a backtracking parameter $p$ and significantly enhance tokenization ability. Extensive experiments on CIFAR10 and LSUN Bedroom datasets demonstrate that gDDCM achieves comprehensive compatibility with mainstream diffusion variants and significantly outperforms DDCM in terms of reconstruction quality and perceptual fidelity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Finer the Better: Towards Granular-aware Open-set Domain Generalization</title>
<link>https://arxiv.org/abs/2511.16979</link>
<guid>https://arxiv.org/abs/2511.16979</guid>
<content:encoded><![CDATA[
arXiv:2511.16979v2 Announce Type: replace 
Abstract: Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title>
<link>https://arxiv.org/abs/2511.20629</link>
<guid>https://arxiv.org/abs/2511.20629</guid>
<content:encoded><![CDATA[
arXiv:2511.20629v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defense That Attacks: How Robust Models Become Better Attackers</title>
<link>https://arxiv.org/abs/2512.02830</link>
<guid>https://arxiv.org/abs/2512.02830</guid>
<content:encoded><![CDATA[
arXiv:2512.02830v3 Announce Type: replace 
Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant symmetry-aware head pose estimation for fetal MRI</title>
<link>https://arxiv.org/abs/2512.04890</link>
<guid>https://arxiv.org/abs/2512.04890</guid>
<content:encoded><![CDATA[
arXiv:2512.04890v3 Announce Type: replace 
Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Learning for Scalable Representation of High-Dimensional Medical Data</title>
<link>https://arxiv.org/abs/2409.13115</link>
<guid>https://arxiv.org/abs/2409.13115</guid>
<content:encoded><![CDATA[
arXiv:2409.13115v2 Announce Type: replace-cross 
Abstract: Integrating artificial intelligence (AI) with healthcare data is rapidly transforming medical diagnostics and driving progress toward precision medicine. However, effectively leveraging multimodal data, particularly digital pathology whole slide images (WSIs) and genomic sequencing, remains a significant challenge due to the intrinsic heterogeneity of these modalities and the need for scalable and interpretable frameworks. Existing diagnostic models typically operate on unimodal data, overlooking critical cross-modal interactions that can yield richer clinical insights. We introduce MarbliX (Multimodal Association and Retrieval with Binary Latent Indexed matriX), a self-supervised framework that learns to embed WSIs and immunogenomic profiles into compact, scalable binary codes, termed ``monogram.'' By optimizing a triplet contrastive objective across modalities, MarbliX captures high-resolution patient similarity in a unified latent space, enabling efficient retrieval of clinically relevant cases and facilitating case-based reasoning. \textcolor{black}{In lung cancer, MarbliX achieves 85-89\% across all evaluation metrics, outperforming histopathology (69-71\%) and immunogenomics (73-76\%). In kidney cancer, real-valued monograms yield the strongest performance (F1: 80-83\%, Accuracy: 87-90\%), with binary monograms slightly lower (F1: 78-82\%).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Denoising Diffusion Models for Anomaly Localization in Medical Images</title>
<link>https://arxiv.org/abs/2410.23834</link>
<guid>https://arxiv.org/abs/2410.23834</guid>
<content:encoded><![CDATA[
arXiv:2410.23834v2 Announce Type: replace-cross 
Abstract: This review explores anomaly localization in medical images using denoising diffusion models. After providing a brief methodological background of these models, including their application to image reconstruction and their conditioning using guidance mechanisms, we provide an overview of available datasets and evaluation metrics suitable for their application to anomaly localization in medical images. In this context, we discuss supervision schemes ranging from fully supervised segmentation to semi-supervised, weakly supervised, self-supervised, and unsupervised methods, and provide insights into the effectiveness and limitations of these approaches. Furthermore, we highlight open challenges in anomaly localization, including detection bias, domain shift, computational cost, and model interpretability. Our goal is to provide an overview of the current state of the art in the field, outline research gaps, and highlight the potential of diffusion models for robust anomaly localization in medical images.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding</title>
<link>https://arxiv.org/abs/2503.09348</link>
<guid>https://arxiv.org/abs/2503.09348</guid>
<content:encoded><![CDATA[
arXiv:2503.09348v2 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have demonstrated significant potential as generalists in vision-language (VL) tasks. However, adoption of LMMs in real-world tasks is hindered by their poor performance in tasks that require a combination of VL capabilities, as well as in tasks that involve the grounding of complex text or visual instructions. To thoroughly investigate this gap and its underlying causes, we propose MOAT, a diverse benchmark with 1005 complex real-world vision questions that are straightforward for humans but challenging for LMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist problem solving by integrating VL capabilities such as reading text, counting, understanding spatial relations, grounding textual and visual instructions, etc. All these abilities fit into a taxonomy proposed by us that contains 9 VL capabilities, enabling MOAT to provide a fine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first benchmark to explicitly evaluate LMMs' ability to ground complex text and visual instructions, which is essential for many real-world applications. We evaluated 17 proprietary and open source LMMs, finding that the best performing LMM (Gemini 2.5 Pro) achieved only 44% accuracy, far below what would be acceptable in real-world applications. To guide future model development, we analyze common trends in our results and discuss the underlying causes of poor performance, focusing on the impact of text-centric reasoning, which VL capabilities form bottlenecks in complex tasks, and the potential harmful effects of tiling. Code and data are available at https://cambrian-yzt.github.io/MOAT/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction</title>
<link>https://arxiv.org/abs/2506.19139</link>
<guid>https://arxiv.org/abs/2506.19139</guid>
<content:encoded><![CDATA[
arXiv:2506.19139v2 Announce Type: replace-cross 
Abstract: Recent advances in 3D Gaussian representations have significantly improved the quality and efficiency of image-based scene reconstruction. Their explicit nature facilitates real-time rendering and fast optimization, yet extracting accurate surfaces - particularly in large-scale, unbounded environments - remains a difficult task. Many existing methods rely on approximate depth estimates and global sorting heuristics, which can introduce artifacts and limit the fidelity of the reconstructed mesh. In this paper, we present Sorted Opacity Fields (SOF), a method designed to recover detailed surfaces from 3D Gaussians with both speed and precision. Our approach improves upon prior work by introducing hierarchical resorting and a robust formulation of Gaussian depth, which better aligns with the level-set. To enhance mesh quality, we incorporate a level-set regularizer operating on the opacity field and introduce losses that encourage geometrically-consistent primitive shapes. In addition, we develop a parallelized Marching Tetrahedra algorithm tailored to our opacity formulation, reducing meshing time by up to an order of magnitude. As demonstrated by our quantitative evaluation, SOF achieves higher reconstruction accuracy while cutting total processing time by more than a factor of three. These results mark a step forward in turning efficient Gaussian-based rendering into equally efficient geometry extraction.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-wise Balancing Data Replay for Federated Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2507.07712</link>
<guid>https://arxiv.org/abs/2507.07712</guid>
<content:encoded><![CDATA[
arXiv:2507.07712v4 Announce Type: replace-cross 
Abstract: Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model's overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-LATTE: Latent Space 3D Editing from Textual Instructions</title>
<link>https://arxiv.org/abs/2509.00269</link>
<guid>https://arxiv.org/abs/2509.00269</guid>
<content:encoded><![CDATA[
arXiv:2509.00269v3 Announce Type: replace-cross 
Abstract: Despite the recent success of multi-view diffusion models for text/image-based 3D asset generation, instruction-based editing of 3D assets lacks surprisingly far behind the quality of generation models. The main reason is that recent approaches using 2D priors suffer from view-inconsistent editing signals. Going beyond 2D prior distillation methods and multi-view editing strategies, we propose a training-free editing method that operates within the latent space of a native 3D diffusion model, allowing us to directly manipulate 3D geometry. We guide the edit synthesis by blending 3D attention maps from the generation with the source object. Coupled with geometry-aware regularization guidance, a spectral modulation strategy in the Fourier domain and a refinement step for 3D enhancement, our method outperforms previous 3D editing methods enabling high-fidelity and precise edits across a wide range of shapes and semantic manipulations. Our project webpage is https://mparelli.github.io/3d-latte
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning</title>
<link>https://arxiv.org/abs/2510.21019</link>
<guid>https://arxiv.org/abs/2510.21019</guid>
<content:encoded><![CDATA[
arXiv:2510.21019v2 Announce Type: replace-cross 
Abstract: Zeroth-order (ZO) optimization has gained attention as a memory-efficient alternative to first-order (FO) methods, particularly in settings where gradient computation is expensive or even impractical. Beyond its memory efficiency, in this work, we investigate ZO optimization for continual learning (CL) as a novel approach to address the plasticity-stability-efficiency trilemma. Through theoretical analysis and empirical evidence, we show that ZO optimization naturally leads to flatter loss landscapes, which in turn reduce forgetting in CL. However, this stability comes at a cost of plasticity: due to its imprecise gradient estimates and slower convergence, ZO optimization tends to be less effective than FO in acquiring new task-specific knowledge, particularly under constrained training budgets. To better understand this trade-off, we conduct a holistic evaluation of ZO optimization applied to various existing CL methods. Our findings reveal that ZO optimization enhances stability but often undermines plasticity, particularly when used with learnable classifiers. Motivated by this insight, we propose ZO-FC, a simple but effective approach that applies ZO optimization to a single adapter-based PEFT module with FO optimized classifier. This design leverages the stability benefits of ZO while preserving the adaptability of FO updates with negligible memory overhead. Experiments demonstrate that ZO-FC achieves an effective balance between stability and plasticity, offering a practical and memory-efficient solution for on-device CL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</title>
<link>https://arxiv.org/abs/2511.15351</link>
<guid>https://arxiv.org/abs/2511.15351</guid>
<content:encoded><![CDATA[
arXiv:2511.15351v2 Announce Type: replace-cross 
Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement</title>
<link>https://arxiv.org/abs/2512.00396</link>
<guid>https://arxiv.org/abs/2512.00396</guid>
<content:encoded><![CDATA[
arXiv:2512.00396v2 Announce Type: replace-cross 
Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dressing the Imagination: A Dataset for AI-Powered Translation of Text into Fashion Outfits and A Novel NeRA Adapter for Enhanced Feature Adaptation</title>
<link>https://arxiv.org/abs/2411.13901</link>
<guid>https://arxiv.org/abs/2411.13901</guid>
<content:encoded><![CDATA[
<div> Keywords: FLORA dataset, fashion AI, generative models, NeRA adapter, Kolmogorov-Arnold Networks<br /><br />Summary:<br /><br />This work introduces FLORA (Fashion Language Outfit Representation for Apparel Generation), a novel dataset with 4,330 curated pairs of fashion outfit images and detailed textual descriptions, rich in industry-specific terminology used by professional fashion designers. The dataset emphasizes capturing subtle stylistic elements and delicate features to support the creation of high-fidelity fashion designs. By fine-tuning generative AI models on FLORA, the authors achieve significantly improved image generation quality, producing more accurate and stylistically rich fashion sketches from text inputs. In addition to the dataset, the paper presents NeRA (Nonlinear low-rank Expressive Representation Adapter), a new adapter architecture grounded in Kolmogorov-Arnold Networks. Unlike existing parameter-efficient fine-tuning methods such as LoRA, LoKR, DoRA, and LoHA, which rely on MLP adapters, NeRA employs learnable spline-based nonlinear transformations for better semantic modeling. This design leads to stronger fidelity, faster convergence, and improved semantic alignment when training generative models. Extensive experiments on FLORA and the large-scale LAION-5B dataset validate NeRA’s superior performance compared to prior adapter techniques. The authors plan to open-source both the FLORA dataset and the implementation code, aiming to foster advanced AI models that deeply comprehend and generate sophisticated fashion designs, empowering both designers and end-users alike. <div>
arXiv:2411.13901v5 Announce Type: replace 
Abstract: Specialized datasets that capture the fashion industry's rich language and styling elements can boost progress in AI-driven fashion design. We present FLORA, (Fashion Language Outfit Representation for Apparel Generation), the first comprehensive dataset containing 4,330 curated pairs of fashion outfits and corresponding textual descriptions. Each description utilizes industry-specific terminology and jargon commonly used by professional fashion designers, providing precise and detailed insights into the outfits. Hence, the dataset captures the delicate features and subtle stylistic elements necessary to create high-fidelity fashion designs.
  We demonstrate that fine-tuning generative models on the FLORA dataset significantly enhances their capability to generate accurate and stylistically rich images from textual descriptions of fashion sketches. FLORA will catalyze the creation of advanced AI models capable of comprehending and producing subtle, stylistically rich fashion designs. It will also help fashion designers and end-users to bring their ideas to life.
  As a second orthogonal contribution, we introduce NeRA (Nonlinear low-rank Expressive Representation Adapter), a novel adapter architecture based on Kolmogorov-Arnold Networks (KAN). Unlike traditional PEFT techniques such as LoRA, LoKR, DoRA, and LoHA that use MLP adapters, NeRA uses learnable spline-based nonlinear transformations, enabling superior modeling of complex semantic relationships, achieving strong fidelity, faster convergence and semantic alignment. Extensive experiments on our proposed FLORA and LAION-5B datasets validate the superiority of NeRA over existing adapters.
  We will open-source both the FLORA dataset and our implementation code.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-centric Token Compression in Large Language Model</title>
<link>https://arxiv.org/abs/2502.00791</link>
<guid>https://arxiv.org/abs/2502.00791</guid>
<content:encoded><![CDATA[
<div> Token Compression, Vision Encoder, Large Language Models, In-Context Learning, Probability-Informed Visual Enhancement  

<br /><br />Summary:  
The paper addresses the challenge of escalating compute and memory costs associated with large language models (LLMs) handling extremely long context windows that span hundreds of thousands of tokens. To tackle this, the authors propose Vision Centric Token Compression (Vist), a novel slow-fast compression framework inspired by human reading behavior. In Vist, distant tokens are transformed into images and processed by a frozen, lightweight vision encoder through a fast pathway, enabling efficient skimming of low-salience context. Meanwhile, the nearby tokens are fed through a slow path into the LLM for detailed reasoning. The approach incorporates a Probability-Informed Visual Enhancement (PVE) training objective that masks high-frequency tokens, guiding the Resampler to focus on semantically meaningful regions, akin to how skilled readers overlook function words. Evaluation on eleven in-context learning benchmarks demonstrates that Vist maintains comparable accuracy using 2.3 times fewer tokens, while reducing floating-point operations (FLOPs) by 16% and memory usage by 50%. Furthermore, Vist outperforms the strongest existing text encoder-based compression baseline, CEPE, by an average of 7.6% across benchmarks such as TriviaQA, NQ, PopQA, NLUI, and CLIN. This method sets a new state-of-the-art in token efficiency for large language models, and the authors provide the implementation at their public GitHub repository. <div>
arXiv:2502.00791v5 Announce Type: replace-cross 
Abstract: Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuromorphic Eye Tracking for Low-Latency Pupil Detection</title>
<link>https://arxiv.org/abs/2512.09969</link>
<guid>https://arxiv.org/abs/2512.09969</guid>
<content:encoded><![CDATA[
<div> Eye tracking, wearable systems, spiking neural networks, neuromorphic sensors, low latency  

<br /><br />Summary:  
1. Eye tracking in wearable systems requires low latency and low power consumption (milliwatt-level) for effective use in AR and VR applications, where user gaze tracking is critical for immersive and responsive interactions.  
2. Traditional frame-based eye-tracking methods suffer from issues like motion blur, high computational cost, and limited temporal resolution, limiting their suitability for real-time wearable use.  
3. Neuromorphic sensors combined with spiking neural networks (SNNs) present an alternative, but previous SNN solutions are either too specialized or do not match the performance of state-of-the-art artificial neural network (ANN) models.  
4. This paper introduces a neuromorphic adaptation of leading event-based eye-tracking models by replacing recurrent and attention mechanisms with lightweight leaky integrate-and-fire (LIF) layers, and incorporates depth-wise separable convolutions to significantly reduce model complexity.  
5. The resulting models achieve mean error rates of 3.7-4.1 pixels, close to the application-specific Retina system’s 3.24 pixels, yet reduce model size by 20 times and theoretical computation by 850 times compared to the closest ANN equivalent.  
6. The proposed models are estimated to operate at 3.9-4.9 mW with 3 ms latency at a sampling rate of 1 kHz, making them highly efficient and suitable for real-time deployment in wearable eye-tracking devices. <div>
arXiv:2512.09969v1 Announce Type: new 
Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects</title>
<link>https://arxiv.org/abs/2512.10031</link>
<guid>https://arxiv.org/abs/2512.10031</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly supervised oriented object detection, adaptive bounding box scaling, symmetry-prior angle loss, horizontal bounding box supervision, aerial object detection<br /><br />Summary: This paper addresses weaknesses in weakly supervised oriented object detection (WS-OOD) that use horizontal bounding box (HBox) annotations by introducing ABBSPO, a new framework. First, it identifies a problem in existing methods where ground truth HBoxes are directly compared to the minimum circumscribed rectangles of predicted rotated boxes (RBoxes), resulting in inaccurate scale predictions. To fix this, the authors propose Adaptive Bounding Box Scaling (ABBS), which dynamically adjusts the size of ground truth HBoxes to better match the predicted RBoxes, leading to more precise scale estimation. Second, the paper presents a Symmetric Prior Angle (SPA) loss that leverages the natural symmetry found in aerial objects for self-supervised learning. This loss function mitigates the collapse problem seen in previous methods when predictions for the original, rotated, and flipped views are all incorrect. The combination of ABBS and SPA within ABBSPO improves robustness and accuracy in oriented object detection tasks. Extensive experiments demonstrate that ABBSPO achieves state-of-the-art performance, outperforming former approaches under weak supervision using only HBox annotations, offering a cost-effective yet accurate solution for aerial oriented object detection. <div>
arXiv:2512.10031v1 Announce Type: new 
Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Is Your Friend in Show, Suggest and Tell</title>
<link>https://arxiv.org/abs/2512.10038</link>
<guid>https://arxiv.org/abs/2512.10038</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Autoregressive generation, Image captioning, COCO dataset, Show Suggest and Tell (SST) <br /><br />Summary:  
This paper addresses the limitations of diffusion denoising models in discrete generative tasks, where they have yet to surpass traditional autoregressive methods. Instead of replacing autoregressive approaches, the authors propose a hybrid paradigm where diffusion models provide suggestions to guide autoregressive generation. This approach leverages the bidirectional and refining strengths of diffusion models while maintaining the strong linguistic structure inherent to autoregressive models. To validate this method, the authors introduce Show, Suggest and Tell (SST), a model designed for image captioning. SST achieves state-of-the-art performance on the COCO dataset, obtaining a CIDEr-D score of 125.1 without employing reinforcement learning. This score surpasses previous state-of-the-art results from both autoregressive and diffusion models by margins of 1.5 and 2.5 points, respectively. Extensive experiments demonstrate that the suggestion module positively impacts caption quality, indicating a strong correlation between suggestion quality and overall performance. The work highlights a promising and previously underexplored research direction that combines the complementary strengths of diffusion and autoregressive models in discrete generation tasks. The authors also commit to releasing the code, facilitating further research on this novel approach. <div>
arXiv:2512.10038v1 Announce Type: new 
Abstract: Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\_suggest\_tell.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata</title>
<link>https://arxiv.org/abs/2512.10041</link>
<guid>https://arxiv.org/abs/2512.10041</guid>
<content:encoded><![CDATA[
<div> MetaVoxel, diffusion modeling, joint distribution, medical imaging, zero-shot inference<br /><br />Summary:  
1. The paper introduces MetaVoxel, a novel generative joint diffusion modeling framework that learns a single diffusion process over both imaging data and clinical metadata.  
2. Unlike traditional models that rely on modeling conditional distributions for specific predictive tasks, MetaVoxel captures the entire joint distribution, enabling unified modeling across multiple tasks.  
3. This unified approach supports flexible zero-shot inference, allowing predictions or generation from arbitrary subsets of inputs without requiring task-specific retraining.  
4. The authors validate MetaVoxel using over 10,000 T1-weighted MRI scans paired with clinical metadata collected from nine different datasets.  
5. The model successfully performs various tasks, including image generation, age estimation, and sex prediction, achieving results comparable to specialized task-specific baseline models.  
6. Additional experiments demonstrate MetaVoxel’s adaptability and versatile inference capabilities.  
7. The findings suggest that joint multimodal diffusion modeling is a promising direction for developing unified medical AI models that are broadly applicable in clinical settings. <div>
arXiv:2512.10041v1 Announce Type: new 
Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Independent Density Estimation</title>
<link>https://arxiv.org/abs/2512.10067</link>
<guid>https://arxiv.org/abs/2512.10067</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Compositional Generalization, Independent Density Estimation, Disentangled Representations, Entropy-based Inference  

<br /><br />Summary:  
Large-scale vision-language models have demonstrated strong performance in tasks like image captioning and conditioned image generation, but still struggle with compositional generalization—the ability to understand and generate novel combinations of known concepts. To address this, the authors introduce Independent Density Estimation (IDE), a method designed to learn explicit connections between each individual word in a sentence and corresponding visual features. Two models based on the IDE framework are proposed: the first uses fully disentangled visual representations as inputs, ensuring clear separation of distinct visual factors; the second employs a Variational Auto-Encoder (VAE) to extract partially disentangled features directly from raw images, balancing representational richness and interpretability. Beyond feature extraction, the authors design an entropy-based compositional inference technique to effectively integrate predictions associated with each word, improving robustness and accuracy. Experimental evaluation across multiple datasets shows that models using IDE consistently outperform existing approaches, particularly on compositions that were unseen during training. This highlights IDE's potential to enhance the compositional generalization capabilities of vision-language systems, bringing them closer to human-like understanding and generation of novel visual-linguistic combinations. <div>
arXiv:2512.10067v1 Announce Type: new 
Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Neverthe- less, these models still encounter difficulties in achieving human-like composi- tional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connec- tion between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy- based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing</title>
<link>https://arxiv.org/abs/2512.10095</link>
<guid>https://arxiv.org/abs/2512.10095</guid>
<content:encoded><![CDATA[
<div> Keywords: TraceFlow, specular reflection, Gaussian Splatting, dynamic scenes, physically accurate rendering<br /><br />Summary: TraceFlow is a novel framework designed for high-fidelity rendering of dynamic specular scenes, addressing the challenges of accurate reflection direction estimation and physically realistic reflection modeling. The method introduces a Residual Material-Augmented 2D Gaussian Splatting representation that effectively models both dynamic geometry and material properties, enabling precise reflection ray calculations. Additionally, TraceFlow incorporates a Dynamic Environment Gaussian alongside a hybrid rendering pipeline that separates rendering into diffuse and specular components. This decomposition allows for physically grounded specular reflection synthesis by combining rasterization and ray tracing techniques. To enhance training, the framework utilizes a coarse-to-fine optimization strategy that improves stability and encourages physically meaningful separation of reflection components. Extensive experiments conducted on various dynamic scene benchmarks demonstrate that TraceFlow surpasses existing approaches both quantitatively and qualitatively. The results highlight the method’s ability to produce sharper, more realistic specular reflections even in complex, dynamic environments, marking a significant advancement in rendering technology for specular effects in dynamic settings. <div>
arXiv:2512.10095v1 Announce Type: new 
Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information</title>
<link>https://arxiv.org/abs/2512.10102</link>
<guid>https://arxiv.org/abs/2512.10102</guid>
<content:encoded><![CDATA[
<div> hierarchical instance tracking, benchmark dataset, video tracking, object-part relationships, multi-category tracking<br /><br />Summary:  
1. The paper introduces a new task called hierarchical instance tracking, which involves tracking all instances of predefined categories of objects and their parts while preserving the hierarchical relationships between them.  
2. To support this task, the authors present the first benchmark dataset, which includes 2,765 unique entities tracked across 552 videos. These entities belong to 40 different categories spanning both objects and their parts.  
3. The new dataset is designed to address the complexity of maintaining hierarchical relationships, making it distinct from traditional instance tracking tasks that usually focus on independent objects.  
4. The authors evaluate seven variants of four tracking models specifically adapted to this task, demonstrating that the dataset poses significant challenges to current state-of-the-art methods.  
5. The proposed dataset is publicly available online at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/, encouraging further research and development in hierarchical instance tracking for complex video understanding applications. <div>
arXiv:2512.10102v1 Announce Type: new 
Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization</title>
<link>https://arxiv.org/abs/2512.10151</link>
<guid>https://arxiv.org/abs/2512.10151</guid>
<content:encoded><![CDATA[
<div> Keywords: Breast cancer, mammography, persistent homology, topological data analysis, ConvNeXt  

<br /><br />Summary:  
1. Breast cancer remains the most commonly diagnosed cancer in women and is a leading cause of cancer-related deaths globally, with mammography screening playing a critical role in mortality reduction.  
2. Despite its importance, mammographic interpretation faces challenges such as high rates of false negatives and false positives, and a notable decline in model accuracy when applied across different imaging devices, modalities, and diverse patient populations.  
3. The authors propose a novel conditioning signal derived from wavelet-based vectorization of persistent homology, utilizing topological data analysis to characterize image structures that persist across intensity thresholds.  
4. This approach converts persistent homology information into spatial, multiscale maps designed to be stable against minor intensity fluctuations, enhancing robustness.  
5. These topological feature maps are incorporated into a two-stage detection pipeline by concatenating them as additional input channels, which improves model generalization.  
6. The method is trained and validated on the CBIS-DDSM digitized film mammography dataset from the US and further tested on two independent full-field digital mammography datasets from Portugal (INbreast) and China (CMMD), with evaluation at the patient level.  
7. Experimental results show a significant improvement on the INbreast dataset, where augmenting a ConvNeXt Tiny model with wavelet persistence channels boosted patient-level AUC from 0.55 to 0.75, especially under limited training data conditions.  
8. This work offers a promising direction for enhancing mammographic screening accuracy and robustness across heterogeneous clinical settings through topologically informed image representation. <div>
arXiv:2512.10151v1 Announce Type: new 
Abstract: Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Coding for Scalable Machine Vision</title>
<link>https://arxiv.org/abs/2512.10209</link>
<guid>https://arxiv.org/abs/2512.10209</guid>
<content:encoded><![CDATA[
<div> Deep Neural Networks, Edge Computing, Feature Coding, MPEG FCM Standard, Bitrate Reduction<br /><br />Summary:<br /><br />This paper addresses the challenge of deploying deep neural networks (DNNs) for machine vision on edge devices, which are constrained by limited computational resources. Traditional methods either run the entire model on the device or offload processing to the cloud, each facing significant trade-offs related to latency, bandwidth, and privacy. A promising alternative is to split inference tasks between the edge and the cloud, but this requires transmitting intermediate features, creating new bandwidth bottlenecks. To solve this, the Moving Picture Experts Group (MPEG) introduced the Feature Coding for Machines (FCM) standard, which defines a bitstream syntax and codec pipeline specifically designed to compress intermediate features efficiently. The authors present the Feature Coding Test Model (FCTM), a tool demonstrating substantial bitrate reductions, averaging 85.14%, across a variety of vision tasks while maintaining model accuracy. The FCM standard and FCTM provide a scalable and interoperable framework that enhances the deployment of intelligent feature coding in bandwidth-limited and privacy-sensitive consumer applications, thus enabling more practical and efficient edge-cloud collaborative inference solutions. <div>
arXiv:2512.10209v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Chain-of-Thought World Modeling for End-to-End Driving</title>
<link>https://arxiv.org/abs/2512.10226</link>
<guid>https://arxiv.org/abs/2512.10226</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, chain-of-thought reasoning, latent language, autonomous driving, reinforcement learning<br /><br />Summary: This paper introduces Latent-CoT-Drive (LCDrive), a novel Vision-Language-Action (VLA) model designed to improve autonomous driving performance via inference-time reasoning. Unlike prior approaches that rely on natural language for chain-of-thought (CoT) reasoning, LCDrive uses a latent language combining action-proposal tokens and world model tokens, both embedded in an action-aligned latent space. This representation enables the model to better capture possible outcomes of driving actions by predicting future scene rollouts. LCDrive is initially supervised with ground-truth future scene data, enabling a "cold start" for its latent CoT reasoning. Subsequently, it undergoes closed-loop reinforcement learning that enhances its reasoning capabilities through interaction. Evaluations on a large-scale end-to-end driving benchmark demonstrate that LCDrive performs faster inference and produces higher quality driving trajectories compared to baselines that use either no reasoning or text-based CoT reasoning. Moreover, it shows greater improvements when further trained with interactive reinforcement learning. Overall, the work presents a unified framework integrating reasoning and decision making in a latent space that aligns actions directly with predicted world outcomes, offering a more efficient and effective approach for VLA models in autonomous driving scenarios. <div>
arXiv:2512.10226v1 Announce Type: new 
Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emerging Standards for Machine-to-Machine Video Coding</title>
<link>https://arxiv.org/abs/2512.10230</link>
<guid>https://arxiv.org/abs/2512.10230</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Coding for Machines, Feature Coding for Machines, HEVC, VVC, bitrate reduction<br /><br />Summary:<br /><br />1. The paper addresses the inefficiencies of current machine-to-machine (M2M) visual data communication, which typically relies on streaming pixel-based video using codecs designed for human viewing, resulting in high bandwidth use and privacy concerns.  
2. MPEG has introduced Video Coding for Machines (VCM) and Feature Coding for Machines (FCM) approaches to optimize video compression and transmission specifically for machine tasks, with FCM focusing on compressing intermediate neural features rather than raw pixels.  
3. Experimental results demonstrate that FCM achieves bitrate reductions while maintaining accuracy comparable to edge inference, indicating benefits in scalability, privacy, and computational offloading.  
4. A comparative analysis of inner codecs used in FCM shows that HEVC (H.265) and VVC (H.266) perform similarly in terms of machine task metrics, with only a minor 1.39% average BD-Rate increase when substituting VVC with HEVC.  
5. In contrast, AVC (H.264) leads to a significant BD-Rate increase of about 32.28% compared to VVC but for tracking tasks codec choice has little impact, with HEVC slightly outperforming VVC, suggesting that current hardware for established codecs is adequate for M2M communications without sacrificing performance. <div>
arXiv:2512.10230v1 Announce Type: new 
Abstract: Machines are increasingly becoming the primary consumers of visual data, yet most deployments of machine-to-machine systems still rely on remote inference where pixel-based video is streamed using codecs optimized for human perception. Consequently, this paradigm is bandwidth intensive, scales poorly, and exposes raw images to third parties. Recent efforts in the Moving Picture Experts Group (MPEG) redesigned the pipeline for machine-to-machine communication: Video Coding for Machines (VCM) is designed to apply task-aware coding tools in the pixel domain, and Feature Coding for Machines (FCM) is designed to compress intermediate neural features to reduce bitrate, preserve privacy, and support compute offload. Experiments show that FCM is capable of maintaining accuracy close to edge inference while significantly reducing bitrate. Additional analysis of H.26X codecs used as inner codecs in FCM reveals that H.265/High Efficiency Video Coding (HEVC) and H.266/Versatile Video Coding (VVC) achieve almost identical machine task performance, with an average BD-Rate increase of 1.39% when VVC is replaced with HEVC. In contrast, H.264/Advanced Video Coding (AVC) yields an average BD-Rate increase of 32.28% compared to VVC. However, for the tracking task, the impact of codec choice is minimal, with HEVC outperforming VVC and achieving BD Rate of -1.81% and 8.79% for AVC, indicating that existing hardware for already deployed codecs can support machine-to-machine communication without degrading performance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-dimensional Preference Alignment by Conditioning Reward Itself</title>
<link>https://arxiv.org/abs/2512.10237</link>
<guid>https://arxiv.org/abs/2512.10237</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Human Feedback, Diffusion Models, Multi Reward, Bradley-Terry Model<br /><br />Summary:<br /><br />1. The paper discusses a significant limitation in the standard Direct Preference Optimization (DPO) approach used for aligning diffusion models with human feedback, where the Bradley-Terry model aggregates multiple evaluation axes (such as aesthetic quality and semantic alignment) into a single scalar reward. This aggregation leads to reward conflicts that force the model to discard desirable features from specific dimensions if the overall sample is not preferred. <br /><br />2. To resolve this issue, the authors propose Multi Reward Conditional DPO (MCDPO), which introduces a disentangled Bradley-Terry objective enabling the model to learn optimization directions independently for each reward axis within one network. <br /><br />3. MCDPO incorporates a preference outcome vector as a training condition and introduces dimensional reward dropout to maintain balanced optimization across multiple reward dimensions, preventing bias toward any particular axis.<br /><br />4. Extensive experiments on Stable Diffusion 1.5 and the newer SDXL model show that MCDPO outperforms previous methods on benchmark tasks, demonstrating effectiveness in handling multi-axis rewards.<br /><br />5. The proposed conditional framework allows for flexible, dynamic control over multiple reward dimensions during inference through Classifier Free Guidance, enabling amplification of specific reward aspects without needing additional training or external reward models. <div>
arXiv:2512.10237v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback has emerged as a standard for aligning diffusion models. However, we identify a fundamental limitation in the standard DPO formulation because it relies on the Bradley-Terry model to aggregate diverse evaluation axes like aesthetic quality and semantic alignment into a single scalar reward. This aggregation creates a reward conflict where the model is forced to unlearn desirable features of a specific dimension if they appear in a globally non-preferred sample. To address this issue, we propose Multi Reward Conditional DPO (MCDPO). This method resolves reward conflicts by introducing a disentangled Bradley-Terry objective. MCDPO explicitly injects a preference outcome vector as a condition during training, which allows the model to learn the correct optimization direction for each reward axis independently within a single network. We further introduce dimensional reward dropout to ensure balanced optimization across dimensions. Extensive experiments on Stable Diffusion 1.5 and SDXL demonstrate that MCDPO achieves superior performance on benchmarks. Notably, our conditional framework enables dynamic and multiple-axis control at inference time using Classifier Free Guidance to amplify specific reward dimensions without additional training or external reward models.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective</title>
<link>https://arxiv.org/abs/2512.10244</link>
<guid>https://arxiv.org/abs/2512.10244</guid>
<content:encoded><![CDATA[
<div> Keywords: Semi-supervised few-shot learning, Vision-Language Models, classifier initialization, temperature tuning, SWIFT

<br /><br />Summary:  
This paper addresses the challenge of semi-supervised few-shot learning (SSFSL), which aims to train models using a few labeled and many unlabeled examples for tasks like auto-annotation. Despite the progress in few-shot learning (FSL) using open-source Vision-Language Models (VLMs), SSFSL methods have not fully leveraged these powerful resources. The authors first apply existing semi-supervised learning (SSL) techniques to fine-tune VLMs but find that these approaches underperform compared to FSL baselines. Their analysis reveals that VLMs tend to output flat softmax probability distributions, causing ineffective use of unlabeled data and weak supervision. To overcome this, they introduce simple yet effective strategies: classifier initialization and temperature tuning, which jointly boost the confidence of pseudo-labels and enhance unlabeled data utilization. Building on these insights, the paper proposes a novel approach called Stage-Wise Finetuning with Temperature Tuning (SWIFT). SWIFT improves the fine-tuning of VLMs by incorporating limited labeled data, abundant unlabeled data, and noisy but task-relevant data retrieved from the VLM’s pretraining set. Comprehensive experiments on five SSFSL benchmarks demonstrate that SWIFT surpasses recent FSL and SSL methods by approximately 5 accuracy points and performs comparably to fully supervised learning using ground-truth labels for unlabeled data. <div>
arXiv:2512.10244v1 Announce Type: new 
Abstract: Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</title>
<link>https://arxiv.org/abs/2512.10248</link>
<guid>https://arxiv.org/abs/2512.10248</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated video, watermark robustness, AIGC detection, RobustSora benchmark, transformer models<br /><br />Summary:<br /><br />1. The paper addresses challenges in detecting AI-generated content (AIGC) videos, particularly focusing on the influence of embedded digital watermarks used by many generative models.<br /><br />2. Existing detection benchmarks do not adequately consider how watermark patterns impact detector performance, potentially biasing results.<br /><br />3. To tackle this, the authors introduce RobustSora, a new benchmark dataset with 6,500 videos categorized into four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW).<br /><br />4. RobustSora defines two evaluation tasks: Task-I measures detection performance on AI videos with watermarks removed, and Task-II evaluates false alarm rates on authentic videos that contain fake watermarks.<br /><br />5. Extensive experiments involving ten different detection models—including specialized AIGC detectors, transformer-based architectures, and multi-modal large language models (MLLMs)—show performance drops between 2 to 8 percentage points when watermark manipulation is introduced.<br /><br />6. Transformer-based models exhibit moderate and consistent dependency on watermark signals (6-8pp variance), whereas MLLMs display more varied responses (2-8pp).<br /><br />7. The findings reveal partial reliance on watermark cues by detectors, underscoring the necessity for watermark-aware training strategies to improve robustness.<br /><br />8. RobustSora thus serves as a vital resource and evaluation tool for advancing the robustness of AI-generated video detection methods. <div>
arXiv:2512.10248v1 Announce Type: new 
Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose</title>
<link>https://arxiv.org/abs/2512.10251</link>
<guid>https://arxiv.org/abs/2512.10251</guid>
<content:encoded><![CDATA[
<div> Keywords: category-level pose estimation, topological prior, hybrid graph fusion, 3D graph convolution, surface embedding<br /><br />Summary:<br /><br />This paper addresses the challenge of category-level 6D object pose estimation where robustness is needed against intra-class variations. Traditional 3D graph convolution (3D-GC) methods primarily use local geometry and depth data, which limits their effectiveness for complex objects and visual ambiguities. The authors propose THE-Pose, a novel framework that incorporates a topological prior through surface embedding to extract consistent and invariant topological features from 2D image data. This approach overcomes the limitations of existing 3D-GC-based methods that rely solely on 3D point-cloud features. A key innovation is the Hybrid Graph Fusion (HGF) module, which adaptively merges topological features from the image domain with 3D point-cloud features, thus bridging 2D contextual information and 3D geometric structure. The fusion strategy enhances the method's stability and accuracy for unseen or complicated objects, especially under significant occlusions. Evaluation on the REAL275 dataset demonstrates a 35.8% improvement over the baseline 3D-GC method HS-Pose, and outperforms prior state-of-the-art approaches by 7.2% across all principal metrics. The authors also provide the implementation code publicly for further research and development. <div>
arXiv:2512.10251v1 Announce Type: new 
Abstract: Category-level object pose estimation requires both global context and local structure to ensure robustness against intra-class variations. However, 3D graph convolution (3D-GC) methods only focus on local geometry and depth information, making them vulnerable to complex objects and visual ambiguities. To address this, we present THE-Pose, a novel category-level 6D pose estimation framework that leverages a topological prior via surface embedding and hybrid graph fusion. Specifically, we extract consistent and invariant topological features from the image domain, effectively overcoming the limitations inherent in existing 3D-GC based methods. Our Hybrid Graph Fusion (HGF) module adaptively integrates the topological features with point-cloud features, seamlessly bridging 2D image context and 3D geometric structure. These fused features ensure stability for unseen or complicated objects, even under significant occlusions. Extensive experiments on the REAL275 dataset show that THE-Pose achieves a 35.8% improvement over the 3D-GC baseline (HS-Pose) and surpasses the previous state-of-the-art by 7.2% across all key metrics. The code is avaialbe on https://github.com/EHxxx/THE-Pose
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule</title>
<link>https://arxiv.org/abs/2512.10252</link>
<guid>https://arxiv.org/abs/2512.10252</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiac segmentation, echocardiography, spatiotemporal modeling, GDKVM, deep learning<br /><br />Summary:<br /><br />Accurate segmentation of cardiac chambers in echocardiography sequences is essential for assessing cardiac function and supporting clinical diagnosis and treatment. Existing segmentation methods leveraging convolutional neural networks, Transformers, and space-time memory networks have improved accuracy but often face challenges balancing long-range spatiotemporal dependency capture with computational efficiency and fine-grained feature representation. This paper proposes GDKVM, a novel deep learning architecture designed specifically for echocardiography video segmentation. The model introduces Linear Key-Value Association (LKVA) to effectively model correlations between frames, and a Gated Delta Rule (GDR) mechanism to efficiently store and update intermediate memory states. Additionally, the Key-Pixel Feature Fusion (KPFF) module integrates local and global features across multiple scales, enhancing the model’s robustness against boundary blurring and noise. Performance evaluations on two prominent echocardiography datasets, CAMUS and EchoNet-Dynamic, demonstrate that GDKVM surpasses current state-of-the-art methods in both segmentation accuracy and robustness. The model also maintains real-time performance, making it practical for clinical applications. The authors have made the code publicly accessible at https://github.com/wangrui2025/GDKVM to facilitate further research and development in this domain. <div>
arXiv:2512.10252v1 Announce Type: new 
Abstract: Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models</title>
<link>https://arxiv.org/abs/2512.10262</link>
<guid>https://arxiv.org/abs/2512.10262</guid>
<content:encoded><![CDATA[
<div> Novel Class Discovery, multimodal framework, semantic prototypes, adaptive clustering, long-tail distribution<br /><br />Summary:<br /><br />Novel Class Discovery (NCD) aims to leverage prior knowledge of known classes to classify and uncover unknown classes within unlabelled data. Existing NCD methods primarily focus on visual features, which often suffer from limited discriminative power and challenges arising from long-tail data distributions. The proposed method, LLM-NCD, introduces a multimodal approach that fuses visual and textual semantics to overcome these limitations. A key contribution of LLM-NCD is the joint optimization of known class image features and their corresponding text features to model both cluster centers and semantic prototypes. This enables a richer semantic understanding during clustering. Furthermore, LLM-NCD employs a dual-phase discovery mechanism that dynamically separates known and novel samples by setting semantic affinity thresholds and applying adaptive clustering techniques. Experimental results on the CIFAR-100 dataset demonstrate that LLM-NCD achieves up to a 25.3% accuracy improvement in identifying unknown classes, outperforming current state-of-the-art methods. Notably, this framework uniquely exhibits robustness to long-tail distributions within the NCD context, addressing a longstanding challenge in the field and marking a significant advancement in class discovery methodologies. <div>
arXiv:2512.10262v1 Announce Type: new 
Abstract: Novel Class Discovery aims to utilise prior knowledge of known classes to classify and discover unknown classes from unlabelled data. Existing NCD methods for images primarily rely on visual features, which suffer from limitations such as insufficient feature discriminability and the long-tail distribution of data. We propose LLM-NCD, a multimodal framework that breaks this bottleneck by fusing visual-textual semantics and prototype guided clustering. Our key innovation lies in modelling cluster centres and semantic prototypes of known classes by jointly optimising known class image and text features, and a dualphase discovery mechanism that dynamically separates known or novel samples via semantic affinity thresholds and adaptive clustering. Experiments on the CIFAR-100 dataset show that compared to the current methods, this method achieves up to 25.3% improvement in accuracy for unknown classes. Notably, our method shows unique resilience to long tail distributions, a first in NCD literature.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction</title>
<link>https://arxiv.org/abs/2512.10267</link>
<guid>https://arxiv.org/abs/2512.10267</guid>
<content:encoded><![CDATA[
<div> Gaussian splatting, Long-LRM++, real-time rendering, implicit representations, novel-view depth prediction  

<br /><br />Summary:  
This paper presents Long-LRM++, a novel model in the field of Gaussian splatting (GS) for 3D scene reconstruction, improving upon the previous Long-LRM framework. The advancement addresses the challenge of predicting millions of Gaussian parameters simultaneously, which often leads to blurring in fine scene details. Unlike heavy implicit methods such as LVSM and LaCT that rely on computationally expensive decompression steps, Long-LRM++ employs a semi-explicit scene representation coupled with a lightweight decoder to achieve high rendering fidelity efficiently. The model matches the rendering quality of LaCT on the DL3DV dataset while enabling real-time rendering at 14 frames per second (FPS) on an NVIDIA A100 GPU, overcoming prior speed bottlenecks. It also scales effectively to handle up to 64 input views at 950×540 resolution, showcasing strong generalization to larger input sets. Additionally, Long-LRM++ demonstrates superior performance in novel-view depth prediction on the ScanNetv2 dataset compared to direct Gaussian depth rendering approaches. Extensive ablation studies validate that each design component contributes effectively to the model’s overall performance, marking significant progress in real-time, high-fidelity 3D reconstruction using Gaussian splatting techniques. <div>
arXiv:2512.10267v1 Announce Type: new 
Abstract: Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\times540$ resolution, achieving 360{\deg} scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential "decompression" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation</title>
<link>https://arxiv.org/abs/2512.10275</link>
<guid>https://arxiv.org/abs/2512.10275</guid>
<content:encoded><![CDATA[
<div> Adversarial Distillation, Robustness Transfer, Transferability, Sample-wise Reweighting, AutoAttack  

<br /><br />Summary:  
This paper addresses the problem of adversarial distillation, where adversarial robustness is transferred from a large, robust teacher network to a smaller student network within a min-max adversarial training framework. The authors note a phenomenon called robust saturation, where stronger teachers do not always produce more robust students, challenging the common belief that capacity gaps solely cause this issue. Instead, they identify adversarial transferability—the extent to which adversarial examples crafted by the student fool the teacher—as a critical factor for effective robustness transfer. Based on this insight, the authors propose Sample-wise Adaptive Adversarial Distillation (SAAD), a method that adaptively reweights training examples according to their transferability scores. SAAD improves robustness without adding computational costs during training. Experimental results on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets demonstrate that SAAD consistently achieves higher robustness against AutoAttack compared to previous approaches. The authors also release their code publicly for reproducibility and further research. This work provides a novel perspective on why robustness transfer can stagnate and offers an effective solution to enhance adversarial distillation outcomes. <div>
arXiv:2512.10275v1 Announce Type: new 
Abstract: Adversarial distillation in the standard min-max adversarial training framework aims to transfer adversarial robustness from a large, robust teacher network to a compact student. However, existing work often neglects to incorporate state-of-the-art robust teachers. Through extensive analysis, we find that stronger teachers do not necessarily yield more robust students-a phenomenon known as robust saturation. While typically attributed to capacity gaps, we show that such explanations are incomplete. Instead, we identify adversarial transferability-the fraction of student-crafted adversarial examples that remain effective against the teacher-as a key factor in successful robustness transfer. Based on this insight, we propose Sample-wise Adaptive Adversarial Distillation (SAAD), which reweights training examples by their measured transferability without incurring additional computational cost. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SAAD consistently improves AutoAttack robustness over prior methods. Our code is available at https://github.com/HongsinLee/saad.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</title>
<link>https://arxiv.org/abs/2512.10284</link>
<guid>https://arxiv.org/abs/2512.10284</guid>
<content:encoded><![CDATA[
<div> Motion-centric image editing, motion transformations, MotionEdit dataset, MotionEdit-Bench, MotionNFT fine-tuning  

<br /><br />Summary:  
1. The paper introduces MotionEdit, a novel dataset designed specifically for motion-centric image editing, focusing on modifying subject actions and interactions while preserving identity, structure, and physical plausibility.  
2. Unlike prior datasets that emphasize static appearance or contain limited, low-quality motion edits, MotionEdit features high-fidelity image pairs that capture realistic motion transformations extracted from continuous video sequences.  
3. To evaluate models on this new task, the authors propose MotionEdit-Bench, a comprehensive benchmark utilizing generative, discriminative, and preference-based metrics to rigorously assess performance on motion-centric edits.  
4. Benchmark results demonstrate that current state-of-the-art diffusion-based image editing models struggle to perform well in motion editing, highlighting the task's difficulty and the gap in existing methods.  
5. To address this, the paper introduces MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that leverages motion alignment rewards to guide models toward accurate motion flow transformations, improving both editing quality and motion fidelity without compromising general editing abilities.  
6. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit datasets confirm that MotionNFT consistently enhances model performance on motion editing tasks, establishing its effectiveness and practical significance for applications like video synthesis and animation. <div>
arXiv:2512.10284v1 Announce Type: new 
Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions</title>
<link>https://arxiv.org/abs/2512.10286</link>
<guid>https://arxiv.org/abs/2512.10286</guid>
<content:encoded><![CDATA[
<div> Keywords: shot transitions, multi-shot video generation, camera control, editing patterns, hierarchical prompting<br /><br />Summary:<br /><br />The paper introduces ShotDirector, a novel framework designed to enhance multi-shot video generation by focusing on shot transitions which are critical for narrative coherence and directorial intent. Unlike prior methods that mainly ensure visual consistency across shots, ShotDirector emphasizes the cinematic design of transitions by integrating parameter-level camera controls (6-DoF poses and intrinsic settings) and hierarchical editing-pattern-aware prompts. This allows precise injection of camera parameters and professional editing knowledge for fine-grained control of shot content and transitions, moving beyond simple sequential shot changes. The framework also employs a shot-aware mask mechanism to support hierarchical prompting, aligning with real-world film editing patterns. To support training and evaluation, the authors introduce ShotWeaver40K, a large dataset capturing film editing priors, alongside new evaluation metrics tailored for controllable multi-shot video generation. Extensive experiments validate the effectiveness of ShotDirector in producing film-like, controllable, and semantically coherent shot transitions that improve overall video narrative quality and storytelling expressiveness. The approach successfully bridges low-level parameter control with high-level semantic guidance for more intentional and professional video editing results. <div>
arXiv:2512.10286v1 Announce Type: new 
Abstract: Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings</title>
<link>https://arxiv.org/abs/2512.10293</link>
<guid>https://arxiv.org/abs/2512.10293</guid>
<content:encoded><![CDATA[
<div> Disentangled360, 3D-aware rendering, anisotropic scattering, hybrid pose anchoring, 360-degree view synthesis

<br /><br />Summary:  
This paper introduces Disentangled360, a novel 3D-aware rendering technology that merges directionally disentangled volume rendering with single-image 360° view synthesis. Unlike traditional approaches that oversimplify anisotropic light effects or struggle with generalization, Disentangled360 distinctly separates isotropic and anisotropic light contributions within a Gaussian Splatting framework. It employs a dual-branch conditioning system—one branch tailored for CT intensity-driven scattering relevant to volumetric medical imaging data, and the other optimized for real-world RGB scenes through normalized camera embeddings. To resolve scale ambiguities and retain structural realism, the method incorporates a hybrid pose-agnostic anchoring technique that adaptively samples scene depth and material transitions, serving as stable reference points during scene distillation. The design seamlessly integrates preoperative radiography simulation with consumer-grade 360° rendering into a unified inference pipeline. This integration enables rapid and photorealistic directional view synthesis. Evaluations on datasets including Mip-NeRF 360, RealEstate10K, and DeepDRR demonstrate superior performance in image quality metrics such as SSIM and LPIPS, alongside competitive runtime efficiency suitable for interactive use cases. Disentangled360 enables new applications in mixed-reality medical supervision, robotic perception, and immersive content creation, all without requiring scene-specific fine-tuning or costly photon simulation processes. <div>
arXiv:2512.10293v1 Announce Type: new 
Abstract: We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360{\deg} unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360{\deg} rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient-VLN: A Training-Efficient Vision-Language Navigation Model</title>
<link>https://arxiv.org/abs/2512.10310</link>
<guid>https://arxiv.org/abs/2512.10310</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Vision-Language Navigation, Training Efficiency, Memory Mechanisms, Exploration-Efficiency Trade-off<br /><br />Summary:<br /><br />The paper addresses the high training overhead faced by multimodal large language models (MLLMs) in Vision-Language Navigation (VLN) tasks. Two main issues are identified: the quadratic computational cost of processing long-horizon historical observations as large token sequences, and the exploration-efficiency trade-off in the DAgger data aggregation method, where increased exploration enhances error recovery but results in longer trajectories during training and inference. To tackle these challenges, the authors propose Efficient-VLN, a training-efficient VLN model. They introduce two novel memory mechanisms to reduce token processing complexity: progressive memory dynamically allocates more tokens to recent observations, while learnable recursive memory employs a key-value cache of learnable tokens to represent memory state. Additionally, a dynamic mixed policy is designed to balance the exploration-efficiency trade-off during learning. Experimental results demonstrate that Efficient-VLN achieves state-of-the-art Success Rate (SR) scores of 64.2% on R2R-CE and 67.0% on RxR-CE benchmarks. Importantly, the model drastically reduces training resource consumption, requiring only 282 H800 GPU hours, which is significantly lower than existing leading methods. This work thus provides a practical and efficient solution for advancing VLN with MLLMs. <div>
arXiv:2512.10310v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation</title>
<link>https://arxiv.org/abs/2512.10314</link>
<guid>https://arxiv.org/abs/2512.10314</guid>
<content:encoded><![CDATA[
<div> prototype-driven learning, weakly supervised semantic segmentation, vision-language alignment, digital pathology, multi-scale pyramid module<br /><br />Summary:<br /><br />This paper addresses the challenge of weakly supervised semantic segmentation (WSSS) in histopathology, which aims to minimize annotation costs by using only image-level labels. The problem is complicated by inter-class similarity, intra-class variability, and limitations of class activation map (CAM)-based supervision caused by region-shrinkage effects. To overcome these issues, the authors propose a prototype-driven framework that leverages vision-language alignment. This approach employs CoOp-style learnable prompt tuning to create text-based prototypes and combines these with learned image prototypes, forming a dual-modal prototype bank that integrates semantic and visual information for better region discovery. Additionally, to counter oversmoothing in Vision Transformer (ViT) features, a multi-scale pyramid module is included to enhance spatial detail and improve localization accuracy. Experimental evaluation on the BCSS-WSSS benchmark demonstrates that the method outperforms current state-of-the-art techniques. Further analysis reveals that diversity in text descriptions, the length of context, and the complementary interplay between text and image prototypes contribute to the improvements. Overall, the study emphasizes that jointly exploiting textual semantics with visual prototype learning offers a promising direction for WSSS in digital pathology. <div>
arXiv:2512.10314v1 Announce Type: new 
Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation</title>
<link>https://arxiv.org/abs/2512.10316</link>
<guid>https://arxiv.org/abs/2512.10316</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised semantic segmentation, histopathology, vision-language models, prototype learning, pseudo-masks

<br /><br />Summary:  
This paper addresses the challenges of weakly supervised semantic segmentation (WSSS) in histopathological images, focusing on capturing the complete spatial extent of tissue structures beyond the most discriminative regions. It leverages the complementary strengths of vision-language models like CONCH, which offer morphology-aware semantic alignment, and SegFormer, a modern segmentation backbone that preserves fine-grained spatial details. The authors propose a novel prototype learning framework that integrates morphology-aware features from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment. A key innovation is the text-guided prototype initialization, using pathology descriptions to generate more complete and semantically accurate pseudo-masks without requiring dense pixel annotations. To maintain fine-grained morphology and tissue boundaries, a structural distillation mechanism is employed to transfer spatial knowledge from SegFormer during prototype learning. The method enhances localization completeness and semantic consistency across different tissue types. Experimental results on BCSS-WSSS datasets demonstrate that the proposed framework outperforms existing WSSS approaches while maintaining computational efficiency through frozen foundation model backbones and lightweight trainable adapters. This work advances weakly supervised segmentation in histopathology by effectively combining language and vision models under limited annotation conditions. <div>
arXiv:2512.10316v1 Announce Type: new 
Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset</title>
<link>https://arxiv.org/abs/2512.10321</link>
<guid>https://arxiv.org/abs/2512.10321</guid>
<content:encoded><![CDATA[
<div> 3D human pose estimation, generative model, point cloud, spatio-temporal encoder, multi-modal dataset<br /><br />Summary:<br /><br />The paper proposes Point2Pose, a novel generative framework designed to improve 3D human pose estimation by effectively modeling the distribution of human poses conditioned on sequential point clouds and pose history. It addresses major challenges in pose estimation, such as the complex geometry of the human body, self-occlusion of joints, and the need for large-scale real-world motion datasets. The approach leverages a spatio-temporal point cloud encoder alongside a pose feature encoder to extract detailed joint-wise features. These features are then processed by an attention-based generative regressor to predict accurate 3D human poses. Additionally, the authors introduce MVPose3D, a large-scale indoor dataset that offers multiple modalities including IMU data capturing non-trivial human motions, dense multi-view point clouds, and RGB images. This comprehensive dataset supports robust training and evaluation of pose estimation models. Experimental results across multiple datasets demonstrate that Point2Pose outperforms existing baseline models, showcasing its superior accuracy and effectiveness in dealing with complex motions and occlusions in 3D human pose estimation. <div>
arXiv:2512.10321v1 Announce Type: new 
Abstract: We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs</title>
<link>https://arxiv.org/abs/2512.10324</link>
<guid>https://arxiv.org/abs/2512.10324</guid>
<content:encoded><![CDATA[
<div> Audio-Visual Large Language Models, Token reduction, Cross-Modal Semantic Sieve, Synchronization-Augmented RoPE, EchoingPixels<br /><br />Summary:<br /><br />This paper addresses the computational challenges faced by Audio-Visual Large Language Models (AV-LLMs) due to the large number of audio and video tokens. Unlike existing token reduction methods that work on unimodal video LLMs, these are inadequate for joint audio-visual data since they fail to exploit cross-modal synergies and use fixed modality budgets, which do not adapt to dynamic information densities. To overcome this, the authors propose EchoingPixels, a novel framework inspired by real-world interactions of visuals and sound. The core innovation is the Cross-Modal Semantic Sieve (CS2), which enables early interaction and co-attention across the combined audio-visual token pool, allowing adaptive token budget allocation and more synergistic token reduction. To preserve essential temporal dependencies during aggressive token pruning, they introduce Synchronization-Augmented RoPE (Sync-RoPE), which enhances temporal modeling for the sparsely selected tokens. Experimental results demonstrate that EchoingPixels achieves comparable performance to strong baselines while using only 5-20% of the original tokens, leading to 2-3 times faster processing and significant memory savings. This work effectively advances efficient token reduction for joint audio-visual large language modeling. <div>
arXiv:2512.10324v1 Announce Type: new 
Abstract: Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology</title>
<link>https://arxiv.org/abs/2512.10326</link>
<guid>https://arxiv.org/abs/2512.10326</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, self-supervised learning, special stains, vision transformer, computational pathology<br /><br />Summary:<br /><br />1. Foundation models trained via self-supervised learning (SSL) on large histological image datasets have advanced computational pathology significantly. 2. These models are commonly used as backbones for analyzing regions-of-interest (ROI) or as patch-level feature extractors in whole-slide images (WSIs) using multiple instance learning (MIL). 3. Existing pathology foundation models (PFMs) are predominantly pre-trained on Hematoxylin-Eosin (H&amp;E) stained images, limiting performance on special stains like immunohistochemistry (IHC), which are prevalent in clinical workflows. 4. To address this limitation, the authors propose StainNet, a specialized foundation model designed for special stains, which is based on the vision transformer (ViT) architecture. 5. StainNet employs a self-distillation SSL training strategy on over 1.4 million image patches extracted from 20,231 publicly available special staining WSIs in the HISTAI database. 6. Evaluation tasks include an in-house slide-level liver malignancy classification and two public ROI-level datasets, demonstrating StainNet’s superior performance. 7. Additional assessments include few-ratio learning and image retrieval experiments, where StainNet outperforms even larger publicly available PFMs. 8. The trained model weights for StainNet have been publicly released at the given Hugging Face link for community use and further research. <div>
arXiv:2512.10326v1 Announce Type: new 
Abstract: Foundation models trained with self-supervised learning (SSL) on large-scale histological images have significantly accelerated the development of computational pathology. These models can serve as backbones for region-of-interest (ROI) image analysis or patch-level feature extractors in whole-slide images (WSIs) based on multiple instance learning (MIL). Existing pathology foundation models (PFMs) are typically pre-trained on Hematoxylin-Eosin (H&amp;E) stained pathology images. However, images with special stains, such as immunohistochemistry, are also frequently used in clinical practice. PFMs pre-trained mainly on H\&amp;E-stained images may be limited in clinical applications involving special stains. To address this issue, we propose StainNet, a specialized foundation model for special stains based on the vision transformer (ViT) architecture. StainNet adopts a self-distillation SSL approach and is trained on over 1.4 million patch images cropping from 20,231 publicly available special staining WSIs in the HISTAI database. To evaluate StainNet, we conduct experiments on an in-house slide-level liver malignancy classification task and two public ROI-level datasets to demonstrate its strong ability. We also perform few-ratio learning and retrieval evaluations, and compare StainNet with recently larger PFMs to further highlight its strengths. We have released the StainNet model weights at: https://huggingface.co/JWonderLand/StainNet.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering</title>
<link>https://arxiv.org/abs/2512.10327</link>
<guid>https://arxiv.org/abs/2512.10327</guid>
<content:encoded><![CDATA[
<div> Keywords: incomplete multi-view data, selective imputation, clustering, variational autoencoder, imputation uncertainty<br /><br />Summary:<br />Incomplete multi-view data pose challenges for clustering due to missing and unbalanced observations across views. Traditional imputation-based methods attempt to fill in missing data but often add noise and bias when data support is insufficient. In contrast, imputation-free methods use only observed data but fail when incompleteness is severe since they lack cross-view complementary information. To overcome these issues, the authors propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC), which evaluates the informativeness of each missing entry based on intra-view similarity and cross-view consistency, performing imputation selectively only when supported by sufficient evidence. ISMVC employs a variational autoencoder with a mixture-of-Gaussians prior to learn clustering-friendly latent representations, allowing distribution-level imputation that stabilizes posterior aggregation and explicitly models imputation uncertainty. This approach enhances robust fusion and prevents overconfident reconstructions. Unlike other cautious imputation methods that rely on training dynamics or model feedback, ISMVC is lightweight, data-driven, and model-agnostic, enabling easy integration as a plug-in module into existing incomplete multi-view clustering (IMC) frameworks. Extensive experiments on multiple benchmark datasets under realistic unbalanced missing scenarios demonstrate that ISMVC consistently outperforms both imputation-based and imputation-free state-of-the-art methods. <div>
arXiv:2512.10327v1 Announce Type: new 
Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images</title>
<link>https://arxiv.org/abs/2512.10334</link>
<guid>https://arxiv.org/abs/2512.10334</guid>
<content:encoded><![CDATA[
<div> Keywords: filament segmentation, deep learning, Pix2Pix, synthetic data generation, filament-aware structural loss  

<br /><br />Summary:  
1. The article addresses the challenge of segmenting thin and elongated filamentous structures like microtubules and actin filaments in biological microscopy images, which is crucial for quantitative biological analysis.  
2. A key difficulty in this domain is the scarcity of high-quality pixel-level annotated datasets due to the dense and complex geometry of filaments, making manual annotation highly labor-intensive and time-consuming.  
3. To overcome the data shortage, the authors propose a conditional generative framework based on the Pix2Pix architecture to synthesize realistic filament images from binary masks, thereby augmenting training datasets.  
4. They introduce a new filament-aware structural loss function designed to enhance structural similarity during image generation, ensuring that synthetic filaments maintain realistic geometric features.  
5. Experimental results demonstrate that models trained with the aid of this synthetic data outperform those trained solely on original real data, validating the effectiveness of the proposed approach in improving filament segmentation performance. <div>
arXiv:2512.10334v1 Announce Type: new 
Abstract: Thin and elongated filamentous structures, such as microtubules and actin filaments, often play important roles in biological systems. Segmenting these filaments in biological images is a fundamental step for quantitative analysis. Recent advances in deep learning have significantly improved the performance of filament segmentation. However, there is a big challenge in acquiring high quality pixel-level annotated dataset for filamentous structures, as the dense distribution and geometric properties of filaments making manual annotation extremely laborious and time-consuming. To address the data shortage problem, we propose a conditional generative framework based on the Pix2Pix architecture to generate realistic filaments in microscopy images from binary masks. We also propose a filament-aware structural loss to improve the structure similarity when generating synthetic images. Our experiments have demonstrated the effectiveness of our approach and outperformed existing model trained without synthetic data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot Adaptation of Stable Diffusion via Plug-in Hierarchical Degradation Representation for Real-World Super-Resolution</title>
<link>https://arxiv.org/abs/2512.10340</link>
<guid>https://arxiv.org/abs/2512.10340</guid>
<content:encoded><![CDATA[
<div> Real-World Image Super-Resolution, Diffusion Models, HD-CLIP, Classifier-Free Guidance, Degradation Embedding<br /><br />Summary:<br /><br />This paper addresses the challenge of Real-World Image Super-Resolution (Real-ISR), where input images suffer from unknown and complex degradations. The authors identify limitations in existing methods that often assume known degradation severity and rely on CLIP text encoders unable to capture numerical severity, thus restricting generalization. To overcome these issues, they introduce HD-CLIP (Hierarchical Degradation CLIP), which decomposes a low-quality image into two parts: a semantic embedding that captures image content and an ordinal degradation embedding that encodes ordered relationships among degradation levels. This enables interpolation across unseen degradation severities. HD-CLIP is integrated into diffusion models through two guidance schemes: the existing classifier-free guidance (CFG) and a novel classifier-free projection guidance (CFPG). The semantic cues guide the generative restoration process, while degradation cues help suppress hallucinations and artifacts. Importantly, HD-CLIP is designed as a plug-and-play module, requiring no additional training when embedded into various super-resolution frameworks. Experimental results demonstrate that HD-CLIP significantly enhances detail fidelity and perceptual realism on diverse real-world datasets, improving generalization to previously unseen degradations. <div>
arXiv:2512.10340v1 Announce Type: new 
Abstract: Real-World Image Super-Resolution (Real-ISR) aims to recover high-quality images from low-quality inputs degraded by unknown and complex real-world factors. Real-world scenarios involve diverse and coupled degradations, making it necessary to provide diffusion models with richer and more informative guidance. However, existing methods often assume known degradation severity and rely on CLIP text encoders that cannot capture numerical severity, limiting their generalization ability. To address this, we propose \textbf{HD-CLIP} (\textbf{H}ierarchical \textbf{D}egradation CLIP), which decomposes a low-quality image into a semantic embedding and an ordinal degradation embedding that captures ordered relationships and allows interpolation across unseen levels. Furthermore, we integrated it into diffusion models via classifier-free guidance (CFG) and proposed classifier-free projection guidance (CFPG). HD-CLIP leverages semantic cues to guide generative restoration while using degradation cues to suppress undesired hallucinations and artifacts. As a \textbf{plug-and-play module}, HD-CLIP can be seamlessly integrated into various super-resolution frameworks without training, significantly improving detail fidelity and perceptual realism across diverse real-world datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates</title>
<link>https://arxiv.org/abs/2512.10342</link>
<guid>https://arxiv.org/abs/2512.10342</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, Sequential Planning, Error Detection, Scene Graph Incremental updates, Corrective Planning<br /><br />Summary:<br /><br />This paper addresses the underexplored area of visual sequential planning within large-scale Vision-Language Models (VLMs), focusing on their ability to execute and correct multi-step actions toward a goal. The authors introduce the Corrective Sequential Planning Benchmark (CoSPlan), designed to evaluate VLMs across four distinct domains: maze navigation, block rearrangement, image reconstruction, and object reorganization. CoSPlan specifically tests two critical skills: Error Detection (identifying non-optimal or erroneous steps) and Step Completion (correcting and finishing the action sequence to successfully reach the goal). Current state-of-the-art VLMs, like Intern-VLM and Qwen2, struggle with this task, often failing to use contextual information effectively. To overcome these limitations, the authors propose a novel, training-free method called Scene Graph Incremental updates (SGI), which introduces intermediate reasoning states between the initial and final goals to support stepwise reasoning. SGI improves VLM performance on CoSPlan by an average of 5.2%, enhancing the models' reliability in handling corrective sequential planning problems. Additionally, SGI demonstrates strong generalization capabilities, showing effectiveness in other traditional planning tasks such as Plan-Bench and Visual Question Answering (VQA), thus highlighting its broader applicability. <div>
arXiv:2512.10342v1 Announce Type: new 
Abstract: Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology-Agnostic Animal Motion Generation from Text Prompt</title>
<link>https://arxiv.org/abs/2512.10352</link>
<guid>https://arxiv.org/abs/2512.10352</guid>
<content:encoded><![CDATA[
<div> Keywords: animal motion, motion generation, skeletal topology, text-driven animation, cross-species transfer<br /><br />Summary:<br /><br />1. The paper addresses a core limitation in motion generation: current methods largely depend on fixed skeletal templates, limiting generalization to different or perturbed skeleton topologies.<br />2. To overcome this, the authors introduce OmniZoo, a large-scale, heterogeneous animal motion dataset containing 32,979 sequences across 140 species, enriched with multimodal annotations.<br />3. Building on this dataset, they propose a generalized autoregressive motion generation framework that can produce text-driven motions adaptable to arbitrary skeletal topologies.<br />4. A key innovation is the Topology-aware Skeleton Embedding Module, which encodes geometric and structural properties of any skeleton into a shared token space, allowing seamless integration with textual semantics.<br />5. The resulting model generates temporally coherent, physically plausible, and semantically aligned motions from text prompts and enables cross-species motion style transfer, advancing motion synthesis flexibility and realism across diverse animal forms. <div>
arXiv:2512.10352v1 Announce Type: new 
Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation</title>
<link>https://arxiv.org/abs/2512.10353</link>
<guid>https://arxiv.org/abs/2512.10353</guid>
<content:encoded><![CDATA[
<div> Weakly supervised semantic segmentation, volumetric medical imaging, Vision Transformer, state space models, Cross-Plane Mamba blocks

<br /><br />Summary: This paper introduces TranSamba, a novel hybrid architecture combining Transformers and Mamba blocks for weakly supervised semantic segmentation in volumetric medical imaging. Traditional methods often use 2D encoders that fail to fully exploit the three-dimensional nature of volumetric data, limiting performance. TranSamba addresses this by augmenting a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which utilize state space models to efficiently enable information exchange across adjacent slices with linear time complexity. This design enhances pairwise self-attention within individual slices, producing more accurate attention maps critical for object localization. The architecture maintains constant memory usage during batch processing, making it scalable for large volumetric inputs. Extensive experiments conducted on three different medical imaging datasets reveal that TranSamba consistently outperforms existing state-of-the-art methods across various imaging modalities and pathological conditions. The study demonstrates the effectiveness of integrating volumetric context through efficient cross-slice communication, offering a label-efficient and computationally practical solution for medical image segmentation tasks. The authors have made their source code and pretrained models publicly available to facilitate further research and application development in this area. <div>
arXiv:2512.10353v1 Announce Type: new 
Abstract: Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar</title>
<link>https://arxiv.org/abs/2512.10357</link>
<guid>https://arxiv.org/abs/2512.10357</guid>
<content:encoded><![CDATA[
<div> Keywords: mmWave radar, static people counting, ultra-low frequency signals, signal processing, indoor spaces<br /><br />Summary:<br /><br />1. mmWave radars face challenges in detecting or counting individuals within dense, static groups because of limited spatial resolution and the traditional dependence on movement for detection.  
2. The proposed system, mmCounter, addresses this challenge by focusing on ultra-low frequency signals (below 1 Hz) generated by subtle body movements such as breathing and micro torso shifts.  
3. mmCounter uses an innovative multi-stage signal processing pipeline to extract these subtle low-frequency signals, isolate them from background noise and static objects, and associate spatial information with each individual to enable accurate counting.  
4. Unlike previous approaches that estimate breathing rates assuming a known number of people, mmCounter does not require prior knowledge of how many people are present.  
5. Experimental evaluations show that mmCounter achieves an average F1 score of 87% with a 0.6 mean absolute error in familiar environments, and a 60% F1 score with a 1.1 mean absolute error in new, untested environments.  
6. The system is capable of counting up to seven individuals in highly dense indoor spaces (three people per square meter), even when people stand without side-by-side spacing and only one meter apart front-to-back. <div>
arXiv:2512.10357v1 Announce Type: new 
Abstract: mmWave radars struggle to detect or count individuals in dense, static (non-moving) groups due to limitations in spatial resolution and reliance on movement for detection. We present mmCounter, which accurately counts static people in dense indoor spaces (up to three people per square meter). mmCounter achieves this by extracting ultra-low frequency (< 1 Hz) signals, primarily from breathing and micro-scale body movements such as slight torso shifts, and applying novel signal processing techniques to differentiate these subtle signals from background noise and nearby static objects. Our problem differs significantly from existing studies on breathing rate estimation, which assume the number of people is known a priori. In contrast, mmCounter utilizes a novel multi-stage signal processing pipeline to extract relevant low-frequency sources along with their spatial information and map these sources to individual people, enabling accurate counting. Extensive evaluations in various environments demonstrate that mmCounter delivers an 87% average F1 score and 0.6 mean absolute error in familiar environments, and a 60% average F1 score and 1.1 mean absolute error in previously untested environments. It can count up to seven individuals in a three square meter space, such that there is no side-by-side spacing and only a one-meter front-to-back distance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task</title>
<link>https://arxiv.org/abs/2512.10359</link>
<guid>https://arxiv.org/abs/2512.10359</guid>
<content:encoded><![CDATA[
<div> VideoQA, Multimodal Large Language Models, Spatiotemporal Reasoning, Video Toolkit, STAR framework<br /><br />Summary: This work addresses the challenge of Video Question Answering (VideoQA), focusing on improving Multimodal Large Language Models' (MLLMs) ability to understand and reason about spatial relationships within frames and temporal dynamics across videos. The authors introduce a comprehensive and extensible Video Toolkit designed to enhance MLLMs' spatiotemporal reasoning capabilities by providing a balanced variety and quantity of tools. To effectively manage tool usage and prevent shortcut issues in toolchains, they propose the Spatiotemporal Reasoning Framework (STAR), which strategically schedules temporal and spatial tools to progressively identify key areas within the video content. By integrating STAR with GPT-4o and leveraging lightweight tools, the method achieves significant performance improvements, showing an 8.2% accuracy gain on the VideoMME benchmark and a 4.6% improvement on the LongVideoBench dataset. The advancements highlight the potential of combining strategic tool invocation with large language models to build more autonomous and intelligent video analysis assistants. The accompanying code for this work is publicly accessible, fostering further research and development in video understanding and reasoning tasks. <div>
arXiv:2512.10359v1 Announce Type: new 
Abstract: Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.10362</link>
<guid>https://arxiv.org/abs/2512.10362</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Contextual Blindness, Visual Funnel, Contextual Anchoring, Entropy-Scaled Portfolio<br /><br />Summary:<br /><br />Multimodal Large Language Models (MLLMs) have strong reasoning skills but often miss fine-grained visual details, which limits their use in tasks requiring precision. Cropping salient image regions helps but introduces "Contextual Blindness," a problem where details from cropped regions become disconnected from the overall image context. This issue is caused not by insufficient information quantity but by a lack of structural diversity in the input. To address this, the paper proposes Visual Funnel, a training-free method consisting of two steps. First, Contextual Anchoring identifies the region of interest in one forward pass. Second, an Entropy-Scaled Portfolio is constructed to maintain hierarchical context—from detailed focal regions to broader surroundings—by dynamically adjusting crop sizes based on attention entropy and refining crop centers. Extensive experiments show Visual Funnel significantly outperforms both naive single-crop and unstructured multi-crop methods. Moreover, simply adding more random crops yields limited or negative effects, highlighting the importance of maintaining hierarchical crop structure to overcome Contextual Blindness effectively. This approach enhances the model's ability to integrate fine visual details without losing the global scene context. <div>
arXiv:2512.10362v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos</title>
<link>https://arxiv.org/abs/2512.10363</link>
<guid>https://arxiv.org/abs/2512.10363</guid>
<content:encoded><![CDATA[
<div> Keywords: Zero-shot Long Video Moment Retrieval, Search-then-Refine, Adaptive Span Generator, Query Decomposition, Temporal Grounding

<br /><br />Summary:  
Zero-shot Long Video Moment Retrieval (ZLVMR) focuses on identifying precise temporal segments within hour-long videos using natural language queries without requiring task-specific training. The core technical challenge is the computational difficulty in processing very long videos in a single pass. The prevailing strategy, known as the 'Search-then-Refine' approach, reduces video segments rapidly in the search phase and analyzes only the shortlisted portions during refinement. However, existing supervised methods for this task suffer from poor generalization and scalability despite high resource demands. Zero-shot methods currently fail due to two main issues: an explosion in candidates during the search phase caused by heuristic strategies and the need for computationally expensive vision-language model (VLM) verification during refinement, which also suffers from semantic discrepancies. To address these, the proposed framework, Point-to-Span (P2S), introduces two key innovations. First, the Adaptive Span Generator mitigates the candidate explosion in the search phase efficiently. Second, Query Decomposition enables candidate refinement without costly VLM verification. P2S is notable as the first zero-shot framework capable of temporal grounding in hour-long videos and surpasses supervised state-of-the-art methods significantly, demonstrating a +3.7% improvement on R5@0.1 on the MAD dataset. <div>
arXiv:2512.10363v1 Announce Type: new 
Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views</title>
<link>https://arxiv.org/abs/2512.10369</link>
<guid>https://arxiv.org/abs/2512.10369</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, sparse views, motion blur, dual-prior strategy, novel view synthesis<br /><br />Summary:  
3D Gaussian Splatting (3DGS) is a leading technique for novel view synthesis but struggles under real-world conditions where input images are sparse and motion-blurred. These limitations cause a negative feedback loop: sparse views reduce multi-view constraints necessary to resolve motion blur, while motion blur removes key high-frequency details needed for accurate view alignment, resulting in poor reconstruction with fragmented and low-frequency biased outputs. To solve this, the authors propose CoherentGS, a novel framework designed for high-fidelity 3D reconstruction from sparse and blurry images. The core idea is a dual-prior strategy integrating two pre-trained generative models: a specialized deblurring network that restores sharp details and provides photometric guidance, alongside a diffusion model that supplies geometric priors to reconstruct unobserved scene regions. Supporting this are innovative components such as a consistency-guided camera exploration module for adaptive generative process control and a depth regularization loss to maintain geometric plausibility. Extensive experiments on synthetic and real-world datasets with as few as 3 to 9 views demonstrate that CoherentGS significantly outperforms current methods, establishing a new state-of-the-art in handling challenging inputs for 3D reconstruction tasks. The authors provide code and demos publicly, facilitating further research and application. <div>
arXiv:2512.10369v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2512.10376</link>
<guid>https://arxiv.org/abs/2512.10376</guid>
<content:encoded><![CDATA[
<div> Radar-LiDAR fusion, scene flow estimation, 4D millimeter wave radar, Dynamic-aware Bidirectional Cross-modal Fusion, automotive dataset<br /><br />Summary:<br /><br />This paper addresses the unexplored fusion of 4D millimeter wave radar and LiDAR sensors for scene flow estimation, an important task in autonomous driving and robotics. Unlike LiDAR, radar is cost-effective, robust in adverse weather, and provides point-wise velocity data, but presents challenges such as noise, low resolution, and sparsity. To overcome the lack of datasets combining radar and LiDAR for this purpose, the authors construct a novel Radar-LiDAR scene flow dataset by leveraging a public automotive dataset and propose preprocessing strategies for radar denoising and accurate scene flow label generation. They introduce RaLiFlow, the first joint learning framework integrating 4D radar and LiDAR data for scene flow estimation. Central to RaLiFlow is the Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module, which incorporates dynamic radar cues via local cross-attention to enhance multimodal contextual information sharing. Additionally, tailored loss functions are designed to suppress unreliable radar data effects during training and improve instance-level consistency in scene flow predictions, particularly for dynamic foreground objects. Extensive experiments demonstrate that RaLiFlow significantly outperforms existing single-modal methods based on either LiDAR or radar alone, validating the benefits of effective radar-LiDAR fusion in scene flow tasks. <div>
arXiv:2512.10376v1 Announce Type: new 
Abstract: Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching</title>
<link>https://arxiv.org/abs/2512.10379</link>
<guid>https://arxiv.org/abs/2512.10379</guid>
<content:encoded><![CDATA[
<div> Keywords: endoscopic image matching, deep learning, self-supervised learning, contrastive learning, surgical navigation  

<br /><br />Summary:  
This paper addresses the challenge of establishing accurate pixel-level correspondences between endoscopic image pairs in minimally invasive surgery, which is crucial for 3D reconstruction, camera tracking, and scene understanding. Traditional computer vision methods struggle in the surgical domain due to weak perspective cues, non-Lambertian reflections, and deformable tissue anatomy. The authors propose a novel deep learning pipeline specifically designed for surgical image matching. Their approach uses a self-supervised optimization framework that employs a novel-view synthesis technique to generate ground-truth inlier correspondences, which are then used to mine triplets for contrastive learning. They enhance the DINOv2 backbone by adding a Transformer layer optimized for producing embeddings that facilitate direct feature matching via cosine similarity thresholding. Experimental results on the SCARED datasets show that the proposed method outperforms state-of-the-art approaches, achieving improved matching precision and reduced epipolar error. This work contributes a targeted solution to the unique challenges of surgical vision and paves the way for more precise and reliable high-level computer vision tasks in surgical endoscopy, such as augmented reality integration and surgical navigation assistance. <div>
arXiv:2512.10379v1 Announce Type: new 
Abstract: Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness. In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation. However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques. While Deep Learning models have shown strong performance in natural scenes, their features are not inherently suited for fine-grained matching in surgical images and require targeted adaptation to meet the demands of this domain. This research presents a novel Deep Learning pipeline for establishing feature correspondences in endoscopic image pairs, alongside a self-supervised optimization framework for model training. The proposed methodology leverages a novel-view synthesis pipeline to generate ground-truth inlier correspondences, subsequently utilized for mining triplets within a contrastive learning paradigm. Through this self-supervised approach, we augment the DINOv2 backbone with an additional Transformer layer, specifically optimized to produce embeddings that facilitate direct matching through cosine similarity thresholding. Experimental evaluation demonstrates that our pipeline surpasses state-of-the-art methodologies on the SCARED datasets improved matching precision and lower epipolar error compared to the related work. The proposed framework constitutes a valuable contribution toward enabling more accurate high-level computer vision applications in surgical endoscopy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies</title>
<link>https://arxiv.org/abs/2512.10384</link>
<guid>https://arxiv.org/abs/2512.10384</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision Language Models, Fine-grained Recognition, FROW Benchmark, Data Construction, Training Optimization<br /><br />Summary: This paper addresses the limitation of current Large Vision Language Models (LVLMs) benchmarks that focus mostly on reasoning tasks while overlooking fine-grained recognition, which is essential for many practical applications. To fill this gap, the authors introduce the Fine-grained Recognition Open World (FROW) benchmark, developed with the assistance of GPT-4o, aimed at providing a detailed evaluation framework for LVLMs in fine-grained recognition tasks. They propose a novel optimization strategy focusing on two main aspects: data construction and training process. The dataset includes mosaic data, which aggregates multiple short-answer responses, and open-world data, generated from real-world questions and answers via GPT-4o, enhancing the comprehensiveness of the evaluation. Experimental results demonstrate that incorporating mosaic data leads to a 1% improvement in category recognition accuracy, while the use of open-world data increases FROW benchmark accuracy by 10%-20% and content accuracy by 6%-12%. Furthermore, integrating fine-grained data into the pre-training phase boosts category recognition accuracy by up to 10%. The benchmark and associated resources are made publicly available at the provided GitHub repository, supporting further research and development in this domain. <div>
arXiv:2512.10384v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</title>
<link>https://arxiv.org/abs/2512.10386</link>
<guid>https://arxiv.org/abs/2512.10386</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud denoising, LiDAR noise reduction, adaptive dual-weight gravitational method, real-time processing, edge preservation  

<br /><br />Summary:  
This paper addresses the challenge of denoising LiDAR-based point cloud data, which is essential for applications like autonomous driving and 3D reconstruction but is often plagued by noisy points that degrade detection and recognition accuracy. The authors identify that existing denoising methods either compromise on computational efficiency or fail to preserve object boundaries and fine structural details, making it difficult to achieve simultaneous high denoising accuracy, edge preservation, and real-time processing. To overcome these limitations, they propose an adaptive dual-weight gravitational-based point cloud denoising method. The approach begins by using an octree for spatial partitioning to enable parallel acceleration of the denoising process. Within each spatial subdivision (leaf node), adaptive voxel-based occupancy statistics combined with k-nearest neighbor (kNN) density estimation quickly filter out isolated and low-density noise points, reducing the candidate pool for detailed analysis. A novel gravitational scoring function, integrating both density and adaptive distance weights, is then employed to accurately differentiate noise from valid object points. Extensive experiments on the Stanford 3D Scanning Repository, CADC dataset, and in-house FMCW LiDAR data demonstrate that this method outperforms existing techniques in terms of F1 score, PSNR, and Chamfer Distance across various noisy conditions, while also reducing per-frame processing time. This validates the approach’s strong denoising accuracy, robustness, edge preservation, and real-time capability in multi-noise environments. <div>
arXiv:2512.10386v1 Announce Type: new 
Abstract: High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos</title>
<link>https://arxiv.org/abs/2512.10408</link>
<guid>https://arxiv.org/abs/2512.10408</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal hate speech, weak supervision, temporal localisation, cross-modal fusion, multiple instance learning<br /><br />Summary:<br /><br />1. The paper addresses the growing issue of multimodal hate speech on video platforms like TikTok and YouTube, where hateful content appears subtly and asynchronously across visual, acoustic, and textual streams. 2. Unlike prior work focused mostly on video-level hate speech classification, this study emphasizes the more practical and challenging task of temporal localisation, which pinpoints the exact time segments containing hate speech within a video. 3. The authors highlight the difficulties posed by weak supervision—where only video-level labels are given—making it hard for static fusion or classification methods to effectively capture temporal and cross-modal dynamics. 4. They propose MultiHateLoc, a novel framework specifically designed for weakly-supervised multimodal hate speech localisation. The framework incorporates modality-aware temporal encoders, dynamic cross-modal fusion to leverage the most informative modalities over time, and a cross-modal contrastive alignment strategy for consistent multimodal features. 5. Additionally, MultiHateLoc uses a modality-aware multiple instance learning (MIL) objective to identify discriminative hateful segments using only coarse video labels, enabling fine-grained, interpretable frame-level hate speech detection. Experimental results on the HateMM and MultiHateClip datasets demonstrate that MultiHateLoc achieves state-of-the-art performance in temporal hate speech localisation. <div>
arXiv:2512.10408v1 Announce Type: new 
Abstract: The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction</title>
<link>https://arxiv.org/abs/2512.10416</link>
<guid>https://arxiv.org/abs/2512.10416</guid>
<content:encoded><![CDATA[
<div> road extraction, off-road mapping, WildRoad dataset, MaGRoad, path-centric framework<br /><br />Summary:<br /><br />1. The paper tackles the challenges of vectorized road extraction in off-road environments, which are less studied compared to urban areas due to domain gaps and data scarcity.<br /><br />2. Existing methods like SAM-Road rely on node-centric approaches focusing on sparse endpoints, leading to fragility against occlusions and ambiguous road junctions in wild terrains, causing topological errors.<br /><br />3. To address these issues, the authors present WildRoad, a large-scale global off-road road network dataset created with an interactive annotation tool designed specifically for efficient and accurate road-network labeling.<br /><br />4. They propose MaGRoad (Mask-aware Geodesic Road network extractor), a novel path-centric approach that aggregates multi-scale visual evidence along candidate paths to robustly infer road connectivity.<br /><br />5. Experimental results demonstrate that MaGRoad not only achieves state-of-the-art accuracy on the WildRoad benchmark but also generalizes effectively to urban road extraction tasks while delivering approximately 2.5 times faster inference, enhancing its practical usability.<br /><br />Ultimately, this work establishes a new dataset and a robust, efficient framework, significantly advancing road mapping capabilities in complex off-road scenarios. <div>
arXiv:2512.10416v1 Announce Type: new 
Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning</title>
<link>https://arxiv.org/abs/2512.10419</link>
<guid>https://arxiv.org/abs/2512.10419</guid>
<content:encoded><![CDATA[
<div> Keywords: aerial-ground localization, LiDAR, cross-modal attention, bird's-eye-view, contrastive learning<br /><br />Summary:<br /><br />Aerial-ground localization poses significant challenges due to the large viewpoint and modality differences between ground-level LiDAR data and overhead aerial imagery. To address this, the paper introduces TransLocNet, a novel cross-modal attention framework that effectively fuses LiDAR geometry with semantic context extracted from aerial images. TransLocNet first projects LiDAR scans into a bird's-eye-view representation, enabling spatial alignment with aerial features through a bidirectional attention mechanism. This alignment enhances the network's ability to capture correspondences between the modalities. The framework includes a likelihood map decoder that outputs spatial probability distributions over a vehicle's position and orientation, allowing precise localization estimates. Additionally, a contrastive learning module is incorporated to enforce a shared embedding space, which further improves cross-modal alignment and generalization. Experimental evaluation on both synthetic data from CARLA and real-world KITTI datasets demonstrates that TransLocNet significantly outperforms existing state-of-the-art baselines, reducing localization errors by up to 63%. The model achieves sub-meter translation accuracy and sub-degree orientation accuracy, highlighting its robustness and effectiveness. Overall, TransLocNet provides a generalizable approach to aerial-ground localization applicable to diverse environments and sensor modalities. <div>
arXiv:2512.10419v1 Announce Type: new 
Abstract: Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Collapse in Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.10421</link>
<guid>https://arxiv.org/abs/2512.10421</guid>
<content:encoded><![CDATA[
<div> Neural Collapse, Sample-wise Alignment, Test-Time Adaptation, Domain Shift, Pseudo-label Reliability<br /><br />Summary: This paper investigates the degradation of deep neural network performance under domain shifts during test-time adaptation (TTA). It builds upon the concept of Neural Collapse (NC), a geometric property where feature embeddings align with classifier weights, by extending it to a sample-wise level and identifying a novel phenomenon called Sample-wise Alignment Collapse (NC3+). The authors demonstrate that performance drops are caused by misalignment between sample features and classifier weights, which worsens with larger distribution shifts. To counter this, they argue that realigning features to classifier weights is necessary for robust adaptation. However, pseudo-labels—which are often used in adaptation—become unreliable under such shifts, complicating realignment efforts. To overcome this challenge, the authors propose NCTTA, a new method that combines geometric closeness of features to classifier weights with predictive confidence, forming hybrid targets to minimize the negative impact of incorrect pseudo-labels. Extensive experiments validate NCTTA’s effectiveness, showing significant improvement over existing approaches. Specifically, NCTTA outperforms the prior state-of-the-art method Tent by a margin of 14.52% on the ImageNet-C corrupted dataset, highlighting its robustness against diverse domain corruptions and distribution changes. <div>
arXiv:2512.10421v1 Announce Type: new 
Abstract: Test-Time Adaptation (TTA) enhances model robustness to out-of-distribution (OOD) data by updating the model online during inference, yet existing methods lack theoretical insights into the fundamental causes of performance degradation under domain shifts. Recently, Neural Collapse (NC) has been proposed as an emergent geometric property of deep neural networks (DNNs), providing valuable insights for TTA. In this work, we extend NC to the sample-wise level and discover a novel phenomenon termed Sample-wise Alignment Collapse (NC3+), demonstrating that a sample's feature embedding, obtained by a trained model, aligns closely with the corresponding classifier weight. Building on NC3+, we identify that the performance degradation stems from sample-wise misalignment in adaptation which exacerbates under larger distribution shifts. This indicates the necessity of realigning the feature embeddings with their corresponding classifier weights. However, the misalignment makes pseudo-labels unreliable under domain shifts. To address this challenge, we propose NCTTA, a novel feature-classifier alignment method with hybrid targets to mitigate the impact of unreliable pseudo-labels, which blends geometric proximity with predictive confidence. Extensive experiments demonstrate the effectiveness of NCTTA in enhancing robustness to domain shifts. For example, NCTTA outperforms Tent by 14.52% on ImageNet-C.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time</title>
<link>https://arxiv.org/abs/2512.10437</link>
<guid>https://arxiv.org/abs/2512.10437</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time identification, physiotherapy exercises, pose estimation, dynamic programming, mobile devices<br /><br />Summary:<br /><br />This article introduces an efficient algorithmic framework designed for real-time identification, classification, and evaluation of human physiotherapy exercises via mobile devices. It interprets kinetic movements as sequences of static poses, which are estimated from camera inputs using a pose-estimation neural network. The extracted body keypoints are transformed into trigonometric angle-based features and classified using lightweight supervised models to produce frame-level pose predictions along with accuracy scores. To recognize entire exercise movements and detect deviations from prescribed patterns, the framework employs a dynamic programming approach based on a modified Levenshtein distance algorithm, enabling robust sequence matching and precise localization of inaccuracies. The system functions entirely on the client side, which ensures scalability and real-time performance without relying on remote servers. Experimental evaluation validates the methodology's effectiveness, demonstrating its potential in enhancing remote physiotherapy supervision. Additionally, the framework is well-suited for mobile health (m-health) applications, providing a scalable and user-friendly solution for remote exercise monitoring and correction. This combination of pose estimation, feature transformation, classification, and sequence analysis makes the approach robust and practical for real-world physiotherapy and healthcare scenarios. <div>
arXiv:2512.10437v1 Announce Type: new 
Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment</title>
<link>https://arxiv.org/abs/2512.10450</link>
<guid>https://arxiv.org/abs/2512.10450</guid>
<content:encoded><![CDATA[
<div> Keywords: learned video compression, temporal alignment, error propagation, mixture-of-expert, rate-distortion performance<br /><br />Summary: This paper addresses the challenges in learned video compression, specifically the trade-off between inaccurate temporal alignment and error propagation in motion estimation and compensation (ME/MC). The authors identify limitations in existing frameworks: the separate-transform framework achieves good rate-distortion (R-D) performance but suffers from error propagation, while the unified-transform framework avoids error propagation but underperforms in ME/MC. To overcome these issues, they propose a novel unified-transform framework with dual-domain progressive temporal alignment and a quality-conditioned mixture-of-expert (QCMoE) module. The dual-domain alignment includes coarse pixel-domain alignment using optical flow from a single reference frame for simple motions, and refined latent-domain alignment with a Flow-Guided Deformable Transformer (FGDT) across multiple references for complex, long-term motion refinement. The QCMoE module enables continuous bitrate adaptation by dynamically adjusting quantization steps per pixel based on quality targets and content, replacing the traditional single-step quantization. This approach allows quality-consistent, error-propagation-free streaming with competitive R-D results. Experimental evaluations demonstrate the method effectively eliminates error propagation while maintaining or improving compression efficiency compared to state-of-the-art learned video compression techniques. <div>
arXiv:2512.10450v1 Announce Type: new 
Abstract: Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network</title>
<link>https://arxiv.org/abs/2512.10498</link>
<guid>https://arxiv.org/abs/2512.10498</guid>
<content:encoded><![CDATA[
<div> Shape-from-Focus, depth estimation, Directional Dilated Laplacian, multi-scale GRU, convex upsampling<br /><br />Summary:<br /><br />This paper addresses limitations in current deep learning-based Shape-from-Focus (SFF) methods for depth estimation, which typically use a two-stage approach involving focus volume extraction with heavy encoders and simple aggregation that leads to artifacts and noise amplification. The authors propose a novel hybrid framework that first computes multi-scale focus volumes using handcrafted Directional Dilated Laplacian (DDL) kernels to better capture long-range and directional focus variations, resulting in more robust focus volumes. These volumes are input into a lightweight, multi-scale GRU-based module that iteratively refines a low-resolution initial depth estimate, improving computational efficiency. To recover high-resolution depth maps with fine details and sharp boundaries, a learned convex upsampling module is embedded within the recurrent network. Extensive experiments performed on both synthetic and real-world datasets demonstrate that this method outperforms existing state-of-the-art deep learning and traditional approaches. The proposed framework achieves higher accuracy and better generalization across various focal conditions, addressing common artifacts and noise issues found in previous works. Overall, the combination of handcrafted focus volume extraction and learned iterative refinement offers an effective solution for improved passive depth estimation from focal stacks. <div>
arXiv:2512.10498v1 Announce Type: new 
Abstract: Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Blood Pulsation Maps</title>
<link>https://arxiv.org/abs/2512.10517</link>
<guid>https://arxiv.org/abs/2512.10517</guid>
<content:encoded><![CDATA[
arXiv:2512.10517v1 Announce Type: new 
Abstract: We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA</title>
<link>https://arxiv.org/abs/2512.10521</link>
<guid>https://arxiv.org/abs/2512.10521</guid>
<content:encoded><![CDATA[
arXiv:2512.10521v1 Announce Type: new 
Abstract: Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding</title>
<link>https://arxiv.org/abs/2512.10548</link>
<guid>https://arxiv.org/abs/2512.10548</guid>
<content:encoded><![CDATA[
arXiv:2512.10548v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential "blink-like" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Everything in Tokens for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.10554</link>
<guid>https://arxiv.org/abs/2512.10554</guid>
<content:encoded><![CDATA[
arXiv:2512.10554v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks</title>
<link>https://arxiv.org/abs/2512.10562</link>
<guid>https://arxiv.org/abs/2512.10562</guid>
<content:encoded><![CDATA[
arXiv:2512.10562v1 Announce Type: new 
Abstract: Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner</title>
<link>https://arxiv.org/abs/2512.10571</link>
<guid>https://arxiv.org/abs/2512.10571</guid>
<content:encoded><![CDATA[
arXiv:2512.10571v1 Announce Type: new 
Abstract: Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2512.10581</link>
<guid>https://arxiv.org/abs/2512.10581</guid>
<content:encoded><![CDATA[
arXiv:2512.10581v1 Announce Type: new 
Abstract: All-in-one image restoration aims to handle diverse degradations (e.g., noise, blur, adverse weather) within a unified framework, yet existing methods increasingly rely on complex architectures (e.g., Mixture-of-Experts, diffusion models) and elaborate degradation prompt strategies. In this work, we reveal a critical insight: well-crafted feature extraction inherently encodes degradation-carrying information, and a symmetric U-Net architecture is sufficient to unleash these cues effectively. By aligning feature scales across encoder-decoder and enabling streamlined cross-scale propagation, our symmetric design preserves intrinsic degradation signals robustly, rendering simple additive fusion in skip connections sufficient for state-of-the-art performance. Our primary baseline, SymUNet, is built on this symmetric U-Net and achieves better results across benchmark datasets than existing approaches while reducing computational cost. We further propose a semantic enhanced variant, SE-SymUNet, which integrates direct semantic injection from frozen CLIP features via simple cross-attention to explicitly amplify degradation priors. Extensive experiments on several benchmarks validate the superiority of our methods. Both baselines SymUNet and SE-SymUNet establish simpler and stronger foundations for future advancements in all-in-one image restoration. The source code is available at https://github.com/WenlongJiao/SymUNet.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Salient Object Detection in Complex Weather Conditions via Noise Indicators</title>
<link>https://arxiv.org/abs/2512.10592</link>
<guid>https://arxiv.org/abs/2512.10592</guid>
<content:encoded><![CDATA[
arXiv:2512.10592v1 Announce Type: new 
Abstract: Salient object detection (SOD), a foundational task in computer vision, has advanced from single-modal to multi-modal paradigms to enhance generalization. However, most existing SOD methods assume low-noise visual conditions, overlooking the degradation of segmentation accuracy caused by weather-induced noise in real-world scenarios. In this paper, we propose a SOD framework tailored for diverse weather conditions, encompassing a specific encoder and a replaceable decoder. To enable handling of varying weather noises, we introduce a one-hot vector as a noise indicator to represent different weather types and design a Noise Indicator Fusion Module (NIFM). The NIFM takes both semantic features and the noise indicator as dual inputs and is inserted between consecutive stages of the encoder to embed weather-aware priors via adaptive feature modulation. Critically, the proposed specific encoder retains compatibility with mainstream SOD decoders. Extensive experiments are conducted on the WXSOD dataset under varying training data scales (100%, 50%, 30% of the full training set), three encoder and seven decoder configurations. Results show that the proposed SOD framework (particularly the NIFM-enhanced specific encoder) improves segmentation accuracy under complex weather conditions compared to a vanilla encoder.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval</title>
<link>https://arxiv.org/abs/2512.10596</link>
<guid>https://arxiv.org/abs/2512.10596</guid>
<content:encoded><![CDATA[
arXiv:2512.10596v1 Announce Type: new 
Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos</title>
<link>https://arxiv.org/abs/2512.10607</link>
<guid>https://arxiv.org/abs/2512.10607</guid>
<content:encoded><![CDATA[
arXiv:2512.10607v1 Announce Type: new 
Abstract: We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation</title>
<link>https://arxiv.org/abs/2512.10608</link>
<guid>https://arxiv.org/abs/2512.10608</guid>
<content:encoded><![CDATA[
arXiv:2512.10608v1 Announce Type: new 
Abstract: In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the "black-box" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces</title>
<link>https://arxiv.org/abs/2512.10617</link>
<guid>https://arxiv.org/abs/2512.10617</guid>
<content:encoded><![CDATA[
arXiv:2512.10617v1 Announce Type: new 
Abstract: We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM</title>
<link>https://arxiv.org/abs/2512.10619</link>
<guid>https://arxiv.org/abs/2512.10619</guid>
<content:encoded><![CDATA[
arXiv:2512.10619v1 Announce Type: new 
Abstract: Document parsing aims to transform unstructured PDF images into semi-structured data, facilitating the digitization and utilization of information in diverse domains. While vision language models (VLMs) have significantly advanced this task, achieving reliable, high-quality parsing in real-world scenarios remains challenging. Common practice often selects the top-performing model on standard benchmarks. However, these benchmarks may carry dataset-specific biases, leading to inconsistent model rankings and limited correlation with real-world performance. Moreover, benchmark metrics typically provide only overall scores, which can obscure distinct error patterns in output. This raises a key challenge: how can we reliably and comprehensively assess document parsing quality in the wild? We address this problem with DOCR-Inspector, which formalizes document parsing assessment as fine-grained error detection and analysis. Leveraging VLM-as-a-Judge, DOCR-Inspector analyzes a document image and its parsed output, identifies all errors, assigns them to one of 28 predefined types, and produces a comprehensive quality assessment. To enable this capability, we construct DOCRcase-200K for training and propose the Chain-of-Checklist reasoning paradigm to enable the hierarchical structure of parsing quality assessment. For empirical validation, we introduce DOCRcaseBench, a set of 882 real-world document parsing cases with manual annotations. On this benchmark, DOCR-Inspector-7B outperforms commercial models like Gemini 2.5 Pro, as well as leading open-source models. Further experiments demonstrate that its quality assessments provide valuable guidance for parsing results refinement, making DOCR-Inspector both a practical evaluator and a driver for advancing document parsing systems at scale. Model and code are released at: https://github.com/ZZZZZQT/DOCR-Inspector.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices</title>
<link>https://arxiv.org/abs/2512.10628</link>
<guid>https://arxiv.org/abs/2512.10628</guid>
<content:encoded><![CDATA[
arXiv:2512.10628v1 Announce Type: new 
Abstract: Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection</title>
<link>https://arxiv.org/abs/2512.10652</link>
<guid>https://arxiv.org/abs/2512.10652</guid>
<content:encoded><![CDATA[
arXiv:2512.10652v1 Announce Type: new 
Abstract: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation</title>
<link>https://arxiv.org/abs/2512.10660</link>
<guid>https://arxiv.org/abs/2512.10660</guid>
<content:encoded><![CDATA[
arXiv:2512.10660v1 Announce Type: new 
Abstract: The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XDen-1K: A Density Field Dataset of Real-World Objects</title>
<link>https://arxiv.org/abs/2512.10668</link>
<guid>https://arxiv.org/abs/2512.10668</guid>
<content:encoded><![CDATA[
arXiv:2512.10668v1 Announce Type: new 
Abstract: A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching</title>
<link>https://arxiv.org/abs/2512.10674</link>
<guid>https://arxiv.org/abs/2512.10674</guid>
<content:encoded><![CDATA[
arXiv:2512.10674v1 Announce Type: new 
Abstract: Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal transport unlocks end-to-end learning for single-molecule localization</title>
<link>https://arxiv.org/abs/2512.10683</link>
<guid>https://arxiv.org/abs/2512.10683</guid>
<content:encoded><![CDATA[
arXiv:2512.10683v1 Announce Type: new 
Abstract: Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Monocular View Synthesis in Less Than a Second</title>
<link>https://arxiv.org/abs/2512.10685</link>
<guid>https://arxiv.org/abs/2512.10685</guid>
<content:encoded><![CDATA[
arXiv:2512.10685v1 Announce Type: new 
Abstract: We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images</title>
<link>https://arxiv.org/abs/2512.10715</link>
<guid>https://arxiv.org/abs/2512.10715</guid>
<content:encoded><![CDATA[
arXiv:2512.10715v1 Announce Type: new 
Abstract: Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.10719</link>
<guid>https://arxiv.org/abs/2512.10719</guid>
<content:encoded><![CDATA[
arXiv:2512.10719v1 Announce Type: new 
Abstract: End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Depth Propagation</title>
<link>https://arxiv.org/abs/2512.10725</link>
<guid>https://arxiv.org/abs/2512.10725</guid>
<content:encoded><![CDATA[
arXiv:2512.10725v1 Announce Type: new 
Abstract: Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation</title>
<link>https://arxiv.org/abs/2512.10730</link>
<guid>https://arxiv.org/abs/2512.10730</guid>
<content:encoded><![CDATA[
arXiv:2512.10730v1 Announce Type: new 
Abstract: Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation</title>
<link>https://arxiv.org/abs/2512.10750</link>
<guid>https://arxiv.org/abs/2512.10750</guid>
<content:encoded><![CDATA[
arXiv:2512.10750v1 Announce Type: new 
Abstract: Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography</title>
<link>https://arxiv.org/abs/2512.10765</link>
<guid>https://arxiv.org/abs/2512.10765</guid>
<content:encoded><![CDATA[
arXiv:2512.10765v1 Announce Type: new 
Abstract: Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What matters for Representation Alignment: Global Information or Spatial Structure?</title>
<link>https://arxiv.org/abs/2512.10794</link>
<guid>https://arxiv.org/abs/2512.10794</guid>
<content:encoded><![CDATA[
arXiv:2512.10794v1 Announce Type: new 
Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading</title>
<link>https://arxiv.org/abs/2512.10808</link>
<guid>https://arxiv.org/abs/2512.10808</guid>
<content:encoded><![CDATA[
arXiv:2512.10808v1 Announce Type: new 
Abstract: Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Ensemble Post Learning for Noisy Domain Generalization</title>
<link>https://arxiv.org/abs/2512.10818</link>
<guid>https://arxiv.org/abs/2512.10818</guid>
<content:encoded><![CDATA[
arXiv:2512.10818v1 Announce Type: new 
Abstract: While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning</title>
<link>https://arxiv.org/abs/2512.10840</link>
<guid>https://arxiv.org/abs/2512.10840</guid>
<content:encoded><![CDATA[
arXiv:2512.10840v1 Announce Type: new 
Abstract: 6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation</title>
<link>https://arxiv.org/abs/2512.10860</link>
<guid>https://arxiv.org/abs/2512.10860</guid>
<content:encoded><![CDATA[
arXiv:2512.10860v1 Announce Type: new 
Abstract: Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.10863</link>
<guid>https://arxiv.org/abs/2512.10863</guid>
<content:encoded><![CDATA[
arXiv:2512.10863v1 Announce Type: new 
Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.10867</link>
<guid>https://arxiv.org/abs/2512.10867</guid>
<content:encoded><![CDATA[
arXiv:2512.10867v1 Announce Type: new 
Abstract: This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos</title>
<link>https://arxiv.org/abs/2512.10881</link>
<guid>https://arxiv.org/abs/2512.10881</guid>
<content:encoded><![CDATA[
arXiv:2512.10881v1 Announce Type: new 
Abstract: Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction</title>
<link>https://arxiv.org/abs/2512.10888</link>
<guid>https://arxiv.org/abs/2512.10888</guid>
<content:encoded><![CDATA[
arXiv:2512.10888v1 Announce Type: new 
Abstract: Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance</title>
<link>https://arxiv.org/abs/2512.10894</link>
<guid>https://arxiv.org/abs/2512.10894</guid>
<content:encoded><![CDATA[
arXiv:2512.10894v1 Announce Type: new 
Abstract: Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos</title>
<link>https://arxiv.org/abs/2512.10927</link>
<guid>https://arxiv.org/abs/2512.10927</guid>
<content:encoded><![CDATA[
arXiv:2512.10927v1 Announce Type: new 
Abstract: Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models</title>
<link>https://arxiv.org/abs/2512.10932</link>
<guid>https://arxiv.org/abs/2512.10932</guid>
<content:encoded><![CDATA[
arXiv:2512.10932v1 Announce Type: new 
Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any4D: Unified Feed-Forward Metric 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.10935</link>
<guid>https://arxiv.org/abs/2512.10935</guid>
<content:encoded><![CDATA[
arXiv:2512.10935v1 Announce Type: new 
Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.10939</link>
<guid>https://arxiv.org/abs/2512.10939</guid>
<content:encoded><![CDATA[
arXiv:2512.10939v1 Announce Type: new 
Abstract: Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis</title>
<link>https://arxiv.org/abs/2512.10940</link>
<guid>https://arxiv.org/abs/2512.10940</guid>
<content:encoded><![CDATA[
arXiv:2512.10940v1 Announce Type: new 
Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mull-Tokens: Modality-Agnostic Latent Thinking</title>
<link>https://arxiv.org/abs/2512.10941</link>
<guid>https://arxiv.org/abs/2512.10941</guid>
<content:encoded><![CDATA[
arXiv:2512.10941v1 Announce Type: new 
Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</title>
<link>https://arxiv.org/abs/2512.10942</link>
<guid>https://arxiv.org/abs/2512.10942</guid>
<content:encoded><![CDATA[
arXiv:2512.10942v1 Announce Type: new 
Abstract: We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation</title>
<link>https://arxiv.org/abs/2512.10943</link>
<guid>https://arxiv.org/abs/2512.10943</guid>
<content:encoded><![CDATA[
arXiv:2512.10943v1 Announce Type: new 
Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</title>
<link>https://arxiv.org/abs/2512.10945</link>
<guid>https://arxiv.org/abs/2512.10945</guid>
<content:encoded><![CDATA[
arXiv:2512.10945v1 Announce Type: new 
Abstract: This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving</title>
<link>https://arxiv.org/abs/2512.10947</link>
<guid>https://arxiv.org/abs/2512.10947</guid>
<content:encoded><![CDATA[
arXiv:2512.10947v1 Announce Type: new 
Abstract: We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClusIR: Towards Cluster-Guided All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2512.10948</link>
<guid>https://arxiv.org/abs/2512.10948</guid>
<content:encoded><![CDATA[
arXiv:2512.10948v1 Announce Type: new 
Abstract: All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</title>
<link>https://arxiv.org/abs/2512.10949</link>
<guid>https://arxiv.org/abs/2512.10949</guid>
<content:encoded><![CDATA[
arXiv:2512.10949v1 Announce Type: new 
Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</title>
<link>https://arxiv.org/abs/2512.10950</link>
<guid>https://arxiv.org/abs/2512.10950</guid>
<content:encoded><![CDATA[
arXiv:2512.10950v1 Announce Type: new 
Abstract: Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration</title>
<link>https://arxiv.org/abs/2512.10954</link>
<guid>https://arxiv.org/abs/2512.10954</guid>
<content:encoded><![CDATA[
arXiv:2512.10954v1 Announce Type: new 
Abstract: In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization</title>
<link>https://arxiv.org/abs/2512.10955</link>
<guid>https://arxiv.org/abs/2512.10955</guid>
<content:encoded><![CDATA[
arXiv:2512.10955v1 Announce Type: new 
Abstract: Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</title>
<link>https://arxiv.org/abs/2512.10956</link>
<guid>https://arxiv.org/abs/2512.10956</guid>
<content:encoded><![CDATA[
arXiv:2512.10956v1 Announce Type: new 
Abstract: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.
  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</title>
<link>https://arxiv.org/abs/2512.10957</link>
<guid>https://arxiv.org/abs/2512.10957</guid>
<content:encoded><![CDATA[
arXiv:2512.10957v1 Announce Type: new 
Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</title>
<link>https://arxiv.org/abs/2512.10958</link>
<guid>https://arxiv.org/abs/2512.10958</guid>
<content:encoded><![CDATA[
arXiv:2512.10958v1 Announce Type: new 
Abstract: Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</title>
<link>https://arxiv.org/abs/2512.10959</link>
<guid>https://arxiv.org/abs/2512.10959</guid>
<content:encoded><![CDATA[
arXiv:2512.10959v1 Announce Type: new 
Abstract: We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CC-GRMAS: A Multi-Agent Graph Neural System for Spatiotemporal Landslide Risk Assessment in High Mountain Asia</title>
<link>https://arxiv.org/abs/2510.20875</link>
<guid>https://arxiv.org/abs/2510.20875</guid>
<content:encoded><![CDATA[
arXiv:2510.20875v1 Announce Type: cross 
Abstract: Landslides are a growing climate induced hazard with severe environmental and human consequences, particularly in high mountain Asia. Despite increasing access to satellite and temporal datasets, timely detection and disaster response remain underdeveloped and fragmented. This work introduces CC-GRMAS, a framework leveraging a series of satellite observations and environmental signals to enhance the accuracy of landslide forecasting. The system is structured around three interlinked agents Prediction, Planning, and Execution, which collaboratively enable real time situational awareness, response planning, and intervention. By incorporating local environmental factors and operationalizing multi agent coordination, this approach offers a scalable and proactive solution for climate resilient disaster preparedness across vulnerable mountainous terrains.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting</title>
<link>https://arxiv.org/abs/2512.09944</link>
<guid>https://arxiv.org/abs/2512.09944</guid>
<content:encoded><![CDATA[
arXiv:2512.09944v1 Announce Type: cross 
Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Domain Generalization with Latent Space Inversion</title>
<link>https://arxiv.org/abs/2512.10224</link>
<guid>https://arxiv.org/abs/2512.10224</guid>
<content:encoded><![CDATA[
arXiv:2512.10224v1 Announce Type: cross 
Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot</title>
<link>https://arxiv.org/abs/2512.10319</link>
<guid>https://arxiv.org/abs/2512.10319</guid>
<content:encoded><![CDATA[
arXiv:2512.10319v1 Announce Type: cross 
Abstract: Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mode-Seeking for Inverse Problems with Diffusion Models</title>
<link>https://arxiv.org/abs/2512.10524</link>
<guid>https://arxiv.org/abs/2512.10524</guid>
<content:encoded><![CDATA[
arXiv:2512.10524v1 Announce Type: cross 
Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title>
<link>https://arxiv.org/abs/2512.10675</link>
<guid>https://arxiv.org/abs/2512.10675</guid>
<content:encoded><![CDATA[
arXiv:2512.10675v1 Announce Type: cross 
Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10691</link>
<guid>https://arxiv.org/abs/2512.10691</guid>
<content:encoded><![CDATA[
arXiv:2512.10691v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metaphor-based Jailbreaking Attacks on Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.10766</link>
<guid>https://arxiv.org/abs/2512.10766</guid>
<content:encoded><![CDATA[
arXiv:2512.10766v1 Announce Type: cross 
Abstract: Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable and Steerable Concept Bottleneck Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.10805</link>
<guid>https://arxiv.org/abs/2512.10805</guid>
<content:encoded><![CDATA[
arXiv:2512.10805v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values</title>
<link>https://arxiv.org/abs/2512.10817</link>
<guid>https://arxiv.org/abs/2512.10817</guid>
<content:encoded><![CDATA[
arXiv:2512.10817v1 Announce Type: cross 
Abstract: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agile Deliberation: Concept Deliberation for Subjective Visual Classification</title>
<link>https://arxiv.org/abs/2512.10821</link>
<guid>https://arxiv.org/abs/2512.10821</guid>
<content:encoded><![CDATA[
arXiv:2512.10821v1 Announce Type: cross 
Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2512.10938</link>
<guid>https://arxiv.org/abs/2512.10938</guid>
<content:encoded><![CDATA[
arXiv:2512.10938v1 Announce Type: cross 
Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(\alpha x + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional Normalizing Flow: From Data to Noise and Back</title>
<link>https://arxiv.org/abs/2512.10953</link>
<guid>https://arxiv.org/abs/2512.10953</guid>
<content:encoded><![CDATA[
arXiv:2512.10953v1 Announce Type: cross 
Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Cluster Contrastive learning for Object Re-Identification</title>
<link>https://arxiv.org/abs/2112.04662</link>
<guid>https://arxiv.org/abs/2112.04662</guid>
<content:encoded><![CDATA[
arXiv:2112.04662v4 Announce Type: replace 
Abstract: Recently, cluster contrastive learning has been proven effective for object ReID by computing the contrastive loss between the individual features and the cluster memory. However, existing methods that use the individual features to momentum update the cluster memory will fluctuate over the training examples, especially for the outlier samples. Unlike the individual-based updating mechanism, the centroid-based updating mechanism that applies the mean feature of each cluster to update the cluster memory can reduce the impact of individual samples. Therefore, we formulate the individual-based updating and centroid-based updating mechanisms in a unified cluster contrastive framework, named Dual Cluster Contrastive framework (DCC), which maintains two types of memory banks: individual and centroid cluster memory banks. Significantly, the individual cluster memory considers just one individual at a time to take a single step for updating. The centroid cluster memory applies the mean feature of each cluster to update the corresponding cluster memory. During optimization, besides the vallina contrastive loss of each memory, a cross-view consistency constraint is applied to exchange the benefits of two memories for generating a discriminative description for the object ReID. Note that DCC can be easily applied for unsupervised or supervised object ReID by using ground-truth labels or the generated pseudo-labels. Extensive experiments on three benchmarks, \emph{e.g.,} Market-1501, MSMT17, and VeRi-776, under \textbf{supervised Object ReID} and \textbf{unsupervised Object ReID} demonstrate the superiority of the proposed DCC.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Online Exam Proctoring by Combining Lightweight Face Detection and Deep Recognition</title>
<link>https://arxiv.org/abs/2206.13356</link>
<guid>https://arxiv.org/abs/2206.13356</guid>
<content:encoded><![CDATA[
arXiv:2206.13356v2 Announce Type: replace 
Abstract: Online exams, conducted via video conferencing platforms such as Zoom, have become popular in educational institutions since COVID-19. While convenient, ensuring the integrity and security of online exams remains challenging, as traditional invigilation struggles to effectively monitor multiple student video feeds in real time. In this paper, we present iExam, an effective online exam proctoring and analysis system that combines lightweight face detection and deep recognition. iExam employs real-time face detection to assist invigilators in continuously monitoring student presence, and leverages deep face recognition for post-exam video analysis to identify abnormal behaviors--including face disappearance, face rotation, and identity substitution. To realize this system, we address three core challenges: (i) designing a lightweight approach to efficiently capture and analyze exam video streams in real time; (ii) developing an enhanced OCR method to automatically extract student identities from dynamically positioned Zoom name tags, enabling reliable ground truth labeling without manual intervention; and (iii) optimizing the training and inference pipeline to significantlyreduce resource and time requirements on ordinary teacher devices. Extensive experiments demonstrate that iExam achieves 90.4% accuracy for real-time face detection and 98.4% accuracy for post-exam face recognition, while maintaining low overhead. These results show that iExam can substantially enhance the automation and reliability of online exam proctoring in practice.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints</title>
<link>https://arxiv.org/abs/2312.08591</link>
<guid>https://arxiv.org/abs/2312.08591</guid>
<content:encoded><![CDATA[
arXiv:2312.08591v3 Announce Type: replace 
Abstract: 3D human generation is increasingly significant in various applications. However, the direct use of 2D generative methods in 3D generation often results in losing local details, while methods that reconstruct geometry from generated images struggle with global view consistency. In this work, we introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details. To achieve this, we employ the Fourier occupancy field (FOF) representation, enabling the direct generation of 3D shapes as preliminary results with 2D generative models. With the proposed high-frequency enhancer and the multi-view recarving strategy, our method can seamlessly integrate the details from different views into a uniform global shape. To better utilize the 3D human prior and enhance control over the generated geometry, we introduce a compact spherical embedding of 3D joints. This allows for an effective guidance of pose during the generation process. Additionally, our method can generate 3D humans guided by textual inputs. Our experimental results demonstrate the capability of our method to ensure global structure, local details, high resolution, and low computational cost simultaneously. More results and the code can be found on our project page at http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>l0-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion</title>
<link>https://arxiv.org/abs/2411.04519</link>
<guid>https://arxiv.org/abs/2411.04519</guid>
<content:encoded><![CDATA[
arXiv:2411.04519v2 Announce Type: replace 
Abstract: Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an $\ell_0$-regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the $\ell_0$-regularized CSC problem, we design a learnable $\ell_0$-regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an $\ell_0$-regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet's training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection \textcolor[rgb]{ 0, 0, 0}{and semantic segmentation} in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/gargi884/FNet-MMIF.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-like emergent properties in deep networks: impact of network architecture, datasets and training</title>
<link>https://arxiv.org/abs/2411.16326</link>
<guid>https://arxiv.org/abs/2411.16326</guid>
<content:encoded><![CDATA[
arXiv:2411.16326v3 Announce Type: replace 
Abstract: Despite the rapid pace at which deep networks are improving on standardized vision benchmarks, they are still outperformed by humans on real-world vision tasks. One solution to this problem is to make deep networks more brain-like. Although there are several benchmarks that compare the ability of deep networks to predict brain responses on natural images, they do not capture subtle but important emergent properties present in brains. It is also unclear which design principle -- architecture, training data, or training regime -- would have the greatest impact on these emergent properties. To investigate these issues, we systematically evaluated over 30 state-of-the-art networks with varying network architectures, training datasets, and training regimes for the presence or absence of brain-like properties. Our main findings are as follows. First, network architecture had the strongest impact on brain-like properties compared to dataset and training regime variations. Second, networks varied widely in their alignment to the brain with no single network outperforming all others. Taken together, our results offer a principled and interpretable path toward closing the gap between artificial and human vision.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpotLight: Shadow-Guided Object Relighting via Diffusion</title>
<link>https://arxiv.org/abs/2411.18665</link>
<guid>https://arxiv.org/abs/2411.18665</guid>
<content:encoded><![CDATA[
arXiv:2411.18665v3 Announce Type: replace 
Abstract: Recent work has shown that diffusion models can serve as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. However, unlike typical physics-based renderers, these neural rendering engines are limited by the lack of manual control over the lighting, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise and controllable lighting can be achieved without any additional training, simply by supplying a coarse shadow hint for the object. Indeed, we show that injecting only the desired shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, is entirely training-free and leverages existing neural rendering approaches to achieve controllable relighting. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting. We also demonstrate other applications, such as hand-scribbling shadows and full-image relighting, demonstrating its versatility.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Reliability of Predictions in Detection Transformers: Object-Level Calibration and Image-Level Uncertainty</title>
<link>https://arxiv.org/abs/2412.01782</link>
<guid>https://arxiv.org/abs/2412.01782</guid>
<content:encoded><![CDATA[
arXiv:2412.01782v3 Announce Type: replace 
Abstract: DETR and its variants have emerged as promising architectures for object detection, offering an end-to-end prediction pipeline. In practice, however, DETRs generate hundreds of predictions that far outnumber the actual objects present in an image. This raises a critical question: which of these predictions could be trusted? Addressing this concern, we provide empirical and theoretical evidence that predictions within the same image play distinct roles, resulting in varying reliability levels. Our analysis reveals that DETRs employ an optimal specialist strategy: one prediction per object is trained to be well-calibrated, while the remaining predictions are trained to suppress their foreground confidence to near zero, even when maintaining accurate localization. We show that this strategy emerges as the loss-minimizing solution to the Hungarian matching algorithm, fundamentally shaping DETRs' outputs. While selecting the well-calibrated predictions is ideal, they are unidentifiable at inference time. This means that any post-processing algorithm poses a risk of outputting a set of predictions with mixed calibration levels. Therefore, practical deployment necessitates a joint evaluation of both the model's calibration quality and the effectiveness of the post-processing algorithm. However, we demonstrate that existing metrics like average precision and expected calibration error are inadequate for this task. To address this issue, we further introduce Object-level Calibration Error (OCE): This object-centric design penalizes both retaining suppressed predictions and missed ground truth foreground objects, making OCE suitable for both evaluating models and identifying reliable prediction subsets. Finally, we present a post hoc uncertainty quantification framework that predicts per-image model accuracy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts</title>
<link>https://arxiv.org/abs/2412.02912</link>
<guid>https://arxiv.org/abs/2412.02912</guid>
<content:encoded><![CDATA[
arXiv:2412.02912v2 Announce Type: replace 
Abstract: We introduce ShapeWords, an approach for synthesizing images based on 3D shape guidance and text prompts. ShapeWords incorporates target 3D shape information within specialized tokens embedded together with the input text, effectively blending 3D shape awareness with textual context to guide the image synthesis process. Unlike conventional shape guidance methods that rely on depth maps restricted to fixed viewpoints and often overlook full 3D structure or textual context, ShapeWords generates diverse yet consistent images that reflect both the target shape's geometry and the textual description. Experimental results show that ShapeWords produces images that are more text-compliant, aesthetically plausible, while also maintaining 3D shape awareness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Worse is Better: Navigating the compression-generation tradeoff in visual tokenization</title>
<link>https://arxiv.org/abs/2412.16326</link>
<guid>https://arxiv.org/abs/2412.16326</guid>
<content:encoded><![CDATA[
arXiv:2412.16326v2 Announce Type: replace 
Abstract: Current image generation methods are based on a two-stage training approach. In stage 1, an auto-encoder is trained to compress an image into a latent space; in stage 2, a generative model is trained to learn a distribution over that latent space. This reveals a fundamental trade-off, do we compress more aggressively to make the latent distribution easier for the stage 2 model to learn even if it makes reconstruction worse? We study this problem in the context of discrete, auto-regressive image generation. Through the lens of scaling laws, we show that smaller stage 2 models can benefit from more compressed stage 1 latents even if reconstruction performance worsens, demonstrating that generation modeling capacity plays a role in this trade-off. Diving deeper, we rigorously study the connection between compute scaling and the stage 1 rate-distortion trade-off. Next, we introduce Causally Regularized Tokenization (CRT), which uses knowledge of the stage 2 generation modeling procedure to embed useful inductive biases in stage 1 latents. This regularization improves stage 2 generation performance better by making the tokens easier to model without affecting the stage 1 compression rate and marginally affecting distortion: we are able to improve compute efficiency 2-3$\times$ over baseline. Finally, we use CRT with further optimizations to the visual tokenizer setup to result in a generative pipeline that matches LlamaGen-3B generation performance (2.18 FID) with half the tokens per image (256 vs. 576) and a fourth the total model parameters (775M vs. 3.1B) while using the same architecture and inference procedure.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Multiple Foundation Models for Advanced Computational Pathology</title>
<link>https://arxiv.org/abs/2503.00736</link>
<guid>https://arxiv.org/abs/2503.00736</guid>
<content:encoded><![CDATA[
arXiv:2503.00736v3 Announce Type: replace 
Abstract: Foundation models have advanced computational pathology by learning transferable visual representations from large histological datasets, yet recent evaluations reveal substantial variability in their performance across tasks. This inconsistency arises from differences in training data diversity and is further constrained by the reliance of many high-performing models on proprietary datasets that cannot be shared or expanded. Offline distillation offers a partial remedy but depends heavily on the size and heterogeneity of the distillation corpus and requires full retraining to incorporate new models. To address these limitations, we propose Shazam, a task-specific online integration framework that unifies multiple pretrained pathology foundation models within a single flexible inference system. Shazam fuses multi-level representations through adaptive expert weighting and learns task-aligned features via online distillation. Across spatial transcriptomics prediction, survival prognosis, tile classification, and visual question answering, Shazam consistently outperforms strong individual models, highlighting its promise as a scalable approach for harnessing the rapid evolution of pathology foundation models in a unified and adaptable manner.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling</title>
<link>https://arxiv.org/abs/2504.05537</link>
<guid>https://arxiv.org/abs/2504.05537</guid>
<content:encoded><![CDATA[
arXiv:2504.05537v2 Announce Type: replace 
Abstract: Motion Transfer is a technique that synthesizes videos by transferring motion dynamics from a driving video to a source image. In this work we propose a deep learning-based framework to enable real-time video motion transfer which is critical for enabling bandwidth-efficient applications such as video conferencing, remote health monitoring, virtual reality interaction, and vision-based anomaly detection. This is done using keypoints which serve as semantically meaningful, compact representations of motion across time. To enable bandwidth savings during video transmission we perform forecasting of keypoints using two generative time series models VRNN and GRU-NF. The predicted keypoints are transformed into realistic video frames using an optical flow-based module paired with a generator network, thereby enabling efficient, low-frame-rate video transmission. Based on the application this allows the framework to either generate a deterministic future sequence or sample a diverse set of plausible futures. Experimental results demonstrate that VRNN achieves the best point-forecast fidelity (lowest MAE) in applications requiring stable and accurate multi-step forecasting and is particularly competitive in higher-uncertainty, multi-modal settings. This is achieved by introducing recurrently conditioned stochastic latent variables that carry past contexts to capture uncertainty and temporal variation. On the other hand the GRU-NF model enables richer diversity of generated videos while maintaining high visual quality. This is realized by learning an invertible, exact-likelihood mapping between the keypoints and their latent representations which supports rich and controllable sampling of diverse yet coherent keypoint sequences. Our work lays the foundation for next-generation AI systems that require real-time, bandwidth-efficient, and semantically controllable video generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation</title>
<link>https://arxiv.org/abs/2504.21336</link>
<guid>https://arxiv.org/abs/2504.21336</guid>
<content:encoded><![CDATA[
arXiv:2504.21336v3 Announce Type: replace 
Abstract: The integration of AI-assisted biomedical image analysis into clinical practice demands AI-generated findings that are not only accurate but also interpretable to clinicians. However, existing biomedical AI models generally lack the ability to simultaneously generate diagnostic findings and localize corresponding biomedical objects. This limitation makes it challenging for clinicians to correlate AI-generated findings with visual evidence (e.g., tiny lesions) in images and interpret the results of AI models. To address this challenge, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation, which is capable of generating accurate diagnostic findings and simultaneously segmenting the corresponding biomedical targets. UniBiomed is based on a novel integration of Multi-modal Large Language Model and Segment Anything Model, which can effectively unify diverse biomedical tasks in universal training for advancing grounded interpretation. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, region annotations, and text descriptions across ten biomedical imaging modalities. Extensive validation on 70 internal and 14 external datasets demonstrated the state-of-the-art performance of UniBiomed in diverse biomedical tasks, including image segmentation, disease recognition, region-aware diagnosis, vision question answering, and report generation. In summary, UniBiomed is a powerful and versatile biomedical foundation model, unlocking the untapped grounded interpretation capability for optimizing AI-assisted biomedical image analysis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Panoramic Out-of-Distribution Segmentation</title>
<link>https://arxiv.org/abs/2505.03539</link>
<guid>https://arxiv.org/abs/2505.03539</guid>
<content:encoded><![CDATA[
arXiv:2505.03539v3 Announce Type: replace 
Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception, which is critical to applications, such as autonomous driving and augmented reality, etc. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to pixel distortions and background clutter. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), with the aim of achieving comprehensive and safe scene understanding. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities and advances the development of panoramic understanding. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>diffDemorph: Extending Reference-Free Demorphing to Unseen Faces</title>
<link>https://arxiv.org/abs/2505.14527</link>
<guid>https://arxiv.org/abs/2505.14527</guid>
<content:encoded><![CDATA[
arXiv:2505.14527v4 Announce Type: replace 
Abstract: A face morph is created by combining two face images corresponding to two identities to produce a composite that successfully matches both the constituent identities. Reference-free (RF) demorphing reverses this process using only the morph image, without the need for additional reference images. Previous RF demorphing methods are overly constrained, as they rely on assumptions about the distributions of training and testing morphs such as the morphing technique used (e.g., landmark-based) and face image style (e.g., passport photos). In this paper, we introduce a novel diffusion-based approach, referred to as diffDeMorph, that effectively disentangles component images from a composite morph image with high visual fidelity. Our method is the first to generalize across morph techniques and face styles, beating the current state of the art by $\geq 59.46\%$ under a common training protocol across all datasets tested. We train our method on morphs created using synthetically generated face images and test on real morphs, thereby enhancing the practicality of the technique. Experiments on six datasets and two face matchers establish the utility and efficacy of our method.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.17012</link>
<guid>https://arxiv.org/abs/2505.17012</guid>
<content:encoded><![CDATA[
arXiv:2505.17012v2 Announce Type: replace 
Abstract: Existing evaluations of multimodal large language models (MLLMs) on spatial intelligence are typically fragmented and limited in scope. In this work, we aim to conduct a holistic assessment of the spatial understanding capabilities of modern MLLMs and propose complementary data-driven and agent-based solutions. Specifically, we make the following contributions: (i) we introduce SpatialScore, to our knowledge, the most comprehensive and diverse benchmark for multimodal spatial intelligence to date. It covers multiple visual data types, input modalities, and question-answering formats, and contains approximately 5K manually verified samples spanning 30 distinct tasks; (ii) using SpatialScore, we extensively evaluate 40 representative MLLMs, revealing persistent challenges and a substantial gap between current models and human-level spatial intelligence; (iii) to advance model capabilities, we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks and significantly improves the performance of existing models (e.g., Qwen3-VL); (iv) to complement this data-driven route with a training-free paradigm, we develop SpatialAgent, a multi-agent system equipped with 12 specialized spatial perception tools that supports both Plan-Execute and ReAct reasoning, enabling substantial gains in spatial reasoning without additional model training. Extensive experiments and in-depth analyses demonstrate the effectiveness of our benchmark, corpus, and agent framework. We expect these resources to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence. All data, code, and models will be released to the research community.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts</title>
<link>https://arxiv.org/abs/2505.17476</link>
<guid>https://arxiv.org/abs/2505.17476</guid>
<content:encoded><![CDATA[
arXiv:2505.17476v2 Announce Type: replace 
Abstract: The detection and grounding of multimedia manipulation has emerged as a critical challenge in combating AI-generated disinformation. While existing methods have made progress in recent years, we identify two fundamental limitations in current approaches: (1) Underestimation of MLLM-driven deception risk: prevailing techniques primarily address rule-based text manipulations, yet fail to account for sophisticated misinformation synthesized by multimodal large language models (MLLMs) that can dynamically generate semantically coherent, contextually plausible yet deceptive narratives conditioned on manipulated images; (2) Unrealistic misalignment artifacts: currently focused scenarios rely on artificially misaligned content that lacks semantic coherence, rendering them easily detectable. To address these gaps holistically, we propose a new adversarial pipeline that leverages MLLMs to generate high-risk disinformation. Our approach begins with constructing the MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered using state-of-the-art editing techniques and then paired with MLLM-generated deceptive texts that maintain semantic consistency with the visual manipulations. Building upon this foundation, we present the Artifact-aware Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations: Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning, to tame MLLMs for the MDSM problem. Comprehensive experiments validate our framework's superior generalization capabilities as a unified architecture for detecting MLLM-powered multimodal deceptions. In cross-domain testing on the MDSM dataset, AMD achieves the best average performance, with 88.18 ACC, 60.25 mAP, and 61.02 mIoU scores.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes</title>
<link>https://arxiv.org/abs/2505.17951</link>
<guid>https://arxiv.org/abs/2505.17951</guid>
<content:encoded><![CDATA[
arXiv:2505.17951v4 Announce Type: replace 
Abstract: We present SplatCo, a structure-view collaborative Gaussian splatting framework for high-fidelity rendering of complex outdoor scenes. SplatCo builds upon three novel components: 1) a cross-structure collaboration module that combines global tri-plane representations, which capture coarse scene layouts, with local context grid features representing fine details. This fusion is achieved through a hierarchical compensation mechanism, ensuring both global spatial awareness and local detail preservation; 2) a cross-view pruning mechanism that removes overfitted or inaccurate Gaussians based on structural consistency, thereby improving storage efficiency and preventing rendering artifacts; 3) a structure view co-learning module that aggregates structural gradients with view gradients,thereby steering the optimization of Gaussian geometric and appearance attributes more robustly. By combining these key components, SplatCo effectively achieves high-fidelity rendering for large-scale scenes. Code and project page are available at https://splatco-tech.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Distillation for Continual Model Adaptation</title>
<link>https://arxiv.org/abs/2506.02671</link>
<guid>https://arxiv.org/abs/2506.02671</guid>
<content:encoded><![CDATA[
arXiv:2506.02671v2 Announce Type: replace 
Abstract: Deep neural networks often suffer performance degradation upon deployment due to distribution shifts. Continual Test-Time Adaptation (CTTA) aims to address this issue in an unsupervised manner, yet existing methods, which rely on self-supervision, are prone to an inherent self-referential feedback loop that amplifies initial prediction errors, leading to model drift. We revisit this limitation and propose Test-Time Distillation (TTD), which reframes adaptation as a distillation process guided by a frozen Vision-Language Model (VLM) as an external signal. While promising, we find that direct distillation is fraught with two pitfalls: the Generalist Trap, where the VLM's broad but non-specialized knowledge leads to suboptimal performance on specific tasks and shifts, and the Entropy Bias, where naive model fusion techniques based on entropy fail due to the disparate calibration of heterogeneous models. These pitfalls motivate our insight: the key is to build a robust supervisory signal and leverage it to guide the target model toward stable adaptation. Hence, we present CoDiRe, a Continual Distillation and Rectification framework for TTD. CoDiRe first constructs a robust blended teacher by dynamically fusing the predictions of the VLM and the target model. Critically, it circumvents the Entropy Bias by leveraging Maximum Softmax Probability (MSP) as a more reliable confidence metric for weighting each model's expertise. Then applies an Optimal Transport based rectification to further align predictions with the blended teacher, enabling continuous and stable adaptation. Extensive experiments show that CoDiRe outperforms state-of-the-art baselines, exceeding CoTTA by 10.55% while using only 48% of its time cost on ImageNet-C.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MokA: Multimodal Low-Rank Adaptation for MLLMs</title>
<link>https://arxiv.org/abs/2506.05191</link>
<guid>https://arxiv.org/abs/2506.05191</guid>
<content:encoded><![CDATA[
arXiv:2506.05191v2 Announce Type: replace 
Abstract: In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExAct: A Video-Language Benchmark for Expert Action Analysis</title>
<link>https://arxiv.org/abs/2506.06277</link>
<guid>https://arxiv.org/abs/2506.06277</guid>
<content:encoded><![CDATA[
arXiv:2506.06277v2 Announce Type: replace 
Abstract: We present ExAct, a new video-language benchmark for expert-level understanding of skilled physical human activities. Our new benchmark contains 3521 expert-curated video question-answer pairs spanning 11 physical activities in 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct requires the correct answer to be selected from five carefully designed candidate options, thus necessitating a nuanced, fine-grained, expert-level understanding of physical human skills. Evaluating the recent state-of-the-art VLMs on ExAct reveals a substantial performance gap relative to human expert performance. Specifically, the best-performing GPT-4o model achieves only 44.70% accuracy, well below the 82.02% attained by trained human specialists/experts. We believe that ExAct will be beneficial for developing and evaluating VLMs capable of precise understanding of human skills in various physical and procedural domains. Dataset and code are available at https://texaser.github.io/exact_project_page/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning</title>
<link>https://arxiv.org/abs/2506.07811</link>
<guid>https://arxiv.org/abs/2506.07811</guid>
<content:encoded><![CDATA[
arXiv:2506.07811v2 Announce Type: replace 
Abstract: Video Question Answering (VideoQA) aims to answer natural language questions based on the given video, with prior work primarily focusing on identifying the duration of relevant segments, referred to as explicit visual evidence. However, explicit visual evidence is not always directly available, particularly when questions target symbolic meanings or deeper intentions, leading to significant performance degradation. To fill this gap, we introduce a novel task and dataset, $\textbf{I}$mplicit $\textbf{V}$ideo $\textbf{Q}$uestion $\textbf{A}$nswering (I-VQA), which focuses on answering questions in scenarios where explicit visual evidence is inaccessible. Given an implicit question and its corresponding video, I-VQA requires answering based on the contextual visual cues present within the video. To tackle I-VQA, we propose a novel reasoning framework, IRM (Implicit Reasoning Model), incorporating dual-stream modeling of contextual actions and intent clues as implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the Visual Enhancement Module (VEM). AIM deduces and preserves question-related dual clues by generating clue candidates and performing relation deduction. VEM enhances contextual visual representation by leveraging key contextual clues. Extensive experiments validate the effectiveness of our IRM in I-VQA tasks, outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\%$, $1.37\%$, and $4.87\%$, respectively. Additionally, IRM performs SOTA on similar implicit advertisement understanding and future prediction in traffic-VQA. Datasets and codes are available for double-blind review in anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2506.09881</link>
<guid>https://arxiv.org/abs/2506.09881</guid>
<content:encoded><![CDATA[
arXiv:2506.09881v3 Announce Type: replace 
Abstract: Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting with Confidence: Accurate Pest Monitoring in Water Traps</title>
<link>https://arxiv.org/abs/2506.22438</link>
<guid>https://arxiv.org/abs/2506.22438</guid>
<content:encoded><![CDATA[
arXiv:2506.22438v2 Announce Type: replace 
Abstract: Accurate pest population monitoring and tracking their dynamic changes are crucial for precision agriculture decision-making. A common limitation in existing vision-based automatic pest counting research is that models are typically evaluated on datasets with ground truth but deployed in real-world scenarios without assessing the reliability of counting results due to the lack of ground truth. To this end, this paper proposed a method for comprehensively evaluating pest counting confidence in the image, based on information related to counting results and external environmental conditions. First, a pest detection network is used for pest detection and counting, extracting counting result-related information. Then, the pest images undergo image quality assessment, image complexity assessment, and pest distribution uniformity assessment. And the changes in image clarity caused by stirring during image acquisition are quantified by calculating the average gradient magnitude. Notably, we designed a hypothesis-driven multi-factor sensitivity analysis method to select the optimal image quality assessment and image complexity assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for pest distribution uniformity assessment. Finally, the obtained information related to counting results and external environmental conditions is input into a regression model for prediction, resulting in the final pest counting confidence. To the best of our knowledge, this is the first study dedicated to comprehensively evaluating counting confidence in counting tasks, and quantifying the relationship between influencing factors and counting confidence through a model. Experimental results show our method reduces MSE by 31.7% and improves R2 by 15.2% on the pest counting confidence test set, compared to the baseline built primarily on information related to counting results.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation</title>
<link>https://arxiv.org/abs/2507.00752</link>
<guid>https://arxiv.org/abs/2507.00752</guid>
<content:encoded><![CDATA[
arXiv:2507.00752v2 Announce Type: replace 
Abstract: Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Open-World Human Action Segmentation Using Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2507.00756</link>
<guid>https://arxiv.org/abs/2507.00756</guid>
<content:encoded><![CDATA[
arXiv:2507.00756v2 Announce Type: replace 
Abstract: Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples.
  We evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models</title>
<link>https://arxiv.org/abs/2507.13162</link>
<guid>https://arxiv.org/abs/2507.13162</guid>
<content:encoded><![CDATA[
arXiv:2507.13162v2 Announce Type: replace 
Abstract: Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</title>
<link>https://arxiv.org/abs/2508.08177</link>
<guid>https://arxiv.org/abs/2508.08177</guid>
<content:encoded><![CDATA[
arXiv:2508.08177v2 Announce Type: replace 
Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction</title>
<link>https://arxiv.org/abs/2509.17329</link>
<guid>https://arxiv.org/abs/2509.17329</guid>
<content:encoded><![CDATA[
arXiv:2509.17329v2 Announce Type: replace 
Abstract: Smoke in real-world scenes can severely degrade image quality and hamper visibility. Recent image restoration methods either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from multi-view video sequences. Our method uses thermal and RGB images, leveraging the reduced scattering in thermal images to see through smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene into smoke and non-smoke components. Unlike prior work, SmokeSeer handles a broad range of smoke densities and adapts to temporally varying smoke. We validate our method on synthetic data and a new real-world smoke dataset with RGB and thermal images. We provide an open-source implementation and data on the project website.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Segmentation of Polyps and Visual Explainability Analysis</title>
<link>https://arxiv.org/abs/2509.18159</link>
<guid>https://arxiv.org/abs/2509.18159</guid>
<content:encoded><![CDATA[
arXiv:2509.18159v3 Announce Type: replace 
Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis</title>
<link>https://arxiv.org/abs/2509.20295</link>
<guid>https://arxiv.org/abs/2509.20295</guid>
<content:encoded><![CDATA[
arXiv:2509.20295v4 Announce Type: replace 
Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: https://github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks</title>
<link>https://arxiv.org/abs/2509.22258</link>
<guid>https://arxiv.org/abs/2509.22258</guid>
<content:encoded><![CDATA[
arXiv:2509.22258v2 Announce Type: replace 
Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalizable Shape Completion with SIM(3) Equivariance</title>
<link>https://arxiv.org/abs/2509.26631</link>
<guid>https://arxiv.org/abs/2509.26631</guid>
<content:encoded><![CDATA[
arXiv:2509.26631v3 Announce Type: replace 
Abstract: 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream</title>
<link>https://arxiv.org/abs/2510.07752</link>
<guid>https://arxiv.org/abs/2510.07752</guid>
<content:encoded><![CDATA[
arXiv:2510.07752v2 Announce Type: replace 
Abstract: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</title>
<link>https://arxiv.org/abs/2510.17790</link>
<guid>https://arxiv.org/abs/2510.17790</guid>
<content:encoded><![CDATA[
arXiv:2510.17790v2 Announce Type: replace 
Abstract: Computer-use agents face a fundamental limitation. They rely exclusively on primitive GUI actions (click, type, scroll), creating brittle execution chains prone to cascading failures. While API-driven agents harness rich capabilities through structured interfaces and tools, computer-use agents remain constrained to low-level visual interactions. We present UltraCUA, a foundation model that transcends this limitation through hybrid action-seamlessly unifying primitive GUI operations with high-level tool execution. Our innovation rests on four critical advances. First, an automated pipeline extracts and scales tool capabilities from software documentation and code repositories. Second, a synthetic data engine produces 17,000+ verifiable tasks capturing real-world computer-use complexity. Third, comprehensive hybrid action trajectory collection incorporates both GUI primitives and strategic tool calls. Fourth, a two-stage training methodology combines supervised fine-tuning with online reinforcement learning, enabling intelligent action selection between GUI and API. Evaluation with our 7B and 32B UltraCUA models reveals transformative performance gains. On OSWorld, UltraCUA achieves 22% relative improvement while executing 11% faster than existing approaches, averagely. Cross-domain validation on WindowsAgentArena demonstrates robust generalization with 21.7% success rate, surpassing Windows-trained baselines. The hybrid action paradigm proves essential, reducing error propagation while improving execution efficiency. This work establishes a scalable paradigm bridging primitive GUI interactions and high-level tool intelligence, enabling more resilient and adaptable computer use agents for diverse environments and complex real-world tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</title>
<link>https://arxiv.org/abs/2510.19195</link>
<guid>https://arxiv.org/abs/2510.19195</guid>
<content:encoded><![CDATA[
arXiv:2510.19195v3 Announce Type: replace 
Abstract: Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
<link>https://arxiv.org/abs/2511.00090</link>
<guid>https://arxiv.org/abs/2511.00090</guid>
<content:encoded><![CDATA[
arXiv:2511.00090v3 Announce Type: replace 
Abstract: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</title>
<link>https://arxiv.org/abs/2511.02427</link>
<guid>https://arxiv.org/abs/2511.02427</guid>
<content:encoded><![CDATA[
arXiv:2511.02427v2 Announce Type: replace 
Abstract: Video Understanding, Scene Interpretation and Commonsense Reasoning are highly challenging tasks enabling the interpretation of visual information, allowing agents to perceive, interact with and make rational decisions in its environment. Large Language Models (LLMs) and Visual Language Models (VLMs) have shown remarkable advancements in these areas in recent years, enabling domain-specific applications as well as zero-shot open vocabulary tasks, combining multiple domains. However, the required computational complexity poses challenges for their application on edge devices and in the context of Mobile Robotics, especially considering the trade-off between accuracy and inference time. In this paper, we investigate the capabilities of state-of-the-art VLMs for the task of Scene Interpretation and Action Recognition, with special regard to small VLMs capable of being deployed to edge devices in the context of Mobile Robotics. The proposed pipeline is evaluated on a diverse dataset consisting of various real-world cityscape, on-campus and indoor scenarios. The experimental evaluation discusses the potential of these small models on edge devices, with particular emphasis on challenges, weaknesses, inherent model biases and the application of the gained information. Supplementary material is provided via the following repository: https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data</title>
<link>https://arxiv.org/abs/2511.02541</link>
<guid>https://arxiv.org/abs/2511.02541</guid>
<content:encoded><![CDATA[
arXiv:2511.02541v2 Announce Type: replace 
Abstract: Shearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Changes in Real Time: Online Scene Change Detection with Multi-View Fusion</title>
<link>https://arxiv.org/abs/2511.12370</link>
<guid>https://arxiv.org/abs/2511.12370</guid>
<content:encoded><![CDATA[
arXiv:2511.12370v2 Announce Type: replace 
Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance</title>
<link>https://arxiv.org/abs/2511.12419</link>
<guid>https://arxiv.org/abs/2511.12419</guid>
<content:encoded><![CDATA[
arXiv:2511.12419v2 Announce Type: replace 
Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.16203</link>
<guid>https://arxiv.org/abs/2511.16203</guid>
<content:encoded><![CDATA[
arXiv:2511.16203v3 Announce Type: replace 
Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2511.17844</link>
<guid>https://arxiv.org/abs/2511.17844</guid>
<content:encoded><![CDATA[
arXiv:2511.17844v2 Announce Type: replace 
Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsistCompose: Unified Multimodal Layout Control for Image Composition</title>
<link>https://arxiv.org/abs/2511.18333</link>
<guid>https://arxiv.org/abs/2511.18333</guid>
<content:encoded><![CDATA[
arXiv:2511.18333v2 Announce Type: replace 
Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.18729</link>
<guid>https://arxiv.org/abs/2511.18729</guid>
<content:encoded><![CDATA[
arXiv:2511.18729v2 Announce Type: replace 
Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be in https://github.com/liulin815/GuideFlow.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Ahead: Foresight Intelligence in MLLMs and World Models</title>
<link>https://arxiv.org/abs/2511.18735</link>
<guid>https://arxiv.org/abs/2511.18735</guid>
<content:encoded><![CDATA[
arXiv:2511.18735v2 Announce Type: replace 
Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttenDence: Maximizing Attention Confidence for Test Time Adaptation</title>
<link>https://arxiv.org/abs/2511.18925</link>
<guid>https://arxiv.org/abs/2511.18925</guid>
<content:encoded><![CDATA[
arXiv:2511.18925v2 Announce Type: replace 
Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective. This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HunyuanOCR Technical Report</title>
<link>https://arxiv.org/abs/2511.19575</link>
<guid>https://arxiv.org/abs/2511.19575</guid>
<content:encoded><![CDATA[
arXiv:2511.19575v2 Announce Type: replace 
Abstract: This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.21606</link>
<guid>https://arxiv.org/abs/2511.21606</guid>
<content:encoded><![CDATA[
arXiv:2511.21606v2 Announce Type: replace 
Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecture Decoupling Is Not All You Need For Unified Multimodal Model</title>
<link>https://arxiv.org/abs/2511.22663</link>
<guid>https://arxiv.org/abs/2511.22663</guid>
<content:encoded><![CDATA[
arXiv:2511.22663v2 Announce Type: replace 
Abstract: Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data</title>
<link>https://arxiv.org/abs/2512.00087</link>
<guid>https://arxiv.org/abs/2512.00087</guid>
<content:encoded><![CDATA[
arXiv:2512.00087v2 Announce Type: replace 
Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent</title>
<link>https://arxiv.org/abs/2512.00846</link>
<guid>https://arxiv.org/abs/2512.00846</guid>
<content:encoded><![CDATA[
arXiv:2512.00846v2 Announce Type: replace 
Abstract: There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.02498</link>
<guid>https://arxiv.org/abs/2512.02498</guid>
<content:encoded><![CDATA[
arXiv:2512.02498v2 Announce Type: replace 
Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glance: Accelerating Diffusion Models with 1 Sample</title>
<link>https://arxiv.org/abs/2512.02899</link>
<guid>https://arxiv.org/abs/2512.02899</guid>
<content:encoded><![CDATA[
arXiv:2512.02899v2 Announce Type: replace 
Abstract: Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</title>
<link>https://arxiv.org/abs/2512.03040</link>
<guid>https://arxiv.org/abs/2512.03040</guid>
<content:encoded><![CDATA[
arXiv:2512.03040v2 Announce Type: replace 
Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus</title>
<link>https://arxiv.org/abs/2512.03346</link>
<guid>https://arxiv.org/abs/2512.03346</guid>
<content:encoded><![CDATA[
arXiv:2512.03346v2 Announce Type: replace 
Abstract: The detection of weak, spatially distributed anomalies in volumetric medical imaging remains challenging due to the difficulty of integrating subtle signals across non-adjacent regions. This study presents a controlled comparison of sixteen architectures spanning convolutional, hybrid, and transformer families for subclinical keratoconus detection from three-dimensional anterior segment optical coherence tomography (AS-OCT). The results demonstrate that hierarchical architectures achieve 21-23% higher sensitivity and specificity, particularly in the difficult subclinical regime, outperforming both convolutional neural networks (CNNs) and global-attention Vision Transformer (ViT) baselines. Mechanistic analyses indicate that this advantage arises from spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate extent of subclinical abnormalities, avoiding the excessive locality observed in convolutional models and the diffuse integration characteristic of pure global attention. Attention-distance measurements show that subclinical cases require longer spatial integration than healthy or overtly pathological volumes, with hierarchical models exhibiting lower variance and more anatomically coherent focus. Representational similarity further indicates that hierarchical attention learns a distinct feature space that balances local structure sensitivity with flexible long-range interactions. Auxiliary age and sex prediction tasks demonstrate moderately high cross-task consistency, supporting the generalizability of these inductive principles. The findings provide design guidance for volumetric anomaly detection and highlight hierarchical attention as a principled approach for early pathological change analysis in medical imaging.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.03454</link>
<guid>https://arxiv.org/abs/2512.03454</guid>
<content:encoded><![CDATA[
arXiv:2512.03454v2 Announce Type: replace 
Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.03477</link>
<guid>https://arxiv.org/abs/2512.03477</guid>
<content:encoded><![CDATA[
arXiv:2512.03477v2 Announce Type: replace 
Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms. Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation</title>
<link>https://arxiv.org/abs/2512.05104</link>
<guid>https://arxiv.org/abs/2512.05104</guid>
<content:encoded><![CDATA[
arXiv:2512.05104v2 Announce Type: replace 
Abstract: All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</title>
<link>https://arxiv.org/abs/2411.03752</link>
<guid>https://arxiv.org/abs/2411.03752</guid>
<content:encoded><![CDATA[
arXiv:2411.03752v3 Announce Type: replace-cross 
Abstract: Recent studies have shown that deep learning models are very vulnerable to poisoning attacks. Many defense methods have been proposed to address this issue. However, traditional poisoning attacks are not as threatening as commonly believed. This is because they often cause differences in how the model performs on the training set compared to the validation set. Such inconsistency can alert defenders that their data has been poisoned, allowing them to take the necessary defensive actions. In this paper, we introduce a more threatening type of poisoning attack called the Deferred Poisoning Attack. This new attack allows the model to function normally during the training and validation phases but makes it very sensitive to evasion attacks or even natural noise. We achieve this by ensuring the poisoned model's loss function has a similar value as a normally trained model at each input sample but with a large local curvature. A similar model loss ensures that there is no obvious inconsistency between the training and validation accuracy, demonstrating high stealthiness. On the other hand, the large curvature implies that a small perturbation may cause a significant increase in model loss, leading to substantial performance degradation, which reflects a worse robustness. We fulfill this purpose by making the model have singular Hessian information at the optimal point via our proposed Singularization Regularization term. We have conducted both theoretical and empirical analyses of the proposed method and validated its effectiveness through experiments on image classification tasks. Furthermore, we have confirmed the hazards of this form of poisoning attack under more general scenarios using natural noise, offering a new perspective for research in the field of security.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Test-Time Training with Operator Sketching for Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2411.05771</link>
<guid>https://arxiv.org/abs/2411.05771</guid>
<content:encoded><![CDATA[
arXiv:2411.05771v5 Announce Type: replace-cross 
Abstract: Equivariant Imaging (EI) regularization has become the de-facto technique for unsupervised training of deep imaging networks, without any need of ground-truth data. Observing that the EI-based unsupervised training paradigm currently has significant computational redundancy leading to inefficiency in high-dimensional applications, we propose a sketched EI regularization which leverages the randomized sketching techniques for acceleration. We apply our sketched EI regularization to develop an accelerated deep internal learning framework, which can be efficiently applied for test-time network adaptation. Additionally, for network adaptation tasks, we propose a parameter-efficient approach to accelerate both EI and Sketched-EI via optimizing only the normalization layers. Our numerical study on X-ray CT and multicoil magnetic resonance image reconstruction tasks demonstrate that our approach can achieve significant computational acceleration over the standard EI counterpart, especially in test-time training tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-centric proto-symbolic behavioural reasoning from pixels</title>
<link>https://arxiv.org/abs/2411.17438</link>
<guid>https://arxiv.org/abs/2411.17438</guid>
<content:encoded><![CDATA[
arXiv:2411.17438v3 Announce Type: replace-cross 
Abstract: Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels -- ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, as well as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \land C)$ and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sublinear Variational Optimization of Gaussian Mixture Models with Millions to Billions of Parameters</title>
<link>https://arxiv.org/abs/2501.12299</link>
<guid>https://arxiv.org/abs/2501.12299</guid>
<content:encoded><![CDATA[
arXiv:2501.12299v2 Announce Type: replace-cross 
Abstract: Gaussian Mixture Models (GMMs) range among the most frequently used models in machine learning. However, training large, general GMMs becomes computationally prohibitive for datasets that have many data points $N$ of high-dimensionality $D$. For GMMs with arbitrary covariances, we here derive a highly efficient variational approximation, which is then integrated with mixtures of factor analyzers (MFAs). For GMMs with $C$ components, our proposed algorithm substantially reduces runtime complexity from $\mathcal{O}(NCD^2)$ per iteration to a complexity scaling linearly with $D$ and sublinearly with $NC$. In numerical experiments, we first validate that the complexity reduction results in a sublinear scaling for the entire GMM optimization process. Second, we show on large-scale benchmarks that the sublinear algorithm results in speed-ups of an order-of-magnitude compared to the state-of-the-art. Third, as a proof of concept, we finally train GMMs with over 10 billion parameters on about 100 million images, observing training times of less than nine hours on a single state-of-the-art CPU. Finally, and forth, we demonstrate the effectiveness of large-scale GMMs on the task of zero-shot image denoising, where sublinear training results in state-of-the-art denoising times while competitive denoising performance is maintained.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures</title>
<link>https://arxiv.org/abs/2503.11352</link>
<guid>https://arxiv.org/abs/2503.11352</guid>
<content:encoded><![CDATA[
arXiv:2503.11352v2 Announce Type: replace-cross 
Abstract: The ability of robots to recognize human gestures facilitates a natural and accessible human-robot collaboration. However, most work in gesture recognition remains rooted in reference frame-dependent representations. This poses a challenge when reference frames vary due to different work cell layouts, imprecise frame calibrations, or other environmental changes. This paper investigated the use of invariant trajectory descriptors for robust hand palm motion gesture recognition under reference frame changes. First, a novel dataset of recorded Hand Palm Motion (HPM) gestures is introduced. The motion gestures in this dataset were specifically designed to be distinguishable without dependence on specific reference frames or directional cues. Afterwards, multiple invariant trajectory descriptor approaches were benchmarked to assess how their performances generalize to this novel HPM dataset. After this offline benchmarking, the best scoring approach is validated for online recognition by developing a real-time Proof of Concept (PoC). In this PoC, hand palm motion gestures were used to control the real-time movement of a manipulator arm. The PoC demonstrated a high recognition reliability in real-time operation, achieving an $F_1$-score of 92.3%. This work demonstrates the effectiveness of the invariant descriptor approach as a standalone solution. Moreover, we believe that the invariant descriptor approach can also be utilized within other state-of-the-art pattern recognition and learning systems to improve their robustness against reference frame variations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FE-MCFormer: An interpretable fault diagnosis framework for rotating machinery under strong noise based on time-frequency fusion transformer</title>
<link>https://arxiv.org/abs/2505.06285</link>
<guid>https://arxiv.org/abs/2505.06285</guid>
<content:encoded><![CDATA[
arXiv:2505.06285v2 Announce Type: replace-cross 
Abstract: Many fault diagnosis methods of rotating machines are based on discriminative features extracted from signals collected from the key components such as bearings. However, under complex operating conditions, periodic impulsive characteristics in the signal related to weak fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn interpretable fault-related features in such scenarios. This paper proposes a novel transformer framework (FE-MCFormer) to extract interpretable time-frequency features, with the aim of improving the fault detection accuracy and intrepretability of rotating machines under strong noise. First, a Fourier adaptive reconstruction embedding layer is introduced as a global information encoder in the model. Subsequently, a time-frequency fusion module is designed, further improve the model robustness and interpretability. The effectiveness of FE-MCFormer in machine fault diagnosis is validated through three case studies.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Regularity in Deterministic Sampling Dynamics of Diffusion-based Generative Models</title>
<link>https://arxiv.org/abs/2506.10177</link>
<guid>https://arxiv.org/abs/2506.10177</guid>
<content:encoded><![CDATA[
arXiv:2506.10177v3 Announce Type: replace-cross 
Abstract: Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics of diffusion generative models: each simulated sampling trajectory along the gradient field lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical boomerang shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing deterministic numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only 5 - 10 function evaluations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling</title>
<link>https://arxiv.org/abs/2507.09441</link>
<guid>https://arxiv.org/abs/2507.09441</guid>
<content:encoded><![CDATA[
arXiv:2507.09441v2 Announce Type: replace-cross 
Abstract: High-resolution image synthesis with diffusion models often suffers from energy instabilities and guidance artifacts that degrade visual quality. We analyze the latent energy landscape during sampling and propose adaptive classifier-free guidance (CFG) schedules that maintain stable energy trajectories. Our approach introduces energy-aware scheduling strategies that modulate guidance strength over time, achieving superior stability scores (0.9998) and consistency metrics (0.9873) compared to fixed-guidance approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling yields optimal performance, providing sharper, more faithful images while reducing artifacts. Our energy profiling framework serves as a powerful diagnostic tool for understanding and improving diffusion model behavior.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</title>
<link>https://arxiv.org/abs/2511.14396</link>
<guid>https://arxiv.org/abs/2511.14396</guid>
<content:encoded><![CDATA[
arXiv:2511.14396v2 Announce Type: replace-cross 
Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models</title>
<link>https://arxiv.org/abs/2511.19877</link>
<guid>https://arxiv.org/abs/2511.19877</guid>
<content:encoded><![CDATA[
arXiv:2511.19877v2 Announce Type: replace-cross 
Abstract: Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Generated Human Videos to Physically Plausible Robot Trajectories</title>
<link>https://arxiv.org/abs/2512.05094</link>
<guid>https://arxiv.org/abs/2512.05094</guid>
<content:encoded><![CDATA[
arXiv:2512.05094v2 Announce Type: replace-cross 
Abstract: Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Happens When: Learning Temporal Orders of Events in Videos</title>
<link>https://arxiv.org/abs/2512.08979</link>
<guid>https://arxiv.org/abs/2512.08979</guid>
<content:encoded><![CDATA[
<div> Keywords: VLMMs, temporal order, VECTOR benchmark, MECOT, chain-of-thought  

<br /><br />Summary:  
Video Large Multimodal Models (VLMMs) have demonstrated strong video understanding capabilities but struggle to accurately capture the temporal order of multiple events. Through comprehensive experiments, the authors found that VLMMs often perform well even when video frames are scrambled, suggesting these models rely on prior knowledge of typical scenarios rather than genuine sequential processing. To explicitly measure temporal understanding, the paper introduces VECTOR, a new benchmark designed to test a model's ability to identify the correct temporal order of events. Various VLMMs tested on VECTOR frequently fail to properly understand event sequences, highlighting a gap in current models. To overcome this, the authors propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which enhances temporal awareness by training models on detailed, event-by-event video descriptions and employing chain-of-thought prompts during inference. MECOT achieves superior performance on VECTOR and also improves results on existing video benchmarks, demonstrating its effectiveness in fostering temporal understanding. The authors provide their code, model, and datasets publicly, enabling further research and development in temporal video understanding. <div>
arXiv:2512.08979v1 Announce Type: new 
Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multi-Image Vision Agents via End2End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08980</link>
<guid>https://arxiv.org/abs/2512.08980</guid>
<content:encoded><![CDATA[
<div> Multi-image QA, Vision-language model, Reinforcement learning, Tool-use, Visual reflection<br /><br />Summary: The paper introduces IMAgent, an open-source vision-language agent designed to handle complex multi-image question answering (QA) tasks, overcoming the limitation of most existing VLM-based agents that only process single images. IMAgent is trained end-to-end via reinforcement learning within a multi-agent framework that generates challenging and visually-rich multi-image QA pairs. To support training and evaluation, the authors present MIFG-QA, a manually verified dataset containing 10,000 samples focused on multi-image reasoning. Recognizing that deeper reasoning steps often cause models to neglect visual inputs, the authors develop two specialized tools for visual reflection and confirmation, enhancing the agent’s ability to reallocate attention to image content during inference. A novel action-trajectory two-level mask strategy is employed to stabilize tool use through pure reinforcement learning without reliance on costly supervised fine-tuning data. Extensive experiments show that IMAgent not only retains strong performance on standard single-image benchmarks but also demonstrates significant improvements on the multi-image QA task with MIFG-QA. The authors plan to release both the codes and dataset to benefit the wider research community, providing actionable insights into developing more effective multi-image vision-language agents. <div>
arXiv:2512.08980v1 Announce Type: new 
Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding</title>
<link>https://arxiv.org/abs/2512.08981</link>
<guid>https://arxiv.org/abs/2512.08981</guid>
<content:encoded><![CDATA[
<div> Face recognition, demographic bias, unified text-image embedding, vision-language models, fair verification<br /><br />Summary:<br /><br />This paper focuses on addressing demographic bias in face recognition (FR) systems, which often arises from the entanglement of demographic-specific information with identity features in facial embeddings. Such biases are particularly problematic in multicultural urban environments where biometric systems are widely used. To mitigate this, the authors propose a novel method called Unified Text-Image Embedding (UTIE) that induces demographic ambiguity by enriching face embeddings with textual demographic features from other groups. This enrichment encourages the embeddings to prioritize identity-relevant information and reduce demographic influence. UTIE leverages the zero-shot learning and cross-modal semantic alignment capabilities of Vision-Language Models (VLMs), specifically using CLIP, OpenCLIP, and SigLIP in their experiments. The method was evaluated on two standard benchmarks for bias assessment in face recognition, RFW and BFW. Results demonstrate that UTIE consistently reduces bias metrics across demographic groups while maintaining or even improving face verification accuracy. This approach thus offers a promising direction to promote fairer FR systems without compromising performance. <div>
arXiv:2512.08981v1 Announce Type: new 
Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement</title>
<link>https://arxiv.org/abs/2512.08982</link>
<guid>https://arxiv.org/abs/2512.08982</guid>
<content:encoded><![CDATA[
<div> Diffusion Models, Low-Light Enhancement, Consistency Models, Retinex Decomposition, One-Step Sampling  

<br /><br />Summary:  
This paper introduces Consist-Retinex, the first framework to adapt consistency modeling for Retinex-based low-light image enhancement, addressing the inefficiency of diffusion models requiring hundreds of sampling steps. Unlike unconditional synthesis, conditional enhancement demands different training dynamics since it relies on large-noise regimes connecting degraded inputs to enhanced outputs. The authors propose two main innovations: a dual-objective consistency loss combining temporal consistency and ground-truth alignment with randomized time sampling to ensure full-spectrum supervision and stable training convergence; and an adaptive noise-emphasized sampling strategy that focuses training on large-noise regions critical for effective one-step conditional enhancement. Evaluated on the VE-LOL-L dataset, Consist-Retinex achieves state-of-the-art results, attaining a PSNR of 25.51 and FID of 44.73, outperforming the previous Diff-Retinex++ model which scored 23.41 PSNR and 49.59 FID, all using only a single sampling step. Additionally, the training cost is significantly reduced, requiring just one-eighth of the budget compared to the traditional 1000-step Diff-Retinex baseline, demonstrating both efficiency and performance improvements in low-light image enhancement tasks. <div>
arXiv:2512.08982v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification</title>
<link>https://arxiv.org/abs/2512.08983</link>
<guid>https://arxiv.org/abs/2512.08983</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV identification, Radio Frequency Fingerprint Identification, model pruning, spectral clustering, deep learning  

<br /><br />Summary: With the growing use of Unmanned Aerial Vehicles (UAVs) and increasing complexity in low-altitude security threats, traditional UAV identification methods face challenges in extracting reliable signal features and achieving real-time performance. Deep learning-based Radio Frequency Fingerprint Identification (RFFI) methods have enhanced recognition accuracy but suffer from large model sizes and high computational requirements, complicating deployment on resource-constrained edge devices. To address this, the paper proposes HSCP (Hierarchical Spectral Clustering Pruning), a novel framework integrating layer and channel pruning to attain significant model compression, improved performance, and efficient inference. HSCP first applies spectral clustering guided by Centered Kernel Alignment (CKA) to remove redundant layers, followed by channel dimension pruning for fine-grained redundancy elimination. A noise-robust fine-tuning strategy is employed to maintain model robustness. Experiments on the UAV-M100 benchmark demonstrate HSCP’s effectiveness, achieving 86.39% parameter reduction and 84.44% FLOPs reduction on ResNet18, while improving accuracy by 1.49% over the unpruned baseline. Furthermore, HSCP maintains superior robustness in environments with low signal-to-noise ratios, confirming its practical utility for UAV identification in complex and resource-limited scenarios. <div>
arXiv:2512.08983v1 Announce Type: new 
Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.08984</link>
<guid>https://arxiv.org/abs/2512.08984</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Retrieval-Augmented Framework, Large Language Models, Prompt Optimization, Unseen Activity Recognition  

<br /><br />Summary:  
Human Activity Recognition (HAR) is crucial for applications such as healthcare, rehabilitation, fitness tracking, and smart environments. Traditional deep learning methods require extensive dataset-specific training, large labeled datasets, and high computational costs. The paper introduces RAG-HAR, a novel training-free retrieval-augmented framework that utilizes large language models (LLMs) for HAR. RAG-HAR works by computing lightweight statistical descriptors from input data and retrieving semantically similar samples from a pre-built vector database to provide contextual evidence for LLM-based activity identification. To enhance its performance, the authors integrate prompt optimization and develop an LLM-based activity descriptor that enriches the vector database with context-aware information. RAG-HAR achieves state-of-the-art results across six diverse HAR benchmarks without any training or fine-tuning of models, highlighting its robustness and practicality. Importantly, this approach extends beyond recognizing previously known activities, enabling the detection and meaningful labeling of multiple unseen human activities. As a result, RAG-HAR offers a scalable and efficient solution for accurate activity recognition without the traditional heavy training requirements. <div>
arXiv:2512.08984v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Test-Time Scaling Approach for Image Generation</title>
<link>https://arxiv.org/abs/2512.08985</link>
<guid>https://arxiv.org/abs/2512.08985</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, test-time compute, diffusion models, compute allocation, Verifier-Threshold method

<br /><br />Summary:  
1. The paper addresses the challenge of improving efficiency in image generation by optimizing the allocation of test-time computational resources during inference.  
2. It focuses on diffusion and flow models where searching across noise samples demands significant compute, and existing methods allocate compute budgets non-uniformly but rely on greedy algorithms that are inefficient.  
3. The authors propose a novel approach called the Verifier-Threshold method, which dynamically reallocates computational effort across denoising steps to improve inference efficiency.  
4. This new method shows substantial gains, achieving a 2-4 times reduction in computational time compared to the current state-of-the-art methods when evaluated on the GenEval benchmark, without sacrificing performance.  
5. The proposed solution advances the practicality of large generative image models by enabling faster and more resource-efficient inference, benefiting real-world applications requiring high-quality image synthesis. <div>
arXiv:2512.08985v1 Announce Type: new 
Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy</title>
<link>https://arxiv.org/abs/2512.08986</link>
<guid>https://arxiv.org/abs/2512.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: Diabetic Retinopathy, quality control, image annotation, explainable classifier, contrastive learning<br /><br />Summary:<br /><br />Diabetic Retinopathy (DR) is a severe complication affecting individuals with long-term diabetes, potentially leading to vision loss if not diagnosed early. Fundus photography is a critical tool for capturing retinal structures and identifying abnormalities corresponding to various stages of DR. Artificial Intelligence (AI) can aid clinicians in detecting lesions from these images, thereby reducing the manual workload. However, training effective AI models demands high-quality annotated datasets, which are challenging to obtain due to the complexity of retinal structures and possible errors during image acquisition and manual annotation. To address these challenges, the authors propose a quality-control framework that guarantees only high-quality data is used for AI training and evaluation. First, the framework deploys an explainable feature-based classifier to filter out inadequate images, employing features extracted through both traditional image processing techniques and advanced contrastive learning. Subsequently, enhanced images are presented for annotation with the assistance of deep learning tools to improve accuracy and efficiency. Finally, the framework calculates annotator agreement using derived formulas to evaluate the usability of the annotations. This systematic approach ensures improved reliability and effectiveness in AI models for DR diagnosis. <div>
arXiv:2512.08986v1 Announce Type: new 
Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization</title>
<link>https://arxiv.org/abs/2512.08987</link>
<guid>https://arxiv.org/abs/2512.08987</guid>
<content:encoded><![CDATA[
<div> 3D inverse design, latent representation, physics-aware optimization, gradient-guided diffusion, topology-preserving refinement<br /><br />Summary:<br /><br />The paper addresses the challenge of inverse design in 3D domains, where the design space is vast and traditional exhaustive search methods are impractical. Recent deep learning approaches have helped by introducing generative priors and surrogate models, but they often rely on 2D projections or modification of existing 3D shapes, limiting volumetric detail and design freedom. To overcome these limitations, the authors propose a novel 3D Inverse Design (3DID) framework that directly explores the 3D design space from scratch. The key innovation involves learning a unified latent representation that jointly encodes both geometric shape and related physical field data compactly. The optimization process occurs in two stages: first, a gradient-guided diffusion sampler globally explores the continuous latent manifold; second, an objective-driven, topology-preserving refinement step fine-tunes the candidates, sculpting them towards the desired physical or functional targets. This combination enables generation of high-fidelity 3D geometries with better solution quality and greater design versatility compared to existing methods. The framework represents a significant advance in enabling fully 3D, physics-aware design optimization, opening pathways to more complex and detailed inverse design tasks in engineering and scientific applications. <div>
arXiv:2512.08987v1 Announce Type: new 
Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration</title>
<link>https://arxiv.org/abs/2512.08989</link>
<guid>https://arxiv.org/abs/2512.08989</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral image classification, knowledge transfer, spectral variations, semantic inconsistencies, cross-scene integration<br /><br />Summary:  
This paper addresses the challenges of knowledge transfer in hyperspectral image (HSI) classification across different domains, focusing on two main issues: spectral variations caused by different sensors and semantic inconsistencies present in heterogeneous scenes. Existing approaches generally assume either homogeneous domains or heterogeneous ones with overlapping label spaces, limiting their effectiveness when label spaces do not coincide and neglecting target-private information. To overcome these limitations, the authors propose a novel Cross-scene Knowledge Integration (CKI) framework designed for fully heterogeneous transfer settings. CKI consists of three core components: (1) Alignment of Spectral Characteristics (ASC), which mitigates spectral discrepancies through a domain-agnostic projection technique; (2) Cross-scene Knowledge Sharing Preference (CKSP), incorporating a Source Similarity Mechanism (SSM) to resolve semantic mismatches between source and target domains; and (3) Complementary Information Integration (CII), which enhances performance by leveraging target-specific complementary cues. Extensive experiments demonstrate that CKI achieves state-of-the-art performance and exhibits robust stability across a variety of challenging cross-scene HSI scenarios, validating its effectiveness in enabling more generalized and accurate HSI classification under diverse and fully heterogeneous domain conditions. <div>
arXiv:2512.08989v1 Announce Type: new 
Abstract: Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic World Models for Verification of Closed-loop Vision-based Systems</title>
<link>https://arxiv.org/abs/2512.08991</link>
<guid>https://arxiv.org/abs/2512.08991</guid>
<content:encoded><![CDATA[
<div> Deterministic World Model, vision-based control, generative models, reachability analysis, conformal prediction<br /><br />Summary:<br /><br />1. The paper addresses the challenge of verifying closed-loop vision-based control systems, which is difficult due to image high dimensionality and complex visual environment modeling. <br />2. Conventional generative models used as camera surrogates introduce overapproximation errors caused by stochastic latent variables, limiting verification precision. <br />3. To overcome this, the authors propose a Deterministic World Model (DWM) that directly maps system states to images without latent variables, enabling precise input bounds. <br />4. The DWM is trained using a dual-objective loss combining pixel-level reconstruction accuracy with a control difference loss, ensuring that the model's behavior matches the real system. <br />5. Integrating DWM into a verification pipeline combined with Star-based reachability analysis (StarV) and conformal prediction yields rigorous statistical bounds on errors between the model and actual vision-based system trajectories. <br />6. Experimental results on benchmarks demonstrate that this approach produces significantly tighter reachable sets and improved verification performance compared to methods using latent-variable generative models. <div>
arXiv:2512.08991v1 Announce Type: new 
Abstract: Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Generative AI helps Radiotherapy Planning with User Preference</title>
<link>https://arxiv.org/abs/2512.08996</link>
<guid>https://arxiv.org/abs/2512.08996</guid>
<content:encoded><![CDATA[
<div> Keywords: radiotherapy planning, 3D dose prediction, generative model, user-defined preferences, treatment planning systems  

<br /><br />Summary:  
This article addresses the complexity and variability in radiotherapy planning across different institutions and planners. It highlights how current deep learning models for 3D dose prediction often depend on reference plans as ground truth, which can introduce biases reflecting specific planning styles or institutional preferences. To overcome this limitation, the authors propose a novel generative model that predicts 3D dose distributions based purely on customizable user-defined preference flavors. These preferences allow planners to emphasize particular trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), enabling more personalized and flexible treatment plans tailored to individual patient needs. The model is designed for easy integration into existing clinical treatment planning systems, supporting efficient generation of high-quality radiotherapy plans. Comparative evaluation results indicate that, in some scenarios, this approach can outperform the Varian RapidPlan model regarding both adaptability to user preferences and overall plan quality. This work represents a significant step towards more personalized, flexible, and potentially higher-quality radiotherapy treatment planning using machine learning techniques. <div>
arXiv:2512.08996v1 Announce Type: new 
Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction</title>
<link>https://arxiv.org/abs/2512.08999</link>
<guid>https://arxiv.org/abs/2512.08999</guid>
<content:encoded><![CDATA[
<div> Keywords: Computed Tomography, Metal Artifact Reduction, Diffusion Model, Implicit Neural Representation, Data Fidelity<br /><br />Summary:<br /><br />Computed tomography (CT) images often suffer from severe artifacts caused by the presence of metals, which degrade image quality and diagnostic accuracy. Existing supervised metal artifact reduction (MAR) methods rely heavily on paired metal-clean training data, leading to unstable performance and limited clinical usefulness. Unsupervised methods addressing MAR face two main challenges: firstly, they fail to effectively integrate CT physical geometry into the reduction process, risking lack of data fidelity; secondly, traditional heuristic regularization techniques do not fully leverage the rich prior knowledge available in the data. To address these issues, the authors propose a framework combining implicit neural representation with a diffusion model to regularize the MAR process. The implicit neural representation enforces physical constraints and ensures data fidelity by embedding the CT geometry. Meanwhile, the pre-trained diffusion model introduces powerful prior knowledge to guide artifact reduction. Experiments on both simulated and real clinical CT datasets show the proposed method improves artifact removal quality and generalizes well across data types. The results suggest that this novel approach has strong potential for practical clinical deployment, offering a more stable and informed solution to metal artifact reduction in CT imaging. <div>
arXiv:2512.08999v1 Announce Type: new 
Abstract: Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography</title>
<link>https://arxiv.org/abs/2512.09001</link>
<guid>https://arxiv.org/abs/2512.09001</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, lithography defects, dataset generation, pixel-level annotation, Mask R-CNN<br /><br />Summary:<br />1. The research addresses the critical limitation in AI applications for micro/nano manufacturing caused by the lack of accessible, high-quality, and physically grounded training data for lithography defect inspection.<br />2. A novel methodology is proposed for generating large-scale and physically valid defect datasets with pixel-accurate segmentation masks, facilitating precise defect delineation.<br />3. The dataset generation framework synthesizes defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to original design layouts.<br />4. The synthesized layouts and their defect-free counterparts are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography, followed by optical micrograph imaging.<br />5. By comparing defect sample images with defect-free references, consistent pixel-level annotations for four defect classes (bridge, burr, pinch, contamination) are produced.<br />6. The resulting dataset comprises 3,530 optical micrographs with 13,365 annotated defect instances, each with full contour and geometry preserved.<br />7. Evaluation using segmentation Mask R-CNN demonstrates superior detection performance (AP@0.5 near or above 0.96) compared to Faster R-CNN, yielding about 34% mean average precision improvement for bridge, burr, and pinch, and approximately 42% improvement for contamination.<br />8. This methodology successfully enables robust, AI-driven measurement and inspection workflows in semiconductor fabrication through physically valid, pixel-level annotated lithography defect datasets. <div>
arXiv:2512.09001v1 Announce Type: new 
Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques</title>
<link>https://arxiv.org/abs/2512.09005</link>
<guid>https://arxiv.org/abs/2512.09005</guid>
<content:encoded><![CDATA[
<div> Keywords: body motion, face motion, generative modeling, multi-modal learning, motion generation<br /><br />Summary:<br /><br />This survey addresses the generation of body and face motion, highlighting their critical role in human communication by conveying verbal and non-verbal cues as well as personality traits. It reviews fundamental concepts and representation techniques essential for modeling motion dynamics effectively. The paper discusses contemporary generative approaches and how multi-modal learning frameworks incorporate diverse signals such as speech, conversational context, and visual inputs to produce motion. A variety of datasets used in the domain are examined to understand their contributions and limitations. Evaluation metrics for assessing the quality, coherence, and expressiveness of generated motions are also surveyed. Importantly, this work is noted as the first comprehensive review that jointly covers both body and face motion generation, emphasizing their interplay in dyadic interactions. The survey concludes by outlining future research directions aimed at enhancing the realism, expressiveness, and coherence of avatar motion in interactive settings. To support further study, the authors provide an extensive list of related resources and materials, available online at their dedicated webpage. <div>
arXiv:2512.09005v1 Announce Type: new 
Abstract: Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Lossless Ultimate Vision Token Compression for VLMs</title>
<link>https://arxiv.org/abs/2512.09010</link>
<guid>https://arxiv.org/abs/2512.09010</guid>
<content:encoded><![CDATA[
<div> Keywords: visual language models, token compression, iterative merging, spectrum pruning, FlashAttention  

<br /><br />Summary:  
This article addresses computational efficiency and latency challenges in visual language models (VLMs) caused by redundancy in token representations of high-resolution images and videos. Existing attention or similarity-based compression algorithms suffer from position bias and class imbalance, leading to accuracy loss, and perform poorly on shallow large language model (LLM) layers that have weaker cross-modal interactions. To overcome these issues, the authors propose extending token compression into the visual encoder using an iterative merging scheme orthogonal in spatial axes, which accelerates computation across the entire VLM. Additionally, they introduce a spectrum pruning unit within the LLM that utilizes an attention/similarity-free low-pass filter to gradually prune redundant visual tokens, fully compatible with modern FlashAttention techniques. Combining these methods, the paper presents the Lossless Ultimate Vision tokens Compression (LUVC) framework, which systematically compresses visual tokens until their complete elimination at the final LLM layer. This enables the progressive fusion of high-dimensional visual features into multimodal queries. Experiments demonstrate that LUVC achieves a 2× speedup in inference time with negligible accuracy degradation, and its training-free design allows for immediate deployment across multiple visual language models. <div>
arXiv:2512.09010v1 Announce Type: new 
Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Approach for Detection of Entities in Dynamic Media Contents</title>
<link>https://arxiv.org/abs/2512.09011</link>
<guid>https://arxiv.org/abs/2512.09011</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, character detection, video sequence, supervised learning, national security<br /><br />Summary: This paper presents a novel approach to detecting specific entities, particularly characters, in video sequences using deep learning techniques based on artificial neural networks. The study addresses the complexity involved in identifying target individuals within videos due to the presence of multiple objects in the data. The proposed method leverages supervised learning algorithms structured to utilize simple features of the target character, achieving significant successes compared to existing state-of-the-art techniques. Results demonstrate that this approach efficiently locates desired individuals from both private and public image databases. The authors emphasize the practical application of their classifier in the context of Angola, where it has the potential to enhance national security systems. Specifically, it can be used to match database records of persons of interest—such as missing individuals or criminals—with video data from the Integrated Public Security Centre (CISP). This fusion of video analysis and deep learning could provide strong support for law enforcement and public safety efforts, marking a valuable advancement in computer vision applications for security. <div>
arXiv:2512.09011v1 Announce Type: new 
Abstract: The notion of learning underlies almost every evolution of Intelligent Agents. In this paper, we present an approach for searching and detecting a given entity in a video sequence. Specifically, we study how the deep learning technique by artificial neuralnetworks allows us to detect a character in a video sequence. The technique of detecting a character in a video is a complex field of study, considering the multitude of objects present in the data under analysis. From the results obtained, we highlight the following, compared to state of the art: In our approach, within the field of Computer Vision, the structuring of supervised learning algorithms allowed us to achieve several successes from simple characteristics of the target character. Our results demonstrate that is new approach allows us to locate, in an efficient way, wanted individuals from a private or public image base. For the case of Angola, the classifier we propose opens the possibility of reinforcing the national security system based on the database of target individuals (disappeared, criminals, etc.) and the video sequences of the Integrated Public Security Centre (CISP).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Remove Lens Flare in Event Camera</title>
<link>https://arxiv.org/abs/2512.09016</link>
<guid>https://arxiv.org/abs/2512.09016</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, lens flare, E-Deflare, restoration, benchmark<br /><br />Summary:<br /><br />1. Event cameras offer advantages like high temporal resolution and dynamic range but suffer from lens flare, an optical artifact causing significant image degradation in event data.<br /><br />2. Lens flare in event streams produces a complex spatio-temporal distortion that has not been adequately addressed in prior research.<br /><br />3. The authors introduce E-Deflare, the first comprehensive framework designed specifically to remove lens flare from event camera outputs.<br /><br />4. They derive a physics-based forward model explaining the nonlinear suppression mechanisms involved in lens flare, forming the theoretical foundation of their approach.<br /><br />5. Leveraging this foundation, the team creates the E-Deflare Benchmark, which includes a large-scale simulated training dataset called E-Flare-2.7K and the first paired real-world test set named E-Flare-R, collected using a novel optical system.<br /><br />6. Using these resources, they develop E-DeflareNet, a deep learning network that delivers state-of-the-art performance in restoring flare-corrupted event camera data.<br /><br />7. Extensive experiments confirm the effectiveness of E-DeflareNet and demonstrate significant improvements in subsequent vision tasks.<br /><br />8. The authors have made their code and datasets publicly accessible to facilitate further research and applications in this domain. <div>
arXiv:2512.09016v1 Announce Type: new 
Abstract: Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors</title>
<link>https://arxiv.org/abs/2512.09056</link>
<guid>https://arxiv.org/abs/2512.09056</guid>
<content:encoded><![CDATA[
<div> Keywords: Object pose estimation, vision-language model, zero-shot, 3D concept maps, 6DoF relative pose<br /><br />Summary:<br /><br />1. Object pose estimation is a crucial task in computer vision and robotics, but traditional methods typically require extensive training on specific datasets.  
2. ConceptPose is introduced as a novel framework that is both training-free and model-free, leveraging the zero-shot capabilities of large-scale vision-language models (VLMs).  
3. The framework creates open-vocabulary 3D concept maps by tagging each 3D point with a concept vector derived from saliency maps generated by the VLM.  
4. By establishing robust 3D-to-3D correspondences across these concept maps, ConceptPose enables precise estimation of the 6DoF (six degrees of freedom) relative pose between objects.  
5. Without any object-specific or dataset-specific training, ConceptPose surpasses existing methods on zero-shot relative pose estimation benchmarks, achieving state-of-the-art accuracy with over 62% improvement in ADD(-S) score compared to prior approaches, including those that require extensive training. <div>
arXiv:2512.09056v1 Announce Type: new 
Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding</title>
<link>https://arxiv.org/abs/2512.09062</link>
<guid>https://arxiv.org/abs/2512.09062</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, construction site, 3D perception, dataset, segmentation<br /><br />Summary:  
This paper introduces SIP (Site in Pieces), a novel dataset designed specifically for 3D scene interpretation in active construction environments. Unlike existing datasets that rely on densely fused LiDAR scans with uniform sampling and complete visibility, SIP captures the practical limitations of real-world construction site data acquisition, including isolated single-station LiDAR views, radial density decay, and fragmented geometry. The dataset encompasses both indoor and outdoor scenes, annotated at the point level using a taxonomy tailored to construction: Built Environment, Construction Operations, and Site Surroundings. SIP includes structural elements and temporary, slender objects such as scaffolding, MEP piping, and scissor lifts, which are challenging to segment due to occlusions and sparsity. The creators detail the scanning protocol, annotation workflow, and quality control processes to ensure consistency and reliability. SIP is openly accessible and supported by a Git repository offering adaptable class configurations, facilitating integration with modern 3D deep learning frameworks. By providing data that reflect realistic sensing conditions and material complexities in construction sites, SIP aims to enable more robust benchmarking and advance the development of construction-oriented 3D computer vision tasks such as progress monitoring, safety assessment, and digital twin creation. <div>
arXiv:2512.09062v1 Announce Type: new 
Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</title>
<link>https://arxiv.org/abs/2512.09069</link>
<guid>https://arxiv.org/abs/2512.09069</guid>
<content:encoded><![CDATA[
<div> Keywords: Age-related macular degeneration, Optical coherence tomography, Knowledge distillation, EfficientNet-B2, Deep learning compression<br /><br />Summary:<br /><br />1. Age-related macular degeneration (AMD) and choroidal neovascularization (CNV) are major causes of vision loss globally, with optical coherence tomography (OCT) playing a crucial role in early diagnosis and management.<br /><br />2. State-of-the-art deep learning models like ConvNeXtV2-Large offer high diagnostic accuracy but face challenges in real-time clinical deployment due to their heavy computational requirements.<br /><br />3. This study presents KD-OCT, a novel knowledge distillation framework designed to compress a high-performance ConvNeXtV2-Large teacher model into a lightweight EfficientNet-B2 student model while maintaining diagnostic performance.<br /><br />4. The teacher model is enhanced with advanced augmentations, stochastic weight averaging, and focal loss, while KD-OCT employs real-time distillation using a combined loss function balancing soft teacher knowledge transfer and hard ground-truth supervision.<br /><br />5. Evaluation on the Noor Eye Hospital (NEH) dataset with patient-level cross-validation demonstrates that KD-OCT achieves near-teacher-level accuracy with significant reductions in model size and inference time, outperforming comparable multi-scale or feature-fusion OCT classifiers.<br /><br />6. The compressed student model enables efficient edge deployment for AMD screening, overcoming previous computational bottlenecks in clinical environments.<br /><br />7. The code for KD-OCT is publicly available at https://github.com/erfan-nourbakhsh/KD-OCT. <div>
arXiv:2512.09069v1 Announce Type: new 
Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</title>
<link>https://arxiv.org/abs/2512.09071</link>
<guid>https://arxiv.org/abs/2512.09071</guid>
<content:encoded><![CDATA[
<div> Visual place recognition, threshold selection, Gaussian mixture model, image descriptors, robot navigation<br /><br />Summary:<br /><br />1. Visual Place Recognition (VPR) is crucial for camera-based mapping and navigation but is challenging due to varying conditions such as seasonal changes, illumination, structural changes, and transient objects like pedestrians or vehicles.<br /><br />2. Traditional VPR methods evaluate performance using metrics like recall@K and ROC curves, but in real-world robotic applications, deciding a suitable matching threshold is usually done manually.<br /><br />3. Manually setting a threshold is difficult because it may not generalize well across diverse visual scenarios encountered by robots.<br /><br />4. The paper proposes an automatic threshold selection approach using statistics derived from the 'negative' Gaussian mixture model, which represents image statistics from places that do not match the query location.<br /><br />5. Experimental results demonstrate that this method reliably selects effective thresholds for various image databases and descriptor types, improving robustness and practicality in real robotic VPR systems. <div>
arXiv:2512.09071v1 Announce Type: new 
Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.09081</link>
<guid>https://arxiv.org/abs/2512.09081</guid>
<content:encoded><![CDATA[
<div> Keywords: AgentComp, compositionality, text-to-image models, large language models, preference optimization  

<br /><br />Summary:  
This paper addresses the challenge of compositionality in text-to-image generative models, which struggle to accurately represent object relationships, attribute bindings, and fine-grained prompt details. The main limitation identified is that existing models are not explicitly trained to distinguish between compositionally similar prompts and images, leading to outputs that deviate subtly from intended descriptions. To overcome this, the authors introduce AgentComp, a novel framework designed to improve models' compositional reasoning abilities. AgentComp utilizes large language models equipped with image generation, editing, and visual question answering (VQA) tools to autonomously create diverse compositional datasets. These datasets enable an agentic preference optimization process during fine-tuning, helping text-to-image models better differentiate between similar compositional variations. Empirical results demonstrate that AgentComp achieves state-of-the-art performance on compositionality benchmarks like T2I-CompBench. Notably, this improved compositional understanding does not come at the cost of image quality, which has been a common issue in previous methods. Furthermore, the approach exhibits strong generalization capabilities to related tasks such as text rendering, even though these were not explicitly targeted during training, indicating the robustness of the proposed method. <div>
arXiv:2512.09081v1 Announce Type: new 
Abstract: Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters</title>
<link>https://arxiv.org/abs/2512.09092</link>
<guid>https://arxiv.org/abs/2512.09092</guid>
<content:encoded><![CDATA[
<div> Underground mining, disaster response, multimodal learning, vision-language models, image captioning<br /><br />Summary:  
This article introduces MDSE (Multimodal Disaster Situation Explainer), a novel vision-language framework designed to generate detailed textual descriptions of post-disaster underground mining environments. The framework addresses challenges such as darkness, dust, and collapses that impair visual clarity and situational awareness in underground mining disasters. MDSE incorporates three key innovations: (i) Context-Aware Cross-Attention that ensures robust alignment between visual and textual features despite severe image degradation; (ii) a Segmentation-aware dual pathway visual encoder that merges global and region-specific image embeddings to enhance understanding of critical scene regions; and (iii) a Resource-Efficient Transformer-Based Language Model that produces expressive captions with reduced computational cost. To facilitate the development and evaluation of the system, the authors introduce the Underground Mine Disaster (UMD) dataset, the first image-caption corpus featuring real underground disaster scenes. Experimental results on the UMD dataset and other related benchmarks demonstrate that MDSE markedly outperforms existing state-of-the-art image captioning models by generating more accurate, detailed, and contextually relevant descriptions. These improvements have the potential to significantly enhance situational awareness for underground emergency responders, supporting safer and more effective disaster response efforts. The source code is publicly available on GitHub. <div>
arXiv:2512.09092v1 Announce Type: new 
Abstract: Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Food Image Generation on Multi-Noun Categories</title>
<link>https://arxiv.org/abs/2512.09095</link>
<guid>https://arxiv.org/abs/2512.09095</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-noun food categories, image generation, text encoder, FoCULR, spatial layout refinement  

<br /><br />Summary: Generating realistic images for food categories with compound names involving multiple nouns is a difficult task because conventional generative models often misinterpret these multi-noun prompts. For example, the phrase "egg noodle" might lead to images showing distinct eggs and noodles separately, which is semantically incorrect. Such multi-noun categories are prevalent in real-world food datasets and benchmarks like UEC-256, making this issue widely relevant. The core problems arise from insufficient domain-specific knowledge in the text encoders and the inability to correctly understand or represent the relationships between nouns, resulting in flawed spatial arrangements of ingredients or objects. To address these challenges, the authors propose a new approach named FoCULR (Food Category Understanding and Layout Refinement). FoCULR integrates explicit food domain knowledge into the generation pipeline and introduces key food concepts early in the image synthesis process. This strategy helps the model better comprehend multi-noun food categories and generate more accurate images with proper spatial layouts. Experiments indicate that incorporating FoCULR significantly enhances the quality and semantic correctness of generated food images, demonstrating the effectiveness of domain knowledge infusion and layout refinement in food image synthesis tasks. <div>
arXiv:2512.09095v1 Announce Type: new 
Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GimbalDiffusion: Gravity-Aware Camera Control for Video Generation</title>
<link>https://arxiv.org/abs/2512.09112</link>
<guid>https://arxiv.org/abs/2512.09112</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video generation, camera control, GimbalDiffusion, gravity-aligned coordinates, null-pitch conditioning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of fine-grained control over camera motion and orientation in text-to-video generation, which has been limited by relative or ambiguous camera trajectory representations in existing methods. 2. GimbalDiffusion is introduced as a novel framework that grounds camera control in absolute physical-world coordinates, using gravity as a universal reference, enabling precise and interpretable manipulation of camera parameters. 3. Unlike prior approaches reliant on relative frame-to-frame motion, this method defines camera trajectories in an absolute coordinate system, removing the need for an initial reference frame. 4. The authors utilize panoramic 360-degree videos to create diverse camera trajectories that go beyond the common straight, forward-facing paths typical in standard video datasets. 5. To improve camera guidance, a null-pitch conditioning strategy is proposed to reduce conflicts between text-driven content and camera orientation (e.g., preventing unrealistic content like grass appearing when the camera points to the sky). 6. A new benchmark is established by rebalancing the SpatialVID-HQ dataset to evaluate text-to-video models under wide variations in camera pitch, enhancing assessment of camera-aware generation. Together, these contributions significantly improve the controllability and robustness of generative text-to-video frameworks by enabling precise, gravity-aligned camera manipulation. <div>
arXiv:2512.09112v1 Announce Type: new 
Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperF: Neural Implicit Fields for Multi-Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.09115</link>
<guid>https://arxiv.org/abs/2512.09115</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-image super-resolution, coordinate-based neural networks, implicit neural representation, sub-pixel alignment, test-time optimization<br /><br />Summary:  
1. The paper addresses the challenge of enhancing image resolution in scenarios affected by sensor limitations, atmospheric conditions, and cost, such as satellite remote sensing and smartphone cameras.  
2. Traditional single-image super-resolution methods rely on learned priors or auxiliary high-resolution guides but often introduce unrealistic "hallucinated" details.  
3. Multi-image super-resolution (MISR) improves resolution by leveraging multiple low-resolution images captured with sub-pixel shifts, offering a more physically grounded reconstruction.  
4. The authors propose SuperF, a novel MISR technique using coordinate-based neural networks (implicit neural representations) to represent continuous signals and optimize the reconstruction.  
5. SuperF uniquely shares a single implicit neural representation across multiple shifted frames and simultaneously optimizes frame alignment via parameterized affine transformations at sub-pixel precision.  
6. Optimization occurs through a super-sampled coordinate grid that matches the target high-resolution output, enhancing reconstruction fidelity.  
7. Experiments on simulated satellite bursts and handheld camera images demonstrate effective upsampling up to 8× without requiring any high-resolution training data, marking a significant advantage over learning-based approaches. <div>
arXiv:2512.09115v1 Announce Type: new 
Abstract: High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to "hallucinated" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task.
  The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation</title>
<link>https://arxiv.org/abs/2512.09134</link>
<guid>https://arxiv.org/abs/2512.09134</guid>
<content:encoded><![CDATA[
<div> Keywords: coronary angiography, fractional flow reserve, deep learning, quantitative flow ratio, virtual stenting<br /><br />Summary:<br /><br />1. Coronary angiography is the primary method for assessing coronary artery disease, but visual assessment of stenosis varies widely and correlates only moderately with ischemia.<br />2. Wire-based fractional flow reserve (FFR) improves lesion assessment but is not routinely used in clinical practice.<br />3. Angiography-derived indices such as quantitative flow ratio (QFR) provide a wire-free physiological evaluation, but existing tools are often workflow-intensive and lack integration with automated anatomical analysis and virtual percutaneous coronary intervention (PCI) planning.<br />4. The authors developed AngioAI-QFR, an end-to-end angiography-only pipeline combining deep learning-based stenosis detection, lumen segmentation, centerline and diameter extraction, per millimeter relative flow capacity (RFC) profiling, and virtual stenting with automatic recalculation of angiography-derived QFR.<br />5. When evaluated in 100 consecutive vessels against invasive FFR, AngioAI-QFR demonstrated excellent stenosis detection (precision 0.97), lumen segmentation (Dice 0.78), and strong correlation with FFR (r = 0.89, mean absolute error 0.045).<br />6. The diagnostic performance for detecting FFR ≤ 0.80 was high, with an area under the curve (AUC) of 0.93, sensitivity 0.88, and specificity 0.86.<br />7. The pipeline ran fully automatically in 93% of vessels, with a median processing time of 41 seconds.<br />8. RFC profiling enabled differentiation between focal and diffuse capacity loss, and virtual stenting predicted greater QFR improvement in focal lesions compared to diffuse disease.<br />9. Overall, AngioAI-QFR offers a practical, near real-time, integrated solution combining computer vision, functional flow profiling, and virtual PCI planning from angiographic images alone. <div>
arXiv:2512.09134v1 Announce Type: new 
Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars</title>
<link>https://arxiv.org/abs/2512.09162</link>
<guid>https://arxiv.org/abs/2512.09162</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, head avatars, UV texture mapping, relighting, monocular video<br /><br />Summary:  
This work presents a novel approach to photorealistic head avatar reconstruction by combining the precise accuracy of 2D Gaussian Splatting with the user-friendly properties of UV texture mapping. The proposed method embeds each canonical Gaussian primitive's local frame into UV space patches of a template mesh, allowing for continuous and editable material textures. Unlike traditional mesh-based methods, this approach enables intuitive editing without sacrificing fidelity. Importantly, reconstruction is performed from a single monocular video, making the process more accessible with conventional equipment. The method also integrates an efficient physically based reflectance model that supports advanced relighting and editing of intrinsic material maps, enhancing visual realism and flexibility. Extensive experimental comparisons with state-of-the-art techniques validate the accuracy and quality of the reconstructions and relighting results. Additionally, the approach provides intuitive controls for modifying an avatar's appearance and geometry through texture mapping, all achieved without requiring further optimization steps. This combination paves the way for practical applications in visual effects, videoconferencing, and virtual reality by overcoming previous limitations related to editability and ease of use. <div>
arXiv:2512.09162v1 Announce Type: new 
Abstract: Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WonderZoom: Multi-Scale 3D World Generation</title>
<link>https://arxiv.org/abs/2512.09164</link>
<guid>https://arxiv.org/abs/2512.09164</guid>
<content:encoded><![CDATA[
<div> Keywords: WonderZoom, multi-scale 3D scenes, scale-adaptive Gaussian surfels, progressive detail synthesizer, single image 3D generation<br /><br />Summary:<br /><br />1. WonderZoom is a novel method designed to generate 3D scenes with content spanning multiple spatial scales, all derived from a single input image. 2. Unlike existing 3D world generation models that are restricted to single-scale synthesis, WonderZoom enables coherent scene synthesis across varying levels of granularity. 3. The core challenge tackled by this approach is the absence of a scale-aware 3D representation capable of both generating and rendering extremely diverse spatial sizes within the same scene. 4. Two major innovations are introduced: first, the use of scale-adaptive Gaussian surfels that allow real-time generation and rendering of multi-scale 3D scenes, and second, a progressive detail synthesizer that iteratively adds finer-scale 3D content in a zoom-in manner. 5. This framework enables users to zoom into any 3D region and synthesize previously non-existent detailed features, ranging from broad landscapes down to microscopic elements. 6. Experimental results demonstrate that WonderZoom surpasses state-of-the-art video and 3D generation models in terms of output quality and spatial alignment. 7. The authors provide video demonstrations and an interactive online viewer showcasing the multi-scale 3D worlds generated by WonderZoom at https://wonderzoom.github.io/. <div>
arXiv:2512.09164v1 Announce Type: new 
Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Compositional Zero-Shot Learning, vision-language models, prompt-based learning, knowledge retention, multimodal fusion

<br /><br />Summary: This paper addresses the challenge of Continual Compositional Zero-Shot Learning (CCZSL), focusing on adapting vision-language models (VLMs) to recognize new attributes, objects, and their unique compositions, while avoiding forgetting previously learned knowledge. Unlike classical continual learning where classes are distinct and non-overlapping, CCZSL is more complex because attributes and objects may reoccur across learning sessions, though their compositions remain unique. The authors propose PromptCCZSL, a prompt-based framework built on a frozen VLM backbone that leverages recency-weighted multi-teacher distillation to retain prior knowledge effectively. The method uses session-aware compositional prompts to integrate multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain consistent global semantics. The Cosine Anchor Loss (CAL) further stabilizes knowledge retention. To improve adaptation during the current session, two additional losses are introduced: Orthogonal Projection Loss (OPL) to keep new attribute and object embeddings distinct from previous ones, and Intra-Session Diversity Loss (IDL) to encourage variety within current session embeddings for more discriminative representations. A new evaluation protocol is also introduced to jointly measure catastrophic forgetting and compositional generalization. Experiments on UT-Zappos and C-GQA datasets demonstrate that PromptCCZSL significantly outperforms previous VLM-based and non-VLM methods, setting a new benchmark for CCZSL in closed-world settings. <div>
arXiv:2512.09172v1 Announce Type: new 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation</title>
<link>https://arxiv.org/abs/2512.09185</link>
<guid>https://arxiv.org/abs/2512.09185</guid>
<content:encoded><![CDATA[
<div> Keywords: disease progression, flow matching, latent alignment, auto-encoders, MRI benchmarks<br /><br />Summary:<br /><br />1. The study addresses the challenge of understanding disease progression, which is crucial for early diagnosis and personalized treatment strategies. 2. Traditional generative models fail to capture the continuous and monotonic nature of disease dynamics, often producing latent representations that lack semantic coherence, while diffusion-based models disrupt continuity through random denoising. 3. The authors propose treating disease dynamics as a velocity field and apply Flow Matching (FM) to better align the temporal evolution of patient data, making the progression more interpretable compared to prior methods. 4. A key limitation addressed is the misalignment of latent spaces produced by Auto-Encoders, which neither ensure consistency across patients nor correlate well with clinical severity indicators like age or disease conditions. 5. To overcome this, the framework introduces patient-specific latent alignment that constrains patient trajectories along a defined axis, with progression magnitude increasing monotonically with disease severity, resulting in a consistent and semantically meaningful latent space. 6. The framework, named Δ-LFM, was evaluated on three longitudinal MRI benchmarks, showing strong empirical results and providing a novel approach for interpreting and visualizing disease dynamics over time. <div>
arXiv:2512.09185v1 Announce Type: new 
Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $\Delta$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $\Delta$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs</title>
<link>https://arxiv.org/abs/2512.09215</link>
<guid>https://arxiv.org/abs/2512.09215</guid>
<content:encoded><![CDATA[
<div> 3D visual grounding, zero-shot, vision-language models, scene graph, spatial reasoning<br /><br />Summary:<br /><br />1. The paper addresses 3D visual grounding (3DVG), which involves identifying objects in 3D scenes based on language descriptions.  
2. Existing zero-shot 3DVG approaches rely on 2D vision-language models (VLMs) by converting 3D spatial information into composite inputs like view renderings or videos, which creates entangled visual representations that complicate reasoning.  
3. The authors propose a new paradigm, VLM x SI, which externalizes 3D spatial information into a structured format, allowing the VLM to selectively retrieve relevant cues incrementally rather than processing all information at once.  
4. They introduce the View-on-Graph (VoG) method that organizes the 3D scene into a multi-modal, multi-layer scene graph, enabling the VLM to explore and reason over the scene graph actively and transparently.  
5. This structured approach reduces reasoning difficulty for the VLM and produces interpretable step-by-step reasoning traces.  
6. Extensive experiments demonstrate that VoG achieves state-of-the-art zero-shot performance in 3D visual grounding, validating structured scene exploration as an effective strategy in this domain. <div>
arXiv:2512.09215v1 Announce Type: new 
Abstract: 3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Next-Generation Consumer Experience with Feature Coding for Machines</title>
<link>https://arxiv.org/abs/2512.09232</link>
<guid>https://arxiv.org/abs/2512.09232</guid>
<content:encoded><![CDATA[
<div> Feature Coding for Machines, MPEG-AI, Neural Network Compression, Edge Computing, Remote Inference Efficiency<br /><br />Summary: This paper introduces the Feature Coding for Machines (FCM) standard, a recent development by the Moving Picture Experts Group (MPEG) under the MPEG-AI framework, designed to optimize data transfer for AI-driven applications. FCM enables the efficient extraction, compression, and transmission of intermediate neural network features, which facilitates the offloading of complex computational tasks from low-powered consumer devices to high-capacity base servers. This division of labor allows devices with limited resources to benefit from large deep learning models without the typical computational overhead. The study provides an overview of FCM’s methodology and implementation, emphasizing its role in supporting increasingly intelligent and interconnected consumer devices. Experimental results demonstrate that the FCM standard achieves comparable accuracy levels to traditional remote inference approaches while significantly reducing bitrate requirements by approximately 75.90%. These advances highlight the potential of FCM to enhance the efficiency of machine-to-machine communication in various AI applications, reducing latency and bandwidth consumption, and thereby supporting more scalable and practical deployment of deep learning models in resource-constrained environments. Overall, FCM represents a promising step forward in efficient neural feature coding for machine tasks at the network edge. <div>
arXiv:2512.09232v1 Announce Type: new 
Abstract: As consumer devices become increasingly intelligent and interconnected, efficient data transfer solutions for machine tasks have become essential. This paper presents an overview of the latest Feature Coding for Machines (FCM) standard, part of MPEG-AI and developed by the Moving Picture Experts Group (MPEG). FCM supports AI-driven applications by enabling the efficient extraction, compression, and transmission of intermediate neural network features. By offloading computationally intensive operations to base servers with high computing resources, FCM allows low-powered devices to leverage large deep learning models. Experimental results indicate that the FCM standard maintains the same level of accuracy while reducing bitrate requirements by 75.90% compared to remote inference.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Feature Compression for Machines with Global Statistics Preservation</title>
<link>https://arxiv.org/abs/2512.09235</link>
<guid>https://arxiv.org/abs/2512.09235</guid>
<content:encoded><![CDATA[
<div> Keywords: split-inference, feature compression, Z-score normalization, Feature Coding for Machines (FCM), bitrate reduction

<br /><br />Summary:  
The paper addresses the challenge of compressing intermediate feature data in split-inference AI models, where an AI model is divided into two parts requiring data transfer between them. The authors propose using Z-score normalization to enhance the recovery of compressed feature data at the decoder side, aiming to improve compression efficiency. This method is integrated into the emerging MPEG Feature Coding for Machines (FCM) codec standard, replacing the existing scaling method used in the current development phase. The proposed approach reduces overhead bits involved in transmission and simultaneously improves end-task accuracy, making it more effective than previous methods. To accommodate different scenarios and further reduce overhead, a simplified variant of the method is also introduced. Experimental results demonstrate that the new method achieves an average bitrate reduction of 17.09% across multiple AI tasks without loss of task accuracy. For specific tasks like object tracking, the bitrate reduction can be as high as 65.69%. This indicates significant compression improvements that do not compromise the performance of AI models relying on compressed intermediate feature data transfer. <div>
arXiv:2512.09235v1 Announce Type: new 
Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI</title>
<link>https://arxiv.org/abs/2512.09244</link>
<guid>https://arxiv.org/abs/2512.09244</guid>
<content:encoded><![CDATA[
<div> Chronic Kidney Disease, Deep Convolutional Neural Network, CT Kidney Images, SMOTE, Grad-CAM  

<br /><br />Summary:  
This study addresses the global health challenge posed by Chronic Kidney Disease (CKD), characterized by gradual renal function decline and systemic fluid imbalance. It emphasizes the need for early and reliable diagnostic methods to reduce morbidity and mortality associated with CKD. The authors introduce a deep convolutional neural network (CNN) framework designed specifically for early detection of CKD using CT kidney images. To overcome class imbalance inherent in medical imaging data, the Synthetic Minority Over-sampling Technique (SMOTE) was applied, enhancing the model's ability to generalize across minority classes. The dataset used comprised 12,446 CT images categorized into cyst, normal, stone, and tumor classes, ensuring a robust and comprehensive evaluation. Interpretability of the CNN was facilitated through Gradient-weighted Class Activation Mapping (Grad-CAM), enabling clinical practitioners to visually assess and verify the areas influencing the model’s decisions. The developed CNN demonstrated exceptional performance, achieving 100% accuracy in CKD detection. This highlights the model’s potential as a powerful tool for early CKD diagnosis, promising improvements in clinical workflows and patient outcomes by enabling timely interventions and appropriate medical management. <div>
arXiv:2512.09244v1 Announce Type: new 
Abstract: Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniPSD: Layered PSD Generation with Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.09247</link>
<guid>https://arxiv.org/abs/2512.09247</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, PSD generation, transparency, in-context learning, RGBA-VAE<br /><br />Summary: Recent progress in diffusion models has significantly enhanced image generation and editing, but generating or reconstructing layered PSD files with transparent alpha channels remains a complex challenge. This paper introduces OmniPSD, a unified diffusion framework developed within the Flux ecosystem that supports both text-to-PSD generation and image-to-PSD decomposition via in-context learning. For text-to-PSD generation, OmniPSD spatially arranges multiple target layers into a single canvas and leverages spatial attention to learn their compositional relationships, resulting in semantically coherent and hierarchically organized layers. In terms of image-to-PSD decomposition, the method employs iterative in-context editing that progressively extracts and erases textual and foreground elements, enabling the reconstruction of editable PSD layers from a single flattened image. A novel RGBA-VAE auxiliary module is integrated to maintain transparency information without compromising structural learning. Extensive experiments conducted on a newly introduced RGBA-layered dataset demonstrate that OmniPSD can achieve high fidelity in generation, structural consistency across layers, and strong transparency awareness. Overall, OmniPSD represents a new paradigm for layered design generation and decomposition, employing diffusion transformers to effectively handle complex PSD files with transparency and multi-layered content. <div>
arXiv:2512.09247v1 Announce Type: new 
Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.09251</link>
<guid>https://arxiv.org/abs/2512.09251</guid>
<content:encoded><![CDATA[
<div> Keywords: Glacial lake monitoring, segmentation, large language models, spatial reasoning, remote sensing  

<br /><br />Summary:  
1. The paper addresses the critical issue of glacial lake monitoring to reduce risks associated with Glacial Lake Outburst Floods (GLOFs).  
2. Existing segmentation techniques using CNNs and Vision Transformers (ViTs) are limited to pixel-level predictions and lack global context and human-interpretable reasoning.  
3. The authors propose GLACIA, a novel framework combining large language models with segmentation to produce both accurate masks and spatial reasoning outputs, enhancing interpretability.  
4. They introduce the GLake-Pos dataset pipeline, offering diverse, spatially grounded question-answer pairs to enable instance-aware positional reasoning, which was previously missing in remote sensing datasets.  
5. Comparative experiments demonstrate GLACIA's superiority with a mean Intersection over Union (mIoU) of 87.30, outperforming state-of-the-art CNN, ViT, geo-foundation, and reasoning-based segmentation methods.  
6. The integrated natural language interaction functionality allows for intuitive disaster preparedness and supports policy-making in rapidly changing glacial environments, making decision-making more efficient and interpretable.  
7. The code and resources are publicly available at the provided GitHub repository, promoting further research and application development in this area. <div>
arXiv:2512.09251v1 Announce Type: new 
Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROI-Packing: Efficient Region-Based Compression for Machine Vision</title>
<link>https://arxiv.org/abs/2512.09258</link>
<guid>https://arxiv.org/abs/2512.09258</guid>
<content:encoded><![CDATA[
<div> ROI Packing, image compression, machine vision, object detection, instance segmentation<br /><br />Summary:<br /><br />This paper presents ROI-Packing, an innovative image compression technique designed specifically for machine vision applications. The method focuses on identifying and prioritizing regions of interest (ROI) that are essential for maintaining end-task accuracy, such as object detection and instance segmentation, while efficiently packing these regions and discarding less relevant image data. Unlike prior approaches, ROI-Packing does not require retraining or fine-tuning the end-task models, making it readily deployable. The authors evaluate the method comprehensively across five different datasets and two common vision tasks. Results demonstrate that ROI-Packing can reduce the bitrate by up to 44.10% without any loss in accuracy on the tasks. Furthermore, when compared to the state-of-the-art Versatile Video Coding (VVC) codec, which is standardized by the Moving Picture Experts Group (MPEG), ROI-Packing achieves an 8.88% improvement in accuracy at equivalent bitrates. This work highlights the potential of targeted compression strategies that focus on task-relevant image content to enhance both compression efficiency and machine vision performance. <div>
arXiv:2512.09258v1 Announce Type: new 
Abstract: This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification</title>
<link>https://arxiv.org/abs/2512.09270</link>
<guid>https://arxiv.org/abs/2512.09270</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D Gaussian Splatting, long-range dynamic scenes, Anchor Relay-based Bidirectional Blending, temporal coherence, hierarchical densification<br /><br />Summary: Recent progress in 4D Gaussian Splatting (4DGS) extends the capabilities of 3D Gaussian Splatting by enabling real-time rendering of dynamic scenes through temporal modeling. However, modeling long-range motion dynamics poses challenges such as memory explosion, temporal flickering, and occlusion handling failures when using naive extensions. To overcome these, the paper introduces MoRel, a novel 4DGS framework that incorporates an Anchor Relay-based Bidirectional Blending (ARBB) mechanism. This method progressively constructs locally canonical anchor spaces at key-frame indices and models inter-frame deformations at the anchor level to enhance temporal consistency. MoRel learns bidirectional deformations between key-frame anchors and blends them adaptively with learnable opacity control, effectively reducing temporal discontinuities and flickering. Additionally, a Feature-variance-guided Hierarchical Densification (FHD) scheme is proposed to densify key-frame anchors selectively based on feature variance, balancing rendering quality and efficiency. To validate the approach, the authors present SelfCap\(_{\text{LR}}\), a new dataset featuring long-range 4D dynamic motions captured over wider spatial domains and larger motion magnitudes than existing datasets. MoRel achieves temporally coherent, flicker-free long-range 4D reconstructions while maintaining bounded memory usage, demonstrating scalability and efficiency in dynamic Gaussian-based scene representations. <div>
arXiv:2512.09270v1 Announce Type: new 
Abstract: Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations</title>
<link>https://arxiv.org/abs/2512.09271</link>
<guid>https://arxiv.org/abs/2512.09271</guid>
<content:encoded><![CDATA[
<div> Keywords: Long Text-to-Image, Alignment Evaluation, Graph-structured Annotations, Multi-modal Large Language Models, Chain-of-Thought

<br /><br />Summary:  
The paper addresses the challenge of evaluating image-text alignment in long Text-to-Image (T2I) generation scenarios, where current benchmarks largely cater to short prompts and offer only coarse-grained MOS or Likert scale annotations. To overcome this, the authors introduce LongT2IBench, a dataset consisting of 14,000 long text-image pairs enriched with detailed graph-structured human annotations that capture entities, attributes, and relations within prompts. They propose a Generate-Refine-Qualify protocol to convert complex long prompts into interpretable textual graph structures, enabling fine-grained alignment assessment. These graph annotations are then transformed into alignment scores and interpretations to support the development of more precise T2I evaluation models. Building on this dataset, the authors develop LongT2IExpert, an evaluator leveraging multi-modal large language models (MLLMs) fine-tuned via an instruction-tuning process that incorporates a Hierarchical Alignment Chain-of-Thought (CoT). This approach allows LongT2IExpert to deliver both quantitative alignment scores and rich structured interpretations. Extensive experiments demonstrate that LongT2IExpert outperforms existing methods in both evaluating and interpreting long T2I alignment. The dataset and code have been made publicly available, promoting further research in this domain. <div>
arXiv:2512.09271v1 Announce Type: new 
Abstract: The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis</title>
<link>https://arxiv.org/abs/2512.09276</link>
<guid>https://arxiv.org/abs/2512.09276</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's disease, hypomimia, facial expression analysis, CLIP architecture, LSTM classification<br /><br />Summary:  
Parkinson's disease (PD) is a common neurodegenerative disorder that impairs patients' daily life and social interactions. This study introduces a novel auxiliary diagnostic method for PD based on dynamic facial expression analysis. The focus is on hypomimia, a distinctive clinical symptom characterized by reduced facial expressivity and facial rigidity. The method captures these two manifestations by analyzing patients' facial expressions during various emotional performances. A multimodal facial expression analysis network is developed, which employs the CLIP architecture to effectively combine visual and textual features while maintaining the temporal dynamics of facial expressions. The extracted expression intensity features are then fed into an LSTM-based classification network designed for PD diagnosis. Experimental results demonstrate that this approach achieves a high diagnostic accuracy of 93.1%, surpassing the performance of other existing in-vitro PD diagnostic methods. The proposed technique provides a more accessible and convenient option for early screening and diagnosis of PD, potentially enhancing the overall diagnostic experience for patients by offering a less intrusive and faster detection method. <div>
arXiv:2512.09276v1 Announce Type: new 
Abstract: Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoGoColor: Local-Global 3D Colorization for 360{\deg} Scenes</title>
<link>https://arxiv.org/abs/2512.09278</link>
<guid>https://arxiv.org/abs/2512.09278</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D colorization, multi-view consistency, color diversity, diffusion model, 360° scenes

<br /><br />Summary: The paper addresses the challenge of 3D colorization in single-channel 3D reconstruction, which is essential for visualization in robotics and medical imaging. Traditional methods rely on 2D image colorization models but suffer from an inherent inconsistency that causes colors to be averaged during training, producing monotonous and oversimplified color results, especially in complex 360° scenes. To overcome this, the authors propose LoGoColor, a novel pipeline that preserves color diversity by generating a new set of consistently colorized training views, thus bypassing the averaging process. The main challenge with this approach is maintaining strict multi-view consistency across all colorized views. LoGoColor tackles this by adopting a Local-Global strategy, partitioning the scene into subscenes and addressing both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. Experimental results demonstrate that LoGoColor achieves more consistent and plausible 3D colorization in complex environments compared to existing methods. Additionally, the study introduces a new metric, the Color Diversity Index, to quantitatively validate the superior color diversity provided by their approach. <div>
arXiv:2512.09278v1 Announce Type: new 
Abstract: Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360{\deg} scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360{\deg} scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model</title>
<link>https://arxiv.org/abs/2512.09282</link>
<guid>https://arxiv.org/abs/2512.09282</guid>
<content:encoded><![CDATA[
<div> Keywords: image restoration, foundation model, data mixture proportions, diffusion-based model, Mixture-of-Experts (MoE)

<br /><br />Summary:  
This paper presents FoundIR-v2, a high-capacity diffusion-based image restoration foundation model that addresses the challenge of balancing diverse restoration tasks in training data. The authors highlight the critical role of data mixture proportions from different restoration tasks in determining the overall performance of all-in-one image restoration models. To optimize this, they introduce a data equilibrium scheduling paradigm that dynamically adjusts the mixture proportions of datasets from various tasks during training, ensuring balanced dataset composition. This approach leverages the data mixing law to achieve consistent generalization and comprehensive performance across tasks. Furthermore, the model incorporates a Mixture-of-Experts (MoE)-driven scheduler in generative pre-training, which allocates task-adaptive diffusion priors for each restoration task. This ensures that the model can handle the distinct degradation types and severity levels specific to each task effectively. Extensive experiments show that FoundIR-v2 can successfully address over 50 sub-tasks across a wide range of real-world image restoration scenarios and delivers competitive performance compared to state-of-the-art methods, demonstrating its flexibility and robustness in multi-task image restoration. <div>
arXiv:2512.09282v1 Announce Type: new 
Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MelanomaNet: Explainable Deep Learning for Skin Lesion Classification</title>
<link>https://arxiv.org/abs/2512.09289</link>
<guid>https://arxiv.org/abs/2512.09289</guid>
<content:encoded><![CDATA[
<div> Keywords: skin lesion classification, deep learning, interpretability, uncertainty quantification, melanoma

<br /><br />Summary:  
This article presents MelanomaNet, an explainable deep learning system designed for multi-class skin lesion classification, addressing the challenge of model interpretability that limits clinical adoption. The system integrates four key interpretability mechanisms: an EfficientNet V2 backbone for feature extraction, GradCAM++ for attention visualization, automated extraction of the ABCDE clinical criteria (Asymmetry, Border, Color, Diameter, Evolution), and Fast Concept Activation Vectors (FastCAV) for concept-based explanations. Additionally, MelanomaNet incorporates Monte Carlo Dropout to quantify uncertainty, separating epistemic (model) and aleatoric (data) uncertainties, which helps flag unreliable predictions for further clinical review. The model was evaluated on a large-scale dataset, ISIC 2019, containing 25,331 dermoscopic images classified into nine diagnostic categories. MelanomaNet achieved an accuracy of 85.61% and a weighted F1 score of 0.8564, demonstrating strong classification performance. Importantly, the interpretability features ensure that the model's attention aligns with established dermatological criteria, making the explanations clinically meaningful. The combination of high predictive performance, interpretability, and uncertainty estimation aims to increase trust and adoption of deep learning tools in dermatology workflows. The authors have made the source code publicly accessible for further research and clinical application development. <div>
arXiv:2512.09289v1 Announce Type: new 
Abstract: Automated skin lesion classification using deep learning has shown remarkable accuracy, yet clinical adoption remains limited due to the "black box" nature of these models. We present MelanomaNet, an explainable deep learning system for multi-class skin lesion classification that addresses this gap through four complementary interpretability mechanisms. Our approach combines an EfficientNet V2 backbone with GradCAM++ attention visualization, automated ABCDE clinical criterion extraction, Fast Concept Activation Vectors (FastCAV) for concept-based explanations, and Monte Carlo Dropout uncertainty quantification. We evaluate our system on the ISIC 2019 dataset containing 25,331 dermoscopic images across 9 diagnostic categories. Our model achieves 85.61% accuracy with a weighted F1 score of 0.8564, while providing clinically meaningful explanations that align model attention with established dermatological assessment criteria. The uncertainty quantification module decomposes prediction confidence into epistemic and aleatoric components, enabling automatic flagging of unreliable predictions for clinical review. Our results demonstrate that high classification performance can be achieved alongside comprehensive interpretability, potentially facilitating greater trust and adoption in clinical dermatology workflows. The source code is available at https://github.com/suxrobgm/explainable-melanoma
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.09296</link>
<guid>https://arxiv.org/abs/2512.09296</guid>
<content:encoded><![CDATA[
<div> Small Target Recognition, YOLOv8n-SPTS, Feature Extraction, Multi-scale Fusion, Triple-Stage Feature Pyramid  

<br /><br />Summary:  
This paper addresses the challenge of small target recognition in autonomous driving, focusing on improving detection performance under issues like missing small target information, scale imbalance, and occlusion. First, the authors optimize the feature extraction module by replacing four traditional convolution modules in the YOLOv8n backbone bottleneck structure with Space-to-Depth Convolution (SPD-Conv) modules, which preserve fine-grained spatial details and improve feature capturing for low-resolution small targets. Second, to enhance feature fusion, they introduce the Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module, which combines multi-scale feature extraction from Spatial Pyramid Pooling with the Cross Stage Partial Connection mechanism, boosting contextual understanding and multi-scale feature expression. Third, the model design is tailored for small target detection by proposing a Triple-Stage Feature Pyramid (TSFP) that adds a 160×160 resolution detection head dedicated to small targets while removing some large target heads to maintain computational balance. Evaluations on the VisDrone2019-DET dataset demonstrate that YOLOv8n-SPTS leads with precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Lastly, visualization confirms that the model notably reduces miss rates for small, occluded, and densely packed objects such as pedestrians and bicycles. <div>
arXiv:2512.09296v1 Announce Type: new 
Abstract: This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VABench: A Comprehensive Benchmark for Audio-Video Generation</title>
<link>https://arxiv.org/abs/2512.09299</link>
<guid>https://arxiv.org/abs/2512.09299</guid>
<content:encoded><![CDATA[
<div> audio-video synchronization, video generation benchmark, multi-dimensional evaluation, text-to-audio-video, lip-speech consistency

<br /><br />Summary:<br /><br />This paper introduces VABench, a novel benchmark framework designed to evaluate synchronous audio-video generation models comprehensively. The benchmark addresses the current lack of convincing evaluation metrics for synchronized audio and video outputs in existing video generation benchmarks. VABench covers three main task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It incorporates two major evaluation modules spanning 15 dimensions, which assess pairwise similarities such as text-video, text-audio, and video-audio, as well as critical aspects like audio-video synchronization and lip-speech consistency. Additionally, the framework includes carefully curated audio and video question-answering (QA) pairs to enhance the evaluation depth. The benchmark also spans seven major content categories, including animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds, ensuring diverse and comprehensive testing scenarios. Alongside proposing the benchmark, the authors provide systematic analysis and visualization of evaluation results, setting a new standard for assessing video generation models with synchronous audio capabilities. VABench aims to stimulate more thorough and multi-dimensional progress within the field of synchronous audio-video generation. <div>
arXiv:2512.09299v1 Announce Type: new 
Abstract: Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation</title>
<link>https://arxiv.org/abs/2512.09307</link>
<guid>https://arxiv.org/abs/2512.09307</guid>
<content:encoded><![CDATA[
<div> Polyp segmentation, colonoscopy, foundation models, knowledge distillation, lightweight models<br /><br />Summary:<br /><br />Accurate polyp segmentation in colonoscopy is crucial for early colorectal cancer detection but remains challenging due to variations in polyp size, shape, color, and their camouflaged appearance. Lightweight segmentation models like U-Net, U-Net++, and PraNet are computationally efficient and easy to deploy yet struggle to handle these challenges effectively. Large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former show strong generalization in natural images but are difficult to apply directly to medical imaging due to limited relevant data and domain-specific complexities. To address this, the authors propose Polyp-DiFoM, a novel distillation framework that transfers semantic knowledge from these foundation models into lightweight baseline architectures, improving accuracy while maintaining efficiency. Key innovations include integrating semantic priors from foundation models into canonical models like U-Net and U-Net++, and introducing frequency domain encoding to enhance distillation effectiveness. Experiments on five benchmark polyp segmentation datasets—Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300—demonstrate that Polyp-DiFoM significantly outperforms both baseline and state-of-the-art methods with about nine times less computational cost. The framework thus offers an effective solution for accurate, real-time polyp segmentation suitable for clinical implementation. The code is publicly available on GitHub. <div>
arXiv:2512.09307v1 Announce Type: new 
Abstract: Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance</title>
<link>https://arxiv.org/abs/2512.09311</link>
<guid>https://arxiv.org/abs/2512.09311</guid>
<content:encoded><![CDATA[
<div> Keywords: Suspiciousness estimation, USE50k dataset, DeepUSEvision, YOLOv12, multimodal fusion<br /><br />Summary:<br /><br />1. This work addresses the crucial task of suspiciousness estimation, which is vital for proactive threat detection and ensuring public safety in complex, uncontrolled environments.<br /><br />2. The authors introduce USE50k, a large-scale annotated dataset comprising 65,500 images collected from diverse public settings such as airports, railway stations, restaurants, and parks. The dataset captures various suspicious cues, including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures.<br /><br />3. Building upon this dataset, the paper presents DeepUSEvision, a computationally efficient and modular vision-based framework designed for real-time suspiciousness analysis.<br /><br />4. DeepUSEvision integrates three core components: an enhanced YOLOv12-based Suspicious Object Detector, dual deep convolutional neural networks (DCNN-I and DCNN-II) focused on facial expression and body language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses these multimodal outputs.<br /><br />5. Extensive experimental evaluations demonstrate that DeepUSEvision outperforms state-of-the-art methods in accuracy, robustness, and interpretability, providing an effective and scalable solution for intelligent surveillance and real-time risk assessment in safety-critical scenarios. <div>
arXiv:2512.09311v1 Announce Type: new 
Abstract: Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook</title>
<link>https://arxiv.org/abs/2512.09315</link>
<guid>https://arxiv.org/abs/2512.09315</guid>
<content:encoded><![CDATA[
<div> Label noise, medical imaging, robustness, benchmark, noisy labels<br /><br />Summary:<br /><br />Learning from noisy labels poses significant challenges in medical image analysis due to the expertise required for annotation and substantial inter-observer variability, which often causes inconsistent or erroneous labeling. Despite substantial work on learning with noisy labels (LNL), existing methods have not been systematically evaluated for robustness in medical imaging contexts. To fill this gap, the authors introduce LNMBench, a comprehensive benchmark designed specifically for evaluating label noise issues in medical imaging. LNMBench includes 10 representative LNL methods assessed across 7 diverse datasets, spanning 6 different imaging modalities and 3 distinct noise patterns. This setup provides a unified and reproducible framework for robustness evaluation under realistic noise conditions. Experiments on LNMBench demonstrate that current LNL methods experience substantial performance degradation when exposed to high or real-world noise, underscoring ongoing challenges related to class imbalance and domain variability typical of medical data. Motivated by these insights, the authors propose a straightforward yet effective improvement to enhance model robustness under such adverse conditions. To promote reproducibility, standardized evaluation, and further research, the LNMBench codebase is publicly released and accessible via GitHub at https://github.com/myyy777/LNMBench. This resource aims to drive advancements in noise-resilient algorithms for both academic research and practical medical applications. <div>
arXiv:2512.09315v1 Announce Type: new 
Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking</title>
<link>https://arxiv.org/abs/2512.09327</link>
<guid>https://arxiv.org/abs/2512.09327</guid>
<content:encoded><![CDATA[
arXiv:2512.09327v1 Announce Type: new 
Abstract: Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video</title>
<link>https://arxiv.org/abs/2512.09335</link>
<guid>https://arxiv.org/abs/2512.09335</guid>
<content:encoded><![CDATA[
arXiv:2512.09335v1 Announce Type: new 
Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment</title>
<link>https://arxiv.org/abs/2512.09350</link>
<guid>https://arxiv.org/abs/2512.09350</guid>
<content:encoded><![CDATA[
arXiv:2512.09350v1 Announce Type: new 
Abstract: Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding</title>
<link>https://arxiv.org/abs/2512.09354</link>
<guid>https://arxiv.org/abs/2512.09354</guid>
<content:encoded><![CDATA[
arXiv:2512.09354v1 Announce Type: new 
Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</title>
<link>https://arxiv.org/abs/2512.09363</link>
<guid>https://arxiv.org/abs/2512.09363</guid>
<content:encoded><![CDATA[
arXiv:2512.09363v1 Announce Type: new 
Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.09364</link>
<guid>https://arxiv.org/abs/2512.09364</guid>
<content:encoded><![CDATA[
arXiv:2512.09364v1 Announce Type: new 
Abstract: Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement</title>
<link>https://arxiv.org/abs/2512.09373</link>
<guid>https://arxiv.org/abs/2512.09373</guid>
<content:encoded><![CDATA[
arXiv:2512.09373v1 Announce Type: new 
Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log NeRF: Comparing Spaces for Learning Radiance Fields</title>
<link>https://arxiv.org/abs/2512.09375</link>
<guid>https://arxiv.org/abs/2512.09375</guid>
<content:encoded><![CDATA[
arXiv:2512.09375v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perception-Inspired Color Space Design for Photo White Balance Editing</title>
<link>https://arxiv.org/abs/2512.09383</link>
<guid>https://arxiv.org/abs/2512.09383</guid>
<content:encoded><![CDATA[
arXiv:2512.09383v1 Announce Type: new 
Abstract: White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.
  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.
  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography</title>
<link>https://arxiv.org/abs/2512.09393</link>
<guid>https://arxiv.org/abs/2512.09393</guid>
<content:encoded><![CDATA[
arXiv:2512.09393v1 Announce Type: new 
Abstract: Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.
  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.
  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.
  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein-Aligned Hyperbolic Multi-View Clustering</title>
<link>https://arxiv.org/abs/2512.09402</link>
<guid>https://arxiv.org/abs/2512.09402</guid>
<content:encoded><![CDATA[
arXiv:2512.09402v1 Announce Type: new 
Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Point Cloud Registration</title>
<link>https://arxiv.org/abs/2512.09407</link>
<guid>https://arxiv.org/abs/2512.09407</guid>
<content:encoded><![CDATA[
arXiv:2512.09407v1 Announce Type: new 
Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping</title>
<link>https://arxiv.org/abs/2512.09417</link>
<guid>https://arxiv.org/abs/2512.09417</guid>
<content:encoded><![CDATA[
arXiv:2512.09417v1 Announce Type: new 
Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis</title>
<link>https://arxiv.org/abs/2512.09418</link>
<guid>https://arxiv.org/abs/2512.09418</guid>
<content:encoded><![CDATA[
arXiv:2512.09418v1 Announce Type: new 
Abstract: Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography</title>
<link>https://arxiv.org/abs/2512.09422</link>
<guid>https://arxiv.org/abs/2512.09422</guid>
<content:encoded><![CDATA[
arXiv:2512.09422v1 Announce Type: new 
Abstract: Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \(69.38\%\) using only \(25\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds</title>
<link>https://arxiv.org/abs/2512.09423</link>
<guid>https://arxiv.org/abs/2512.09423</guid>
<content:encoded><![CDATA[
arXiv:2512.09423v1 Announce Type: new 
Abstract: Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents</title>
<link>https://arxiv.org/abs/2512.09435</link>
<guid>https://arxiv.org/abs/2512.09435</guid>
<content:encoded><![CDATA[
arXiv:2512.09435v1 Announce Type: new 
Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model</title>
<link>https://arxiv.org/abs/2512.09441</link>
<guid>https://arxiv.org/abs/2512.09441</guid>
<content:encoded><![CDATA[
arXiv:2512.09441v1 Announce Type: new 
Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation</title>
<link>https://arxiv.org/abs/2512.09446</link>
<guid>https://arxiv.org/abs/2512.09446</guid>
<content:encoded><![CDATA[
arXiv:2512.09446v1 Announce Type: new 
Abstract: Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework</title>
<link>https://arxiv.org/abs/2512.09461</link>
<guid>https://arxiv.org/abs/2512.09461</guid>
<content:encoded><![CDATA[
arXiv:2512.09461v1 Announce Type: new 
Abstract: Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing</title>
<link>https://arxiv.org/abs/2512.09463</link>
<guid>https://arxiv.org/abs/2512.09463</guid>
<content:encoded><![CDATA[
arXiv:2512.09463v1 Announce Type: new 
Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach</title>
<link>https://arxiv.org/abs/2512.09471</link>
<guid>https://arxiv.org/abs/2512.09471</guid>
<content:encoded><![CDATA[
arXiv:2512.09471v1 Announce Type: new 
Abstract: Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Color encoding in Latent Space of Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2512.09477</link>
<guid>https://arxiv.org/abs/2512.09477</guid>
<content:encoded><![CDATA[
arXiv:2512.09477v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images</title>
<link>https://arxiv.org/abs/2512.09489</link>
<guid>https://arxiv.org/abs/2512.09489</guid>
<content:encoded><![CDATA[
arXiv:2512.09489v1 Announce Type: new 
Abstract: Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio</title>
<link>https://arxiv.org/abs/2512.09492</link>
<guid>https://arxiv.org/abs/2512.09492</guid>
<content:encoded><![CDATA[
arXiv:2512.09492v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Guided Learning Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2512.09497</link>
<guid>https://arxiv.org/abs/2512.09497</guid>
<content:encoded><![CDATA[
arXiv:2512.09497v1 Announce Type: new 
Abstract: Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction</title>
<link>https://arxiv.org/abs/2512.09525</link>
<guid>https://arxiv.org/abs/2512.09525</guid>
<content:encoded><![CDATA[
arXiv:2512.09525v1 Announce Type: new 
Abstract: Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.09546</link>
<guid>https://arxiv.org/abs/2512.09546</guid>
<content:encoded><![CDATA[
arXiv:2512.09546v1 Announce Type: new 
Abstract: This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.09555</link>
<guid>https://arxiv.org/abs/2512.09555</guid>
<content:encoded><![CDATA[
arXiv:2512.09555v1 Announce Type: new 
Abstract: Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection</title>
<link>https://arxiv.org/abs/2512.09565</link>
<guid>https://arxiv.org/abs/2512.09565</guid>
<content:encoded><![CDATA[
arXiv:2512.09565v1 Announce Type: new 
Abstract: Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.09573</link>
<guid>https://arxiv.org/abs/2512.09573</guid>
<content:encoded><![CDATA[
arXiv:2512.09573v1 Announce Type: new 
Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis</title>
<link>https://arxiv.org/abs/2512.09576</link>
<guid>https://arxiv.org/abs/2512.09576</guid>
<content:encoded><![CDATA[
arXiv:2512.09576v1 Announce Type: new 
Abstract: Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title>
<link>https://arxiv.org/abs/2512.09579</link>
<guid>https://arxiv.org/abs/2512.09579</guid>
<content:encoded><![CDATA[
arXiv:2512.09579v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation</title>
<link>https://arxiv.org/abs/2512.09580</link>
<guid>https://arxiv.org/abs/2512.09580</guid>
<content:encoded><![CDATA[
arXiv:2512.09580v1 Announce Type: new 
Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision</title>
<link>https://arxiv.org/abs/2512.09583</link>
<guid>https://arxiv.org/abs/2512.09583</guid>
<content:encoded><![CDATA[
arXiv:2512.09583v1 Announce Type: new 
Abstract: Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CS3D: An Efficient Facial Expression Recognition via Event Vision</title>
<link>https://arxiv.org/abs/2512.09592</link>
<guid>https://arxiv.org/abs/2512.09592</guid>
<content:encoded><![CDATA[
arXiv:2512.09592v1 Announce Type: new 
Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Chain-of-Thought Reasoning for Videos</title>
<link>https://arxiv.org/abs/2512.09616</link>
<guid>https://arxiv.org/abs/2512.09616</guid>
<content:encoded><![CDATA[
arXiv:2512.09616v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation</title>
<link>https://arxiv.org/abs/2512.09617</link>
<guid>https://arxiv.org/abs/2512.09617</guid>
<content:encoded><![CDATA[
arXiv:2512.09617v1 Announce Type: new 
Abstract: Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder</title>
<link>https://arxiv.org/abs/2512.09626</link>
<guid>https://arxiv.org/abs/2512.09626</guid>
<content:encoded><![CDATA[
arXiv:2512.09626v1 Announce Type: new 
Abstract: Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking SAM2-based Trackers on FMOX</title>
<link>https://arxiv.org/abs/2512.09633</link>
<guid>https://arxiv.org/abs/2512.09633</guid>
<content:encoded><![CDATA[
arXiv:2512.09633v1 Announce Type: new 
Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments</title>
<link>https://arxiv.org/abs/2512.09644</link>
<guid>https://arxiv.org/abs/2512.09644</guid>
<content:encoded><![CDATA[
arXiv:2512.09644v1 Announce Type: new 
Abstract: Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification</title>
<link>https://arxiv.org/abs/2512.09646</link>
<guid>https://arxiv.org/abs/2512.09646</guid>
<content:encoded><![CDATA[
arXiv:2512.09646v1 Announce Type: new 
Abstract: Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</title>
<link>https://arxiv.org/abs/2512.09663</link>
<guid>https://arxiv.org/abs/2512.09663</guid>
<content:encoded><![CDATA[
arXiv:2512.09663v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OxEnsemble: Fair Ensembles for Low-Data Classification</title>
<link>https://arxiv.org/abs/2512.09665</link>
<guid>https://arxiv.org/abs/2512.09665</guid>
<content:encoded><![CDATA[
arXiv:2512.09665v1 Announce Type: new 
Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence</title>
<link>https://arxiv.org/abs/2512.09670</link>
<guid>https://arxiv.org/abs/2512.09670</guid>
<content:encoded><![CDATA[
arXiv:2512.09670v1 Announce Type: new 
Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized</title>
<link>https://arxiv.org/abs/2512.09687</link>
<guid>https://arxiv.org/abs/2512.09687</guid>
<content:encoded><![CDATA[
arXiv:2512.09687v1 Announce Type: new 
Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.09700</link>
<guid>https://arxiv.org/abs/2512.09700</guid>
<content:encoded><![CDATA[
arXiv:2512.09700v1 Announce Type: new 
Abstract: Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stylized Meta-Album: Group-bias injection with style transfer to study robustness against distribution shifts</title>
<link>https://arxiv.org/abs/2512.09773</link>
<guid>https://arxiv.org/abs/2512.09773</guid>
<content:encoded><![CDATA[
arXiv:2512.09773v1 Announce Type: new 
Abstract: We introduce Stylized Meta-Album (SMA), a new image classification meta-dataset comprising 24 datasets (12 content datasets, and 12 stylized datasets), designed to advance studies on out-of-distribution (OOD) generalization and related topics. Created using style transfer techniques from 12 subject classification datasets, SMA provides a diverse and extensive set of 4800 groups, combining various subjects (objects, plants, animals, human actions, textures) with multiple styles. SMA enables flexible control over groups and classes, allowing us to configure datasets to reflect diverse benchmark scenarios. While ideally, data collection would capture extensive group diversity, practical constraints often make this infeasible. SMA addresses this by enabling large and configurable group structures through flexible control over styles, subject classes, and domains-allowing datasets to reflect a wide range of real-world benchmark scenarios. This design not only expands group and class diversity, but also opens new methodological directions for evaluating model performance across diverse group and domain configurations-including scenarios with many minority groups, varying group imbalance, and complex domain shifts-and for studying fairness, robustness, and adaptation under a broader range of realistic conditions. To demonstrate SMA's effectiveness, we implemented two benchmarks: (1) a novel OOD generalization and group fairness benchmark leveraging SMA's domain, class, and group diversity to evaluate existing benchmarks. Our findings reveal that while simple balancing and algorithms utilizing group information remain competitive as claimed in previous benchmarks, increasing group diversity significantly impacts fairness, altering the superiority and relative rankings of algorithms. We also propose to use \textit{Top-M worst group accuracy} as a new hyperparameter tuning metric, demonstrating broader fairness during optimization and delivering better final worst-group accuracy for larger group diversity. (2) An unsupervised domain adaptation (UDA) benchmark utilizing SMA's group diversity to evaluate UDA algorithms across more scenarios, offering a more comprehensive benchmark with lower error bars (reduced by 73\% and 28\% in closed-set setting and UniDA setting, respectively) compared to existing efforts. These use cases highlight SMA's potential to significantly impact the outcomes of conventional benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation</title>
<link>https://arxiv.org/abs/2512.09792</link>
<guid>https://arxiv.org/abs/2512.09792</guid>
<content:encoded><![CDATA[
arXiv:2512.09792v1 Announce Type: new 
Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of "apparent rotation", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation</title>
<link>https://arxiv.org/abs/2512.09801</link>
<guid>https://arxiv.org/abs/2512.09801</guid>
<content:encoded><![CDATA[
arXiv:2512.09801v1 Announce Type: new 
Abstract: Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing</title>
<link>https://arxiv.org/abs/2512.09806</link>
<guid>https://arxiv.org/abs/2512.09806</guid>
<content:encoded><![CDATA[
arXiv:2512.09806v1 Announce Type: new 
Abstract: U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.09814</link>
<guid>https://arxiv.org/abs/2512.09814</guid>
<content:encoded><![CDATA[
arXiv:2512.09814v1 Announce Type: new 
Abstract: Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composing Concepts from Images and Videos via Concept-prompt Binding</title>
<link>https://arxiv.org/abs/2512.09824</link>
<guid>https://arxiv.org/abs/2512.09824</guid>
<content:encoded><![CDATA[
arXiv:2512.09824v1 Announce Type: new 
Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities</title>
<link>https://arxiv.org/abs/2512.09847</link>
<guid>https://arxiv.org/abs/2512.09847</guid>
<content:encoded><![CDATA[
arXiv:2512.09847v1 Announce Type: new 
Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.09864</link>
<guid>https://arxiv.org/abs/2512.09864</guid>
<content:encoded><![CDATA[
arXiv:2512.09864v1 Announce Type: new 
Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI</title>
<link>https://arxiv.org/abs/2512.09867</link>
<guid>https://arxiv.org/abs/2512.09867</guid>
<content:encoded><![CDATA[
arXiv:2512.09867v1 Announce Type: new 
Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling</title>
<link>https://arxiv.org/abs/2512.09871</link>
<guid>https://arxiv.org/abs/2512.09871</guid>
<content:encoded><![CDATA[
arXiv:2512.09871v1 Announce Type: new 
Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs</title>
<link>https://arxiv.org/abs/2512.09874</link>
<guid>https://arxiv.org/abs/2512.09874</guid>
<content:encoded><![CDATA[
arXiv:2512.09874v1 Announce Type: new 
Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisualActBench: Can VLMs See and Act like a Human?</title>
<link>https://arxiv.org/abs/2512.09907</link>
<guid>https://arxiv.org/abs/2512.09907</guid>
<content:encoded><![CDATA[
arXiv:2512.09907v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway</title>
<link>https://arxiv.org/abs/2512.09913</link>
<guid>https://arxiv.org/abs/2512.09913</guid>
<content:encoded><![CDATA[
arXiv:2512.09913v1 Announce Type: new 
Abstract: We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splatent: Splatting Diffusion Latents for Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.09923</link>
<guid>https://arxiv.org/abs/2512.09923</guid>
<content:encoded><![CDATA[
arXiv:2512.09923v1 Announce Type: new 
Abstract: Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning</title>
<link>https://arxiv.org/abs/2512.09924</link>
<guid>https://arxiv.org/abs/2512.09924</guid>
<content:encoded><![CDATA[
arXiv:2512.09924v1 Announce Type: new 
Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures</title>
<link>https://arxiv.org/abs/2512.09925</link>
<guid>https://arxiv.org/abs/2512.09925</guid>
<content:encoded><![CDATA[
arXiv:2512.09925v1 Announce Type: new 
Abstract: Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agreement Disagreement Guided Knowledge Transfer for Cross-Scene Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2512.08990</link>
<guid>https://arxiv.org/abs/2512.08990</guid>
<content:encoded><![CDATA[
arXiv:2512.08990v1 Announce Type: cross 
Abstract: Knowledge transfer plays a crucial role in cross-scene hyperspectral imaging (HSI). However, existing studies often overlook the challenges of gradient conflicts and dominant gradients that arise during the optimization of shared parameters. Moreover, many current approaches fail to simultaneously capture both agreement and disagreement information, relying only on a limited shared subset of target features and consequently missing the rich, diverse patterns present in the target scene. To address these issues, we propose an Agreement Disagreement Guided Knowledge Transfer (ADGKT) framework that integrates both mechanisms to enhance cross-scene transfer. The agreement component includes GradVac, which aligns gradient directions to mitigate conflicts between source and target domains, and LogitNorm, which regulates logit magnitudes to prevent domination by a single gradient source. The disagreement component consists of a Disagreement Restriction (DiR) and an ensemble strategy, which capture diverse predictive target features and mitigate the loss of critical target information. Extensive experiments demonstrate the effectiveness and superiority of the proposed method in achieving robust and balanced knowledge transfer across heterogeneous HSI scenes.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning</title>
<link>https://arxiv.org/abs/2512.08992</link>
<guid>https://arxiv.org/abs/2512.08992</guid>
<content:encoded><![CDATA[
arXiv:2512.08992v1 Announce Type: cross 
Abstract: The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant</title>
<link>https://arxiv.org/abs/2512.08998</link>
<guid>https://arxiv.org/abs/2512.08998</guid>
<content:encoded><![CDATA[
arXiv:2512.08998v1 Announce Type: cross 
Abstract: Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Attribution of Model Performance Gaps in Medical Imaging Under Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.09094</link>
<guid>https://arxiv.org/abs/2512.09094</guid>
<content:encoded><![CDATA[
arXiv:2512.09094v1 Announce Type: cross 
Abstract: Deep learning models for medical image segmentation suffer significant performance drops due to distribution shifts, but the causal mechanisms behind these drops remain poorly understood. We extend causal attribution frameworks to high-dimensional segmentation tasks, quantifying how acquisition protocols and annotation variability independently contribute to performance degradation. We model the data-generating process through a causal graph and employ Shapley values to fairly attribute performance changes to individual mechanisms. Our framework addresses unique challenges in medical imaging: high-dimensional outputs, limited samples, and complex mechanism interactions. Validation on multiple sclerosis (MS) lesion segmentation across 4 centers and 7 annotators reveals context-dependent failure modes: annotation protocol shifts dominate when crossing annotators (7.4% $\pm$ 8.9% DSC attribution), while acquisition shifts dominate when crossing imaging centers (6.5% $\pm$ 9.1%). This mechanism-specific quantification enables practitioners to prioritize targeted interventions based on deployment context.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual Primitive Fitting of 3D Shapes with SuperFrusta</title>
<link>https://arxiv.org/abs/2512.09201</link>
<guid>https://arxiv.org/abs/2512.09201</guid>
<content:encoded><![CDATA[
arXiv:2512.09201v1 Announce Type: cross 
Abstract: We introduce a framework for converting 3D shapes into compact and editable assemblies of analytic primitives, directly addressing the persistent trade-off between reconstruction fidelity and parsimony. Our approach combines two key contributions: a novel primitive, termed SuperFrustum, and an iterative fiting algorithm, Residual Primitive Fitting (ResFit). SuperFrustum is an analytical primitive that is simultaneously (1) expressive, being able to model various common solids such as cylinders, spheres, cones & their tapered and bent forms, (2) editable, being compactly parameterized with 8 parameters, and (3) optimizable, with a sign distance field differentiable w.r.t. its parameters almost everywhere. ResFit is an unsupervised procedure that interleaves global shape analysis with local optimization, iteratively fitting primitives to the unexplained residual of a shape to discover a parsimonious yet accurate decompositions for each input shape. On diverse 3D benchmarks, our method achieves state-of-the-art results, improving IoU by over 9 points while using nearly half as many primitives as prior work. The resulting assemblies bridge the gap between dense 3D data and human-controllable design, producing high-fidelity and editable shape programs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge</title>
<link>https://arxiv.org/abs/2512.09309</link>
<guid>https://arxiv.org/abs/2512.09309</guid>
<content:encoded><![CDATA[
arXiv:2512.09309v1 Announce Type: cross 
Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration</title>
<link>https://arxiv.org/abs/2512.09340</link>
<guid>https://arxiv.org/abs/2512.09340</guid>
<content:encoded><![CDATA[
arXiv:2512.09340v1 Announce Type: cross 
Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane</title>
<link>https://arxiv.org/abs/2512.09343</link>
<guid>https://arxiv.org/abs/2512.09343</guid>
<content:encoded><![CDATA[
arXiv:2512.09343v1 Announce Type: cross 
Abstract: QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rates and architectures for learning geometrically non-trivial operators</title>
<link>https://arxiv.org/abs/2512.09376</link>
<guid>https://arxiv.org/abs/2512.09376</guid>
<content:encoded><![CDATA[
arXiv:2512.09376v1 Announce Type: cross 
Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos</title>
<link>https://arxiv.org/abs/2512.09406</link>
<guid>https://arxiv.org/abs/2512.09406</guid>
<content:encoded><![CDATA[
arXiv:2512.09406v1 Announce Type: cross 
Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments</title>
<link>https://arxiv.org/abs/2512.09447</link>
<guid>https://arxiv.org/abs/2512.09447</guid>
<content:encoded><![CDATA[
arXiv:2512.09447v1 Announce Type: cross 
Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiePrune: Lie Group and Quantum Geometric Dual Representation for One-Shot Structured Pruning of Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2512.09469</link>
<guid>https://arxiv.org/abs/2512.09469</guid>
<content:encoded><![CDATA[
arXiv:2512.09469v1 Announce Type: cross 
Abstract: Quantum neural networks (QNNs) and parameterized quantum circuits (PQCs) are key building blocks for near-term quantum machine learning. However, their scalability is constrained by excessive parameters, barren plateaus, and hardware limitations. We propose LiePrune, the first mathematically grounded one-shot structured pruning framework for QNNs that leverages Lie group structure and quantum geometric information. Each gate is jointly represented in a Lie group--Lie algebra dual space and a quantum geometric feature space, enabling principled redundancy detection and aggressive compression. Experiments on quantum classification (MNIST, FashionMNIST), quantum generative modeling (Bars-and-Stripes), and quantum chemistry (LiH VQE) show that LiePrune achieves over $10\times$ compression with negligible or even improved task performance, while providing provable guarantees on redundancy detection, functional approximation, and computational complexity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics</title>
<link>https://arxiv.org/abs/2512.09510</link>
<guid>https://arxiv.org/abs/2512.09510</guid>
<content:encoded><![CDATA[
arXiv:2512.09510v1 Announce Type: cross 
Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</title>
<link>https://arxiv.org/abs/2512.09607</link>
<guid>https://arxiv.org/abs/2512.09607</guid>
<content:encoded><![CDATA[
arXiv:2512.09607v1 Announce Type: cross 
Abstract: Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation</title>
<link>https://arxiv.org/abs/2512.09610</link>
<guid>https://arxiv.org/abs/2512.09610</guid>
<content:encoded><![CDATA[
arXiv:2512.09610v1 Announce Type: cross 
Abstract: People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthPix: A lightspeed PIV images generator</title>
<link>https://arxiv.org/abs/2512.09664</link>
<guid>https://arxiv.org/abs/2512.09664</guid>
<content:encoded><![CDATA[
arXiv:2512.09664v1 Announce Type: cross 
Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation</title>
<link>https://arxiv.org/abs/2512.09779</link>
<guid>https://arxiv.org/abs/2512.09779</guid>
<content:encoded><![CDATA[
arXiv:2512.09779v1 Announce Type: cross 
Abstract: Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&amp;Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronusOmni: Improving Time Awareness of Omni Large Language Models</title>
<link>https://arxiv.org/abs/2512.09841</link>
<guid>https://arxiv.org/abs/2512.09841</guid>
<content:encoded><![CDATA[
arXiv:2512.09841v1 Announce Type: cross 
Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</title>
<link>https://arxiv.org/abs/2512.09851</link>
<guid>https://arxiv.org/abs/2512.09851</guid>
<content:encoded><![CDATA[
arXiv:2512.09851v1 Announce Type: cross 
Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Heading Prediction for Autonomous Aerial Vehicles</title>
<link>https://arxiv.org/abs/2512.09898</link>
<guid>https://arxiv.org/abs/2512.09898</guid>
<content:encoded><![CDATA[
arXiv:2512.09898v1 Announce Type: cross 
Abstract: The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506{\deg} and a root mean squared error of 0.1957{\deg}, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</title>
<link>https://arxiv.org/abs/2512.09903</link>
<guid>https://arxiv.org/abs/2512.09903</guid>
<content:encoded><![CDATA[
arXiv:2512.09903v1 Announce Type: cross 
Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating</title>
<link>https://arxiv.org/abs/2512.09920</link>
<guid>https://arxiv.org/abs/2512.09920</guid>
<content:encoded><![CDATA[
arXiv:2512.09920v1 Announce Type: cross 
Abstract: Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Causal Principles for Improving Visual Dialog</title>
<link>https://arxiv.org/abs/1911.10496</link>
<guid>https://arxiv.org/abs/1911.10496</guid>
<content:encoded><![CDATA[
arXiv:1911.10496v3 Announce Type: replace 
Abstract: This paper unravels the design tricks adopted by us, the champion team MReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for improving Visual Dialog (VisDial). By "improving", we mean that they can promote almost every existing VisDial model to the state-of-the-art performance on the leader-board. Such a major improvement is only due to our careful inspection on the causality behind the model and data, finding that the community has overlooked two causalities in VisDial. Intuitively, Principle 1 suggests: we should remove the direct input of the dialog history to the answer model, otherwise a harmful shortcut bias will be introduced; Principle 2 says: there is an unobserved confounder for history, question, and answer, leading to spurious correlations from training data. In particular, to remove the confounder suggested in Principle 2, we propose several causal intervention algorithms, which make the training fundamentally different from the traditional likelihood estimation. Note that the two principles are model-agnostic, so they are applicable in any VisDial model. The code is available at https://github.com/simpleshinobu/visdial-principles.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Infrared Small Target Detection: A Feature-Enhanced and Sensitivity-Tunable Framework</title>
<link>https://arxiv.org/abs/2407.20090</link>
<guid>https://arxiv.org/abs/2407.20090</guid>
<content:encoded><![CDATA[
arXiv:2407.20090v3 Announce Type: replace 
Abstract: Recently, single-frame infrared small target (SIRST) detection technology has attracted widespread attention. Different from most existing deep learning-based methods that focus on improving network architectures, we propose a feature-enhanced and sensitivity-tunable (FEST) framework, which is compatible with existing SIRST detection networks and further enhances their detection performance. The FEST framework improves the model's robustness from two aspects: feature enhancement and target confidence regulation. For feature enhancement, we employ a multi-scale fusion strategy to improve the model's perception to multi-scale features of multi-size targets, and design an edge enhancement difficulty mining (EEDM) loss to guide the network to continuously focus on challenging target regions and edge features during training. For target confidence regulation, an adjustable sensitivity (AS) strategy is proposed for network post-processing. This strategy enhances the model's adaptability in complex scenarios and significantly improves the detection rate of infrared small targets while maintaining segmentation accuracy. Extensive experimental results show that our FEST framework can effectively enhance the performance of existing SIRST detection networks. The code is available at https://github.com/YuChuang1205/FEST-Framework
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial-Robustness-Guided Graph Pruning</title>
<link>https://arxiv.org/abs/2411.12331</link>
<guid>https://arxiv.org/abs/2411.12331</guid>
<content:encoded><![CDATA[
arXiv:2411.12331v2 Announce Type: replace 
Abstract: Graph learning plays a central role in many data mining and machine learning tasks, such as manifold learning, data representation and analysis, dimensionality reduction, clustering, and visualization. In this work, we propose a highly scalable, adversarial-robustness-guided graph pruning framework for learning graph topologies from data. By performing a spectral adversarial robustness evaluation, our method aims to learn sparse, undirected graphs that help the underlying algorithms resist noise and adversarial perturbations. In particular, we explicitly identify and prune edges that are most vulnerable to adversarial attacks. We use spectral clustering, one of the most representative graph-based machine learning algorithms, to evaluate the proposed framework. Compared with prior state-of-the-art graph learning approaches, the proposed method is more scalable and significantly improves both the computational efficiency and the solution quality of spectral clustering.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations</title>
<link>https://arxiv.org/abs/2412.01826</link>
<guid>https://arxiv.org/abs/2412.01826</guid>
<content:encoded><![CDATA[
arXiv:2412.01826v2 Announce Type: replace 
Abstract: We present RELOCATE, a simple training-free baseline designed to perform the challenging task of visual query localization in long videos. To eliminate the need for task-specific training and efficiently handle long videos, RELOCATE leverages a region-based representation derived from pretrained vision models. At a high level, it follows the classic object localization approach: (1) identify all objects in each video frame, (2) compare the objects with the given query and select the most similar ones, and (3) perform bidirectional tracking to get a spatio-temporal response. However, we propose some key enhancements to handle small objects, cluttered scenes, partial visibility, and varying appearances. Notably, we refine the selected objects for accurate localization and generate additional visual queries to capture visual variations. We evaluate RELOCATE on the challenging Ego4D Visual Query 2D Localization dataset, establishing a new baseline that outperforms prior task-specific methods by 49% (relative improvement) in spatio-temporal average precision.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence models for continuous cell cycle stage prediction from brightfield images</title>
<link>https://arxiv.org/abs/2502.02182</link>
<guid>https://arxiv.org/abs/2502.02182</guid>
<content:encoded><![CDATA[
arXiv:2502.02182v2 Announce Type: replace 
Abstract: Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the Fucci system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous Fucci signals using non-fluorescence brightfield imaging, a widely available label-free modality. To that end, we generated a large dataset of 1.3 M images of dividing RPE1 cells with full cell cycle trajectories to quantitatively compare the predictive performance of distinct model categories including single time-frame models, causal state space models and bidirectional transformer models. We show that both causal and transformer-based models significantly outperform single- and fixed frame approaches, enabling the prediction of visually imperceptible transitions like G1/S within 1h resolution. Our findings underscore the importance of sequence models for accurate predictions of cell cycle dynamics and highlight their potential for label-free imaging.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization</title>
<link>https://arxiv.org/abs/2502.05593</link>
<guid>https://arxiv.org/abs/2502.05593</guid>
<content:encoded><![CDATA[
arXiv:2502.05593v2 Announce Type: replace 
Abstract: Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight Space Representation Learning on Diverse NeRF Architectures</title>
<link>https://arxiv.org/abs/2502.09623</link>
<guid>https://arxiv.org/abs/2502.09623</guid>
<content:encoded><![CDATA[
arXiv:2502.09623v3 Announce Type: replace 
Abstract: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network. Recent studies have demonstrated that these weights can be used as input for frameworks designed to address deep learning tasks; however, such frameworks require NeRFs to adhere to a specific, predefined architecture. In this paper, we introduce the first framework capable of processing NeRFs with diverse architectures and performing inference on architectures unseen at training time. We achieve this by training a Graph Meta-Network within an unsupervised representation learning framework, and show that a contrastive objective is conducive to obtaining an architecture-agnostic latent space. In experiments conducted across 13 NeRF architectures belonging to three families (MLPs, tri-planes, and, for the first time, hash tables), our approach demonstrates robust performance in classification, retrieval, and language tasks involving multiple architectures, even unseen at training time, while also matching or exceeding the results of existing frameworks limited to single architectures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v5 Announce Type: replace 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Motion Unlearning</title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
arXiv:2503.18674v3 Announce Type: replace 
Abstract: We introduce Human Motion Unlearning and motivate it through the concrete task of preventing violent 3D motion synthesis, an important safety requirement given that popular text-to-motion datasets (HumanML3D and Motion-X) contain from 7\% to 15\% violent sequences spanning both atomic gestures (e.g., a single punch) and highly compositional actions (e.g., loading and swinging a leg to kick). By focusing on violence unlearning, we demonstrate how removing a challenging, multifaceted concept can serve as a proxy for the broader capability of motion "forgetting." To enable systematic evaluation of Human Motion Unlearning, we establish the first motion unlearning benchmark by automatically filtering HumanML3D and Motion-X datasets to create distinct forget sets (violent motions) and retain sets (safe motions). We introduce evaluation metrics tailored to sequential unlearning, measuring both suppression efficacy and the preservation of realism and smooth transitions. We adapt two state-of-the-art, training-free image unlearning methods (UCE and RECE) to leading text-to-motion architectures (MoMask and BAMM), and propose Latent Code Replacement (LCR), a novel, training-free approach that identifies violent codes in a discrete codebook representation and substitutes them with safe alternatives. Our experiments show that unlearning violent motions is indeed feasible and that acting on latent codes strikes the best trade-off between violence suppression and preserving overall motion quality. This work establishes a foundation for advancing safe motion synthesis across diverse applications. Website: https://www.pinlab.org/hmu.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset</title>
<link>https://arxiv.org/abs/2503.19804</link>
<guid>https://arxiv.org/abs/2503.19804</guid>
<content:encoded><![CDATA[
arXiv:2503.19804v2 Announce Type: replace 
Abstract: Low-light image enhancement is crucial for a myriad of applications, from night vision and surveillance, to autonomous driving. However, due to the inherent limitations that come in hand with capturing images in low-illumination environments, the task of enhancing such scenes still presents a formidable challenge. To advance research in this field, we introduce our Low Exposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure benchmark dataset for low-light image enhancement comprising of over 230K frames showcasing 24K real-world indoor and outdoor, with-and without human, scenes. Captured using 3 different camera sensors, LENVIZ offers a wide range of lighting conditions, noise levels, and scene complexities, making it the largest publicly available up-to 4K resolution benchmark in the field. LENVIZ includes high quality human-generated ground truth, for which each multi-exposure low-light scene has been meticulously curated and edited by expert photographers to ensure optimal image quality. Furthermore, we also conduct a comprehensive analysis of current state-of-the-art low-light image enhancement techniques on our dataset and highlight potential areas of improvement.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation</title>
<link>https://arxiv.org/abs/2504.03166</link>
<guid>https://arxiv.org/abs/2504.03166</guid>
<content:encoded><![CDATA[
arXiv:2504.03166v2 Announce Type: replace 
Abstract: The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Polarization Multiplexing: Single-Shot Invisible Shape and Reflectance Recovery</title>
<link>https://arxiv.org/abs/2504.13177</link>
<guid>https://arxiv.org/abs/2504.13177</guid>
<content:encoded><![CDATA[
arXiv:2504.13177v2 Announce Type: replace 
Abstract: We propose spatial polarization multiplexing (SPM) for joint sensing of shape and reflectance of a static or dynamic deformable object, which is also invisible to the naked eye. Past structured-light methods are limited to shape acquisition and cannot recover reflectance as they alter scene appearance. Our key idea is to spatially multiplex a polarization pattern to encode the incident ray and also densely sample the reflected light. We derive a quantized polarized light pattern that can be robustly and uniquely decoded from the reflected Angle of Linear Polarization (AoLP) values. It also enables single-shot disentanglement of polarimetric diffuse and specular reflections for accurate BRDF estimation. We achieve this spatial polarization multiplexing (SPM) with a constrained de Bruijn sequence. We validate this novel invisible single-shot shape and reflectance method with real static and dynamic objects. The results demonstrate the effectiveness of SPM for accurate shape and BRDF measurement which opens new avenues of application for 3D sensing thanks to its invisibility and ability to jointly recover the radiometric properties.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing</title>
<link>https://arxiv.org/abs/2505.19148</link>
<guid>https://arxiv.org/abs/2505.19148</guid>
<content:encoded><![CDATA[
arXiv:2505.19148v2 Announce Type: replace 
Abstract: Resolving closely-spaced small targets in dense clusters presents a significant challenge in infrared imaging, as the overlapping signals hinder precise determination of their quantity, sub-pixel positions, and radiation intensities. While deep learning has advanced the field of infrared small target detection, its application to closely-spaced infrared small targets has not yet been explored. This gap exists primarily due to the complexity of separating superimposed characteristics and the lack of an open-source infrastructure. In this work, we propose the Dynamic Iterative Shrinkage Thresholding Network (DISTA-Net), which reconceptualizes traditional sparse reconstruction within a dynamic framework. DISTA-Net adaptively generates convolution weights and thresholding parameters to tailor the reconstruction process in real time. To the best of our knowledge, DISTA-Net is the first deep learning model designed specifically for the unmixing of closely-spaced infrared small targets, achieving superior sub-pixel detection accuracy. Moreover, we have established the first open-source ecosystem to foster further research in this field. This ecosystem comprises three key components: (1) CSIST-100K, a publicly available benchmark dataset; (2) CSO-mAP, a custom evaluation metric for sub-pixel detection; and (3) GrokCSO, an open-source toolkit featuring DISTA-Net and other models. Our code and dataset are available at https://github.com/GrokCV/GrokCSO.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Infer Parameterized Representations of Plants from 3D Scans</title>
<link>https://arxiv.org/abs/2505.22337</link>
<guid>https://arxiv.org/abs/2505.22337</guid>
<content:encoded><![CDATA[
arXiv:2505.22337v2 Announce Type: replace 
Abstract: Plants frequently contain numerous organs, organized in 3D branching systems defining the plant's architecture. Reconstructing the architecture of plants from unstructured observations is challenging because of self-occlusion and spatial proximity between organs, which are often thin structures. To achieve the challenging task, we propose an approach that allows to infer a parameterized representation of the plant's architecture from a given 3D scan of a plant. In addition to the plant's branching structure, this representation contains parametric information for each plant organ, and can therefore be used directly in a variety of tasks. In this data-driven approach, we train a recursive neural network with virtual plants generated using a procedural model. After training, the network allows to infer a parametric tree-like representation based on an input 3D point cloud. Our method is applicable to any plant that can be represented as binary axial tree. We quantitatively evaluate our approach on Chenopodium Album plants on reconstruction, segmentation and skeletonization, which are important problems in plant phenotyping. In addition to carrying out several tasks at once, our method achieves results on-par with strong baselines for each task. We apply our method, trained exclusively on synthetic data, to 3D scans and show that it generalizes well.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2506.02022</link>
<guid>https://arxiv.org/abs/2506.02022</guid>
<content:encoded><![CDATA[
arXiv:2506.02022v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) show reasoning promise, yet their visual perception is a critical bottleneck. Strikingly, MLLMs can produce correct answers even while misinterpreting crucial visual elements, masking these underlying failures. Our preliminary study on a joint perception-reasoning dataset revealed that for one leading MLLM, 29% of its correct answers to reasoning questions still exhibited visual perception errors. To systematically address this, we introduce "Do You See Me", a scalable benchmark with 1,758 images and 2,612 questions. It spans seven human-psychology inspired subtasks in 2D and 3D, featuring controllable complexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading closed-source and 5 major open-source models reveal a stark deficit: humans achieve 96.49% accuracy, while top MLLMs average below 50%. This performance gap widens rapidly with increased task complexity (e.g., from 12% to 45% in the visual form constancy subtask). Further analysis into the root causes suggests that failures stem from challenges like misallocated visual attention and the instability of internal representations for fine-grained details, especially at or below encoder patch resolution. This underscores an urgent need for MLLMs with truly robust visual perception. The benchmark dataset, source code and evaluation scripts are available at https://github.com/microsoft/Do-You-See-Me.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models</title>
<link>https://arxiv.org/abs/2506.11436</link>
<guid>https://arxiv.org/abs/2506.11436</guid>
<content:encoded><![CDATA[
arXiv:2506.11436v2 Announce Type: replace 
Abstract: Audio-Visual Segmentation (AVS) faces a fundamental challenge of effectively aligning audio and visual modalities. While recent approaches leverage foundation models to address data scarcity, they often rely on single-modality knowledge or combine foundation models in an off-the-shelf manner, failing to address the cross-modal alignment challenge. In this paper, we present TAViS, a novel framework that \textbf{couples} the knowledge of multimodal foundation models (ImageBind) for cross-modal alignment and a segmentation foundation model (SAM2) for precise segmentation. However, effectively combining these models poses two key challenges: the difficulty in transferring the knowledge between SAM2 and ImageBind due to their different feature spaces, and the insufficiency of using only segmentation loss for supervision. To address these challenges, we introduce a text-bridged design with two key components: (1) a text-bridged hybrid prompting mechanism where pseudo text provides class prototype information while retaining modality-specific details from both audio and visual inputs, and (2) an alignment supervision strategy that leverages text as a bridge to align shared semantic concepts within audio-visual modalities. Our approach achieves superior performance on single-source, multi-source, semantic datasets, and excels in zero-shot settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DELTAv2: Accelerating Dense 3D Tracking</title>
<link>https://arxiv.org/abs/2508.01170</link>
<guid>https://arxiv.org/abs/2508.01170</guid>
<content:encoded><![CDATA[
arXiv:2508.01170v2 Announce Type: replace 
Abstract: We propose a novel algorithm for accelerating dense long-term 3D point tracking in videos. Through analysis of existing state-of-the-art methods, we identify two major computational bottlenecks. First, transformer-based iterative tracking becomes expensive when handling a large number of trajectories. To address this, we introduce a coarse-to-fine strategy that begins tracking with a small subset of points and progressively expands the set of tracked trajectories. The newly added trajectories are initialized using a learnable interpolation module, which is trained end-to-end alongside the tracking network. Second, we propose an optimization that significantly reduces the cost of correlation feature computation, another key bottleneck in prior methods. Together, these improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURORA:Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation</title>
<link>https://arxiv.org/abs/2508.02149</link>
<guid>https://arxiv.org/abs/2508.02149</guid>
<content:encoded><![CDATA[
arXiv:2508.02149v2 Announce Type: replace 
Abstract: Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to precisely locate sounding objects by integrating visual, auditory, and textual cues. Existing methods often lack genuine semantic understanding, tending to memorize fixed reasoning patterns. Furthermore, jointly training for reasoning and segmentation can compromise pixel-level precision. To address these issues, we introduce AURORA, a novel framework designed to enhance genuine reasoning and language comprehension in reference audio-visual segmentation. We employ a structured Chain-of-Thought (CoT) prompting mechanism to guide the model through a step-by-step reasoning process and introduce a novel segmentation feature distillation loss to effectively integrate these reasoning abilities without sacrificing segmentation performance. To further cultivate the model's genuine reasoning capabilities, we devise a further two-stage training strategy: first, a ``corrective reflective-style training" stage utilizes self-correction to enhance the quality of reasoning paths, followed by reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster robustness in challenging scenarios. Experiments demonstrate that AURORA achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes effectively to unreferenced segmentation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v2 Announce Type: replace 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</title>
<link>https://arxiv.org/abs/2508.06982</link>
<guid>https://arxiv.org/abs/2508.06982</guid>
<content:encoded><![CDATA[
arXiv:2508.06982v4 Announce Type: replace 
Abstract: We present WeatherDiffusion, a diffusion-based framework for controllable weather editing in intrinsic space. Our framework includes two components based on diffusion priors: an inverse renderer that estimates material properties, scene geometry, and lighting as intrinsic maps from an input image, and a forward renderer that utilizes these geometry and material maps along with a text prompt that describes specific weather conditions to generate a final image. The intrinsic maps enhance controllability compared to traditional pixel-space editing approaches. We propose an intrinsic map-aware attention mechanism that improves spatial correspondence and decomposition quality in large outdoor scenes. For forward rendering, we leverage CLIP-space interpolation of weather prompts to achieve fine-grained weather control. We also introduce a synthetic and a real-world dataset, containing 38k and 18k images under various weather conditions, each with intrinsic map annotations. WeatherDiffusion outperforms state-of-the-art pixel-space editing approaches, weather restoration methods, and rendering-based methods, showing promise for downstream tasks such as autonomous driving, enhancing the robustness of detection and segmentation in challenging weather scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugLift: Uncertainty Aware Depth Descriptors for Robust 2D to 3D Pose Lifting</title>
<link>https://arxiv.org/abs/2508.07112</link>
<guid>https://arxiv.org/abs/2508.07112</guid>
<content:encoded><![CDATA[
arXiv:2508.07112v3 Announce Type: replace 
Abstract: Lifting based 3D human pose estimators infer 3D joints from 2D keypoints, but often struggle to generalize to real world settings with noisy 2D detections. We revisit the input to lifting and propose AugLift, a simple augmentation of standard lifting that enriches each 2D keypoint (x, y) with an Uncertainty Aware Depth Descriptor (UADD). We run a single off the shelf monocular depth estimator to obtain a depth map, and for every keypoint with detector confidence c we extract depth statistics from its confidence scaled neighborhood, forming a compact, interpretable UADD (c, d, d_min, d_max) that captures both local geometry and reliability. AugLift is modular, requires no new sensors or architectural changes, and integrates by expanding the input layer of existing lifting models.
  Across four datasets and four lifting architectures, AugLift boosts cross dataset (out of distribution) performance on unseen data by an average of 10.1 percent, while also improving in distribution performance by 4.0 percent as measured by MPJPE. A post hoc analysis clarifies when and why it helps: gains are largest on novel poses and significantly occluded joints, where depth statistics resolve front back ambiguities while confidence calibrates the spatial neighborhoods from which they are drawn. We also study interaction with recent image feature lifting methods and find the signals are complementary: adding UADD to image conditioned lifting yields both ID and OOD gains. A learned depth feature extension (AugLiftV2) improves performance further while trading off interpretability. Together, these results indicate that lightweight, confidence aware depth cues are a powerful plug in for robust 2D to 3D pose lifting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring</title>
<link>https://arxiv.org/abs/2508.11482</link>
<guid>https://arxiv.org/abs/2508.11482</guid>
<content:encoded><![CDATA[
arXiv:2508.11482v2 Announce Type: replace 
Abstract: The construction industry increasingly relies on visual data to support Artificial Intelligence (AI) and Machine Learning (ML) applications for site monitoring. High-quality, domain-specific datasets, comprising images, videos, and point clouds, capture site geometry and spatiotemporal dynamics, including the location and interaction of objects, workers, and materials. However, despite growing interest in leveraging visual datasets, existing resources vary widely in sizes, data modalities, annotation quality, and representativeness of real-world construction conditions. A systematic review to categorize their data characteristics and application contexts is still lacking, limiting the community's ability to fully understand the dataset landscape, identify critical gaps, and guide future directions toward more effective, reliable, and scalable AI applications in construction. To address this gap, this study conducts an extensive search of academic databases and open-data platforms, yielding 51 publicly available visual datasets that span the 2005-2024 period. These datasets are categorized using a structured data schema covering (i) data fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv) downstream application domains (e.g., progress tracking). This study synthesizes these findings into an open-source catalog, OpenConstruction, supporting data-driven method development. Furthermore, the study discusses several critical limitations in the existing construction dataset landscape and presents a roadmap for future data infrastructure anchored in the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles. By reviewing the current landscape and outlining strategic priorities, this study supports the advancement of data-centric solutions in the construction sector.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix-game 2.0: An open-source real-time and streaming interactive world model</title>
<link>https://arxiv.org/abs/2508.13009</link>
<guid>https://arxiv.org/abs/2508.13009</guid>
<content:encoded><![CDATA[
arXiv:2508.13009v3 Announce Type: replace 
Abstract: Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seedream 4.0: Toward Next-generation Multimodal Image Generation</title>
<link>https://arxiv.org/abs/2509.20427</link>
<guid>https://arxiv.org/abs/2509.20427</guid>
<content:encoded><![CDATA[
arXiv:2509.20427v3 Announce Type: replace 
Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. We further scale our model and data as Seedream 4.5. Seedream 4.0 and Seedream 4.5 are accessible on Volcano Engine https://www.volcengine.com/experience/ark?launch=seedream.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification</title>
<link>https://arxiv.org/abs/2510.08269</link>
<guid>https://arxiv.org/abs/2510.08269</guid>
<content:encoded><![CDATA[
arXiv:2510.08269v2 Announce Type: replace 
Abstract: Multi-label classification (MLC) offers a more comprehensive semantic understanding of Remote Sensing (RS) imagery compared to traditional single-label classification (SLC). However, obtaining complete annotations for MLC is particularly challenging due to the complexity and high cost of the labeling process. As a practical alternative, single-positive multi-label learning (SPML) has emerged, where each image is annotated with only one relevant label, and the model is expected to recover the full set of labels. While scalable, SPML introduces significant supervision ambiguity, demanding specialized solutions for model training. Although various SPML methods have been proposed in the computer vision domain, research in the RS context remains limited. To bridge this gap, we propose Adaptive Gradient Calibration (AdaGC), a novel and generalizable SPML framework tailored to RS imagery. AdaGC adopts a gradient calibration (GC) mechanism with a dual exponential moving average (EMA) module for robust pseudo-label generation. We introduce a theoretically grounded, training-dynamics-based indicator to adaptively trigger GC, which ensures GC's effectiveness by preventing it from being affected by model underfitting or overfitting to label noise. Extensive experiments on two benchmark RS datasets under two distinct label noise types demonstrate that AdaGC achieves state-of-the-art (SOTA) performance while maintaining strong robustness across diverse settings. The codes and data will be released at https://github.com/rslab-unitrento/AdaGC.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</title>
<link>https://arxiv.org/abs/2510.08352</link>
<guid>https://arxiv.org/abs/2510.08352</guid>
<content:encoded><![CDATA[
arXiv:2510.08352v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not "shortsighted", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation</title>
<link>https://arxiv.org/abs/2510.11173</link>
<guid>https://arxiv.org/abs/2510.11173</guid>
<content:encoded><![CDATA[
arXiv:2510.11173v2 Announce Type: replace 
Abstract: Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above the prior state of the art across both validation and test partitions. Extensive experiments demonstrate a strong positive correlation among the CoT trajectory, the generated heatmap, and the decoded mask, supporting an interpretable alignment between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and in more precise mask prediction. Code, checkpoints and logs are released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foveation Improves Payload Capacity in Steganography</title>
<link>https://arxiv.org/abs/2510.13151</link>
<guid>https://arxiv.org/abs/2510.13151</guid>
<content:encoded><![CDATA[
arXiv:2510.13151v2 Announce Type: replace 
Abstract: Steganography finds its use in visual medium such as providing metadata and watermarking. With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits, while achieving better accuracy of up to 1 failure bit out of 2000, at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in creating multi-modal latent representations in steganography.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression</title>
<link>https://arxiv.org/abs/2511.09702</link>
<guid>https://arxiv.org/abs/2511.09702</guid>
<content:encoded><![CDATA[
arXiv:2511.09702v2 Announce Type: replace 
Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation</title>
<link>https://arxiv.org/abs/2511.14184</link>
<guid>https://arxiv.org/abs/2511.14184</guid>
<content:encoded><![CDATA[
arXiv:2511.14184v3 Announce Type: replace 
Abstract: Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoD: A Diffusion Foundation Model for Image Compression</title>
<link>https://arxiv.org/abs/2511.18706</link>
<guid>https://arxiv.org/abs/2511.18706</guid>
<content:encoded><![CDATA[
arXiv:2511.18706v2 Announce Type: replace 
Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion</title>
<link>https://arxiv.org/abs/2511.22048</link>
<guid>https://arxiv.org/abs/2511.22048</guid>
<content:encoded><![CDATA[
arXiv:2511.22048v2 Announce Type: replace 
Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22787</link>
<guid>https://arxiv.org/abs/2511.22787</guid>
<content:encoded><![CDATA[
arXiv:2511.22787v2 Announce Type: replace 
Abstract: In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2512.00368</link>
<guid>https://arxiv.org/abs/2512.00368</guid>
<content:encoded><![CDATA[
arXiv:2512.00368v2 Announce Type: replace 
Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFM-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images</title>
<link>https://arxiv.org/abs/2512.00718</link>
<guid>https://arxiv.org/abs/2512.00718</guid>
<content:encoded><![CDATA[
arXiv:2512.00718v2 Announce Type: replace 
Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary accuracy. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios. The codes are available at https://github.com/wondelyan/VFM-ISRefiner .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalised Medical Phrase Grounding</title>
<link>https://arxiv.org/abs/2512.01085</link>
<guid>https://arxiv.org/abs/2512.01085</guid>
<content:encoded><![CDATA[
arXiv:2512.01085v2 Announce Type: replace 
Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach for Precise Wall Segmentation</title>
<link>https://arxiv.org/abs/2512.02413</link>
<guid>https://arxiv.org/abs/2512.02413</guid>
<content:encoded><![CDATA[
arXiv:2512.02413v2 Announce Type: replace 
Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans necessitates high-precision semantic segmentation of structural elements, particularly walls. However, existing methods often struggle with detecting thin structures and maintaining geometric precision. This study introduces MitUNet, a hybrid neural network combining a Mix-Transformer encoder and a U-Net decoder enhanced with spatial and channel attention blocks. Our approach, optimized with the Tversky loss function, achieves a balance between precision and recall, ensuring accurate boundary recovery. Experiments on the CubiCasa5k dataset and a proprietary regional dataset demonstrate MitUNet's superiority in generating structurally correct masks with high boundary accuracy, outperforming standard models. This tool provides a robust foundation for automated 3D reconstruction pipelines. To ensure reproducibility and facilitate future research, the source code and the proprietary regional dataset are publicly available at https://github.com/aliasstudio/mitunet and https://doi.org/10.5281/zenodo.17871079 respectively.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance</title>
<link>https://arxiv.org/abs/2512.02685</link>
<guid>https://arxiv.org/abs/2512.02685</guid>
<content:encoded><![CDATA[
arXiv:2512.02685v2 Announce Type: replace 
Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Informed Weighting Channel Normalizing Flow for Deep Generative Models</title>
<link>https://arxiv.org/abs/2407.04958</link>
<guid>https://arxiv.org/abs/2407.04958</guid>
<content:encoded><![CDATA[
arXiv:2407.04958v2 Announce Type: replace-cross 
Abstract: Normalizing Flows (NFs) are widely used in deep generative models for their exact likelihood estimation and efficient sampling.
  However, they require substantial memory since the latent space matches the input dimension.
  Multi-scale architectures address this by progressively reducing latent dimensions while preserving reversibility.
  Existing multi-scale architectures use simple, static channel-wise splitting, limiting expressiveness. To improve this, we introduce a regularized, feature-dependent $\mathtt{Shuffle}$ operation and integrate it into vanilla multi-scale architecture.
  This operation adaptively generates channel-wise weights and shuffles latent variables before splitting them.
  We observe that such operation guides the variables to evolve in the direction of entropy increase, hence we refer to NFs with the $\mathtt{Shuffle}$ operation as \emph{Entropy-Informed Weighting Channel Normalizing Flow} (EIW-Flow).
  Extensive experiments on CIFAR-10, CelebA, ImageNet, and LSUN demonstrate that EIW-Flow achieves state-of-the-art density estimation and competitive sample quality for deep generative modeling, with minimal computational overhead.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INRetouch: Context Aware Implicit Neural Representation for Photography Retouching</title>
<link>https://arxiv.org/abs/2412.03848</link>
<guid>https://arxiv.org/abs/2412.03848</guid>
<content:encoded><![CDATA[
arXiv:2412.03848v4 Announce Type: replace-cross 
Abstract: Professional photo editing remains challenging, requiring extensive knowledge of imaging pipelines and significant expertise. While recent deep learning approaches, particularly style transfer methods, have attempted to automate this process, they often struggle with output fidelity, editing control, and complex retouching capabilities. We propose a novel retouch transfer approach that learns from professional edits through before-after image pairs, enabling precise replication of complex editing operations. We develop a context-aware Implicit Neural Representation that learns to apply edits adaptively based on image content and context, and is capable of learning from a single example. Our method extracts implicit transformations from reference edits and adaptively applies them to new images. To facilitate this research direction, we introduce a comprehensive Photo Retouching Dataset comprising 100,000 high-quality images edited using over 170 professional Adobe Lightroom presets. Through extensive evaluation, we demonstrate that our approach not only surpasses existing methods in photo retouching but also enhances performance in related image reconstruction tasks like Gamut Mapping and Raw Reconstruction. By bridging the gap between professional editing capabilities and automated solutions, our work presents a significant step toward making sophisticated photo editing more accessible while maintaining high-fidelity results. The source code and the dataset are publicly available at https://omaralezaby.github.io/inretouch .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment</title>
<link>https://arxiv.org/abs/2503.10287</link>
<guid>https://arxiv.org/abs/2503.10287</guid>
<content:encoded><![CDATA[
arXiv:2503.10287v3 Announce Type: replace-cross 
Abstract: Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-modal task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, we propose a method called MACS to conduct multi-source audio-to-image generation. To our best knowledge, this is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, effective image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 out of the 21 evaluation indexes on all tasks and delivers superior visual quality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Pretrained Transformers May Be Universally Robust In-Context Learners</title>
<link>https://arxiv.org/abs/2505.14042</link>
<guid>https://arxiv.org/abs/2505.14042</guid>
<content:encoded><![CDATA[
arXiv:2505.14042v2 Announce Type: replace-cross 
Abstract: Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we present the first theoretical analysis suggesting that adversarially pretrained transformers can serve as universally robust foundation models -- models that can robustly adapt to diverse downstream tasks with only lightweight tuning. Specifically, we demonstrate that single-layer linear transformers, after adversarial pretraining across a variety of classification tasks, can robustly generalize to unseen classification tasks through in-context learning from clean demonstrations (i.e., without requiring additional adversarial training or examples). This universal robustness stems from the model's ability to adaptively focus on robust features within given tasks. We also show the two open challenges for attaining robustness: accuracy--robustness trade-off and sample-hungry training. This study initiates the discussion on the utility of universally robust foundation models. While their training is expensive, the investment would prove worthwhile as downstream tasks can enjoy free adversarial robustness. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.23589</link>
<guid>https://arxiv.org/abs/2509.23589</guid>
<content:encoded><![CDATA[
arXiv:2509.23589v2 Announce Type: replace-cross 
Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 7.72% over prior arts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</title>
<link>https://arxiv.org/abs/2509.25270</link>
<guid>https://arxiv.org/abs/2509.25270</guid>
<content:encoded><![CDATA[
arXiv:2509.25270v3 Announce Type: replace-cross 
Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</title>
<link>https://arxiv.org/abs/2510.09660</link>
<guid>https://arxiv.org/abs/2510.09660</guid>
<content:encoded><![CDATA[
arXiv:2510.09660v4 Announce Type: replace-cross 
Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Failures: Rethinking Foundation Models in Pathology</title>
<link>https://arxiv.org/abs/2510.23807</link>
<guid>https://arxiv.org/abs/2510.23807</guid>
<content:encoded><![CDATA[
arXiv:2510.23807v4 Announce Type: replace-cross 
Abstract: Despite their successes in vision and language, foundation models have stumbled in pathology, revealing low accuracy, instability, and heavy computational demands. These shortcomings stem not from tuning problems but from deeper conceptual mismatches: dense embeddings cannot represent the combinatorial richness of tissue, and current architectures inherit flaws in self-supervision, patch design, and noise-fragile pretraining. Biological complexity and limited domain innovation further widen the gap. The evidence is clear-pathology requires models explicitly designed for biological images rather than adaptations of large-scale natural-image methods whose assumptions do not hold for tissue.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporality for Sketch Representation Learning</title>
<link>https://arxiv.org/abs/2512.04007</link>
<guid>https://arxiv.org/abs/2512.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: sketches, temporal aspect, positional encodings, sequence modeling, decoders<br /><br />Summary:<br /><br />This work explores the role of temporal information in sketch representation learning, evaluating whether sketches should be treated as sequences. The research confirms that traditional positional encodings are valid for modeling sketches, with absolute coordinate encodings performing better than relative ones. Additionally, the study compares decoder architectures, finding that non-autoregressive decoders consistently outperform autoregressive decoders in representing sketches. Importantly, the study reveals that the significance of temporality in sketches is not universal but depends heavily on the specific stroke order used and the task being performed. Overall, the findings provide insight into how internal stroke order and temporal structure affect the quality of sketch representations, informing better model designs for various applications. <div>
arXiv:2512.04007v2 Announce Type: replace 
Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Cyberbullying in GIF using AI</title>
<link>https://arxiv.org/abs/2512.07838</link>
<guid>https://arxiv.org/abs/2512.07838</guid>
<content:encoded><![CDATA[
<div> Keywords: Cyberbullying, GIFs, Deep learning, VGG16, Twitter  

<br /><br />Summary:  
This study addresses the escalating issue of cyberbullying on social media platforms, with an emphasis on detecting cyberbullying in GIFs and stickers, an area with limited research compared to text and images. The researchers collected a diverse dataset of over 4100 GIFs from Twitter by extracting hashtags related to cyberbullying and using the GIPHY API to download relevant files. The dataset includes both cyberbullying and non-cyberbullying GIFs to facilitate effective training and evaluation. For detection, the study employed the pre-trained deep learning model VGG16, known for its robust performance in image processing tasks. The model achieved a high accuracy of 97% in classifying GIFs as cyberbullying or non-cyberbullying content. This highlights the potential of deep learning techniques for multimedia cyberbullying detection beyond text-based methods. Additionally, the authors provide the constructed GIF dataset to the research community, enabling further studies and improvements in this under-explored area. The work contributes to broadening the scope of cyberbullying detection by incorporating dynamic media formats like GIFs, thereby enhancing online safety measures on social networking platforms. <div>
arXiv:2512.07838v1 Announce Type: new 
Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-real time fires detection using satellite imagery in Sudan conflict</title>
<link>https://arxiv.org/abs/2512.07925</link>
<guid>https://arxiv.org/abs/2512.07925</guid>
<content:encoded><![CDATA[
<div> Keywords: Sudan conflict, deep learning, satellite imagery, fire damage monitoring, Planet Labs<br /><br />Summary:<br />1. The ongoing war in Sudan necessitates rapid and efficient monitoring methods to assess conflict-related damage.<br />2. The study leverages advances in deep learning alongside readily accessible satellite remote sensing data to achieve near real-time conflict monitoring.<br />3. Specifically, the authors utilize 4-band imagery from Planet Labs combined with a deep learning model to detect and assess fire damage caused by armed conflicts.<br />4. Five case studies in Sudan demonstrate the approach's effectiveness, showing improved accuracy in identifying active fires and charred areas compared to baseline methods.<br />5. The research finds that incorporating higher dimensional data such as 8-band imagery or time series imagery provides only marginal improvements over the 4-band imagery model.<br />6. These results underscore the potential for automated systems using readily available satellite imagery to provide timely insights into conflict zones, supporting humanitarian and strategic decision-making efforts. <div>
arXiv:2512.07925v1 Announce Type: new 
Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality</title>
<link>https://arxiv.org/abs/2512.07951</link>
<guid>https://arxiv.org/abs/2512.07951</guid>
<content:encoded><![CDATA[
<div> Video face swapping, temporal consistency, reference-guided editing, keyframe conditioning, Face2Face dataset<br /><br />Summary: Video face swapping plays an essential role in film and entertainment production but faces challenges in maintaining high fidelity and temporal consistency across long and complex sequences. This work, called LivingSwap, introduces the first video reference-guided face swapping model that leverages rich visual attributes from source videos to improve both fidelity and temporal coherence. The approach uses keyframes as conditioning signals to incorporate the target identity flexibly and controllably, combining them with video reference guidance to perform temporal stitching. This ensures stable identity preservation and high-quality reconstruction throughout lengthy video sequences. To overcome the lack of data for reference-guided training, the authors create a paired face-swapping dataset named Face2Face and augment it by reversing the data pairs to provide reliable ground-truth supervision. Extensive experiments show that LivingSwap delivers state-of-the-art results by seamlessly integrating target identities with the source video's expressions, lighting, and motions. Additionally, the method significantly reduces manual labor in video production workflows. The project webpage provides further resources and details: https://aim-uofa.github.io/LivingSwap <div>
arXiv:2512.07951v1 Announce Type: new 
Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection</title>
<link>https://arxiv.org/abs/2512.07984</link>
<guid>https://arxiv.org/abs/2512.07984</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical semantic segmentation, dental imaging, anatomical hierarchy, TL-pano dataset, feature-wise linear modulation<br /><br />Summary:<br />1. The paper addresses the challenge of accurately understanding anatomical structures in dental disease staging by incorporating explicit anatomical hierarchies into semantic segmentation models.<br />2. The authors propose a general framework that integrates a recurrent, level-wise prediction scheme, restrictive output heads, and top-down feature conditioning to embed anatomical hierarchy explicitly.<br />3. At each hierarchical level, the backbone model processes the original image combined with logits from the previous level; child class features are modulated using Feature-wise Linear Modulation (FiLM) of parent class probabilities to enhance finer detection.<br />4. A probabilistic composition rule is introduced to ensure consistency between parent and descendant class predictions, while a hierarchical loss combining class-weighted Dice, cross-entropy, and consistency terms ensures that parent predictions reflect the sum of their children.<br />5. The approach is validated on the newly proposed TL-pano dataset, consisting of 194 panoramic radiographs annotated for tooth layers and alveolar bone, using UNet and HRNet backbones in a 5-fold cross-validation.<br />6. Hierarchical variants significantly improve intersection-over-union (IoU), Dice scores, and recall, especially for fine-grained anatomical features, producing more anatomically coherent segmentation masks.<br />7. However, the hierarchical models show increased recall with a tendency for more false positives, indicating a trade-off between sensitivity and precision.<br />8. Overall, explicitly incorporating anatomical hierarchies improves segmentation performance and clinical plausibility, particularly under limited data conditions in dental imaging. <div>
arXiv:2512.07984v1 Announce Type: new 
Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08016</link>
<guid>https://arxiv.org/abs/2512.08016</guid>
<content:encoded><![CDATA[
<div> Cartographic reasoning, spatial relations, large vision language models, FRIEDA benchmark, map visual question answering<br /><br />Summary:<br /><br />1. Cartographic reasoning involves interpreting geographic relationships by aligning legends, scales, compass directions, text, and geometries across one or more map images, critical for tasks like disaster response and urban planning but remains under-evaluated.<br /><br />2. Existing large vision language models (LVLMs) often treat maps as a subset of charts, which overlooks the unique complexity of map visual question answering (VQA) that requires understanding layered symbology and spatial relations related to orientation, distance, and multi-map interactions.<br /><br />3. To address this, the authors introduce FRIEDA, a benchmark designed to test complex, open-ended cartographic reasoning in LVLMs using real-world map images from documents and reports across various domains and geographies.<br /><br />4. FRIEDA focuses on three categories of spatial relations from GIS literature: topological (e.g., border, within), metric (distance), and directional (orientation), with questions demanding multi-step inference and often cross-map grounding.<br /><br />5. Evaluation of eleven state-of-the-art LVLMs under direct and contextual settings reveals a significant performance gap, with the best models reaching only around 38% accuracy compared to human performance at nearly 85%, highlighting the challenge and establishing FRIEDA as a benchmark to advance spatial intelligence in LVLMs. <div>
arXiv:2512.08016v1 Announce Type: new 
Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification</title>
<link>https://arxiv.org/abs/2512.08038</link>
<guid>https://arxiv.org/abs/2512.08038</guid>
<content:encoded><![CDATA[
<div> Keywords: Retinopathy of Prematurity, Explainable AI, Sparse and Smooth Explainer, ADMM, Fundus Image Classification<br /><br />Summary:<br />1. The paper addresses the challenge of interpreting neural network models used for diagnosing Retinopathy of Prematurity (ROP) from fundus images, acknowledging the need for reliable model explainers due to their black-box nature.<br />2. It introduces a novel explainer method called Sparse and Smooth Explainer (SSplain), which generates pixel-wise explanations by enforcing smoothness and sparsity to better preserve the input image's structure.<br />3. SSplain relies on solving an optimization problem with combinatorial constraints via the Alternating Direction Method of Multipliers (ADMM), enabling the generation of realistic and interpretable explanations.<br />4. Experimental results demonstrate that SSplain outperforms existing explanation methods both in terms of post-hoc accuracy and maintaining smoothness in the output, thus providing more meaningful and consistent explanations aligned with clinical understanding.<br />5. The method's generalizability is validated by applying SSplain to additional publicly available datasets, and the source code has been made publicly available to facilitate further research and application in medical image explanation. <div>
arXiv:2512.08038v1 Announce Type: new 
Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment</title>
<link>https://arxiv.org/abs/2512.08040</link>
<guid>https://arxiv.org/abs/2512.08040</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language understanding, sign language translation, sign-subtitle alignment, multilingual pretraining, zero-shot generalisation<br /><br />Summary: This work introduces a unified model for sign language understanding that integrates two key tasks: sign language translation (SLT), which converts continuous signing videos into spoken language text, and sign-subtitle alignment (SSA), which temporally aligns signing video frames with subtitles. The model architecture comprises three components: a lightweight visual backbone extracting both manual and non-manual cues from human keypoints and lip-region images with privacy considerations; a Sliding Perceiver mapping network that aggregates visual features into word-level embeddings bridging the visual and textual modalities; and a multi-task scalable training strategy that jointly optimizes SLT and SSA to improve both linguistic and temporal alignment. To ensure cross-linguistic generalization, the model is pretrained on large-scale datasets featuring British Sign Language (BSL) and American Sign Language (ASL), specifically BOBSL and YouTube-SL-25 corpora. This multilingual pretraining coupled with robust model design achieves state-of-the-art performance on the BOBSL dataset for both SLT and SSA tasks. Additionally, the model shows strong zero-shot generalization capabilities and enhanced fine-tuned SLT performance on the How2Sign (ASL) dataset, demonstrating its potential for scalable, cross-lingual sign language translation applications. <div>
arXiv:2512.08040v1 Announce Type: new 
Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</title>
<link>https://arxiv.org/abs/2512.08042</link>
<guid>https://arxiv.org/abs/2512.08042</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, frequency-domain masking, generalization, model pruning, Green AI  

<br /><br />Summary: Universal deepfake detection seeks to identify AI-generated images from a wide range of generative models, including those not seen during training. The main challenge lies in achieving robust generalization to new and evolving deepfake techniques while minimizing computational costs to enable large-scale deployment, aligning with the goals of Green AI. This work proposes a novel training strategy based on frequency-domain masking, contrasted with traditional reliance on spatial features or large pretrained models. The method involves random masking and geometric transformations, with an emphasis on frequency masking due to its better generalization capabilities. Experiments demonstrate that frequency masking improves detection accuracy across various GAN and diffusion-based image generators. Additionally, the approach maintains strong performance even after substantial structured pruning, indicating its scalability and efficiency. The proposed method sets a new state-of-the-art for universal deepfake detection, combining strong generalization with resource-conscious operation. This makes it a promising solution for sustainable and practical deployment in large-scale deepfake screening scenarios. The authors have made the code and trained models publicly available to facilitate further research and application. <div>
arXiv:2512.08042v1 Announce Type: new 
Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning</title>
<link>https://arxiv.org/abs/2512.08048</link>
<guid>https://arxiv.org/abs/2512.08048</guid>
<content:encoded><![CDATA[
<div> Masking, Test-time adaptation, Distribution shift, Image corruption, Consistency loss<br /><br />Summary:<br /><br />1. The paper addresses the problem of distribution shifts at test time that degrade the performance of image classifiers, focusing on continual test-time adaptation (CTTA) methods.<br /><br />2. Existing CTTA approaches often rely on masking mechanisms guided by calibrated uncertainty estimates or stable attention scores, which introduces additional complexity.<br /><br />3. The authors question whether custom masking designs are necessary or if simple random masking can suffice under strong corruptions.<br /><br />4. They propose a novel method called Mask to Adapt (M2A), which uses a short sequence of randomly masked views—either spatial or frequency-based—and adapts the model using two objectives: a mask consistency loss to align predictions across views and an entropy minimization loss to encourage confident outputs.<br /><br />5. The study explores two types of masking: spatial masking (patch vs. pixel) and frequency masking (all vs. low vs. high frequencies), motivated by masked image modeling techniques.<br /><br />6. Experimental results on CIFAR10C, CIFAR100C, and ImageNetC datasets at high corruption severity demonstrate that M2A with spatial masking achieves significant improvements, outperforming or matching strong CTTA baselines, whereas frequency masking underperforms.<br /><br />7. Ablation studies confirm that simple random masking is both effective and robust, indicating that complex uncertainty- or attention-based masking schemes are not necessary.<br /><br />8. The findings suggest that combining random masking with consistency and entropy-based losses provides a straightforward yet powerful approach to test-time adaptation for image classifiers under distribution shifts. <div>
arXiv:2512.08048v1 Announce Type: new 
Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models</title>
<link>https://arxiv.org/abs/2512.08075</link>
<guid>https://arxiv.org/abs/2512.08075</guid>
<content:encoded><![CDATA[
<div> Keywords: Amazon Rainforest, deforestation detection, machine learning, satellite imagery, self-attention mechanisms<br /><br />Summary:<br /><br />1. The Amazon Rainforest's preservation is critical for global climate change mitigation, biodiversity protection, and indigenous culture safeguarding.<br />2. The PRODES project by INPE annually monitors deforestation in the Brazilian Amazon and other biomes using satellite data.<br />3. Recent machine learning models utilize PRODES data for deforestation detection by analyzing multitemporal satellite images as a change detection problem.<br />4. Current methods face challenges including suboptimal performance, limited use of modern architectures like self-attention-based networks, and lack of standardized evaluation protocols for direct comparison.<br />5. This work evaluates multiple change detection models on a unified dataset, comparing fully convolutional networks and Transformer-based self-attention architectures.<br />6. The study explores the effects of various pre- and post-processing techniques such as connected component filtering, texture replacement, and image enhancement, demonstrating significant improvement in model effectiveness.<br />7. Combining models through different fusion strategies yields better performance than individual models, achieving an F1-score of 80.41%, comparable to recent state-of-the-art results.<br /><br />This comprehensive evaluation addresses existing gaps, provides methodological standardization, and highlights the potential of advanced deep learning methods and processing techniques for improved deforestation monitoring in the Amazon using satellite imagery. <div>
arXiv:2512.08075v1 Announce Type: new 
Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2512.08135</link>
<guid>https://arxiv.org/abs/2512.08135</guid>
<content:encoded><![CDATA[
<div> Keywords: central-peripheral vision, multimodal model, spatial reasoning, 3D scene understanding, allocentric grid<br /><br />Summary:<br /><br />This paper introduces a novel central-peripheral vision-inspired framework (CVP) designed to enhance spatial reasoning in 3D scene understanding tasks. The approach is motivated by the human visual system, which uses central vision to focus on important details and peripheral vision to capture broader contextual information. Existing models typically rely on unstructured data representations such as point clouds or voxels, with scene context added implicitly via coordinate embeddings, limiting their ability to reason spatially in a structured manner. To overcome this, CVP incorporates two key components: the target-affinity token, serving as an analog to central vision by directing attention to query-relevant objects, and the allocentric grid, which acts like peripheral vision by encoding the global spatial layout and context of the scene. These elements operate together within a Large Multimodal Model framework to provide an explicit, structured understanding of complex 3D environments. Experimental results demonstrate that CVP outperforms existing state-of-the-art methods across several benchmarks for 3D scene understanding, validating the effectiveness of combining central and peripheral vision-inspired representations for improved spatial reasoning. <div>
arXiv:2512.08135v1 Announce Type: new 
Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing</title>
<link>https://arxiv.org/abs/2512.08161</link>
<guid>https://arxiv.org/abs/2512.08161</guid>
<content:encoded><![CDATA[
<div> Image dehazing, Fourier-RWKV, Multi-State Perception, Deformable Quad-directional Token Shift, Semantic Bridge Module<br /><br />Summary: The paper addresses the challenge of image dehazing under complex, non-uniform haze conditions that degrade visual perception. It introduces Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel framework employing a Multi-State Perception paradigm to model haze degradation comprehensively with linear computational complexity. The framework integrates three key perceptual states: (1) Spatial-form Perception, achieved through the Deformable Quad-directional Token Shift (DQ-Shift), which dynamically adjusts receptive fields to adapt locally to haze variations; (2) Frequency-domain Perception, realized in the Fourier Mix block by extending the RWKV-based WKV attention mechanism into the Fourier domain to maintain long-range dependencies while reducing spatial attenuation; and (3) Semantic-relation Perception, enabled by the Semantic Bridge Module (SBM) that uses Dynamic Semantic Kernel Fusion (DSK-Fusion) to align encoder-decoder features effectively and suppress reconstruction artifacts. Extensive experiments on various benchmarks show that Fourier-RWKV achieves state-of-the-art dehazing results across diverse haze scenarios. Notably, the method significantly lowers computational costs compared to typical Transformer-based approaches, striking a favorable balance between high restoration quality and real-time efficiency. The authors also provide code publicly at the linked repository for reproducibility and further research. <div>
arXiv:2512.08161v1 Announce Type: new 
Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators</title>
<link>https://arxiv.org/abs/2512.08163</link>
<guid>https://arxiv.org/abs/2512.08163</guid>
<content:encoded><![CDATA[
<div> Keywords: Monocular depth estimation, human perception, deep neural networks, model accuracy, KITTI dataset  

<br /><br />Summary:  
1. This study focuses on monocular depth estimation, which is crucial for applications like autonomous driving and robotics.  
2. Although deep neural networks have achieved superhuman accuracy on sensor-based benchmarks, aligning their representations with human perception remains a challenge.  
3. Prior research in object recognition has found a trade-off between accuracy and human-like behavior, prompting an investigation into whether a similar pattern exists for depth estimation, especially in outdoor natural scenes.  
4. The authors evaluated 69 monocular depth estimation models using the KITTI dataset and dissected error patterns through affine fitting to create interpretable components of prediction errors.  
5. Results showed that while humans and DNNs share some estimation biases reflected in positive error correlations, improvements in accuracy do not necessarily make model behavior more human-like.  
6. This divergence underlines the importance of developing new, multifaceted evaluation methods that focus on human-centric metrics beyond traditional accuracy measures for robust and interpretable monocular depth estimation models. <div>
arXiv:2512.08163v1 Announce Type: new 
Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoLoom: High-quality Geometric Diagram Generation from Textual Input</title>
<link>https://arxiv.org/abs/2512.08180</link>
<guid>https://arxiv.org/abs/2512.08180</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric diagram generation, autoformalization, GeoLingua, Monte Carlo optimization, constraint-based evaluation  

<br /><br />Summary: High-quality geometric diagram generation requires precise spatial accuracy and benefits from well-defined constraints, presenting unique challenges and opportunities. To address this, the paper introduces GeoLoom, a novel framework designed for generating geometric diagrams from natural language descriptions. GeoLoom consists of two main components: an autoformalization module that converts natural language into GeoLingua, a custom formal language tailored for diagram generation, and a coordinate solver that uses Monte Carlo optimization to compute exact diagram coordinates from formal constraints. To facilitate development and evaluation, the authors present GeoNF, a dataset that pairs natural language geometric problem statements with corresponding GeoLingua formalizations. Additionally, the work proposes a new constraint-based evaluation metric to mathematically quantify structural deviations in generated diagrams, enabling more principled and interpretable assessment and iterative refinement. Empirical evaluations demonstrate that GeoLoom significantly outperforms existing baseline methods in maintaining structural fidelity, offering a scalable and interpretable approach to text-to-diagram generation in geometric domains. These contributions collectively provide a strong foundation for future research bridging natural language understanding, formal reasoning, and precise geometric visualization. <div>
arXiv:2512.08180v1 Announce Type: new 
Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animal Re-Identification on Microcontrollers</title>
<link>https://arxiv.org/abs/2512.08198</link>
<guid>https://arxiv.org/abs/2512.08198</guid>
<content:encoded><![CDATA[
<div> Keywords: Animal Re-Identification, Microcontroller Units, MobileNetV2, On-device Inference, Wildlife Monitoring  

<br /><br />Summary:  
This work addresses the challenge of performing Animal Re-Identification (Animal Re-ID) directly on low-power, memory-constrained devices such as collar tags and microcontroller units (MCUs), essential for wildlife monitoring and precision livestock management in remote environments with limited connectivity. First, the authors analyze the performance gap between current state-of-the-art Animal Re-ID models and the capabilities of MCU-class hardware, revealing that traditional knowledge distillation from large teacher models offers limited improvements under strict constraints on memory and input resolution. Guided by these insights, they design a new, high-accuracy Animal Re-ID model by systematically scaling a MobileNetV2 backbone tailored for low-resolution inputs characteristic of MCU devices. The framework is validated on six public Animal Re-ID datasets, where the compact model achieves competitive retrieval accuracy while reducing the model size by more than 100 times compared to larger models. Additionally, a data-efficient fine-tuning strategy is proposed, enabling rapid adaptation to new environments with only three images per animal identity. On a self-collected cattle dataset, the system performs fully on-device inference with minimal accuracy loss and unchanged Top-1 accuracy compared to the cluster-based model. This demonstrates the practical feasibility of deploying adaptable, scalable Animal Re-ID solutions on MCU-class devices for real field applications. <div>
arXiv:2512.08198v1 Announce Type: new 
Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement</title>
<link>https://arxiv.org/abs/2512.08215</link>
<guid>https://arxiv.org/abs/2512.08215</guid>
<content:encoded><![CDATA[
<div> Keywords: human avatars, neural rendering, diffusion models, novel-view synthesis, 3D consistency<br /><br />Summary: The paper presents Blur2Sharp, an innovative framework designed to create lifelike human avatars with realistic pose variation and viewpoint flexibility. Traditional methods often produce images that lack geometric consistency across multiple views or suffer from low photorealism, leading to blurred outputs during complex motions and diverse viewing angles. Blur2Sharp addresses these challenges by integrating 3D-aware neural rendering via a Human NeRF model, which generates geometrically coherent multi-view images based on a single reference image, providing explicit 3D structural guidance. Following this, a diffusion model refines these renderings, enhancing image sharpness and detail preservation to maintain structural fidelity. The method further improves visual quality through hierarchical feature fusion that incorporates texture, normal vectors, and semantic priors extracted from parametric SMPL models. This combined approach enhances both global coherence and local detail accuracy in the synthesized images. Extensive experimental results show that Blur2Sharp outperforms existing state-of-the-art techniques in novel pose and novel view generation tasks. It is particularly robust in challenging situations that involve loose clothing and occlusions, demonstrating its efficacy and potential for advanced human avatar synthesis in computer vision and graphics applications. <div>
arXiv:2512.08215v1 Announce Type: new 
Abstract: The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisKnow: Constructing Visual Knowledge Base for Object Understanding</title>
<link>https://arxiv.org/abs/2512.08221</link>
<guid>https://arxiv.org/abs/2512.08221</guid>
<content:encoded><![CDATA[
<div> Visual Knowledge Base, Multi-modal data, Object understanding, AnimalKB, Knowledge graph completion<br /><br />Summary:<br /><br />1. The paper addresses the challenge of achieving in-depth object understanding in computer vision, moving beyond simple object recognition to comprehensively perceive object categories including components, appearance, relationships, and context.<br /><br />2. It emphasizes the need for systematic multi-modal data combining visual annotations (parts, attributes, co-occurrences) and textual knowledge to support advanced reasoning and question-answering tasks.<br /><br />3. The authors propose a Visual Knowledge Base structured as graphs, along with a construction framework named VisKnow, which extracts and integrates multi-modal, object-level knowledge from images and text, leveraging expert design and large-scale model applications.<br /><br />4. As a case study, they build AnimalKB, a visual knowledge base containing 406 animal categories, 22K textual knowledge triplets from encyclopedic texts, 420K images, and detailed region annotations for objects and parts.<br /><br />5. Experimental results demonstrate AnimalKB’s utility for improving zero-shot recognition, fine-grained visual question answering (VQA), and serving as a benchmark for knowledge graph completion and part segmentation, highlighting the potential of automated visual knowledge bases for enhancing visual understanding and practical applications. <div>
arXiv:2512.08221v1 Announce Type: new 
Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.08223</link>
<guid>https://arxiv.org/abs/2512.08223</guid>
<content:encoded><![CDATA[
<div> 3D object detection, prompt tuning, large language models, transfer learning, Scene-Oriented Prompt Pool (SOP²)  

<br /><br />Summary:  
This paper explores the application of prompt tuning methods, typically used in Natural Language Processing (NLP), within the field of 3D object detection. It investigates if a foundation model trained on a large-scale dataset like Waymo can be effectively adapted to various 3D detection scenarios using minimal parameter adjustments. The study specifically analyzes the impact of prompt tokens and prompt generators on model performance in 3D object detection tasks. Furthermore, the authors introduce a novel concept called the Scene-Oriented Prompt Pool (SOP²), designed to enhance the adaptability and effectiveness of prompt pools within 3D environments. The proposed SOP² demonstrates promising results, showing improvements in model tuning without requiring extensive retraining. By bridging techniques from language model tuning to 3D detection, the work highlights the strong potential for prompt-based methods to advance performance in this field. Ultimately, the paper aims to inspire further research into prompt tuning strategies tailored for complex 3D data, encouraging deeper exploration of how prompts can serve as a powerful tool for efficient and flexible adaptation of large-scale 3D detection models. <div>
arXiv:2512.08223v1 Announce Type: new 
Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New VVC profiles targeting Feature Coding for Machines</title>
<link>https://arxiv.org/abs/2512.08227</link>
<guid>https://arxiv.org/abs/2512.08227</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.08227v1  

Keywords: Versatile Video Coding, Feature Compression, MPEG-AI, Split Inference, BD-Rate  

<br /><br />Summary:  
1. Traditional video codecs are optimized for preserving perceptual quality based on human visual system models, but these approaches are not suitable for compressing intermediate neural network features in split inference systems.  
2. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity metrics irrelevant for compression evaluation.  
3. The paper investigates the application of Versatile Video Coding (VVC) to compress intermediate features under the MPEG-AI Feature Coding for Machines (FCM) standard.  
4. A detailed tool-level analysis is conducted to assess how individual VVC coding components affect compression efficiency and the accuracy of downstream vision tasks.  
5. Leveraging these insights, the authors propose three lightweight VVC profiles—Fast, Faster, and Fastest—tailored for feature compression:  
   - Fast profile yields a 2.96% BD-Rate improvement and decreases encoding time by 21.8%.  
   - Faster profile achieves a 1.85% BD-Rate improvement with a 51.5% speedup in encoding.  
   - Fastest profile reduces encoding time drastically by 95.6%, incurring only a 1.71% loss in BD-Rate.  
This work provides practical and efficient coding profiles for encoding neural network intermediate features effectively in machine-centric video coding scenarios. <div>
arXiv:2512.08227v1 Announce Type: new 
Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models</title>
<link>https://arxiv.org/abs/2512.08228</link>
<guid>https://arxiv.org/abs/2512.08228</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought, visual grounding, logical coherence, multimodal models, benchmark<br /><br />Summary:<br /><br />1. The paper introduces MM-CoT, a new diagnostic benchmark designed to evaluate Chain-of-Thought (CoT) reasoning in multimodal models (MMs) by focusing on both visual grounding and logical coherence rather than just generative capabilities.<br /><br />2. MM-CoT requires models to select a single event chain that meets two orthogonal constraints: visual consistency, ensuring each reasoning step is based on observable evidence, and logical coherence, ensuring causal and commonsense validity.<br /><br />3. The benchmark incorporates adversarial distractors that violate either visual consistency or logical coherence, enabling precise identification of reasoning failures in vision-language models.<br /><br />4. Evaluation of leading vision-language models on MM-CoT reveals that even state-of-the-art systems struggle, highlighting a significant gap between generative fluency and actual reasoning fidelity.<br /><br />5. MM-CoT exhibits low correlation with existing benchmarks, underscoring its unique ability to measure the combined challenge of visual grounding and logical reasoning, thereby providing a foundation for developing more faithful and coherent multimodal reasoning systems. <div>
arXiv:2512.08228v1 Announce Type: new 
Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems</title>
<link>https://arxiv.org/abs/2512.08229</link>
<guid>https://arxiv.org/abs/2512.08229</guid>
<content:encoded><![CDATA[
<div> Keywords: depth completion, RGB-D sensors, sparse depth sampling, surface normals, geometry-aware sampling<br /><br />Summary:<br />1. Accurate 3D perception is crucial for industrial robotic tasks such as manipulation, inspection, and navigation, yet existing RGB-D and stereo sensors often produce noisy, incomplete, or biased depth maps. <br />2. Depth completion techniques aim to generate dense and reliable depth maps by combining sparse depth data with RGB images; however, current pipelines typically use unrealistic sparse depth sampling methods that assume uniform randomness rather than reflecting real sensor behaviors. <br />3. This work introduces a novel normal-guided sparse depth sampling strategy that uses PCA-based surface normal estimation from the RGB-D point cloud to create a per-pixel depth reliability measure. <br />4. Using this geometry-aware reliability distribution, the sparse depth samples are drawn more realistically, better replicating sensor-specific reliability patterns influenced by surface geometry. <br />5. The proposed method is integrated with the Marigold-DC diffusion-based depth completion model and evaluated on the NYU Depth v2 dataset, demonstrating improved accuracy, reduction of artifacts near edges and discontinuities, and more realistic training conditions that reflect actual sensor behavior. <div>
arXiv:2512.08229v1 Announce Type: new 
Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastBEV++: Fast by Algorithm, Deployable by Design</title>
<link>https://arxiv.org/abs/2512.08237</link>
<guid>https://arxiv.org/abs/2512.08237</guid>
<content:encoded><![CDATA[
<div> FastBEV++, Bird's-Eye-View, view transformation, depth-aware fusion, real-time performance<br /><br />Summary:<br /><br />1. FastBEV++ addresses the challenge in camera-only Bird's-Eye-View (BEV) perception, balancing high performance with ease of deployment on vehicles by overcoming reliance on costly view transformations and specialized platform-specific kernels.<br /><br />2. The framework introduces a novel "Deployable by Design" view transformation method that breaks down complex projections into a standard Index-Gather-Reshape pipeline, executed solely with basic native operators like gather and matrix multiplication.<br /><br />3. This approach removes the requirement for custom CUDA kernels, enabling full compatibility with TensorRT and making the system highly portable across platforms.<br /><br />4. On the algorithmic side, FastBEV++ integrates a depth-aware fusion mechanism that is jointly learned, supported by temporal aggregation and robust data augmentation, enhancing the geometric accuracy of BEV representations.<br /><br />5. Validations on the nuScenes benchmark demonstrate that FastBEV++ achieves a new state-of-the-art NDS score of 0.359 and real-time speeds exceeding 134 FPS on automotive-grade hardware such as Tesla T4, making it a mature, scalable, and efficient solution for production-level autonomous systems. <div>
arXiv:2512.08237v1 Announce Type: new 
Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridToken-VLM: Hybrid Token Compression for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08240</link>
<guid>https://arxiv.org/abs/2512.08240</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, hybrid compression, semantic disentanglement, MGVQ quantization, voco token<br /><br />Summary:<br /><br />1. Vision-language models (VLMs) face computational challenges due to the quadratic cost of processing hundreds of visual patch tokens when integrated with large language models (LLMs), resulting in memory strain and limited context windows.<br /><br />2. Existing compression strategies either use continuous compression, which reduces semantic clarity like object identities, or discrete quantization, which loses fine-grained visual details such as textures.<br /><br />3. The proposed HTC-VLM framework introduces a hybrid dual-channel approach that separates semantic information and appearance: a continuous pathway processes fine-grained ViT patches, while a discrete pathway uses multi-granularity vector quantization (MGVQ) to produce symbolic anchors represented by four tokens.<br /><br />4. These dual representations combine into a 580-token hybrid sequence and are further compressed into a single voco token, enabled by a disentanglement attention mask and bottleneck mechanism, which balances efficiency and grounded semantic representation.<br /><br />5. HTC-VLM demonstrates superior performance with an average retention of 87.2% across seven vision-language benchmarks, outperforming continuous baseline methods at 81.0%, while achieving a 580-to-1 compression ratio. Attention studies confirm the voco token favors discrete semantic anchors, validating the hybrid framework’s effectiveness in resolving the trade-off between efficiency and fidelity for scalable VLMs. <div>
arXiv:2512.08240v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI</title>
<link>https://arxiv.org/abs/2512.08243</link>
<guid>https://arxiv.org/abs/2512.08243</guid>
<content:encoded><![CDATA[
<div> Keywords: Residual-SwinCA-Net, breast lesion segmentation, Swin Transformer, Multi-Scale Channel Attention, BUSI dataset<br /><br />Summary:  
This study proposes a novel deep hybrid segmentation framework called Residual-SwinCA-Net designed for breast lesion segmentation in ultrasound images. The framework integrates locally correlated feature extraction through residual CNN modules and global dependency learning via customized Swin Transformer blocks with internal residual pathways, which improve gradient stability and facilitate global feature fusion. To enhance tissue continuity, suppress ultrasound noise, and emphasize fine structural transitions, a Laplacian-of-Gaussian regional operator is applied, along with a boundary-oriented operator to maintain the morphological integrity of malignant lesion contours. The model employs a stage-wise contraction strategy by progressively reducing feature maps to capture scale invariance and improve robustness to structural variability. Each decoder level incorporates a Multi-Scale Channel Attention and Squeezing (MSCAS) module that selectively emphasizes salient encoder features, retains discriminative global and complementary local information, and suppresses redundant activations efficiently. Finally, a Pixel-Attention module adaptively weighs malignant lesion pixels to encode class-relevant spatial cues while reducing background noise interference. The Residual-SwinCA-Net was tested on the publicly available BUSI dataset, outperforming existing CNN and ViT models with a mean accuracy of 99.29%, IoU of 98.74%, and Dice score of 0.9041. The framework significantly enhances lesion diagnostic performance, aiding timely and accurate clinical decision-making. <div>
arXiv:2512.08243v1 Announce Type: new 
Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.08247</link>
<guid>https://arxiv.org/abs/2512.08247</guid>
<content:encoded><![CDATA[
<div> Future Temporal Knowledge Distillation, 3D Object Detection, Autonomous Driving, Knowledge Distillation, NuScenes Dataset<br /><br />Summary: This paper addresses the challenge of improving online camera-based temporal 3D object detection for autonomous driving by transferring knowledge from offline models that utilize future frames. Existing knowledge distillation (KD) methods mainly focus on spatial features or temporal relations but fail to effectively incorporate future frame information due to strict frame alignment requirements. To overcome this, the authors propose Future Temporal Knowledge Distillation (FTKD), a sparse query-based framework that allows an online student model to learn from an offline teacher model’s future frame features without needing strict alignment. FTKD incorporates a future-aware feature reconstruction strategy that encourages capturing future information and introduces future-guided logit distillation to exploit the teacher model’s stable foreground and background context. The method is compatible with two state-of-the-art 3D object detection baselines and demonstrates significant improvements, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset. Additionally, FTKD improves velocity estimation accuracy while maintaining the same inference cost, making it a practical solution for real-time autonomous driving systems. <div>
arXiv:2512.08247v1 Announce Type: new 
Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.08253</link>
<guid>https://arxiv.org/abs/2512.08253</guid>
<content:encoded><![CDATA[
<div> Few-shot learning, 3D point cloud, semantic segmentation, prototype learning, contrastive loss<br /><br />Summary:<br /><br />Few-shot 3D point cloud semantic segmentation (FS-3DSeg) focuses on segmenting new classes using only a few labeled examples. Existing prototype learning methods typically generate class prototypes solely from the support set, which risks prototype bias due to insufficient consideration of the query data distribution. This bias causes prototypes to overfit support set features and reduces segmentation accuracy under distribution shifts between support and query sets. To address this, the paper introduces a Query-aware Hub Prototype (QHP) learning method that models semantic correlations between support and query samples explicitly. The method includes a Hub Prototype Generation (HPG) module that constructs a bipartite graph linking support and query points, identifies frequently connected support hubs, and produces prototypes aligned with query semantics. Additionally, the Prototype Distribution Optimization (PDO) module refines prototypes by leveraging a purity-reweighted contrastive loss, which pulls ambiguous prototypes and poor hubs closer to class centers, mitigating noisy or outlier influences. Experimental validation on the S3DIS and ScanNet datasets demonstrates that QHP significantly outperforms state-of-the-art methods, effectively reducing the semantic gap between support-based prototypes and the query distribution, thereby enhancing few-shot segmentation performance in 3D point clouds. <div>
arXiv:2512.08253v1 Announce Type: new 
Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFP: Real-World Scene Recovery Using Spatial and Frequency Priors</title>
<link>https://arxiv.org/abs/2512.08254</link>
<guid>https://arxiv.org/abs/2512.08254</guid>
<content:encoded><![CDATA[
<div> Spatial prior, Frequency prior, Scene recovery, Image degradation, Weighted fusion<br /><br />Summary: This paper addresses the challenge of real-world scene recovery by proposing a novel method called Spatial and Frequency Priors (SFP). First, it identifies limitations in existing approaches that either use a single prior—insufficient for handling multiple degradations—or rely on complex networks trained on synthetic data that struggle to generalize to diverse real scenarios. Second, in the spatial domain, the authors discover that the inverse of a degraded image projects along its spectral direction in a manner resembling scene transmission. Leveraging this spatial prior, they estimate the transmission map to recover scenes affected by scattering degradation. Third, in the frequency domain, a mask is constructed for adaptive frequency enhancement, guided by two novel priors: one assumes that the mean intensity of the DC components across color channels in the degraded image approximates that of the clear image, while the other observes that low radial frequencies below 0.001 typically make up about 1% of the total spectral magnitude in clear images. Fourth, the method integrates spatial restoration, frequency enhancement, and salient input features through a weighted fusion strategy to produce the final recovered image. Lastly, extensive experiments demonstrate that SFP consistently outperforms existing techniques under various degradation conditions, proving its effectiveness and generalizability in real-world scene recovery tasks. <div>
arXiv:2512.08254v1 Announce Type: new 
Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera</title>
<link>https://arxiv.org/abs/2512.08262</link>
<guid>https://arxiv.org/abs/2512.08262</guid>
<content:encoded><![CDATA[
<div> Keywords: extrinsic calibration, LiDAR, RADAR, camera sensors, deep learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurate extrinsic calibration of LiDAR, RADAR, and camera sensors, which is crucial for reliable perception in autonomous vehicles.<br />2. It identifies that current calibration methods struggle due to mechanical vibrations and cumulative sensor drift in dynamic environments.<br />3. The authors propose RLCNet, a novel end-to-end trainable deep learning framework designed for simultaneous online calibration of multimodal sensors.<br />4. RLCNet is validated on real-world datasets and is optimized for practical deployment, showing robust performance across diverse environments.<br />5. To support real-time operation, the paper introduces an online calibration framework featuring a weighted moving average and outlier rejection to dynamically adjust calibration parameters.<br />6. This framework reduces prediction noise and improves resilience against sensor drift.<br />7. An ablation study is performed to demonstrate the importance of the architectural choices in the proposed network.<br />8. The results indicate that RLCNet outperforms existing methods in terms of accuracy and robustness, making it a superior approach for multimodal sensor calibration in autonomous vehicles. <div>
arXiv:2512.08262v1 Announce Type: new 
Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoX: Egocentric Video Generation from a Single Exocentric Video</title>
<link>https://arxiv.org/abs/2512.08269</link>
<guid>https://arxiv.org/abs/2512.08269</guid>
<content:encoded><![CDATA[
<div> Egocentric perception, exocentric video, video diffusion models, geometry-guided self-attention, egocentric video generation  

<br /><br />Summary:  
This paper introduces EgoX, a novel framework designed to convert exocentric (third-person) videos into egocentric (first-person) videos, addressing the challenges of extreme camera pose variations and minimal view overlap. EgoX builds on pretrained large-scale spatio-temporal video diffusion models, adapting them using lightweight LoRA techniques to effectively generate egocentric views from a single exocentric input. The framework employs a unified conditioning strategy that merges exocentric and egocentric priors through width and channel-wise concatenation, facilitating a more coherent transformation between perspectives. To maintain geometric consistency and enhance visual fidelity, EgoX incorporates a geometry-guided self-attention mechanism that focuses attention on spatially relevant regions of the scene. Experimental results demonstrate that EgoX produces visually realistic and geometrically coherent egocentric videos even when applied to unseen and in-the-wild videos, highlighting its robustness and scalability. This work advances immersive video understanding by enabling faithful content preservation and plausible synthesis of unseen areas, opening new avenues for egocentric video generation from exocentric inputs. <div>
arXiv:2512.08269v1 Announce Type: new 
Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAVAS: Physics-Aware Video-to-Audio Synthesis</title>
<link>https://arxiv.org/abs/2512.08282</link>
<guid>https://arxiv.org/abs/2512.08282</guid>
<content:encoded><![CDATA[
<div> Keywords: Video-to-Audio synthesis, physical reasoning, latent diffusion, object interaction, Audio-Physics Correlation Coefficient (APCC)  

<br /><br />Summary:  
This paper introduces Physics-Aware Video-to-Audio Synthesis (PAVAS), a novel approach that integrates physical reasoning into video-to-audio (V2A) generation. Unlike previous models that rely mainly on appearance-driven correlations, PAVAS incorporates physical parameters to generate more realistic sounds. The method employs a Physics-Driven Audio Adapter (Phy-Adapter) that utilizes object-level physical information, such as mass and velocity, estimated by the Physical Parameter Estimator (PPE). The PPE leverages a Vision-Language Model (VLM) to infer object mass and combines this with a segmentation-based dynamic 3D reconstruction module to recover motion trajectories and compute velocity. These physical inputs help the model produce audio that better reflects real-world sound generation processes. To evaluate physical realism, the authors create VGG-Impact, a new benchmark dataset focused on object-object interactions, and propose the Audio-Physics Correlation Coefficient (APCC) metric to quantify the alignment between physical parameters and generated audio. Extensive experiments demonstrate that PAVAS outperforms current state-of-the-art V2A methods, achieving superior physical plausibility and perceptual coherence in the synthesized sounds. Additional demo videos and resources are available on the project website. <div>
arXiv:2512.08282v1 Announce Type: new 
Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</title>
<link>https://arxiv.org/abs/2512.08294</link>
<guid>https://arxiv.org/abs/2512.08294</guid>
<content:encoded><![CDATA[
<div> Keywords: subject-driven generation, identity fidelity, video-derived dataset, vision-language models, image manipulation<br /><br />Summary:<br /><br />1. The paper addresses challenges in subject-driven image generation where current models often fail to maintain accurate reference identities and struggle with scenes containing multiple subjects.<br />2. To overcome these limitations, the authors introduce OpenSubject, a large-scale video-derived dataset featuring 2.5 million samples and 4.35 million images specifically designed for subject-driven generation and manipulation tasks.<br />3. The dataset construction involves a four-stage pipeline: (i) Video Curation through resolution and aesthetic filtering to select high-quality clips, (ii) Cross-Frame Subject Mining and Pairing using vision-language models (VLMs) to ensure category consensus and diversity-aware pairing of image pairs, (iii) Identity-Preserving Reference Image Synthesis deploying segmentation map-guided outpainting and box-guided inpainting with geometry-aware augmentations to retain subject identity, and (iv) Verification and Captioning where a VLM validates the synthesized samples, re-synthesizes failed ones, and generates descriptive captions.<br />4. The authors also propose a benchmark that evaluates multiple aspects including identity fidelity, prompt adherence, manipulation consistency, and background consistency, all assessed with a VLM-based judge.<br />5. Extensive experiments demonstrate that training models with OpenSubject significantly improves the performance of subject-driven generation and manipulation, especially in complex multi-subject scenes. <div>
arXiv:2512.08294v1 Announce Type: new 
Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation</title>
<link>https://arxiv.org/abs/2512.08309</link>
<guid>https://arxiv.org/abs/2512.08309</guid>
<content:encoded><![CDATA[
<div> Procedural Noise, Diffusion Models, Infinite Generation, Terrain Synthesis, Planetary Scale  

<br /><br />Summary: This article introduces Terrain Diffusion, a novel AI-driven approach that advances traditional procedural noise functions like Perlin noise by leveraging diffusion models for enhanced realism and large-scale coherence in terrain generation. Terrain Diffusion preserves the critical advantages of procedural noise—seamless infinite extent, seed-consistency, and constant-time random access—while addressing its limitations in detail and fidelity. At the heart of the method is InfiniteDiffusion, an innovative algorithm enabling real-time, seamless, and infinite generation of landscapes. The approach uses a hierarchical arrangement of diffusion models to integrate broad planetary context with intricate local terrain features. A compact Laplacian encoding system is employed to maintain output stability across Earth-scale dynamic ranges, ensuring consistent quality at varying scales. Additionally, an open-source infinite-tensor framework supports efficient manipulation of boundless data structures using constant memory, facilitating practical implementation. Few-step consistency distillation further enhances generation efficiency, reducing computational overhead. Collectively, these advancements position diffusion models as a practical and scalable foundation for procedural world generation, capable of coherently and controllably synthesizing entire planets without spatial limitations, making it a significant step forward for applications in gaming, simulation, and virtual world creation. <div>
arXiv:2512.08309v1 Announce Type: new 
Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDM: Geometry-aware Distribution Matching for Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.08317</link>
<guid>https://arxiv.org/abs/2512.08317</guid>
<content:encoded><![CDATA[
<div> Dataset Distillation, Geometry-aware, Distribution Matching, Manifold Learning, Optimal Transport<br /><br />Summary:<br /><br />1. The paper addresses dataset distillation, which aims to create a small synthetic subset of data that enables training models with performance comparable to those trained on the full dataset. 2. Existing distribution-matching methods focus on Euclidean spaces, thus only capturing linear data structures and ignoring important intrinsic geometries such as curvature that real-world high-dimensional data often exhibit. 3. The authors propose GeoDM, a novel geometry-aware distribution-matching framework that models data in a Cartesian product of Euclidean, hyperbolic, and spherical manifolds, capturing flat, hierarchical, and cyclical data structures within a unified framework. 4. GeoDM introduces learnable curvature and weighting parameters for each geometry type, allowing the model to adaptively fit the underlying data manifold geometry during distillation. 5. An optimal transport loss is designed to improve the fidelity of the matched distributions, and theoretical analysis demonstrates that geometry-aware matching across product spaces can achieve tighter generalization error bounds compared to purely Euclidean approaches. 6. Extensive experiments on standard benchmarks verify that GeoDM outperforms state-of-the-art dataset distillation methods and is robust across different distribution-matching strategies and geometric settings. <div>
arXiv:2512.08317v1 Announce Type: new 
Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge</title>
<link>https://arxiv.org/abs/2512.08323</link>
<guid>https://arxiv.org/abs/2512.08323</guid>
<content:encoded><![CDATA[
<div> Teeth landmark detection, 3D intraoral scans, deep learning, orthodontics, MICCAI challenge<br /><br />Summary:  
Teeth landmark detection plays a crucial role in clinical orthodontics by enabling accurate diagnostics, personalized treatment planning, and effective monitoring of dental treatments. The task is complicated by the complex geometry of individual teeth and significant inter-personal variation, making it challenging to achieve precise identification. To overcome these difficulties, the use of advanced techniques such as deep learning is essential for reliable detection of 3D tooth landmarks. Addressing this need, the 3DTeethLand challenge was organized in conjunction with the MICCAI 2024 conference. This challenge aimed to foster the development of algorithms capable of accurately detecting teeth landmarks from intraoral 3D scans. A notable contribution of the challenge is the introduction of the first publicly available dataset dedicated to 3D teeth landmark detection. This dataset provides a valuable benchmark for evaluating the effectiveness of state-of-the-art methods in this domain. Furthermore, the challenge serves to encourage the research community to contribute innovative methodologies that can help address this clinically significant problem, ultimately improving outcomes in dental care through enhanced diagnostic and treatment capabilities. <div>
arXiv:2512.08323v1 Announce Type: new 
Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification</title>
<link>https://arxiv.org/abs/2512.08325</link>
<guid>https://arxiv.org/abs/2512.08325</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Motion Magnification, Diffusion Model, Optical Flow, Noise Suppression, Motion Amplification<br /><br />Summary:<br /><br />1. The paper introduces GeoDiffMM, a novel diffusion-based Lagrangian framework for Video Motion Magnification (VMM) that uses optical flow as a geometric cue to enable structurally consistent motion amplification.<br />2. Existing Eulerian VMM approaches face challenges in separating photon noise from true micro-motions when displacements are very small; GeoDiffMM addresses these challenges by explicitly integrating geometry-aware optical flow.<br />3. The authors propose a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise, providing supervision that improves geometry accuracy and generalization of the optical flow estimation.<br />4. GeoDiffMM employs a Diffusion Motion Magnifier, which conditions the denoising process on optical flow as a geometry prior and a learnable magnification factor, selectively amplifying motion components aligned with scene semantics while suppressing irrelevant noise.<br />5. Finally, the framework includes a Flow-based Video Synthesis module that accurately maps the amplified motion back to the image domain, producing high-fidelity outputs.<br />Extensive experiments on both real and synthetic datasets demonstrate that GeoDiffMM outperforms state-of-the-art VMM methods by significantly enhancing motion magnification quality, especially in low-displacement and noise-prone scenarios. <div>
arXiv:2512.08325v1 Announce Type: new 
Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low Rank Support Quaternion Matrix Machine</title>
<link>https://arxiv.org/abs/2512.08327</link>
<guid>https://arxiv.org/abs/2512.08327</guid>
<content:encoded><![CDATA[
<div> Keywords: Quaternion, Low-rank, Support Quaternion Matrix Machine, Color image classification, ADMM<br /><br />Summary:<br /><br />This article introduces a novel method for color image classification named Low-rank Support Quaternion Matrix Machine (LSQMM), which leverages quaternion algebra to treat RGB channels as pure quaternions, thereby preserving intrinsic inter-channel relationships. Unlike conventional approaches that represent input features as vectors, matrices, or third-order tensors, LSQMM models color images in the quaternion domain to effectively utilize channel coupling. To promote low-rank structures arising from strongly correlated color channels, the authors incorporate a quaternion nuclear norm regularization term, extending the classical matrix nuclear norm concept to quaternions. The classification model is formulated as a hinge loss optimization problem augmented by this quaternion nuclear norm regularizer. To solve the resulting optimization, an iterative algorithm based on the Alternating Direction Method of Multipliers (ADMM) is proposed, tailored for quaternion optimization challenges. The method is evaluated on multiple color image classification datasets, demonstrating superior performance in classification accuracy, robustness against noise and variations, and computational efficiency. Comparative experiments against state-of-the-art classifiers—including support vector machines (SVMs), support matrix machines (SMMs), and support tensor machines (STMs)—highlight the advantages of LSQMM, suggesting its potential as an effective tool for practical color image classification tasks. <div>
arXiv:2512.08327v1 Announce Type: new 
Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08329</link>
<guid>https://arxiv.org/abs/2512.08329</guid>
<content:encoded><![CDATA[
<div> Keywords: image protection, adversarial perturbations, feature-space analysis, frequency-domain, generative models<br /><br />Summary:<br /><br />This paper investigates recent image protection techniques like Glaze and Nightshade, which add subtle adversarial perturbations to thwart text-to-image generative models. While these methods are empirically effective, the study aims to understand their internal structure, detectability, and how they affect image representations. Using a combined white-box and black-box analytical framework, the authors perform latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization. They find that these image protection perturbations are structured, low-entropy signals closely tied to the original image content across representational, spatial, and spectral dimensions. Rather than causing a broad shift in image representation, protected images maintain their content-driven feature organization with distinct protection-specific substructures. Detectability of these perturbations depends on entropy, spatial placement, and frequency alignment, and stacking protections increases the detectable structure. Frequency-domain analysis reveals that Glaze and Nightshade reshape energy along image-aligned frequency axes instead of adding indiscriminate noise. Overall, the study demonstrates that image protection operates via structured feature-level changes instead of overt semantic shifts, explaining how these protective signals remain visually imperceptible yet systematically identifiable. This work enhances the interpretability of adversarial image protections and guides future development of defenses and detection methods for generative AI systems. <div>
arXiv:2512.08329v1 Announce Type: new 
Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08330</link>
<guid>https://arxiv.org/abs/2512.08330</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D representation learning, diffusion model, contrastive learning, self-supervised learning, PointDico  

<br /><br />Summary:  
1. The paper addresses the challenges of self-supervised 3D representation learning caused by the unordered nature and uneven density of 3D data, which limits the effectiveness of existing contrastive and generative methods.  
2. It identifies that contrastive models are prone to overfitting, while 3D Masked Autoencoders face difficulties in processing unordered point clouds.  
3. To overcome these issues, the authors propose PointDico, a novel framework that integrates diffusion-based generative modeling and contrastive learning via knowledge distillation, where the diffusion model guides the contrastive model.  
4. A hierarchical pyramid conditional generator is introduced in PointDico to extract geometric features at multiple scales, capturing detailed and structural information effectively.  
5. The model uses a dual-channel design to merge local and global contextual features, enhancing the overall representation quality.  
6. Experimentally, PointDico sets a new state-of-the-art in 3D representation learning benchmarks with 94.32% accuracy on ScanObjectNN and 86.5% instance mIoU on ShapeNetPart, demonstrating its superior capability in understanding 3D point cloud data. <div>
arXiv:2512.08330v1 Announce Type: new 
Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening</title>
<link>https://arxiv.org/abs/2512.08331</link>
<guid>https://arxiv.org/abs/2512.08331</guid>
<content:encoded><![CDATA[
<div> Pansharpening, adaptive convolution, Bi^2MAC, remote sensing, computational efficiency<br /><br />Summary:<br /><br />Pansharpening is a process that fuses a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral (HRMS) image. Traditional deep learning methods for this task struggle to adapt effectively to regional heterogeneity within feature representations. Existing adaptive convolution approaches try to address this but often result in high computational costs and limited effectiveness in capturing heterogeneous regions. To tackle these issues, the authors propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which leverages different region types while optimally allocating computational resources. Bi^2MAC includes a lightweight module that generates both soft and hard masks; soft masks preliminarily modulate input features, while hard masks guide separated processing branches for different region types. Less important features flow through a compact branch for efficient global processing, whereas complex heterogeneous features are handled by a focused branch that applies more computational power for detailed modeling. Extensive experiments across multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art performance, significantly reduces training time and parameter counts, and maintains the lowest computational cost among adaptive convolution models for pansharpening tasks. <div>
arXiv:2512.08331v1 Announce Type: new 
Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</title>
<link>https://arxiv.org/abs/2512.08334</link>
<guid>https://arxiv.org/abs/2512.08334</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid Splatting, Gaussian primitives, reflection baking, rendering speed, memory optimization<br /><br />Summary: This paper introduces HybridSplat, a novel Hybrid Splatting mechanism designed to improve the rendering of complex reflections in real-world scenes using 3D Gaussian splatting. The core innovation is the reflection-baked Gaussian tracing technique, which integrates view-dependent reflections directly within each Gaussian primitive. This approach utilizes tile-based Gaussian splatting for efficient reflection rendering. HybridSplat unifies reflective Gaussian primitives with base primitives into a hybrid framework, enabling high-fidelity scene reconstruction. Additionally, the authors present a pipeline-level acceleration method along with reflection-sensitive Gaussian pruning to significantly reduce model size. These strategies collectively enhance rendering speed and decrease memory consumption without compromising reflection quality. Extensive evaluations demonstrate that HybridSplat achieves approximately a 7x increase in rendering speed on complex reflective scenes from datasets like Ref-NeRF and NeRF-Casting. Furthermore, it requires four times fewer Gaussian primitives compared to similar ray-tracing-based Gaussian splatting techniques. Overall, HybridSplat establishes a new state-of-the-art performance benchmark in photorealistic novel view synthesis for challenging reflective environments by balancing efficiency, memory usage, and visual fidelity. <div>
arXiv:2512.08334v1 Announce Type: new 
Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation</title>
<link>https://arxiv.org/abs/2512.08337</link>
<guid>https://arxiv.org/abs/2512.08337</guid>
<content:encoded><![CDATA[
<div> Keywords: BOLD image generation, T1-weighted MRI, DINOv3, multi-slice attention, self-supervised learning<br /><br />Summary:  
The study addresses the challenge of generating BOLD (Blood Oxygen Level Dependent) images from T1-weighted (T1w) MRI scans, which is valuable for recovering missing or corrupted BOLD data and facilitating subsequent analysis. The authors introduce DINO-BOLDNet, an innovative framework guided by DINOv3, a self-supervised transformer model. This framework combines a frozen DINOv3 encoder that captures detailed within-slice structural features with a lightweight, trainable decoder. A dedicated slice-attention module integrates contextual information across adjacent slices to enhance spatial consistency. Subsequently, a multi-scale decoder restores detailed functional contrasts in the generated images. The model employs a perceptual loss based on DINO features to ensure structural and textural fidelity between the predicted and ground-truth BOLD images in the transformer feature space. Experiments conducted on a clinical dataset of 248 subjects demonstrate that DINO-BOLDNet outperforms a conditional GAN baseline in terms of PSNR (Peak Signal-to-Noise Ratio) and MS-SSIM (Multi-Scale Structural Similarity Index). This work represents the first successful attempt to directly generate mean BOLD images from T1w images, showcasing the promise of self-supervised transformer guidance in bridging structural to functional MRI representations. <div>
arXiv:2512.08337v1 Announce Type: new 
Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels</title>
<link>https://arxiv.org/abs/2512.08358</link>
<guid>https://arxiv.org/abs/2512.08358</guid>
<content:encoded><![CDATA[
<div> Monocular 3D tracking, dense 3D tracking, world-centric coordinate system, tracking upsampler, camera pose estimation<br /><br />Summary:<br /><br />This paper addresses the challenges in monocular 3D tracking, which aims to capture long-term 3D pixel motion from single-camera videos. Existing methods struggle to separate camera movement from foreground dynamic objects and cannot effectively track newly appearing dynamic subjects densely. To overcome these limitations, the authors propose TrackingWorld, a novel framework that enables dense 3D tracking of nearly all pixels within a world-centric coordinate system. The approach introduces a tracking upsampler that converts sparse 2D tracks into dense 2D tracks efficiently. Furthermore, to handle newly emerging objects, the upsampler is applied across all frames, with redundant tracks in overlapping areas removed to maintain compactness. Finally, the system employs an optimization-based framework to convert the dense 2D tracks into 3D trajectories by jointly estimating camera poses and 3D point coordinates. Evaluations on synthetic and real-world datasets demonstrate the method achieves accurate and dense 3D tracking within a consistent world-centric frame, outperforming existing approaches in handling dynamic scenes and camera motion separation. <div>
arXiv:2512.08358v1 Announce Type: new 
Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation</title>
<link>https://arxiv.org/abs/2512.08362</link>
<guid>https://arxiv.org/abs/2512.08362</guid>
<content:encoded><![CDATA[
<div> Keywords: fire detection, SCU-CGAN, U-Net, CBAM, dataset augmentation

<br /><br />Summary: This paper addresses the challenge of limited fire image datasets for improving household fire detection systems. The authors propose SCU-CGAN, a novel generative adversarial network combining U-Net architecture, Convolutional Block Attention Module (CBAM), and an additional discriminator, designed to generate realistic fire images from non-fire images. The model's performance is evaluated and shown to outperform existing generative models, specifically achieving a 41.5% improvement in the Kernel Inception Distance (KID) score compared to CycleGAN, indicating superior image quality. The augmented dataset produced by SCU-CGAN is then utilized to enhance fire detection model accuracy without modifying their structures. Experiments with the YOLOv5 nano fire detection model demonstrate significant gains, particularly a 56.5% increase in mean average precision (mAP@0.5:0.95), underscoring the effectiveness of the augmented data. This approach highlights the potential of GAN-based data augmentation in overcoming dataset scarcity, ultimately helping to improve early fire detection capabilities in IoT-enabled household environments. <div>
arXiv:2512.08362v1 Announce Type: new 
Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss</title>
<link>https://arxiv.org/abs/2512.08374</link>
<guid>https://arxiv.org/abs/2512.08374</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Pre-Norm architecture, norm disparity, asymmetric update dynamic, LayerNorm correction<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) integrate pre-trained vision encoders with language models, enabling advanced multimodal understanding. 2. The common Pre-Norm architecture in these models causes a significant norm disparity where visual tokens have much higher norms compared to textual tokens. 3. This disparity leads to an asymmetric update dynamic, wherein high-norm visual tokens experience “representational inertia,” updating and adapting more slowly than text tokens during training. 4. This imbalance hampers effective cross-modal feature fusion, reducing the overall performance of MLLMs. 5. The authors provide a formal theoretical analysis of this phenomenon and empirically verify that norm disparity and asymmetric updates persist across various mainstream MLLMs. 6. To address the issue, they propose a simple fix: adding a carefully initialized LayerNorm layer immediately after the visual projector to align the norms of visual and text tokens. 7. Experiments on the LLaVA-1.5 architecture demonstrate that this intervention significantly improves performance on diverse multimodal benchmarks and also boosts text-only tasks like MMLU. 8. The results suggest that correcting the architectural norm imbalance leads to a more balanced and capable multimodal language model overall. <div>
arXiv:2512.08374v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions</title>
<link>https://arxiv.org/abs/2512.08378</link>
<guid>https://arxiv.org/abs/2512.08378</guid>
<content:encoded><![CDATA[
<div> Keywords: image enhancement, noise suppression, gradient-domain weighted guided filter, Retinex model, multi-exposure fusion<br /><br />Summary:<br /><br />This paper addresses the problem of image degradation under challenging lighting conditions, which negatively affects vision-based applications. The authors propose a novel framework that performs simultaneous image enhancement and noise suppression in complex illumination scenarios. The first step involves using a gradient-domain weighted guided filter (GDWGIF) to accurately estimate illumination and improve overall image quality. Subsequently, the Retinex model is applied to decompose the image into illumination and reflection components. These components are processed in parallel: the illumination layer is corrected to enhance lighting conditions, while the reflection layer is enhanced to improve details and visual quality. Finally, the framework optimizes the image dynamic range through a combination of multi-exposure fusion and linear stretching techniques. Experiments conducted on real-world datasets from practical applications demonstrate that this method outperforms state-of-the-art approaches in both contrast enhancement and noise suppression. The proposed solution effectively balances noise reduction and detail preservation, making it suitable for diverse and complex lighting conditions. <div>
arXiv:2512.08378v1 Announce Type: new 
Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Digital Facial Retouching utilizing Face Beauty Information</title>
<link>https://arxiv.org/abs/2512.08397</link>
<guid>https://arxiv.org/abs/2512.08397</guid>
<content:encoded><![CDATA[
<div> Facial Retouching, Face Recognition, Beauty Assessment, Retouching Detection, Artificial Intelligence<br /><br />Summary:<br /><br />1. Facial retouching is commonly used in social media, advertisements, and professional photo studios to enhance appearance by making individuals look younger and removing skin imperfections.<br />2. While retouching serves aesthetic purposes, it poses challenges when retouched images are used as biometric samples in face recognition systems.<br />3. Prior research has demonstrated that facial retouching can reduce the accuracy of face recognition, necessitating effective detection methods for retouching.<br />4. This work analyzes how facial retouching impacts beauty assessment algorithms and explores the use of various artificial intelligence-based feature extraction techniques to improve detection of retouched images.<br />5. The study also investigates whether leveraging face beauty metrics can enhance the performance of retouching detection algorithms.<br />6. In scenarios where the specific retouching algorithm used in an attack is unknown, the proposed approach achieved a Detection Equal Error Rate (D-EER) of 1.1% on single-image detection, indicating high effectiveness in identifying retouched biometric samples. <div>
arXiv:2512.08397v1 Announce Type: new 
Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</title>
<link>https://arxiv.org/abs/2512.08400</link>
<guid>https://arxiv.org/abs/2512.08400</guid>
<content:encoded><![CDATA[
<div> Fish re-identification, Electronic Monitoring, Vision Transformer, Hard triplet mining, AutoFish dataset  

<br /><br />Summary:  
This paper addresses the challenge of automating fish re-identification (Re-ID) to support fisheries management, as Electronic Monitoring (EM) systems produce vast amounts of video data that cannot be manually reviewed at scale. The authors introduce the AutoFish dataset, which simulates an EM system setup featuring conveyor belts and six visually similar fish species for Re-ID evaluation. They develop an optimized deep learning pipeline that significantly improves key identification metrics such as Rank-1 accuracy (R1) and mean average precision at k (mAP@k) by combining hard triplet mining with a specialized image transformation pipeline tailored to the dataset, including custom normalization. Their experiments show that the Vision Transformer-based Swin-T architecture consistently outperforms the traditional CNN-based ResNet-50 architecture, achieving a peak 41.65% mAP@k and 90.43% Rank-1 accuracy. A detailed error analysis highlights that the main difficulty in the task is distinguishing between visually similar individual fish within the same species, where variations in viewpoint have a greater negative impact than partial occlusion. The authors provide their source code and documentation openly to facilitate further research and application. <div>
arXiv:2512.08400v1 Announce Type: new 
Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos</title>
<link>https://arxiv.org/abs/2512.08406</link>
<guid>https://arxiv.org/abs/2512.08406</guid>
<content:encoded><![CDATA[
<div> Human Mesh Recovery, temporal consistency, occlusion robustness, video segmentation, multi-human inference<br /><br />Summary:<br /><br />1. The paper addresses key limitations of existing Human Mesh Recovery (HMR) models, particularly their temporal inconsistency and performance degradation under occlusion when applied to video sequences.<br /><br />2. The authors introduce SAM-Body4D, a training-free framework designed to enhance temporal stability and robustness for 3D human pose and shape reconstruction specifically from videos.<br /><br />3. SAM-Body4D leverages identity-consistent masklets generated by a promptable video segmentation model to utilize the inherent continuity of human motion across frames.<br /><br />4. An Occlusion-Aware module is employed to refine these masklets by recovering missing regions caused by occlusions, improving the accuracy of the reconstructed meshes.<br /><br />5. The refined masklets guide the SAM 3D Body model to produce full-body mesh trajectories that are consistent over time, and a padding-based parallel strategy is introduced to efficiently handle multi-human scenarios in videos.<br /><br />6. Experimental evaluations demonstrate that SAM-Body4D improves temporal stability and occlusion robustness on challenging real-world video data without necessitating any additional training or model retraining.<br /><br />7. The authors have made their code and demo publicly accessible, facilitating further research and practical application of the method. <div>
arXiv:2512.08406v1 Announce Type: new 
Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval</title>
<link>https://arxiv.org/abs/2512.08410</link>
<guid>https://arxiv.org/abs/2512.08410</guid>
<content:encoded><![CDATA[
<div> Video understanding, Multimodal Large Language Models, OneClip-RAG, video chunking, long-video benchmarks<br /><br />Summary:<br /><br />1. Most Multimodal Large Language Models (MLLMs) face challenges processing long videos due to excessive memory overhead, limiting them to only a few frames. 2. The paper introduces One-shot video-Clip based Retrieval Augmentation (OneClip-RAG), an effective and efficient paradigm that leverages video clips to enhance video understanding by maintaining knowledge integrity and semantic coherence. 3. OneClip-RAG incorporates a novel query-guided video chunking algorithm that unifies clip chunking and cross-modal retrieval into a single step, reducing redundant computations. 4. To improve instruction-following capability, the authors propose the SynLongVideo dataset and design a progressive training scheme for OneClip-RAG. 5. OneClip-RAG was integrated with five recent MLLMs and evaluated on long-video benchmarks, demonstrating significant performance improvements—such as boosting InternLV2 8B and Qwen2-VL 7B to GPT-4o-level performance on MLVU—and superior efficiency, enabling LLaVA-Video to understand up to one hour of video in under 2.2 minutes on a single NVIDIA RTX 4090 GPU. <div>
arXiv:2512.08410v1 Announce Type: new 
Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking</title>
<link>https://arxiv.org/abs/2512.08430</link>
<guid>https://arxiv.org/abs/2512.08430</guid>
<content:encoded><![CDATA[
<div> 6D pose estimation, multi-view depth, sparse transformer, TSDF, bin picking<br /><br />Summary:<br /><br />This paper addresses the challenge of accurate 6D pose estimation in densely packed, cluttered industrial bin-picking environments, where occlusions, reflections, and textureless parts pose significant difficulties. The authors introduce a depth-only approach that fuses multi-view depth maps into either a detailed 3D point cloud or a sparse Truncated Signed Distance Field (TSDF) for better scene representation. Central to the framework is a staged heatmap mechanism that generates scene-adaptive attention priors at multiple resolutions, focusing computational resources on foreground regions while managing memory consumption efficiently. A novel density-aware sparse transformer block is proposed to dynamically handle self-occlusions and the uneven spatial distribution of 3D data, enhancing the model’s ability to process close-range robotic perception tasks. By leveraging fully sparse operations, the framework supports high-resolution volumetric representations that capture fine geometric details critical for precise pose estimation in clutter. The method processes entire scenes integrally and uses a per-voxel voting strategy to predict simultaneous 6D poses for multiple target objects, improving efficiency and scalability. Validation on the IPD and MV-YCB multi-view datasets demonstrates competitive performance in challenging industrial and household bin-picking scenarios, confirming the effectiveness of the approach. <div>
arXiv:2512.08430v1 Announce Type: new 
Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training</title>
<link>https://arxiv.org/abs/2512.08439</link>
<guid>https://arxiv.org/abs/2512.08439</guid>
<content:encoded><![CDATA[
arXiv:2512.08439v1 Announce Type: new 
Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multispectral Sensors for Color Correction in Mobile Cameras</title>
<link>https://arxiv.org/abs/2512.08441</link>
<guid>https://arxiv.org/abs/2512.08441</guid>
<content:encoded><![CDATA[
arXiv:2512.08441v1 Announce Type: new 
Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.08445</link>
<guid>https://arxiv.org/abs/2512.08445</guid>
<content:encoded><![CDATA[
arXiv:2512.08445v1 Announce Type: new 
Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery</title>
<link>https://arxiv.org/abs/2512.08467</link>
<guid>https://arxiv.org/abs/2512.08467</guid>
<content:encoded><![CDATA[
arXiv:2512.08467v1 Announce Type: new 
Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention</title>
<link>https://arxiv.org/abs/2512.08477</link>
<guid>https://arxiv.org/abs/2512.08477</guid>
<content:encoded><![CDATA[
arXiv:2512.08477v1 Announce Type: new 
Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</title>
<link>https://arxiv.org/abs/2512.08478</link>
<guid>https://arxiv.org/abs/2512.08478</guid>
<content:encoded><![CDATA[
arXiv:2512.08478v1 Announce Type: new 
Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions</title>
<link>https://arxiv.org/abs/2512.08486</link>
<guid>https://arxiv.org/abs/2512.08486</guid>
<content:encoded><![CDATA[
arXiv:2512.08486v1 Announce Type: new 
Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs</title>
<link>https://arxiv.org/abs/2512.08498</link>
<guid>https://arxiv.org/abs/2512.08498</guid>
<content:encoded><![CDATA[
arXiv:2512.08498v1 Announce Type: new 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2512.08503</link>
<guid>https://arxiv.org/abs/2512.08503</guid>
<content:encoded><![CDATA[
arXiv:2512.08503v1 Announce Type: new 
Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08505</link>
<guid>https://arxiv.org/abs/2512.08505</guid>
<content:encoded><![CDATA[
arXiv:2512.08505v1 Announce Type: new 
Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds</title>
<link>https://arxiv.org/abs/2512.08506</link>
<guid>https://arxiv.org/abs/2512.08506</guid>
<content:encoded><![CDATA[
arXiv:2512.08506v1 Announce Type: new 
Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Images via Self-Calling Agent</title>
<link>https://arxiv.org/abs/2512.08511</link>
<guid>https://arxiv.org/abs/2512.08511</guid>
<content:encoded><![CDATA[
arXiv:2512.08511v1 Announce Type: new 
Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Real Weights: Hypercomplex Representations for Stable Quantization</title>
<link>https://arxiv.org/abs/2512.08524</link>
<guid>https://arxiv.org/abs/2512.08524</guid>
<content:encoded><![CDATA[
arXiv:2512.08524v1 Announce Type: new 
Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVP: Multiple View Prediction Improves GUI Grounding</title>
<link>https://arxiv.org/abs/2512.08529</link>
<guid>https://arxiv.org/abs/2512.08529</guid>
<content:encoded><![CDATA[
arXiv:2512.08529v1 Announce Type: new 
Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation</title>
<link>https://arxiv.org/abs/2512.08534</link>
<guid>https://arxiv.org/abs/2512.08534</guid>
<content:encoded><![CDATA[
arXiv:2512.08534v1 Announce Type: new 
Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement</title>
<link>https://arxiv.org/abs/2512.08535</link>
<guid>https://arxiv.org/abs/2512.08535</guid>
<content:encoded><![CDATA[
arXiv:2512.08535v1 Announce Type: new 
Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.08537</link>
<guid>https://arxiv.org/abs/2512.08537</guid>
<content:encoded><![CDATA[
arXiv:2512.08537v1 Announce Type: new 
Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation</title>
<link>https://arxiv.org/abs/2512.08542</link>
<guid>https://arxiv.org/abs/2512.08542</guid>
<content:encoded><![CDATA[
arXiv:2512.08542v1 Announce Type: new 
Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Iteration-Free Fixed-Point Estimator for Diffusion Inversion</title>
<link>https://arxiv.org/abs/2512.08547</link>
<guid>https://arxiv.org/abs/2512.08547</guid>
<content:encoded><![CDATA[
arXiv:2512.08547v1 Announce Type: new 
Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2512.08557</link>
<guid>https://arxiv.org/abs/2512.08557</guid>
<content:encoded><![CDATA[
arXiv:2512.08557v1 Announce Type: new 
Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain</title>
<link>https://arxiv.org/abs/2512.08560</link>
<guid>https://arxiv.org/abs/2512.08560</guid>
<content:encoded><![CDATA[
arXiv:2512.08560v1 Announce Type: new 
Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Neural Image Signal Processing</title>
<link>https://arxiv.org/abs/2512.08564</link>
<guid>https://arxiv.org/abs/2512.08564</guid>
<content:encoded><![CDATA[
arXiv:2512.08564v1 Announce Type: new 
Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Aware Test-Time Segmentation for Continual Domain Shifts</title>
<link>https://arxiv.org/abs/2512.08569</link>
<guid>https://arxiv.org/abs/2512.08569</guid>
<content:encoded><![CDATA[
arXiv:2512.08569v1 Announce Type: new 
Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis</title>
<link>https://arxiv.org/abs/2512.08572</link>
<guid>https://arxiv.org/abs/2512.08572</guid>
<content:encoded><![CDATA[
arXiv:2512.08572v1 Announce Type: new 
Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery</title>
<link>https://arxiv.org/abs/2512.08577</link>
<guid>https://arxiv.org/abs/2512.08577</guid>
<content:encoded><![CDATA[
arXiv:2512.08577v1 Announce Type: new 
Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Pollen Recognition in Optical and Holographic Microscopy Images</title>
<link>https://arxiv.org/abs/2512.08589</link>
<guid>https://arxiv.org/abs/2512.08589</guid>
<content:encoded><![CDATA[
arXiv:2512.08589v1 Announce Type: new 
Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning</title>
<link>https://arxiv.org/abs/2512.08606</link>
<guid>https://arxiv.org/abs/2512.08606</guid>
<content:encoded><![CDATA[
arXiv:2512.08606v1 Announce Type: new 
Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</title>
<link>https://arxiv.org/abs/2512.08625</link>
<guid>https://arxiv.org/abs/2512.08625</guid>
<content:encoded><![CDATA[
arXiv:2512.08625v1 Announce Type: new 
Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Densification and Depth from Perspective-based Blur</title>
<link>https://arxiv.org/abs/2512.08627</link>
<guid>https://arxiv.org/abs/2512.08627</guid>
<content:encoded><![CDATA[
arXiv:2512.08627v1 Announce Type: new 
Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</title>
<link>https://arxiv.org/abs/2512.08639</link>
<guid>https://arxiv.org/abs/2512.08639</guid>
<content:encoded><![CDATA[
arXiv:2512.08639v1 Announce Type: new 
Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2512.08645</link>
<guid>https://arxiv.org/abs/2512.08645</guid>
<content:encoded><![CDATA[
arXiv:2512.08645v1 Announce Type: new 
Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition</title>
<link>https://arxiv.org/abs/2512.08647</link>
<guid>https://arxiv.org/abs/2512.08647</guid>
<content:encoded><![CDATA[
arXiv:2512.08647v1 Announce Type: new 
Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank</title>
<link>https://arxiv.org/abs/2512.08648</link>
<guid>https://arxiv.org/abs/2512.08648</guid>
<content:encoded><![CDATA[
arXiv:2512.08648v1 Announce Type: new 
Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds</title>
<link>https://arxiv.org/abs/2512.08673</link>
<guid>https://arxiv.org/abs/2512.08673</guid>
<content:encoded><![CDATA[
arXiv:2512.08673v1 Announce Type: new 
Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance</title>
<link>https://arxiv.org/abs/2512.08697</link>
<guid>https://arxiv.org/abs/2512.08697</guid>
<content:encoded><![CDATA[
arXiv:2512.08697v1 Announce Type: new 
Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth</title>
<link>https://arxiv.org/abs/2512.08700</link>
<guid>https://arxiv.org/abs/2512.08700</guid>
<content:encoded><![CDATA[
arXiv:2512.08700v1 Announce Type: new 
Abstract: Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images</title>
<link>https://arxiv.org/abs/2512.08730</link>
<guid>https://arxiv.org/abs/2512.08730</guid>
<content:encoded><![CDATA[
arXiv:2512.08730v1 Announce Type: new 
Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting</title>
<link>https://arxiv.org/abs/2512.08733</link>
<guid>https://arxiv.org/abs/2512.08733</guid>
<content:encoded><![CDATA[
arXiv:2512.08733v1 Announce Type: new 
Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture</title>
<link>https://arxiv.org/abs/2512.08738</link>
<guid>https://arxiv.org/abs/2512.08738</guid>
<content:encoded><![CDATA[
arXiv:2512.08738v1 Announce Type: new 
Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation</title>
<link>https://arxiv.org/abs/2512.08747</link>
<guid>https://arxiv.org/abs/2512.08747</guid>
<content:encoded><![CDATA[
arXiv:2512.08747v1 Announce Type: new 
Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices</title>
<link>https://arxiv.org/abs/2512.08751</link>
<guid>https://arxiv.org/abs/2512.08751</guid>
<content:encoded><![CDATA[
arXiv:2512.08751v1 Announce Type: new 
Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</title>
<link>https://arxiv.org/abs/2512.08765</link>
<guid>https://arxiv.org/abs/2512.08765</guid>
<content:encoded><![CDATA[
arXiv:2512.08765v1 Announce Type: new 
Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps</title>
<link>https://arxiv.org/abs/2512.08774</link>
<guid>https://arxiv.org/abs/2512.08774</guid>
<content:encoded><![CDATA[
arXiv:2512.08774v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fr\'echet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models</title>
<link>https://arxiv.org/abs/2512.08785</link>
<guid>https://arxiv.org/abs/2512.08785</guid>
<content:encoded><![CDATA[
arXiv:2512.08785v1 Announce Type: new 
Abstract: Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance</title>
<link>https://arxiv.org/abs/2512.08789</link>
<guid>https://arxiv.org/abs/2512.08789</guid>
<content:encoded><![CDATA[
arXiv:2512.08789v1 Announce Type: new 
Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning</title>
<link>https://arxiv.org/abs/2512.08820</link>
<guid>https://arxiv.org/abs/2512.08820</guid>
<content:encoded><![CDATA[
arXiv:2512.08820v1 Announce Type: new 
Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincar\'e ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08829</link>
<guid>https://arxiv.org/abs/2512.08829</guid>
<content:encoded><![CDATA[
arXiv:2512.08829v1 Announce Type: new 
Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v1 Announce Type: new 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference</title>
<link>https://arxiv.org/abs/2512.08860</link>
<guid>https://arxiv.org/abs/2512.08860</guid>
<content:encoded><![CDATA[
arXiv:2512.08860v1 Announce Type: new 
Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning</title>
<link>https://arxiv.org/abs/2512.08873</link>
<guid>https://arxiv.org/abs/2512.08873</guid>
<content:encoded><![CDATA[
arXiv:2512.08873v1 Announce Type: new 
Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing</title>
<link>https://arxiv.org/abs/2512.08881</link>
<guid>https://arxiv.org/abs/2512.08881</guid>
<content:encoded><![CDATA[
arXiv:2512.08881v1 Announce Type: new 
Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Rotation-Invariant Convolution for UAV Image Segmentation</title>
<link>https://arxiv.org/abs/2512.08888</link>
<guid>https://arxiv.org/abs/2512.08888</guid>
<content:encoded><![CDATA[
arXiv:2512.08888v1 Announce Type: new 
Abstract: Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers</title>
<link>https://arxiv.org/abs/2512.08889</link>
<guid>https://arxiv.org/abs/2512.08889</guid>
<content:encoded><![CDATA[
arXiv:2512.08889v1 Announce Type: new 
Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation</title>
<link>https://arxiv.org/abs/2512.08897</link>
<guid>https://arxiv.org/abs/2512.08897</guid>
<content:encoded><![CDATA[
arXiv:2512.08897v1 Announce Type: new 
Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Evolving 3D Scene Generation from a Single Image</title>
<link>https://arxiv.org/abs/2512.08905</link>
<guid>https://arxiv.org/abs/2512.08905</guid>
<content:encoded><![CDATA[
arXiv:2512.08905v1 Announce Type: new 
Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception</title>
<link>https://arxiv.org/abs/2512.08912</link>
<guid>https://arxiv.org/abs/2512.08912</guid>
<content:encoded><![CDATA[
arXiv:2512.08912v1 Announce Type: new 
Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration</title>
<link>https://arxiv.org/abs/2512.08922</link>
<guid>https://arxiv.org/abs/2512.08922</guid>
<content:encoded><![CDATA[
arXiv:2512.08922v1 Announce Type: new 
Abstract: Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiently Reconstructing Dynamic Scenes One D4RT at a Time</title>
<link>https://arxiv.org/abs/2512.08924</link>
<guid>https://arxiv.org/abs/2512.08924</guid>
<content:encoded><![CDATA[
arXiv:2512.08924v1 Announce Type: new 
Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment</title>
<link>https://arxiv.org/abs/2512.08930</link>
<guid>https://arxiv.org/abs/2512.08930</guid>
<content:encoded><![CDATA[
arXiv:2512.08930v1 Announce Type: new 
Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
arXiv:2512.08931v1 Announce Type: new 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm</title>
<link>https://arxiv.org/abs/2512.05791</link>
<guid>https://arxiv.org/abs/2512.05791</guid>
<content:encoded><![CDATA[
arXiv:2512.05791v1 Announce Type: cross 
Abstract: Purpose: The Unadjusted Langevin Algorithm (ULA) in combination with diffusion models can generate high quality MRI reconstructions with uncertainty estimation from highly undersampled k-space data. However, sampling methods such as diffusion posterior sampling or likelihood annealing suffer from long reconstruction times and the need for parameter tuning. The purpose of this work is to develop a robust sampling algorithm with fast convergence.
  Theory and Methods: In the reverse diffusion process used for sampling the posterior, the exact likelihood is multiplied with the diffused prior at all noise scales. To overcome the issue of slow convergence, preconditioning is used. The method is trained on fastMRI data and tested on retrospectively undersampled brain data of a healthy volunteer.
  Results: For posterior sampling in Cartesian and non-Cartesian accelerated MRI the new approach outperforms annealed sampling in terms of reconstruction speed and sample quality.
  Conclusion: The proposed exact likelihood with preconditioning enables rapid and reliable posterior sampling across various MRI reconstruction tasks without the need for parameter tuning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model</title>
<link>https://arxiv.org/abs/2512.07855</link>
<guid>https://arxiv.org/abs/2512.07855</guid>
<content:encoded><![CDATA[
arXiv:2512.07855v1 Announce Type: cross 
Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSPN-2: Efficient Parallel Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.07884</link>
<guid>https://arxiv.org/abs/2512.07884</guid>
<content:encoded><![CDATA[
arXiv:2512.07884v1 Announce Type: cross 
Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization</title>
<link>https://arxiv.org/abs/2512.07969</link>
<guid>https://arxiv.org/abs/2512.07969</guid>
<content:encoded><![CDATA[
arXiv:2512.07969v1 Announce Type: cross 
Abstract: Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \textbf{2$\times$--35$\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLD: Visual Language Goal Distance for Reinforcement Learning Navigation</title>
<link>https://arxiv.org/abs/2512.07976</link>
<guid>https://arxiv.org/abs/2512.07976</guid>
<content:encoded><![CDATA[
arXiv:2512.07976v1 Announce Type: cross 
Abstract: Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIP-Net: Continual Interpretable Prototype-based Network</title>
<link>https://arxiv.org/abs/2512.07981</link>
<guid>https://arxiv.org/abs/2512.07981</guid>
<content:encoded><![CDATA[
arXiv:2512.07981v1 Announce Type: cross 
Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIJIT: A Robotic Head for an Active Observer</title>
<link>https://arxiv.org/abs/2512.07998</link>
<guid>https://arxiv.org/abs/2512.07998</guid>
<content:encoded><![CDATA[
arXiv:2512.07998v1 Announce Type: cross 
Abstract: We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</title>
<link>https://arxiv.org/abs/2512.08029</link>
<guid>https://arxiv.org/abs/2512.08029</guid>
<content:encoded><![CDATA[
arXiv:2512.08029v1 Announce Type: cross 
Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizations of the Normalized Radon Cumulative Distribution Transform for Limited Data Recognition</title>
<link>https://arxiv.org/abs/2512.08099</link>
<guid>https://arxiv.org/abs/2512.08099</guid>
<content:encoded><![CDATA[
arXiv:2512.08099v1 Announce Type: cross 
Abstract: The Radon cumulative distribution transform (R-CDT) exploits one-dimensional Wasserstein transport and the Radon transform to represent prominent features in images. It is closely related to the sliced Wasserstein distance and facilitates classification tasks, especially in the small data regime, like the recognition of watermarks in filigranology. Here, a typical issue is that the given data may be subject to affine transformations caused by the measuring process. To make the R-CDT invariant under arbitrary affine transformations, a two-step normalization of the R-CDT has been proposed in our earlier works. The aim of this paper is twofold. First, we propose a family of generalized normalizations to enhance flexibility for applications. Second, we study multi-dimensional and non-Euclidean settings by making use of generalized Radon transforms. We prove that our novel feature representations are invariant under certain transformations and allow for linear separation in feature space. Our theoretical results are supported by numerical experiments based on 2d images, 3d shapes and 3d rotation matrices, showing near perfect classification accuracies and clustering results.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowSteer: Conditioning Flow Field for Consistent Image Restoration</title>
<link>https://arxiv.org/abs/2512.08125</link>
<guid>https://arxiv.org/abs/2512.08125</guid>
<content:encoded><![CDATA[
arXiv:2512.08125v1 Announce Type: cross 
Abstract: Flow-based text-to-image (T2I) models excel at prompt-driven image generation, but falter on Image Restoration (IR), often "drifting away" from being faithful to the measurement. Prior work mitigate this drift with data-specific flows or task-specific adapters that are computationally heavy and not scalable across tasks. This raises the question "Can't we efficiently manipulate the existing generative capabilities of a flow model?" To this end, we introduce FlowSteer (FS), an operator-aware conditioning scheme that injects measurement priors along the sampling path,coupling a frozed flow's implicit guidance with explicit measurement constraints. Across super-resolution, deblurring, denoising, and colorization, FS improves measurement consistency and identity preservation in a strictly zero-shot setting-no retrained models, no adapters. We show how the nature of flow models and their sensitivities to noise inform the design of such a scheduler. FlowSteer, although simple, achieves a higher fidelity of reconstructed images, while leveraging the rich generative priors of flow models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08153</link>
<guid>https://arxiv.org/abs/2512.08153</guid>
<content:encoded><![CDATA[
arXiv:2512.08153v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features</title>
<link>https://arxiv.org/abs/2512.08170</link>
<guid>https://arxiv.org/abs/2512.08170</guid>
<content:encoded><![CDATA[
arXiv:2512.08170v1 Announce Type: cross 
Abstract: In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model</title>
<link>https://arxiv.org/abs/2512.08188</link>
<guid>https://arxiv.org/abs/2512.08188</guid>
<content:encoded><![CDATA[
arXiv:2512.08188v1 Announce Type: cross 
Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title>
<link>https://arxiv.org/abs/2512.08216</link>
<guid>https://arxiv.org/abs/2512.08216</guid>
<content:encoded><![CDATA[
arXiv:2512.08216v1 Announce Type: cross 
Abstract: Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation</title>
<link>https://arxiv.org/abs/2512.08271</link>
<guid>https://arxiv.org/abs/2512.08271</guid>
<content:encoded><![CDATA[
arXiv:2512.08271v1 Announce Type: cross 
Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Reinforced Deep Priors for Reparameterized Full Waveform Inversion</title>
<link>https://arxiv.org/abs/2512.08284</link>
<guid>https://arxiv.org/abs/2512.08284</guid>
<content:encoded><![CDATA[
arXiv:2512.08284v1 Announce Type: cross 
Abstract: Full waveform inversion (FWI) has become a widely adopted technique for high-resolution subsurface imaging. However, its inherent strong nonlinearity often results in convergence toward local minima. Recently, deep image prior-based reparameterized FWI (DIP-FWI) has been proposed to alleviate the dependence on massive training data. By exploiting the spectral bias and implicit regularization in the neural network architecture, DIP-FWI can effectively avoid local minima and reconstruct more geologically plausible velocity models. Nevertheless, existing DIP-FWI typically use a fixed random input throughout the inversion process, which fails to utilize the mapping and correlation between the input and output of the network. Moreover, under complex geological conditions, the lack of informative prior in the input can exacerbate the ill-posedness of the inverse problem, leading to artifacts and unstable reconstructions. To address these limitations, we propose a self-reinforced DIP-FWI (SRDIP-FWI) framework, in which a steering algorithm alternately updates both the network parameters and the input at each iteration using feedback from the current network output. This design allows adaptive structural enhancement and improved regularization, thereby effectively mitigating the ill-posedness in FWI. Additionally, we analyze the spectral bias of the network in SRDIP-FWI and quantify its role in multiscale velocity model building. Synthetic tests and field land data application demonstrate that SRDIP-FWI achieves superior resolution, improved accuracy and greater depth penetration compared to multiscale FWI. More importantly, SRDIP-FWI eliminates the need for manual frequency-band selection and time-window picking, substantially simplifying the inversion workflow. Overall, the proposed method provides a novel, adaptive and robust framework for accurate subsurface velocity model reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2512.08360</link>
<guid>https://arxiv.org/abs/2512.08360</guid>
<content:encoded><![CDATA[
arXiv:2512.08360v1 Announce Type: cross 
Abstract: Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions</title>
<link>https://arxiv.org/abs/2512.08500</link>
<guid>https://arxiv.org/abs/2512.08500</guid>
<content:encoded><![CDATA[
arXiv:2512.08500v1 Announce Type: cross 
Abstract: Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks</title>
<link>https://arxiv.org/abs/2512.08545</link>
<guid>https://arxiv.org/abs/2512.08545</guid>
<content:encoded><![CDATA[
arXiv:2512.08545v1 Announce Type: cross 
Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm</title>
<link>https://arxiv.org/abs/2512.08629</link>
<guid>https://arxiv.org/abs/2512.08629</guid>
<content:encoded><![CDATA[
arXiv:2512.08629v1 Announce Type: cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-domain performance analysis with scores tailored to user preferences</title>
<link>https://arxiv.org/abs/2512.08715</link>
<guid>https://arxiv.org/abs/2512.08715</guid>
<content:encoded><![CDATA[
arXiv:2512.08715v1 Announce Type: cross 
Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</title>
<link>https://arxiv.org/abs/2310.02239</link>
<guid>https://arxiv.org/abs/2310.02239</guid>
<content:encoded><![CDATA[
arXiv:2310.02239v4 Announce Type: replace 
Abstract: The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spike-EVPR: Deep Spiking Residual Networks with SNN-Tailored Representations for Event-Based Visual Place Recognition</title>
<link>https://arxiv.org/abs/2402.10476</link>
<guid>https://arxiv.org/abs/2402.10476</guid>
<content:encoded><![CDATA[
arXiv:2402.10476v2 Announce Type: replace 
Abstract: Event cameras are ideal for visual place recognition (VPR) in challenging environments due to their high temporal resolution and high dynamic range. However, existing methods convert sparse events into dense frame-like representations for Artificial Neural Networks (ANNs), ignoring event sparsity and incurring high computational cost. Spiking Neural Networks (SNNs) complement event data through discrete spike signals to enable energy-efficient VPR, but their application is hindered by the lack of effective spike-compatible representations and deep architectures capable of learning discriminative global descriptors. To address these limitations, we propose Spike-EVPR, a directly trained, end-to-end SNN framework tailored for event-based VPR. First, we introduce two complementary event representations, MCS-Tensor and TSS-Tensor, designed to reduce temporal redundancy while preserving essential spatio-temporal cues. Furthermore, we propose a deep spiking residual architecture that effectively aggregates these features to generate robust place descriptors. Extensive experiments on the Brisbane-Event-VPR and DDD20 datasets demonstrate that Spike-EVPR achieves state-of-the-art performance, improving Recall@1 by 7.61% and 13.20%, respectively, while significantly reducing energy consumption.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning effective pruning at initialization from iterative pruning</title>
<link>https://arxiv.org/abs/2408.14757</link>
<guid>https://arxiv.org/abs/2408.14757</guid>
<content:encoded><![CDATA[
arXiv:2408.14757v2 Announce Type: replace 
Abstract: Pruning at initialization (PaI) reduces training costs by removing weights before training, which becomes increasingly crucial with the growing network size. However, current PaI methods still have a large accuracy gap with iterative pruning, especially at high sparsity levels. This raises an intriguing question: can we get inspiration from iterative pruning to improve the PaI performance? In the lottery ticket hypothesis, the iterative rewind pruning (IRP) finds subnetworks retroactively by rewinding the parameter to the original initialization in every pruning iteration, which means all the subnetworks are based on the initial state. Here, we hypothesise the surviving subnetworks are more important and bridge the initial feature and their surviving score as the PaI criterion. We employ an end-to-end neural network (\textbf{AutoS}parse) to learn this correlation, input the model's initial features, output their score and then prune the lowest score parameters before training. To validate the accuracy and generalization of our method, we performed PaI across various models. Results show that our approach outperforms existing methods in high-sparsity settings. Notably, as the underlying logic of model pruning is consistent in different models, only one-time IRP on one model is needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to VGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural network-based PaI method, we conduct extensive experiments to validate the factors influencing this approach. These results reveal the learning tendencies of neural networks and provide new insights into our understanding and research of PaI from a practical perspective. Our code is available at: https://github.com/ChengYaofeng/AutoSparse.git.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond accuracy: quantifying the reliability of Multiple Instance Learning for Whole Slide Image classification</title>
<link>https://arxiv.org/abs/2409.11110</link>
<guid>https://arxiv.org/abs/2409.11110</guid>
<content:encoded><![CDATA[
arXiv:2409.11110v3 Announce Type: replace 
Abstract: Machine learning models have become integral to many fields, but their reliability, defined as producing dependable, trustworthy, and domain-consistent predictions, remains a critical concern. Multiple Instance Learning (MIL) models designed for Whole Slide Image (WSI) classification in computational pathology are rarely evaluated in terms of reliability, leaving a key gap in understanding their suitability for high-stakes applications like clinical decision-making. In this paper, we address this gap by introducing three quantitative metrics for reliability assessment and applying them to several widely used MIL architectures across three region-wise annotated pathology datasets. Our findings indicate that the mean pooling instance (MEAN-POOL-INS)model demonstrates superior reliability compared to other networks, despite its simple architectural design and computational efficiency. These findings underscore the need of reliability evaluation alongside predictive performance in MIL models and establish MEAN-POOL-INS as a strong, trustworthy baseline for future research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning, Machine Learning -- Digital Signal and Image Processing: From Theory to Application</title>
<link>https://arxiv.org/abs/2410.20304</link>
<guid>https://arxiv.org/abs/2410.20304</guid>
<content:encoded><![CDATA[
arXiv:2410.20304v2 Announce Type: replace 
Abstract: Digital Signal Processing (DSP) and Digital Image Processing (DIP) with Machine Learning (ML) and Deep Learning (DL) are popular research areas in Computer Vision and related fields. We highlight transformative applications in image enhancement, filtering techniques, and pattern recognition. By integrating frameworks like the Discrete Fourier Transform (DFT), Z-Transform, and Fourier Transform methods, we enable robust data manipulation and feature extraction essential for AI-driven tasks. Using Python, we implement algorithms that optimize real-time data processing, forming a foundation for scalable, high-performance solutions in computer vision. This work illustrates the potential of ML and DL to advance DSP and DIP methodologies, contributing to artificial intelligence, automated feature extraction, and applications across diverse domains.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Radiance Fields for the Real World: A Survey</title>
<link>https://arxiv.org/abs/2501.13104</link>
<guid>https://arxiv.org/abs/2501.13104</guid>
<content:encoded><![CDATA[
arXiv:2501.13104v2 Announce Type: replace 
Abstract: Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since release. NeRFs can effectively reconstruct complex 3D scenes from 2D images, advancing different fields and applications such as scene understanding, 3D content generation, and robotics. Despite significant research progress, a thorough review of recent innovations, applications, and challenges is lacking. This survey compiles key theoretical advancements and alternative representations and investigates emerging challenges. It further explores applications on reconstruction, highlights NeRFs' impact on computer vision and robotics, and reviews essential datasets and toolkits. By identifying gaps in the literature, this survey discusses open challenges and offers directions for future research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape and Texture Recognition in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.23062</link>
<guid>https://arxiv.org/abs/2503.23062</guid>
<content:encoded><![CDATA[
arXiv:2503.23062v5 Announce Type: replace 
Abstract: Shapes and textures are the basic building blocks of visual perception. The ability to identify shapes regardless of orientation, texture, or context, and to recognize textures and materials independently of their associated objects, is essential for a general visual understanding of the world. This work introduces the Large Shapes and Textures dataset (LAS&amp;T), a giant collection of highly diverse shapes and textures, created by unsupervised extraction of patterns from natural images. This dataset is used to benchmark how effectively leading Large Vision-Language Models (LVLM/VLM) recognize and represent shapes, textures, and materials in 2D and 3D scenes. For shape recognition, we test the models' ability to match images of identical shapes that differ in orientation, texture, color, or environment. Our results show that the shape-recognition capabilities of LVLMs remain well below human performance, especially when multiple transformations are applied. LVLMs rely predominantly on high-level and semantic features and struggle with abstract shapes lacking class associations. For texture and material recognition, we evaluated the models' ability to identify images with identical textures and materials across different objects and environments. Interestingly, leading LVLMs approach human-level performance in recognizing materials in 3D scenes, yet substantially underperform humans when identifying simpler, more abstract 2D textures and shapes. These results are consistent across a wide range of leading LVLMs (GPT/Gemini/LLama/Qwen) and foundation vision models (DINO/CLIP), exposing major deficiencies in the ability of VLMs to extract low-level visual features. In contrast, humans and simple nets trained directly for these tasks achieve high accuracy. The LAS&amp;T dataset, featuring over 700,000 images for 2D/3D shape and textures recognition and retrieval, is freely available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2504.02316</link>
<guid>https://arxiv.org/abs/2504.02316</guid>
<content:encoded><![CDATA[
arXiv:2504.02316v2 Announce Type: replace 
Abstract: Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent prior view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel method that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise view control; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer can be seamlessly integrated into various 3D representations and score distillation paradigms, effectively mitigating the multi-face Janus problem.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Dataset Condensation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06670</link>
<guid>https://arxiv.org/abs/2505.06670</guid>
<content:encoded><![CDATA[
arXiv:2505.06670v2 Announce Type: replace 
Abstract: In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance, particularly in the video domain. In this paper, we focus on video dataset distillation. We begin by employing a video diffusion model to generate synthetic videos. Since the videos are generated only once, this significantly reduces computational costs. Next, we introduce the Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MELLM: A Flow-Guided Large Language Model for Micro-Expression Understanding</title>
<link>https://arxiv.org/abs/2505.07007</link>
<guid>https://arxiv.org/abs/2505.07007</guid>
<content:encoded><![CDATA[
arXiv:2505.07007v3 Announce Type: replace 
Abstract: Micro-expressions (MEs), brief and low-intensity facial movements revealing concealed emotions, are crucial for affective computing. Despite notable progress in ME recognition, existing methods are largely confined to discrete emotion classification, lacking the capacity for comprehensive ME Understanding (MEU), particularly in interpreting subtle facial dynamics and underlying emotional cues. While Multimodal Large Language Models (MLLMs) offer potential for MEU with their advanced reasoning abilities, they still struggle to perceive such subtle facial affective behaviors. To bridge this gap, we propose a ME Large Language Model (MELLM) that integrates optical flow-based sensitivity to subtle facial motions with the powerful inference ability of LLMs. Specifically, an iterative, warping-based optical-flow estimator, named MEFlowNet, is introduced to precisely capture facial micro-movements. For its training and evaluation, we construct MEFlowDataset, a large-scale optical-flow dataset with 54,611 onset-apex image pairs spanning diverse identities and subtle facial motions. Subsequently, we design a Flow-Guided Micro-Expression Understanding paradigm. Under this framework, the optical flow signals extracted by MEFlowNet are leveraged to build MEU-Instruct, an instruction-tuning dataset for MEU. MELLM is then fine-tuned on MEU-Instruct, enabling it to translate subtle motion patterns into human-readable descriptions and generate corresponding emotional inferences. Experiments demonstrate that MEFlowNet significantly outperforms existing optical flow methods in facial and ME-flow estimation, while MELLM achieves state-of-the-art accuracy and generalization across multiple ME benchmarks. To the best of our knowledge, this work presents two key contributions: MEFlowNet as the first dedicated ME flow estimator, and MELLM as the first LLM tailored for MEU.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multi-Modal Information to Enhance Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.08605</link>
<guid>https://arxiv.org/abs/2505.08605</guid>
<content:encoded><![CDATA[
arXiv:2505.08605v3 Announce Type: replace 
Abstract: Dataset distillation aims to create a small and highly representative synthetic dataset that preserves the essential information of a larger real dataset. Beyond reducing storage and computational costs, related approaches offer a promising avenue for privacy preservation in computer vision by eliminating the need to store or share sensitive real-world images. Existing methods focus solely on optimizing visual representations, overlooking the potential of multi-modal information. In this work, we propose a multi-modal dataset distillation framework that incorporates two key enhancements: caption-guided supervision and object-centric masking. To leverage textual information, we introduce two strategies: caption concatenation, which fuses caption embeddings with visual features during classification, and caption matching, which enforces semantic alignment between real and synthetic data through a caption-based loss. To improve data utility and reduce unnecessary background noise, we employ segmentation masks to isolate target objects and introduce two novel losses: masked feature alignment and masked gradient matching, both aimed at promoting object-centric learning. Extensive evaluations demonstrate that our approach improves downstream performance while promoting privacy protection by minimizing exposure to real data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make LVLMs Focus: Context-Aware Attention Modulation for Better Multimodal In-Context Learning</title>
<link>https://arxiv.org/abs/2505.17097</link>
<guid>https://arxiv.org/abs/2505.17097</guid>
<content:encoded><![CDATA[
arXiv:2505.17097v3 Announce Type: replace 
Abstract: Multimodal in-context learning (ICL) is becoming a key capability that allows large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, which expands their usefulness in many real-world applications. However, ICL performance remains unstable even when the in-context demonstrations (ICDs) are well matched, showing that LVLMs still struggle to make full use of the provided context. While existing work mainly focuses on prompt engineering or post-hoc logit calibration, we study the attention mechanisms inside LVLMs to address their inherent limitations. We identify two important weaknesses in their self-attention that hinder effective ICL. To address these weaknesses, we propose \textbf{Context-Aware Modulated Attention} (CAMA), a training-free and plug-and-play method that dynamically adjusts attention logits based on the input in-context sequence. CAMA uses a two-stage modulation process that strengthens attention to semantically important tokens, especially visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, showing clear effectiveness and generalization. It can also activate the intended benefits of prompt engineering methods and remains robust across different sequence configurations. Therefore, CAMA opens up new directions for improving multimodal reasoning through a deeper understanding of attention dynamics.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Missing Point in Vision Transformers for Universal Image Segmentation</title>
<link>https://arxiv.org/abs/2505.19795</link>
<guid>https://arxiv.org/abs/2505.19795</guid>
<content:encoded><![CDATA[
arXiv:2505.19795v2 Announce Type: replace 
Abstract: Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>50 Years of Automated Face Recognition</title>
<link>https://arxiv.org/abs/2505.24247</link>
<guid>https://arxiv.org/abs/2505.24247</guid>
<content:encoded><![CDATA[
arXiv:2505.24247v3 Announce Type: replace 
Abstract: Over the past five decades, automated face recognition (FR) has progressed from handcrafted geometric and statistical approaches to advanced deep learning architectures that now approach, and in many cases exceed, human performance. This paper traces the historical and technological evolution of FR, encompassing early algorithmic paradigms through to contemporary neural systems trained on extensive real and synthetically generated datasets. We examine pivotal innovations that have driven this progression, including advances in dataset construction, loss function formulation, network architecture design, and feature fusion strategies. Furthermore, we analyze the relationship between data scale, diversity, and model generalization, highlighting how dataset expansion correlates with benchmark performance gains. Recent systems have achieved near-perfect large-scale identification accuracy, with the leading algorithm in the latest NIST FRTE 1:N benchmark reporting a FNIR of 0.15 percent at FPIR of 0.001 on a gallery of over 10 million identities. We delineate key open problems and emerging directions, including scalable training, multi-modal fusion, synthetic data, and interpretable recognition frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection</title>
<link>https://arxiv.org/abs/2506.05872</link>
<guid>https://arxiv.org/abs/2506.05872</guid>
<content:encoded><![CDATA[
arXiv:2506.05872v2 Announce Type: replace 
Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation</title>
<link>https://arxiv.org/abs/2506.17159</link>
<guid>https://arxiv.org/abs/2506.17159</guid>
<content:encoded><![CDATA[
arXiv:2506.17159v2 Announce Type: replace 
Abstract: Medical image analysis is critical yet challenged by the need of jointly segmenting organs or tissues, and numerous instances for anatomical structures and tumor microenvironment analysis. Existing studies typically formulated different segmentation tasks in isolation, which overlooks the fundamental interdependencies between these tasks, leading to suboptimal segmentation performance and insufficient medical image understanding. To address this issue, we propose a Co-Seg++ framework for versatile medical segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing semantic and instance segmentation tasks to mutually enhance each other. We first devise a spatio-sequential prompt encoder (SSP-Encoder) to capture long-range spatial and sequential relationships between segmentation regions and image embeddings as prior spatial constraints. Moreover, we devise a multi-task collaborative decoder (MTC-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, jointly computing semantic and instance segmentation masks. Extensive experiments on diverse CT and histopathology datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts in the semantic, instance, and panoptic segmentation of dental anatomical structures, histopathology tissues, and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg-Plus.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards</title>
<link>https://arxiv.org/abs/2506.18331</link>
<guid>https://arxiv.org/abs/2506.18331</guid>
<content:encoded><![CDATA[
arXiv:2506.18331v4 Announce Type: replace 
Abstract: While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches. Our implementation code is publicly available at: https://github.com/AHHHZ975/Differentiable-Texture-Learning
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning an Ensemble Token from Task-driven Priors in Facial Analysis</title>
<link>https://arxiv.org/abs/2507.01290</link>
<guid>https://arxiv.org/abs/2507.01290</guid>
<content:encoded><![CDATA[
arXiv:2507.01290v3 Announce Type: replace 
Abstract: Facial analysis exhibits task-specific feature variations. While Convolutional Neural Networks (CNNs) have enabled the fine-grained representation of spatial information, Vision Transformers (ViTs) have facilitated the representation of semantic information at the patch level. While advances in backbone architectures have improved over the past decade, combining high-fidelity models often incurs computational costs on feature representation perspective. In this work, we introduce KT-Adapter, a novel methodology for learning knowledge token which enables the integration of high-fidelity feature representation in computationally efficient manner. Specifically, we propose a robust prior unification learning method that generates a knowledge token within a self-attention mechanism, sharing the mutual information across the pre-trained encoders. This knowledge token approach offers high efficiency with negligible computational cost. Our results show improved performance across facial analysis, with statistically significant enhancements observed in the feature representations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.04049</link>
<guid>https://arxiv.org/abs/2507.04049</guid>
<content:encoded><![CDATA[
arXiv:2507.04049v3 Announce Type: replace 
Abstract: Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding</title>
<link>https://arxiv.org/abs/2507.21888</link>
<guid>https://arxiv.org/abs/2507.21888</guid>
<content:encoded><![CDATA[
arXiv:2507.21888v3 Announce Type: replace 
Abstract: We address Embodied Reference Understanding, the task of predicting the object a person in the scene refers to through pointing gesture and language. This requires multimodal reasoning over text, visual pointing cues, and scene context, yet existing methods often fail to fully exploit visual disambiguation signals. We also observe that while the referent often aligns with the head-to-fingertip direction, in many cases it aligns more closely with the wrist-to-fingertip direction, making a single-line assumption overly limiting. To address this, we propose a dual-model framework, where one model learns from the head-to-fingertip direction and the other from the wrist-to-fingertip direction. We introduce a Gaussian ray heatmap representation of these lines and use them as input to provide a strong supervisory signal that encourages the model to better attend to pointing cues. To fuse their complementary strengths, we present the CLIP-Aware Pointing Ensemble module, which performs a hybrid ensemble guided by CLIP features. We further incorporate an auxiliary object center prediction head to enhance referent localization. We validate our approach on YouRefIt, achieving 75.0 mAP at 0.25 IoU, alongside state-of-the-art CLIP and C_D scores, and demonstrate its generality on unseen CAESAR and ISL Pointing, showing robust performance across benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
<link>https://arxiv.org/abs/2508.00518</link>
<guid>https://arxiv.org/abs/2508.00518</guid>
<content:encoded><![CDATA[
arXiv:2508.00518v2 Announce Type: replace 
Abstract: Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</title>
<link>https://arxiv.org/abs/2508.07871</link>
<guid>https://arxiv.org/abs/2508.07871</guid>
<content:encoded><![CDATA[
arXiv:2508.07871v2 Announce Type: replace 
Abstract: Modern large vision-language models (LVLMs) convert each input image into a large set of tokens that far outnumber the text tokens. Although this improves visual perception, it also introduces severe image token redundancy. Because image tokens contain sparse information, many contribute little to reasoning but greatly increase inference cost. Recent image token pruning methods address this issue by identifying important tokens and removing the rest. These methods improve efficiency with only small performance drops. However, most of them focus on single-image tasks and overlook multimodal in-context learning (ICL), where redundancy is higher and efficiency is more important. Redundant tokens weaken the advantage of multimodal ICL for rapid domain adaptation and lead to unstable performance. When existing pruning methods are applied in this setting, they cause large accuracy drops, which exposes a clear gap and the need for new approaches. To address this, we propose Contextually Adaptive Token Pruning (CATP), a training-free pruning method designed for multimodal ICL. CATP uses two stages of progressive pruning that fully reflect the complex cross-modal interactions in the input sequence. After removing 77.8% of the image tokens, CATP achieves an average performance gain of 0.6% over the vanilla model on four LVLMs and eight benchmarks, clearly outperforming all baselines. At the same time, it improves efficiency by reducing inference latency by an average of 10.78%. CATP strengthens the practical value of multimodal ICL and lays the foundation for future progress in interleaved image-text settings.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training</title>
<link>https://arxiv.org/abs/2508.12522</link>
<guid>https://arxiv.org/abs/2508.12522</guid>
<content:encoded><![CDATA[
arXiv:2508.12522v2 Announce Type: replace 
Abstract: Personalized expression recognition (ER) involves adapting a machine learning model to subject-specific data for improved recognition of expressions with considerable interpersonal variability. Subject-specific ER can benefit significantly from multi-source domain adaptation (MSDA) methods, where each domain corresponds to a specific subject to improve model accuracy and robustness. Despite promising results, state-of-the-art MSDA approaches often overlook multimodal information or blend sources into a single domain, limiting subject diversity and failing to explicitly capture unique subject-specific characteristics. To address these limitations, we introduce MuSACo, a multimodal subject-specific selection and adaptation method for ER based on co-training. It leverages complementary information across multiple modalities and multiple source domains for subject-specific adaptation. This makes MuSACo particularly relevant for affective computing applications in digital health, such as patient-specific assessment for stress or pain, where subject-level nuances are crucial. MuSACo selects source subjects relevant to the target and generates pseudo-labels using the dominant modality for class-aware learning, in conjunction with a class-agnostic loss to learn from less confident target samples. Finally, source features from each modality are aligned, while only confident target features are combined. Experimental results on challenging multimodal ER datasets: BioVid, StressID, and BAH show that MuSACo outperforms UDA (blending) and state-of-the-art MSDA methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples</title>
<link>https://arxiv.org/abs/2508.13309</link>
<guid>https://arxiv.org/abs/2508.13309</guid>
<content:encoded><![CDATA[
arXiv:2508.13309v2 Announce Type: replace 
Abstract: Numerous techniques have been proposed for generating adversarial examples in white-box settings under strict Lp-norm constraints. However, such norm-bounded examples often fail to align well with human perception, and only recently have a few methods begun specifically exploring perceptually aligned adversarial examples. Moreover, it remains unclear whether insights from Lp-constrained attacks can be effectively leveraged to improve perceptual efficacy. In this paper, we introduce DAASH, a fully differentiable meta-attack framework that generates effective and perceptually aligned adversarial examples by strategically composing existing Lp-based attack methods. DAASH operates in a multi-stage fashion: at each stage, it aggregates candidate adversarial examples from multiple base attacks using learned, adaptive weights and propagates the result to the next stage. A novel meta-loss function guides this process by jointly minimizing misclassification loss and perceptual distortion, enabling the framework to dynamically modulate the contribution of each base attack throughout the stages. We evaluate DAASH on adversarially trained models across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on Lp-constrained based methods, DAASH significantly outperforms state-of-the-art perceptual attacks such as AdvAD -- achieving higher attack success rates (e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM, LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively). Furthermore, DAASH generalizes well to unseen defenses, making it a practical and strong baseline for evaluating robustness without requiring handcrafted adaptive attacks for each new defense.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
arXiv:2508.15769v2 Announce Type: replace 
Abstract: 3D content generation has recently attracted significant research interest, driven by its critical applications in VR/AR and embodied AI. In this work, we tackle the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for extra optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architecture yields improved generation performance when multiple images are provided; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robustness of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Object Grounding for Time-Sensitive Video Understanding</title>
<link>https://arxiv.org/abs/2509.06335</link>
<guid>https://arxiv.org/abs/2509.06335</guid>
<content:encoded><![CDATA[
arXiv:2509.06335v2 Announce Type: replace 
Abstract: We propose to improve the time-sensitive video understanding (TSV) capability of video large language models (Video-LLMs) with grounded objects (GO). We hypothesize that TSV tasks can benefit from GO within frames, which is supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM for reasoning temporal localization. While augmenting prompts with textual descriptions of these object annotations improves the performance of LITA, it also introduces extra token length and susceptibility to the noise in object-level information. To address this, we propose GO-Tokenizer, a lightweight add-on module for Video-LLMs leveraging off-the-shelf object detectors to encode compact object information on the fly. Experimental results demonstrate that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its counterpart, utilizing textual descriptions of objects in the prompt. The gain generalizes across different models, datasets, and video understanding tasks, such as reasoning temporal localization and dense captioning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zo3T: Zero-Shot 3D-Aware Trajectory-Guided Image-to-Video Generation via Test-Time Training</title>
<link>https://arxiv.org/abs/2509.06723</link>
<guid>https://arxiv.org/abs/2509.06723</guid>
<content:encoded><![CDATA[
arXiv:2509.06723v3 Announce Type: replace 
Abstract: Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data-driven Typology of Vision Models from Integrated Representational Metrics</title>
<link>https://arxiv.org/abs/2509.21628</link>
<guid>https://arxiv.org/abs/2509.21628</guid>
<content:encoded><![CDATA[
arXiv:2509.21628v2 Announce Type: replace 
Abstract: Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity metrics, each capturing a different facet-geometry, unit tuning, or linear decodability-and assess family separability using multiple complementary measures. Metrics preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family discrimination, whereas flexible mappings such as Linear Predictivity show weaker separation. These findings indicate that geometry and tuning carry family-specific signatures, while linearly decodable information is more broadly shared. To integrate these complementary facets, we adapt Similarity Network Fusion (SNF), a method inspired by multi-omics integration. SNF achieves substantially sharper family separation than any individual metric and produces robust composite signatures. Clustering of the fused similarity matrix recovers both expected and surprising patterns: supervised ResNets and ViTs form distinct clusters, yet all self-supervised models group together across architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with masked autoencoders, suggesting convergence between architectural modernization and reconstruction-based training. This biology-inspired framework provides a principled typology of vision models, showing that emergent computational strategies-shaped jointly by architecture and training objective-define representational structure beyond surface design categories.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation</title>
<link>https://arxiv.org/abs/2510.11000</link>
<guid>https://arxiv.org/abs/2510.11000</guid>
<content:encoded><![CDATA[
arXiv:2510.11000v2 Announce Type: replace 
Abstract: Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction</title>
<link>https://arxiv.org/abs/2510.14885</link>
<guid>https://arxiv.org/abs/2510.14885</guid>
<content:encoded><![CDATA[
arXiv:2510.14885v2 Announce Type: replace 
Abstract: Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception</title>
<link>https://arxiv.org/abs/2510.17568</link>
<guid>https://arxiv.org/abs/2510.17568</guid>
<content:encoded><![CDATA[
arXiv:2510.17568v3 Announce Type: replace 
Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://arxiv.org/abs/2510.20812</link>
<guid>https://arxiv.org/abs/2510.20812</guid>
<content:encoded><![CDATA[
arXiv:2510.20812v3 Announce Type: replace 
Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery</title>
<link>https://arxiv.org/abs/2511.16887</link>
<guid>https://arxiv.org/abs/2511.16887</guid>
<content:encoded><![CDATA[
arXiv:2511.16887v2 Announce Type: replace 
Abstract: Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models</title>
<link>https://arxiv.org/abs/2511.22425</link>
<guid>https://arxiv.org/abs/2511.22425</guid>
<content:encoded><![CDATA[
arXiv:2511.22425v2 Announce Type: replace 
Abstract: We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IRPO: Boosting Image Restoration via Post-training GRPO</title>
<link>https://arxiv.org/abs/2512.00814</link>
<guid>https://arxiv.org/abs/2512.00814</guid>
<content:encoded><![CDATA[
arXiv:2512.00814v2 Announce Type: replace 
Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball</title>
<link>https://arxiv.org/abs/2512.01478</link>
<guid>https://arxiv.org/abs/2512.01478</guid>
<content:encoded><![CDATA[
arXiv:2512.01478v2 Announce Type: replace 
Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</title>
<link>https://arxiv.org/abs/2405.17537</link>
<guid>https://arxiv.org/abs/2405.17537</guid>
<content:encoded><![CDATA[
arXiv:2405.17537v5 Announce Type: replace-cross 
Abstract: Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery</title>
<link>https://arxiv.org/abs/2501.06039</link>
<guid>https://arxiv.org/abs/2501.06039</guid>
<content:encoded><![CDATA[
arXiv:2501.06039v2 Announce Type: replace-cross 
Abstract: Spatial proteomics technologies have transformed our understanding of complex tissue architecture in cancer but present unique challenges for computational analysis. Each study uses a different marker panel and protocol, and most methods are tailored to single cohorts, which limits knowledge transfer and robust biomarker discovery. Here we present Virtual Tissues (VirTues), a general-purpose foundation model for spatial proteomics that learns marker-aware, multi-scale representations of proteins, cells, niches and tissues directly from multiplex imaging data. From a single pretrained backbone, VirTues supports marker reconstruction, cell typing and niche annotation, spatial biomarker discovery, and patient stratification, including zero-shot annotation across heterogeneous panels and datasets. In triple-negative breast cancer, VirTues-derived biomarkers predict anti-PD-L1 chemo-immunotherapy response and stratify disease-free survival in an independent cohort, outperforming state-of-the-art biomarkers derived from the same datasets and current clinical stratification schemes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bezier Splatting for Fast and Differentiable Vector Graphics Rendering</title>
<link>https://arxiv.org/abs/2503.16424</link>
<guid>https://arxiv.org/abs/2503.16424</guid>
<content:encoded><![CDATA[
arXiv:2503.16424v4 Announce Type: replace-cross 
Abstract: Differentiable vector graphics (VGs) are widely used in image vectorization and vector synthesis, while existing representations are costly to optimize and struggle to achieve high-quality rendering results for high-resolution images. This work introduces a new differentiable VG representation, dubbed B\'ezier Splatting, that enables fast yet high-fidelity VG rasterization. B\'ezier Splatting samples 2D Gaussians along B\'ezier curves, which naturally provide positional gradients at object boundaries. Thanks to the efficient splatting-based differentiable rasterizer, B\'ezier Splatting achieves 30x and 150x faster per forward and backward rasterization step for open curves compared to DiffVG. Additionally, we introduce an adaptive pruning and densification strategy that dynamically adjusts the spatial distribution of curves to escape local minima, further improving VG quality. Furthermore, our new VG representation supports conversion to standard XML-based SVG format, enhancing interoperability with existing VG tools and pipelines. Experimental results show that B\'ezier Splatting significantly outperforms existing methods with better visual fidelity and significant optimization speedup.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases</title>
<link>https://arxiv.org/abs/2504.07606</link>
<guid>https://arxiv.org/abs/2504.07606</guid>
<content:encoded><![CDATA[
arXiv:2504.07606v3 Announce Type: replace-cross 
Abstract: Heart diseases remain the leading cause of mortality worldwide, implying approximately 18 million deaths according to the WHO. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid, and effective prediction. This work presents an automatic system based on a novel framework which combines Modal Decomposition and Masked Autoencoders (MAE) to extend the application from heart disease classification to the more challenging and specific task of heart failure time prediction, not previously addressed to the best of authors' knowledge. This system comprises two stages. The first one transforms the data from a database of echocardiography video sequences into a large collection of annotated images compatible with the training phase of machine learning-based frameworks and deep learning-based ones. This stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage builds and trains a Vision Transformer (ViT). MAEs based on a combined scheme of self-supervised (SSL) and supervised learning, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the ViT from scratch, even with scarce databases. The designed neural network analyses in real-time images from echocardiography sequences to estimate the time of happening a heart failure. This approach demonstrates to improve prediction accuracy from scarce databases and to be superior to several established ViT and Convolutional Neural Network (CNN) architectures. The source code will be incorporated into the next version release of the ModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion</title>
<link>https://arxiv.org/abs/2504.08937</link>
<guid>https://arxiv.org/abs/2504.08937</guid>
<content:encoded><![CDATA[
arXiv:2504.08937v4 Announce Type: replace-cross 
Abstract: In image fusion tasks, the absence of real fused images as priors forces most deep learning approaches to rely on large-scale paired datasets to extract global weighting features or to generate pseudo-supervised images through algorithmic constructions. Unlike previous methods, this work re-examines prior-guided learning under few-shot conditions by introducing rough set theory. We regard the traditional algorithm as a prior generator, while the network re-inferrs and adaptively optimizes the prior through a dynamic loss function, reducing the inference burden of the network and enabling effective few-shot learning.To provide the prior, we propose the Granular Ball Pixel Computation (GBPC) algorithm. GBPC models pixel pairs in a luminance subspace using meta-granular balls and mines intra-ball information at multiple granular levels. At the fine-grained level, sliding granular balls assign adaptive weights to individual pixels to produce pixel-level prior fusion. At the coarse-grained level, the algorithm performs split computation within a single image to estimate positive and boundary domain distributions, enabling modality awareness and prior confidence estimation, which dynamically guide the loss weighting.The network and the algorithmic prior are coupled through the loss function to form an integrated framework. Thanks to the dynamic weighting mechanism, the network can adaptively adjust to different priors during training, enhancing its perception and fusion capability across modalities. We name this framework GBFF (Granular Ball Fusion Framework). Experiments on four fusion tasks demonstrate that even with only ten training image pairs per task, GBFF achieves superior performance in both visual quality and model compactness. Code is available at: https://github.com/DMinjie/GBFF
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Task-Oriented Flying: Framework, Infrastructure, and Principles</title>
<link>https://arxiv.org/abs/2504.15129</link>
<guid>https://arxiv.org/abs/2504.15129</guid>
<content:encoded><![CDATA[
arXiv:2504.15129v2 Announce Type: replace-cross 
Abstract: Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</title>
<link>https://arxiv.org/abs/2505.11394</link>
<guid>https://arxiv.org/abs/2505.11394</guid>
<content:encoded><![CDATA[
arXiv:2505.11394v2 Announce Type: replace-cross 
Abstract: Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced, a method that is label-free and allows subsequent staining of sections after 3D-PLI measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established in the same section. However, inevitable distortions introduced during the staining process make a costly nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of such samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. We use a supervised setting, building on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method can predict a Cresyl violet staining from 3D-PLI, resulting in a virtual staining that exhibits plausible patterns of cell organization in gray matter, with larger cell bodies being localized at their expected positions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title>
<link>https://arxiv.org/abs/2505.12332</link>
<guid>https://arxiv.org/abs/2505.12332</guid>
<content:encoded><![CDATA[
arXiv:2505.12332v5 Announce Type: replace-cross 
Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PET Image Reconstruction Using Deep Diffusion Image Prior</title>
<link>https://arxiv.org/abs/2507.15078</link>
<guid>https://arxiv.org/abs/2507.15078</guid>
<content:encoded><![CDATA[
arXiv:2507.15078v2 Announce Type: replace-cross 
Abstract: Diffusion models have shown great promise in medical image denoising and reconstruction, but their application to Positron Emission Tomography (PET) imaging remains limited by tracer-specific contrast variability and high computational demands. In this work, we proposed an anatomical prior-guided PET image reconstruction method based on diffusion models, inspired by the deep diffusion image prior (DDIP) framework. The proposed method alternated between diffusion sampling and model fine-tuning guided by the PET sinogram, enabling the reconstruction of high-quality images from various PET tracers using a score function pretrained on a dataset of another tracer. To improve computational efficiency, the half-quadratic splitting (HQS) algorithm was adopted to decouple network optimization from iterative PET reconstruction. The proposed method was evaluated using one simulation and two clinical datasets. For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested on [$^{18}$F]FDG data and amyloid-negative PET data to assess out-of-distribution (OOD) performance. For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one [$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from another tracer. Experiment results show that the proposed PET reconstruction method can generalize robustly across tracer distributions and scanner types, providing an efficient and versatile reconstruction framework for low-dose PET imaging.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding WaveMamba with Frequency Maps for Image Debanding</title>
<link>https://arxiv.org/abs/2508.11331</link>
<guid>https://arxiv.org/abs/2508.11331</guid>
<content:encoded><![CDATA[
arXiv:2508.11331v2 Announce Type: replace-cross 
Abstract: Compression at low bitrates in modern codecs often introduces banding artifacts, especially in smooth regions such as skies. These artifacts degrade visual quality and are common in user-generated content due to repeated transcoding. We propose a banding restoration method that employs the Wavelet State Space Model and a frequency masking map to preserve high-frequency details. Furthermore, we provide a benchmark of open-source banding restoration methods and evaluate their performance on two public banding image datasets. Experimentation on the available datasets suggests that the proposed post-processing approach effectively suppresses banding compared to the state-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving image textures. Visual inspections of the results confirm this. Code and supplementary material are available at: https://github.com/xinyiW915/Debanding-PCS2025.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
<link>https://arxiv.org/abs/2508.19112</link>
<guid>https://arxiv.org/abs/2508.19112</guid>
<content:encoded><![CDATA[
arXiv:2508.19112v3 Announce Type: replace-cross 
Abstract: Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained transformer encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution decoder, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Contexts for Long Video Generation</title>
<link>https://arxiv.org/abs/2508.21058</link>
<guid>https://arxiv.org/abs/2508.21058</guid>
<content:encoded><![CDATA[
arXiv:2508.21058v3 Announce Type: replace-cross 
Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects</title>
<link>https://arxiv.org/abs/2510.02069</link>
<guid>https://arxiv.org/abs/2510.02069</guid>
<content:encoded><![CDATA[
arXiv:2510.02069v2 Announce Type: replace-cross 
Abstract: Accurate reconstruction and relighting of glossy objects remains a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restrict faithful material recovery and limit relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine environment map optimization accelerates convergence, and negative-only environment map clipping preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title>
<link>https://arxiv.org/abs/2510.24411</link>
<guid>https://arxiv.org/abs/2510.24411</guid>
<content:encoded><![CDATA[
arXiv:2510.24411v2 Announce Type: replace-cross 
Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Our code and data are available at https://github.com/OS-Copilot/OS-Sentinel.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.09907</link>
<guid>https://arxiv.org/abs/2511.09907</guid>
<content:encoded><![CDATA[
arXiv:2511.09907v2 Announce Type: replace-cross 
Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</title>
<link>https://arxiv.org/abs/2512.00387</link>
<guid>https://arxiv.org/abs/2512.00387</guid>
<content:encoded><![CDATA[
<div> Keywords: WiseEdit, image editing, cognitive creativity, knowledge-intensive benchmark, evaluation

<br /><br />Summary:  
The paper introduces WiseEdit, a novel knowledge-intensive benchmark designed to comprehensively evaluate advanced image editing models that incorporate cognition and creativity. Existing benchmarks are criticized for their narrow scope and inability to fully assess these sophisticated abilities. WiseEdit addresses this gap by structuring image editing tasks into three cascaded cognitive steps: Awareness, Interpretation, and Imagination, each representing distinct challenges for model evaluation. The benchmark additionally includes complex tasks that require the integrated use of these three steps, pushing models beyond simple editing capabilities. WiseEdit incorporates three fundamental types of knowledge—Declarative, Procedural, and Metacognitive—providing broad knowledge coverage to test models' reasoning and creativity. In total, the benchmark contains 1,220 test cases, enabling a detailed and objective assessment of state-of-the-art image editing models’ limitations in cognitive reasoning and creative composition. The benchmark, along with evaluation code and example generated images from various models, will be made publicly available soon. This work aims to push forward research in intelligent image editing by offering a deep and broad evaluation framework inspired by human cognitive processes in creativity. Further details and resources are accessible on the project webpage. <div>
arXiv:2512.00387v2 Announce Type: replace 
Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</title>
<link>https://arxiv.org/abs/2512.00960</link>
<guid>https://arxiv.org/abs/2512.00960</guid>
<content:encoded><![CDATA[
<div> 4D HOI reconstruction, human-object interaction, monocular videos, human-in-the-loop, dataset

<br /><br />Summary: This paper addresses the challenge of reconstructing accurate and scalable 4D human-object interaction (HOI) data from diverse and large-scale monocular internet videos, which are abundant but difficult to analyze. The authors propose 4DHOISolver, an efficient optimization framework that integrates sparse human-in-the-loop contact point annotations to constrain the ill-posed 4D HOI reconstruction problem, ensuring high spatio-temporal coherence and physical plausibility. Using this method, they introduce Open4DHOI, a large-scale dataset featuring 144 object types and 103 different actions, offering a rich variety for learning generalized robot interactions. The effectiveness of the reconstructed 4D interactions is validated by enabling a reinforcement learning agent to imitate the recovered human-object manipulation motions. Despite these advances, the paper highlights that current 3D foundation models fall short in automatically predicting precise human-object contact correspondences, making this an open problem for the community. This limitation justifies the human-in-the-loop annotation strategy employed. Data and code for 4DHOISolver and Open4DHOI will be made publicly available, advancing research in robotics and HOI understanding from in-the-wild videos. <div>
arXiv:2512.00960v2 Announce Type: replace 
Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-ACT: Learn from Multimodal Parallel Generation to Act</title>
<link>https://arxiv.org/abs/2512.00975</link>
<guid>https://arxiv.org/abs/2512.00975</guid>
<content:encoded><![CDATA[
<div> Keywords: generalist robotic policy, Vision-Language-Action model, multimodal learning, cross-modal learning, robotic task performance<br /><br />Summary:<br /><br />This paper introduces MM-ACT, a unified Vision-Language-Action (VLA) model designed for generalist robotic policies that require semantic understanding and predictive interaction capabilities. The model integrates text, image, and action data within a shared token space and supports generation across these three modalities. To enhance generation efficiency, MM-ACT utilizes a re-mask parallel decoding strategy for text and image outputs and a one-step parallel decoding strategy for action generation. The novel training method, called Context-Shared Multimodal Learning, supervises generation across modalities from a unified contextual representation, boosting the quality of action generation through cross-modal information exchange. The model’s performance was validated on three benchmarks: the LIBERO simulation, real-world Franka robot tasks, and RoboTwin2.0 for bimanual task execution. MM-ACT achieved impressive success rates of 96.3% on LIBERO, 72.0% on three Franka robot tasks, and 52.38% on eight RoboTwin2.0 tasks, demonstrating strong in-domain and out-of-domain capabilities. Notably, the cross-modal learning paradigm contributed an additional 9.25% improvement in task success rates. The authors have made their codes, models, and datasets publicly available at the provided GitHub repository to facilitate further research and development in robotic generalist policies. <div>
arXiv:2512.00975v2 Announce Type: replace 
Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</title>
<link>https://arxiv.org/abs/2512.01302</link>
<guid>https://arxiv.org/abs/2512.01302</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, Multi-Modal Diffusion Transformers, divide-and-conquer, attention masks, noise initialization<br /><br />Summary:<br /><br />This paper addresses the challenge of generating accurate long or multiple texts in text-to-image models, which typically suffer from diluted global attention. The authors propose DCText, a training-free method based on a divide-and-conquer approach that leverages Multi-Modal Diffusion Transformers known for reliable short-text generation. DCText first decomposes complex prompts by extracting and dividing the target text into segments, each assigned to specific regions in the image. To ensure each text segment is accurately rendered and the overall image remains coherent, two novel attention masks, Text-Focus and Context-Expansion, are introduced and applied sequentially during the denoising process. Furthermore, the method employs Localized Noise Initialization to enhance text accuracy and alignment of regions without increasing computational overhead. Extensive experiments on both single- and multi-sentence benchmarks demonstrate that DCText achieves superior text accuracy compared to existing approaches, while maintaining high image quality. Additionally, DCText outperforms others in generation speed, offering the lowest latency. This technique provides an effective and efficient solution to improve text rendering in text-to-image synthesis without requiring additional training. <div>
arXiv:2512.01302v2 Announce Type: replace 
Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians</title>
<link>https://arxiv.org/abs/2512.01306</link>
<guid>https://arxiv.org/abs/2512.01306</guid>
<content:encoded><![CDATA[
<div> Gaussian Swaying, aerodynamic simulation, 3D Gaussians, surface-based framework, realistic motion<br /><br />Summary:<br /><br />This paper introduces Gaussian Swaying, a novel surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike traditional mesh-based methods that require complex and computationally expensive meshing, or particle-based techniques which depend on discrete positional data, this approach models surfaces continuously with 3D Gaussian patches. This continuous representation enables efficient and fine-grained aerodynamic interactions that enhance realism in simulating natural motions such as branches swaying, flags rippling, and boats rocking. A key advantage of the method is its unification of simulation and rendering on the same representation, where Gaussian patches facilitate both force computations for dynamics and normal calculations for lightweight shading simultaneously. The framework is tested across synthetic and real-world datasets and evaluates multiple performance metrics to demonstrate its effectiveness. Results show that Gaussian Swaying achieves state-of-the-art performance and computational efficiency, making it a scalable solution for realistic aerodynamic scene simulation. The research highlights how this method advances the realism and efficiency of aerodynamic effects critical for applications in vision and graphics. <div>
arXiv:2512.01306v2 Announce Type: replace 
Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title>
<link>https://arxiv.org/abs/2512.01821</link>
<guid>https://arxiv.org/abs/2512.01821</guid>
<content:encoded><![CDATA[
<div> Spatial reasoning, Multimodal Large Language Models, Implicit spatial modeling, Relative Positional Encoding, Geometry-aware dataset<br /><br />Summary:<br /><br />This paper addresses the challenge of spatial reasoning in Multimodal Large Language Models (MLLMs), highlighting the limitation of current methods that rely solely on verbal descriptive tuning, leading to visual illiteracy. To overcome this, the authors propose MILO, an implicit spatial world modeling paradigm that enables human-like spatial imagination by integrating a visual generator to provide geometry-aware feedback. This integration grounds symbolic reasoning in perceptual visual experience rather than purely textual symbols. Alongside MILO, the study introduces RePE (Relative Positional Encoding), a novel encoding method that captures relative camera-pose transformations, which proves more effective than absolute coordinate systems for spatial understanding. To facilitate training, the authors develop GeoGen, a large-scale dataset featuring around 2,241 videos and 67,827 observation-action-outcome triplets, designed to be geometry-aware and generative. Experimental results across multiple baselines and benchmarks demonstrate that this combined approach significantly improves the spatial reasoning capabilities of MLLMs, enabling a more comprehensive and holistic understanding of 3D spaces. This work represents an important advancement in bridging symbolic spatial concepts and perceptual visual grounding in multimodal AI systems. <div>
arXiv:2512.01821v2 Announce Type: replace 
Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTArena: A Benchmark for Agentic PowerPoint Editing</title>
<link>https://arxiv.org/abs/2512.03042</link>
<guid>https://arxiv.org/abs/2512.03042</guid>
<content:encoded><![CDATA[
<div> PowerPoint editing, benchmark, PPTArena, PPTPilot, visual fidelity<br /><br />Summary:<br /><br />1. The paper introduces PPTArena, a new benchmark designed to evaluate reliable in-place editing of real PowerPoint slides guided by natural-language instructions. <br /><br />2. Unlike previous approaches focusing on image-PDF renderings or text-to-slide generation, PPTArena emphasizes detailed edits within existing slides covering text, charts, tables, animations, and master-level styles. It comprises 100 presentation decks, 2125 slides, and over 800 targeted edits. <br /><br />3. Each task in PPTArena includes a ground-truth deck along with a fully specified target outcome and utilizes a dual vision-language model (VLM) judge pipeline that separately assesses instruction adherence and visual quality using structural differences and slide image comparisons. <br /><br />4. The authors present PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences and dynamically routes between high-level programmatic tools and deterministic XML operations for precise modifications. It verifies results through an iterative plan-edit-check loop against task constraints. <br /><br />5. Experimental results demonstrate that PPTPilot outperforms strong proprietary and state-of-the-art VLM-based systems by over 10 percentage points, particularly excelling in visual fidelity, layout-sensitive edits, and deck-wide consistency. Nonetheless, long-horizon, document-scale editing tasks in PPTArena remain challenging for all existing agents, indicating significant room for future improvement in reliable automated PPT editing. <div>
arXiv:2512.03042v2 Announce Type: replace 
Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices</title>
<link>https://arxiv.org/abs/2512.05969</link>
<guid>https://arxiv.org/abs/2512.05969</guid>
<content:encoded><![CDATA[
<div> Keywords: video generation models, reasoning, Task Pair design, automated evaluation, reinforcement learning<br /><br />Summary:<br /><br />This paper demonstrates that video generation models have advanced to a stage where they can perform reasoning tasks. The models were tested on diverse reasoning challenges such as chess, maze navigation, Sudoku, mental rotation, and Raven’s Matrices, with leading models like Sora-2 achieving success rates around sixty percent. To facilitate systematic evaluation, the authors introduce a robust experimental paradigm called the "Task Pair" design, which pairs related tasks to test reasoning ability. They developed a code framework supporting 39 pre-integrated models and allowing easy addition of new models and tasks, ensuring scalability and extensibility. The study validates that their automated evaluation metrics strongly correlate with human judgment, indicating that this evaluation approach is reliable and scalable. With this foundation, the paper suggests that reinforcement learning can be applied to further improve the reasoning capabilities of video generation models. The authors provide open access to their raw results via a public website and share their entire evaluation codebase, VMEvalKit, on GitHub, encouraging community involvement and further research in this area. <div>
arXiv:2512.05969v1 Announce Type: new 
Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Dataset Quantization: A New Direction for Dataset Pruning</title>
<link>https://arxiv.org/abs/2512.05987</link>
<guid>https://arxiv.org/abs/2512.05987</guid>
<content:encoded><![CDATA[
<div> Dataset Quantization, Intra-sample Redundancy, Adaptive Quantization, Edge Devices, Compression

<br /><br />Summary:  
This paper proposes a novel dataset quantization approach aimed at reducing storage and communication costs for large-scale datasets, particularly in resource-constrained edge devices. Unlike traditional methods that address inter-sample redundancy, the presented technique focuses on compressing each image by reducing intra-sample redundancy, i.e., redundant or less informative content within individual samples while preserving essential features. The method first uses linear symmetric quantization to determine an initial quantization range and scale for each sample. Following this, an adaptive quantization allocation algorithm is employed to assign different quantization ratios according to samples' precision requirements, ensuring a constant total compression ratio across the dataset. Key contributions include being the first to represent datasets with limited bits for significant storage reduction, introducing a dataset-level quantization algorithm with adaptive ratio allocation, and performing extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. The results demonstrate that this approach maintains model training performance while achieving substantial compression, outperforming conventional quantization and dataset pruning techniques at equivalent compression ratios. This work highlights the potential for effective dataset compression strategies tailored to edge computing scenarios without sacrificing training accuracy. <div>
arXiv:2512.05987v1 Announce Type: new 
Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG3T: Visual Geometry Grounded Gaussian Transformer</title>
<link>https://arxiv.org/abs/2512.05988</link>
<guid>https://arxiv.org/abs/2512.05988</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic occupancy, multi-view fusion, Gaussian representation, Grid-Based Sampling, Positional Refinement<br /><br />Summary:<br /><br />1. The paper addresses the challenge of creating coherent 3D scene representations from multiple images taken at different viewpoints, which is a fundamental problem in 3D reconstruction and scene understanding.<br />2. Existing methods often fail due to fragmented and inconsistent 3D representations caused by processing each view independently, limiting the overall quality and coherence of the 3D output.<br />3. The authors propose VG3T, a novel multi-view feed-forward neural network that predicts a 3D semantic occupancy by directly modeling a set of semantically labeled 3D Gaussian primitives, integrating information from all views jointly rather than view-by-view.<br />4. Two key innovations, Grid-Based Sampling and Positional Refinement, are introduced to reduce the typical distance-dependent density bias arising from pixel-aligned initializations of Gaussian primitives, improving geometric and semantic accuracy.<br />5. VG3T demonstrates superior performance on the challenging nuScenes dataset by achieving a 1.7% absolute improvement in mean Intersection over Union (mIoU) while using 46% fewer Gaussian primitives than the previous state-of-the-art, indicating enhanced efficiency and representational power. <div>
arXiv:2512.05988v1 Announce Type: new 
Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head</title>
<link>https://arxiv.org/abs/2512.05991</link>
<guid>https://arxiv.org/abs/2512.05991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, emotional expression, Emotion-aware Gaussian Diffusion, action unit prompt, text-to-AU emotion controller<br /><br />Summary:<br /><br />This paper addresses the limitations of existing photo-realistic 3D talking head models based on 3D Gaussian Splatting, particularly in manipulating emotional expressions with fine granularity and expansive dynamics via multimodal control. The authors propose EmoDiffTalk, an editable 3D Gaussian talking head framework that introduces a novel Emotion-aware Gaussian Diffusion mechanism. This mechanism incorporates an action unit (AU) prompt Gaussian diffusion process designed for detailed facial animation, enabling precise control over subtle emotional expressions. Furthermore, EmoDiffTalk integrates an accurate text-to-AU emotion controller, allowing expressive and dynamic emotional editing driven by text input. The system has been evaluated on public datasets EmoTalk3D and RenderMe-360, where it demonstrates superior performance in emotional subtlety, lip-sync accuracy, and overall controllability compared to prior works. EmoDiffTalk sets a new standard for high-quality, diffusion-based multimodal 3D talking head synthesis. Notably, this framework is among the first to leverage 3D Gaussian Splatting for talking-head generation that supports continuous and multimodal emotional editing within the AU-based expression space, paving the way for advanced, editable 3D facial animation technologies. <div>
arXiv:2512.05991v1 Announce Type: new 
Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology</title>
<link>https://arxiv.org/abs/2512.05993</link>
<guid>https://arxiv.org/abs/2512.05993</guid>
<content:encoded><![CDATA[
<div> Neuropathology, foundation models, neurodegenerative diseases, whole-slide images, domain-specific AI  

<br /><br />Summary:  
1. Foundation models have revolutionized computational pathology by learning from large histology datasets but are mainly trained on surgical pathology data, which does not represent nervous tissue well.  
2. Neuropathology involves distinct tissue types and pathological features, such as neurons, glia, neurofibrillary tangles, amyloid plaques, Lewy bodies, and unique neurodegeneration patterns, which are not adequately captured by general-purpose models.  
3. To bridge this domain gap, the authors developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue covering various neurodegenerative diseases.  
4. NeuroFM outperforms general foundation models in neuropathology-specific tasks including mixed dementia classification, segmentation of the hippocampal region, and identification of neurodegenerative ataxias such as cerebellar essential tremor and spinocerebellar ataxia subtypes.  
5. This study demonstrates that domain-specialized foundation models tailored to brain pathology provide more accurate and reliable analyses for diagnosing and researching neurodegenerative diseases, setting a precedent for future development of specialized AI models in digital pathology. <div>
arXiv:2512.05993v1 Announce Type: new 
Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</title>
<link>https://arxiv.org/abs/2512.05996</link>
<guid>https://arxiv.org/abs/2512.05996</guid>
<content:encoded><![CDATA[
<div> Keywords: Fish detection, Underwater imagery, Weak supervision, Reinforcement learning, Marine ecology

<br /><br />Summary: Analyzing underwater fish imagery presents challenges due to visual degradation and the high cost of detailed annotations. The paper introduces FishDetector-R1, a novel multi-modal large model (MLLM)-based framework designed to detect, segment, and count fish using weak supervision. Evaluated on the DeepFish dataset, FishDetector-R1 shows significant performance improvements, achieving a 20% increase in Average Precision (AP), 10% higher mean Intersection over Union (mIoU), a 30% reduction in Mean Absolute Error (MAE), and a 35% decrease in the Grid Average Mean Absolute Error (GAME). These gains are attributed to two core innovations: a detect-to-count prompt that ensures spatially consistent fish detections and counts, and a Reinforcement Learning from Verifiable Reward (RLVR) method that uses sparse point labels within a scalable reward framework. Ablation studies confirm the effectiveness of the designed reward system. Additionally, FishDetector-R1 demonstrates strong cross-domain generalization capabilities on other underwater datasets, highlighting its robustness. Overall, this approach offers a reliable and scalable solution to improve marine visual ecological monitoring through weakly supervised learning, reducing annotation costs while maintaining high accuracy. More details and resources are available at the project page: https://umfieldrobotics.github.io/FishDetector-R1. <div>
arXiv:2512.05996v1 Announce Type: new 
Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrunedCaps: A Case For Primary Capsules Discrimination</title>
<link>https://arxiv.org/abs/2512.06003</link>
<guid>https://arxiv.org/abs/2512.06003</guid>
<content:encoded><![CDATA[
<div> Capsule Networks, Pruning, Primary Capsules, Dynamic Routing, Computational Efficiency<br /><br />Summary:<br /><br />This paper addresses the inefficiency of Capsule Networks (CapsNets) caused by the large number of Primary Capsules (PCs), which results in slow training and testing phases. The authors propose pruning the Primary Capsules to enhance the resource efficiency of CapsNets. They conduct experiments on four datasets: MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN. The results demonstrate that pruning up to 95 percent of the Capsules significantly accelerates the CapsNet, achieving up to 9.90 times faster performance compared to the original architecture without any drop in accuracy. Additionally, the pruned CapsNet reduces floating-point operations in the dynamic routing stage by more than 95.36 percent, highlighting substantial computational savings. The study further explores the varying impact of pruning on different datasets, providing insights into why some datasets benefit more from pruning than others. Overall, this research offers a practical approach to make CapsNets more resource-efficient while maintaining their advantages, contributing to faster and less resource-intensive deep learning models suitable for image classification tasks. <div>
arXiv:2512.06003v1 Announce Type: new 
Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization</title>
<link>https://arxiv.org/abs/2512.06006</link>
<guid>https://arxiv.org/abs/2512.06006</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, code adaptation, biomedical imaging, agent design, production pipeline<br /><br />Summary:<br /><br />1. The article addresses the challenge of adapting production-level computer vision tools to specialized scientific datasets, which currently faces significant bottlenecks due to the need for large annotated datasets for fine-tuning or labor-intensive manual code changes.  
2. It proposes the use of AI agents to automate the manual coding process required for adapting these tools, aiming to streamline and accelerate the adaptation to bespoke scientific datasets.  
3. A systematic evaluation framework for agentic code optimization is introduced, enabling objective assessment of different AI agent designs on this task.  
4. The framework is applied to three biomedical imaging pipelines, demonstrating that a simple AI agent architecture can consistently generate adaptation code that outperforms solutions produced by human experts.  
5. The study finds that more complex agent architectures do not always offer advantages, offering practical insights and a roadmap for optimal agent design.  
6. The framework and approach have been open-sourced, and the authors validate their method by successfully deploying agent-generated functions into a real production pipeline, illustrating clear potential for real-world scientific impact. <div>
arXiv:2512.06006v1 Announce Type: new 
Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Flexible Robustness Certificates for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06010</link>
<guid>https://arxiv.org/abs/2512.06010</guid>
<content:encoded><![CDATA[
<div> Keywords: certifiable robustness, semantic segmentation, Lipschitz constraints, adversarial attacks, randomized smoothing<br /><br />Summary: This paper addresses the vulnerability of deep neural networks to small adversarial perturbations, focusing on semantic segmentation tasks rather than classification. The authors introduce a new class of semantic segmentation networks incorporating built-in Lipschitz constraints to ensure certifiable robustness. These networks are efficiently trainable and achieve competitive pixel accuracy on challenging datasets like Cityscapes. A novel framework is proposed to generalize robustness certificates for semantic segmentation, demonstrating both computational efficiency and flexibility of Lipschitz networks in providing robustness guarantees. The approach enables real-time compatible certifiably robust semantic segmentation for the first time, which is a significant advancement in practical deployment. The method also allows for the computation of worst-case performance under \(\ell_2\) adversarial attacks within a specified radius \(\epsilon\), covering a variety of performance metrics. A major highlight is the certification process's speed, which is shown to be around 600 times faster than randomized smoothing methods at inference on an NVIDIA A100 GPU while maintaining comparable certificate quality. Finally, the authors validate the tightness of their worst-case robustness certificates by benchmarking them against state-of-the-art adversarial attacks, confirming the robustness and reliability of the proposed method. <div>
arXiv:2512.06010v1 Announce Type: new 
Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $\epsilon$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2512.06012</link>
<guid>https://arxiv.org/abs/2512.06012</guid>
<content:encoded><![CDATA[
<div> Keywords: Selective Laser Melting, powder morphology, machine learning, clustering, Fourier descriptors  

<br /><br />Summary:  
1. Selective Laser Melting (SLM) is an additive manufacturing technique whose part quality is highly dependent on the morphology of the metallic powder feedstock.  
2. Traditional powder characterization methods are low-throughput and qualitative, often failing to represent the heterogeneity found in industrial-scale powder batches.  
3. The authors propose an automated, machine learning-based framework that integrates high-throughput imaging with shape extraction and clustering to profile powder morphology at scale.  
4. Three clustering pipelines are developed and evaluated: an autoencoder-based pipeline, a shape-descriptor-based pipeline, and a functional-data-based pipeline.  
5. Using a dataset of approximately 126,000 powder particle images ranging from 0.5 to 102 micrometers, internal validity metrics such as the Davies-Bouldin index and Calinski-Harabasz score identify the Fourier-descriptor combined with k-means clustering as the most effective approach.  
6. This Fourier-descriptor + k-means pipeline achieves a very low Davies-Bouldin index and high Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop, demonstrating both effectiveness and efficiency.  
7. Although this study focuses on establishing a robust morphological clustering framework, the identified shape groups provide a foundation for future investigations into how particle shape affects powder flowability, packing density, and final SLM part quality.  
8. The unsupervised learning framework enables rapid, automated morphological assessment and allows monitoring of shape changes across powder reuse cycles, offering a path toward real-time feedstock monitoring within SLM manufacturing workflows. <div>
arXiv:2512.06012v1 Announce Type: new 
Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAT: Vision Action Transformer by Unlocking Full Representation of ViT</title>
<link>https://arxiv.org/abs/2512.06013</link>
<guid>https://arxiv.org/abs/2512.06013</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, robot learning, action tokens, imitation learning, manipulation tasks<br /><br />Summary:<br /><br />1. The paper addresses a limitation in current robot learning methods that use Vision Transformers (ViTs) by relying solely on features from the final layer, which discards valuable information.  
2. It proposes a novel architecture called Vision Action Transformer (VAT), which extends ViT by incorporating action tokens processed with visual features across all transformer layers.  
3. This design enables a deep and progressive fusion of perception and action generation, utilizing the full hierarchical representation of ViT rather than only the last layer's output.  
4. Experimental evaluation on simulated manipulation tasks across four LIBERO benchmarks shows that VAT achieves a 98.15% average success rate, outperforming previous state-of-the-art approaches such as OpenVLA-OFT.  
5. The work highlights the critical importance of leveraging the entire "representation trajectory" in vision models to enhance robotic policy learning and provides publicly available code on GitHub for reproducibility and further research. <div>
arXiv:2512.06013v1 Announce Type: new 
Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets</title>
<link>https://arxiv.org/abs/2512.06014</link>
<guid>https://arxiv.org/abs/2512.06014</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, chest X-ray, medical imaging, embedding models, model benchmarking

<br /><br />Summary:  
This study benchmarks two large-scale chest X-ray embedding models, CXR-Foundation (ELIXR v2.0) and MedImageInsight, using public datasets MIMIC-CR and NIH ChestX-ray14. Both models were evaluated through a unified preprocessing pipeline alongside fixed downstream LightGBM classifiers to ensure reproducibility. Embeddings were extracted directly from pre-trained encoders for multiple disease label prediction tasks. Performance metrics including mean AUROC and F1-score with 95% confidence intervals were reported. Results showed MedImageInsight slightly outperformed CXR-Foundation across most tasks, while CXR-Foundation demonstrated superior cross-dataset stability. Additionally, unsupervised clustering of MedImageInsight embeddings revealed well-defined disease-specific grouping consistent with quantitative performance. The study emphasizes the importance of standardizing evaluations for medical foundation models to enable fair and reproducible comparisons. Finally, it establishes reproducible baseline performances to facilitate future research, particularly in multimodal data integration and clinical applications. <div>
arXiv:2512.06014v1 Announce Type: new 
Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation</title>
<link>https://arxiv.org/abs/2512.06020</link>
<guid>https://arxiv.org/abs/2512.06020</guid>
<content:encoded><![CDATA[
<div> Preference-conditioned image generation, multimodal large language models, visual question answering, inter-user discrimination, diffusion-based image generation<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting image generation models to individual user preferences, aiming to produce images that reflect personal aesthetics beyond the textual prompt. The authors propose a novel multimodal framework that employs multimodal large language models (MLLMs) to extract detailed user representations. These representations are introduced into diffusion-based image generators to guide output personalization. To refine preference extraction, the MLLM is trained on a preference-oriented visual question answering task, enabling it to capture subtle semantic cues. The framework includes two probing tasks: inter-user discrimination, which differentiates between users, and intra-user discrimination, which distinguishes between content liked or disliked by a user. To ensure that the user embeddings align with the diffusion model’s text encoders despite modality differences, a maximum mean discrepancy-based alignment loss is designed, preserving multimodal structure and compatibility. The resulting embeddings effectively condition the image generator, allowing it to honor both the user's preferences and the given text prompts. Extensive experiments validate the approach, showing significant improvements over existing baselines in image quality and preference alignment, demonstrating the strength of enhanced representation extraction and modality alignment for personalized image generation. <div>
arXiv:2512.06020v1 Announce Type: new 
Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing</title>
<link>https://arxiv.org/abs/2512.06024</link>
<guid>https://arxiv.org/abs/2512.06024</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, wave free surface, velocity fields, attention-augmented neural network, ocean waves<br /><br />Summary:<br /><br />1. The paper presents a novel neural network designed for precise three-dimensional reconstruction of ocean wave free surfaces and associated velocity fields, tackling the challenge of long-term ocean wave observations.<br /><br />2. The network architecture is an attention-augmented pyramid tailored to handle the multi-scale and temporally continuous nature of wave motions, improving robustness especially under visual occlusions.<br /><br />3. Physics-based constraints are integrated into the model to enable time-resolved reconstruction of nonlinear 3D velocity fields derived from the evolving free-surface boundary.<br /><br />4. Experimental results from real-sea conditions show millimetre-level accuracy in wave elevation prediction, dominant-frequency errors below 0.01 Hz, and precise estimation of high-frequency spectral power laws.<br /><br />5. The model achieves high-fidelity 3D reconstruction of nonlinear velocity fields efficiently, capable of dense reconstruction of two million points in just 1.35 seconds, outperforming conventional visual reconstruction methods and demonstrating strong generalization even with occlusions, due to global multi-scale attention and learned wave propagation dynamics. <div>
arXiv:2512.06024v1 Announce Type: new 
Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06032</link>
<guid>https://arxiv.org/abs/2512.06032</guid>
<content:encoded><![CDATA[
<div> Segment Anything Models, SAM2, SAM3, prompt-based segmentation, concept-driven segmentation<br /><br />Summary:<br /><br />This paper highlights the fundamental discontinuity between Segment Anything Model versions SAM2 and SAM3. (1) Conceptual Break: SAM2 relies on spatial prompts like points, boxes, and masks for geometric and temporal segmentation, whereas SAM3 utilizes a multimodal fusion framework incorporating vision and language for semantic, open-vocabulary, and concept-driven segmentation. (2) Architectural Divergence: SAM2 is purely vision-temporal with spatial prompt inputs, while SAM3 integrates vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and Mixture-of-Experts to manage ambiguity. (3) Dataset and Annotation Differences: SAM2 is trained on SA-V video masks focused on geometric delineation, whereas SAM3 leverages multimodal corpora annotated with rich semantic concepts. (4) Training and Hyperparameter Distinctions: SAM2’s optimization strategies are unsuitable for SAM3 due to the latter’s complex multimodal objectives and architecture. (5) Evaluation and Failure Modes: SAM2 uses geometric Intersection-over-Union (IoU) metrics, while SAM3 requires semantic, open-vocabulary evaluation to capture concept-level performance. Overall, the analysis positions SAM3 as initiating a new class of segmentation foundation models centered on concept-driven understanding, marking a shift from traditional prompt-based segmentation and indicating future research directions in this domain. <div>
arXiv:2512.06032v1 Announce Type: new 
Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Learning for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.06058</link>
<guid>https://arxiv.org/abs/2512.06058</guid>
<content:encoded><![CDATA[
<div> 3D data, point cloud, supervised learning, self-supervised learning, transfer learning<br /><br />Summary:<br /><br />This dissertation addresses the growing importance of 3D data acquisition and utilization across multiple fields such as computer vision, robotics, and geospatial analysis. It highlights the diverse methods of capturing 3D data, including 3D scanners, LiDARs, and RGB-D cameras, which provide detailed geometric, shape, and scale information. The work emphasizes the benefits of combining 3D data with 2D images to enhance machine understanding of environments, which is crucial for applications like autonomous driving, robotics, remote sensing, and medical treatment. The core focus of the research lies in three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning techniques, and transfer learning by leveraging 2D pre-trained models to improve 3D network training. Importantly, the approach does not rely on simply transforming 2D data into 3D but integrates 2D knowledge to boost 3D understanding effectively. The dissertation includes extensive experiments demonstrating the effectiveness and potential of the proposed methods to advance point cloud representation learning through the integration of 2D and 3D data. <div>
arXiv:2512.06058v1 Announce Type: new 
Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</title>
<link>https://arxiv.org/abs/2512.06065</link>
<guid>https://arxiv.org/abs/2512.06065</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric video editing, instruction-guided, real-time inference, hand-object interactions, dataset  

<br /><br />Summary:  
This work focuses on instruction-guided editing of egocentric videos, targeting interactive augmented reality (AR) applications. Unlike existing AI video editors that perform well on third-person footage, egocentric videos present unique challenges such as rapid egomotion and frequent hand-object interactions, creating a considerable domain gap that complicates editing tasks. Traditional offline editing methods also suffer from high latency, restricting their usability for real-time interactions. To overcome these challenges, the authors introduce a comprehensive ecosystem for egocentric video editing consisting of three key components. First, they create EgoEditData, a manually curated dataset tailored to egocentric editing scenarios which emphasizes preserving hand and object interactions explicitly. Second, they develop EgoEdit, an instruction-following video editor optimized for egocentric footage that supports real-time streaming inference on a single GPU, enabling interactive latency. Third, they propose EgoEditBench, an evaluation suite designed to assess instruction faithfulness, preservation of hands and interactions, and temporal stability despite rapid egomotion. Experimental results demonstrate that EgoEdit delivers temporally stable and instruction-faithful editing outcomes, outperforming existing methods on egocentric editing benchmarks while maintaining competitive performance on general video editing tasks. EgoEditData and EgoEditBench will be publicly released to facilitate further research. <div>
arXiv:2512.06065v1 Announce Type: new 
Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light</title>
<link>https://arxiv.org/abs/2512.06080</link>
<guid>https://arxiv.org/abs/2512.06080</guid>
<content:encoded><![CDATA[
<div> Keywords: single-photon lidar, 3D scene reconstruction, multi-bounce light, multiplexed illumination, deep learning<br /><br />Summary:<br />1. This work tackles the difficult problem of reconstructing 3D scenes, especially in the presence of occluded regions and specular materials such as mirrors, using single-photon lidar technology. <br />2. Single-photon lidars estimate depth by detecting light emitted into the scene and reflected back; uniquely, they also capture multi-bounce light, which carries rich information about occluded geometry and material properties but is challenging to interpret. <br />3. Previous efforts have only succeeded when the laser illuminates one point at a time, while this paper addresses the more practical and complex scenario of simultaneous illumination of multiple scene points, inducing complicated light transport due to multiplexing, two-bounce reflections, shadows, and specularity. <br />4. Analytical inversion of this complex light transport proves impractical, so the authors develop a data-driven approach using deep learning to decompose multi-bounce signals and recover constituent contributions from each laser spot. <br />5. To train their model, they create and release a large simulated dataset with around 100,000 lidar transients from indoor environments, and experimentally validate that their method successfully infers 3D geometry in challenging scenes from a single measurement, demonstrating improved reconstruction in occluded and mirror-containing scenes. <div>
arXiv:2512.06080v1 Announce Type: new 
Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06096</link>
<guid>https://arxiv.org/abs/2512.06096</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Multimodal Language Models, 360-degree BEV, autonomous driving, spatial reasoning  

<br /><br />Summary:  
The paper introduces BeLLA, an innovative end-to-end architecture designed for autonomous driving that integrates unified 360-degree Bird's Eye View (BEV) spatial representations with a large language model to enhance question answering capabilities. Traditional approaches often rely on single-view encoders or aggregated multi-view features, which either fail to fully utilize multi-camera spatial structures or lack a unified spatial framework, limiting effective reasoning about ego-centric directions and object relationships. BeLLA addresses these limitations by providing a consistent and comprehensive spatial representation, enabling more accurate understanding of relative object positions and contextual behavioral patterns crucial for autonomous navigation. The model is rigorously evaluated on two benchmarks—NuScenes-QA and DriveLM—where it demonstrates substantial improvements, especially in tasks demanding complex spatial reasoning, achieving up to a 9.3% absolute gain over prior methods. Additionally, BeLLA remains competitive across a broad spectrum of question types, showcasing its versatility and robustness. Overall, this work advances the integration of vision and language models in driving contexts, pushing forward the interpretability and contextual awareness of autonomous vehicle systems. <div>
arXiv:2512.06096v1 Announce Type: new 
Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360{\deg} BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2512.06103</link>
<guid>https://arxiv.org/abs/2512.06103</guid>
<content:encoded><![CDATA[
<div> Keywords: Iris recognition, Presentation Attack Detection, Multispectral imaging, Vision Transformer, Spoofing detection

<br /><br />Summary: Iris recognition is a highly accurate biometric modality but is vulnerable to presentation attacks (PAs), making effective presentation attack detection (PAD) essential for security. Traditional iris systems typically use near-infrared (NIR) imaging, but multispectral imaging across multiple NIR bands can enhance PAD generalizability by providing complementary reflectance information. This work introduces SpectraIrisPAD, a novel deep learning framework based on a DINOv2 Vision Transformer backbone, which incorporates learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features for robust PAD. To support this, the authors developed MSIrPAD, a new comprehensive multispectral iris PAD dataset containing 18,848 images across eight diverse attack types, including textured contact lenses, print attacks, and display-based attacks, captured at five NIR wavelengths (800 nm, 830 nm, 850 nm, 870 nm, and 980 nm) using a custom multispectral sensor. Extensive experiments under unseen attack scenarios demonstrate that SpectraIrisPAD consistently outperforms state-of-the-art methods on multiple metrics, showcasing its superior robustness and generalization capabilities in detecting a wide variety of iris presentation attacks. <div>
arXiv:2512.06103v1 Announce Type: new 
Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation</title>
<link>https://arxiv.org/abs/2512.06105</link>
<guid>https://arxiv.org/abs/2512.06105</guid>
<content:encoded><![CDATA[
<div> Keywords: melanoma classification, interpretability, contrastive learning, Vision Transformer, clinical trust

<br /><br />Summary: This paper addresses the challenge of model opacity in deep learning-based melanoma classification, which hinders clinical adoption despite high accuracy. The authors propose a Cross-modal Explainable Framework for Melanoma (CEFM) that integrates clinical diagnostic criteria—Asymmetry, Border, and Color (ABC)—directly into the model's embedding space using contrastive learning. By employing dual projection heads within a Vision Transformer architecture, CEFM aligns visual features with clinical semantics, fostering interpretability. These aligned features are then converted into structured textual explanations through natural language generation, creating a transparent connection between raw images and clinical insights. Experimental results on public melanoma datasets show that CEFM achieves a classification accuracy of 92.79% and an AUC of 0.961. The framework also demonstrates significant improvements in interpretability metrics compared to baseline methods. Qualitative analyses further reveal that the spatial arrangement of learned embeddings corresponds well with clinicians’ use of the ABC rule, effectively enhancing trust and understanding of the model's decisions. Overall, CEFM successfully bridges the gap between black-box high-performance melanoma classification models and clinically reliable, interpretable tools for dermatology practice. <div>
arXiv:2512.06105v1 Announce Type: new 
Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation</title>
<link>https://arxiv.org/abs/2512.06158</link>
<guid>https://arxiv.org/abs/2512.06158</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D generation, multi-view video diffusion, point tracking, Gaussian Splatting, dynamic objects  

<br /><br />Summary:  
Generating dynamic 4D objects from sparse multi-view inputs is challenging due to the need for preserving both appearance and motion consistency across views and over time, while avoiding artifacts and temporal drift. The paper identifies an important limitation in existing methods, namely that supervision often relies on pixel- or latent-space video diffusion losses, which do not provide explicit temporally aware, feature-level tracking guidance. To address this, the authors propose Track4DGen, a two-stage framework that integrates a multi-view video diffusion model with a foundational point tracker and a hybrid 4D Gaussian Splatting reconstructor. In the first stage, dense, feature-level point correspondences derived from the tracker are enforced within the diffusion generator to produce temporally consistent features that reduce appearance drift and improve coherence across views. In the second stage, a dynamic 4D Gaussian Splatting model is reconstructed by combining diffusion features carrying the Stage One tracking priors with Hex-plane features, augmented with 4D Spherical Harmonics to better model complex dynamics. Experimentally, Track4DGen outperforms baseline methods on benchmarks for both multi-view video generation and 4D object generation, producing temporally stable and text-editable 4D assets. Additionally, the authors contribute Sketchfab28, a new high-quality, object-centric 4D generation dataset to support future research in this domain. <div>
arXiv:2512.06158v1 Announce Type: new 
Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</title>
<link>https://arxiv.org/abs/2512.06171</link>
<guid>https://arxiv.org/abs/2512.06171</guid>
<content:encoded><![CDATA[
<div> Shearography, defect detection, deep learning, annotation automation, segmentation<br /><br />Summary:<br /><br />1. Shearography is an interferometric method that measures surface displacement gradients, making it highly sensitive for identifying subsurface defects in safety-critical components. 2. A significant barrier to deploying shearography in industrial environments is the absence of extensive, high-quality annotated datasets. Manual annotation is not only time-consuming but also subjective and difficult to standardize across different operators. 3. To address this challenge, the authors propose an automated workflow that leverages deep learning techniques to generate defect annotations directly from shearography data. 4. This workflow produces detailed, high-resolution segmentation masks and bounding-box labels that correspond to defects detected in shearography images. 5. Validation against expert-labeled datasets demonstrates that the automated annotation achieves a level of accuracy adequate for use in weakly supervised training schemes, minimizing the dependency on manual labeling. Overall, this approach facilitates scalable and standardized dataset creation, which in turn supports more robust and efficient defect detection in industrial shearography applications. <div>
arXiv:2512.06171v1 Announce Type: new 
Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction</title>
<link>https://arxiv.org/abs/2512.06174</link>
<guid>https://arxiv.org/abs/2512.06174</guid>
<content:encoded><![CDATA[
<div> Keywords: shadow generation, physical modeling, deep learning, 3D geometry, illumination<br /><br />Summary:  
This paper addresses shadow generation with a focus on producing photorealistic shadows consistent with object geometry and scene illumination. It identifies a gap in existing deep-learning shadow generation methods that rarely incorporate explicit physical modeling based on the physics of shadow formation. The authors propose a novel framework embedding explicit physical modeling of both geometry and illumination into a deep learning pipeline. Starting from a single RGB image, their method estimates approximate 3D geometry represented as dense point maps and predicts a dominant light direction. Using these signals, the framework computes an initial shadow location and shape that adhere to physical principles of shadow casting. This physics-based shadow estimate is then refined within a diffusion model, enhancing realism and ensuring the final shadow maintains coherence with scene geometry and lighting. The model is trained on the DESOBAV2 dataset and demonstrates improved results over prior approaches, notably in challenging scenarios involving complex geometry or ambiguous lighting conditions. Overall, the approach successfully integrates physical insights with data-driven techniques to enhance shadow generation quality and physical correctness. <div>
arXiv:2512.06174v1 Announce Type: new 
Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction</title>
<link>https://arxiv.org/abs/2512.06179</link>
<guid>https://arxiv.org/abs/2512.06179</guid>
<content:encoded><![CDATA[
<div> Keywords: attached shadows, cast shadows, shadow detection, light estimation, geometry-illumination reasoning  

<br /><br />Summary:  
Attached shadows occur on surfaces where light is blocked due to self-occlusion, playing a vital role in perceiving 3D structure and improving scene understanding. Despite their importance, current shadow detection research primarily focuses on cast shadows, lacking specialized datasets and models for attached shadows. To bridge this gap, the authors propose a novel framework that jointly detects both cast and attached shadows by leveraging the interplay between shadows, scene illumination, and geometry. The system comprises a shadow detection module that separately predicts cast and attached shadows, alongside a light estimation module that deduces the light direction from shadow cues. Using the estimated light direction with surface normals, the framework generates a geometry-consistent partial map, highlighting regions prone to self-occlusion. This map is fed back to refine shadow predictions, creating an iterative closed-loop process that enhances both shadow segmentation and light estimation simultaneously. The authors also constructed a dedicated dataset containing 1,458 images with distinct annotations for cast and attached shadows to facilitate robust training and evaluation. Experimental results demonstrate that this geometry-illumination iterative reasoning approach significantly improves attached shadow detection performance, achieving at least a 33% reduction in Balanced Error Rate (BER) while preserving accuracy in detecting cast and full shadows. <div>
arXiv:2512.06179v1 Announce Type: new 
Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling</title>
<link>https://arxiv.org/abs/2512.06185</link>
<guid>https://arxiv.org/abs/2512.06185</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, fooling images, black-box attack, vision transformers, model robustness<br /><br />Summary:<br /><br />1. This study revisits the concept of "fooling images," initially proposed by Nguyen et al. (2015), which are inputs that cause deep neural networks (DNNs) to produce high-confidence incorrect classifications despite bearing no resemblance to natural images. <br /><br />2. The authors re-implement evolutionary fooling attacks using both CPPN-based and direct-encoding-based approaches on modern architectures, including convolutional neural networks and transformer models like ViT-B/16. Their findings confirm that state-of-the-art models remain vulnerable, with the ViT-B/16 transformer being the most susceptible to quick and near-certain misclassifications.<br /><br />3. Introducing SPOOF, a new minimalist and efficient black-box attack, the paper demonstrates that it can generate high-confidence fooling images with minimal pixel changes and significantly lower computation compared to previous methods.<br /><br />4. The research explores robustness improvements by retraining models with fooling images as an additional class. This method only provides limited defense, as SPOOF continues to produce effective fooling images, albeit with a slightly increased number of queries.<br /><br />5. The results highlight the persistent vulnerability and fragility of current deep learning classifiers, including advanced transformer architectures, against adversarial images crafted to deceive them consistently. <div>
arXiv:2512.06185v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</title>
<link>https://arxiv.org/abs/2512.06190</link>
<guid>https://arxiv.org/abs/2512.06190</guid>
<content:encoded><![CDATA[
<div> Keywords: food drying, color trajectory, multi-modal prediction, drying process parameters, model accuracy<br /><br />Summary:<br /><br />Food drying is extensively employed to reduce moisture, enhance safety, and prolong the shelf life of food products. Monitoring the color evolution of food samples is crucial as it serves as an important quality indicator during drying. Existing research has mainly focused on analyzing low-dimensional color features under varying drying conditions, which limits the ability to fully characterize the complex and dynamic color changes that occur. Additionally, current modeling techniques struggle to generalize predictions to drying conditions not previously encountered. To overcome these challenges, the authors propose a novel multi-modal color-trajectory prediction approach that integrates high-dimensional temporal color data with drying process parameters. This integration enables the accurate and data-efficient prediction of color trajectories in food drying. The model’s performance was evaluated on cookie and apple drying tasks under unseen drying conditions, achieving root mean squared errors (RMSE) of 2.12 and 1.29, respectively. This represents a reduction in prediction errors by over 90% compared to baseline models. The experimental results confirm the approach’s superior accuracy, robustness, and broad applicability across different drying scenarios, highlighting its potential for improving food drying quality control. <div>
arXiv:2512.06190v1 Announce Type: new 
Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning</title>
<link>https://arxiv.org/abs/2512.06206</link>
<guid>https://arxiv.org/abs/2512.06206</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Glioma Segmentation, MRI, PID Controller, Medical Imaging  

<br /><br />Summary:  
1. The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024 focused on federated learning (FL) for segmenting glioma sub-regions in multi-parametric MRI scans.  
2. The challenge used a multi-institutional dataset from the BraTS glioma benchmark, including 1,251 training cases, 219 validation cases, and 570 hidden test cases, with segmentation labels for enhancing tumor (ET), tumor core (TC), and whole tumor (WT).  
3. Six teams participated and were evaluated under a standardized federated learning framework using a cumulative scoring system that combined segmentation accuracy (Dice Similarity Coefficient and 95th percentile Hausdorff Distance) and communication efficiency (convergence score).  
4. The winning team employed a PID-controller-based weight aggregation method, achieving the highest overall ranking with mean DSCs of 0.733 (ET), 0.761 (TC), and 0.751 (WT), alongside high HD95 values (around 32-34 mm), and the best communication efficiency with a convergence score of 0.764.  
5. Results demonstrate advancements over previous challenge iterations, highlighting the effectiveness of PID controllers in stabilizing and optimizing weight aggregation in federated learning for medical imaging. The challenge code is openly available at https://github.com/FeTS-AI/Challenge. <div>
arXiv:2512.06206v1 Announce Type: new 
Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study</title>
<link>https://arxiv.org/abs/2512.06221</link>
<guid>https://arxiv.org/abs/2512.06221</guid>
<content:encoded><![CDATA[
<div> SVD, WDR, image compression, reproducibility, JPEG2000<br /><br />Summary:<br /><br />This article presents an independent reproducibility study focused on a lossy image compression technique that combines Singular Value Decomposition (SVD) and Wavelet Difference Reduction (WDR). The original publication claimed that this combined SVD+WDR method outperforms both JPEG2000 and standalone WDR in terms of visual quality and compression ratio. To verify these claims, the author re-implemented the technique, carefully addressing missing implementation details such as quantization and threshold initialization, which were ambiguously described in the original paper. The study replicated the original experiments as closely as possible and extended the evaluation to new image datasets. Performance was assessed using standard metrics, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). Contrary to the original claims, findings indicate that SVD+WDR does not generally exceed JPEG2000 or WDR alone in PSNR, and only shows limited improvement in SSIM when compared to JPEG2000. The study underlines how unclear methodological details can significantly hinder reproducibility and affect the reported effectiveness of image compression methods. This highlights the importance of thorough documentation and transparency for accurate performance assessment in image compression research. <div>
arXiv:2512.06221v1 Announce Type: new 
Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking</title>
<link>https://arxiv.org/abs/2512.06230</link>
<guid>https://arxiv.org/abs/2512.06230</guid>
<content:encoded><![CDATA[
<div> multi-target tracking, labeled random finite sets, GLMB filter, GPU acceleration, multi-detections per object<br /><br />Summary:<br /><br />This article addresses challenges in multi-target tracking by focusing on labeled random finite set (RFS) methods, particularly the Generalized Labeled Multi-Bernoulli (GLMB) filter. Labeled RFS methods are valued for maintaining temporal coherence in object labeling and providing closed-form solutions to the multi-target Bayes filter. However, these methods experience high computational complexity due to the maintenance of multiple hypotheses under standard measurement models, even with hypothesis pruning. The authors propose a variant of the GLMB filter that allows for multiple detections per object from the same sensor, addressing scenarios common in distributed networks of machine learning-based virtual sensors. This variant breaks inter-detection dependencies present in standard GLMB updates, enabling significantly enhanced parallel scalability during filter updates. The improved scalability facilitates efficient implementation on GPU hardware. The paper reports preliminary results from a GPU-accelerated GLMB tracker, demonstrating favorable run-time scalability relative to the number of tracked objects and the maximum number of retained hypotheses. Overall, the work contributes a method that reduces computational burdens and improves deployment efficiency for multi-target tracking in sensor networks leveraging advanced computing architectures. <div>
arXiv:2512.06230v1 Announce Type: new 
Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion: Learning Intuitive Physics May Require More than Visual Data</title>
<link>https://arxiv.org/abs/2512.06232</link>
<guid>https://arxiv.org/abs/2512.06232</guid>
<content:encoded><![CDATA[
<div> Keywords: intuitive physics, video joint embedding predictive architecture, SAYCam dataset, deep learning, data distribution<br /><br />Summary:<br />1. Humans develop intuitive physics through rich internal models based on their everyday experiences and understanding of the physical world.<br />2. Current state-of-the-art deep learning models, despite being trained on enormous internet video datasets, have not reached human-level performance on intuitive physics tasks.<br />3. This study explores whether the type and distribution of training data, rather than just the quantity, can enhance learning of intuitive physics.<br />4. The authors pretrained a Video Joint Embedding Predictive Architecture (V-JEPA) on SAYCam, an egocentric video dataset that realistically captures the visual experiences of young children.<br />5. Despite representing only 0.01% of the data volume used by leading models, training on SAYCam did not produce meaningful improvements on the IntPhys2 benchmark.<br />6. The findings suggest that simply using developmentally realistic datasets is not enough for current architectures to acquire effective intuitive physics representations.<br />7. Consequently, increasing or varying visual data volume and distribution alone appears insufficient to build artificial systems with human-like intuitive physics capabilities. <div>
arXiv:2512.06232v1 Announce Type: new 
Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</title>
<link>https://arxiv.org/abs/2512.06251</link>
<guid>https://arxiv.org/abs/2512.06251</guid>
<content:encoded><![CDATA[
<div> Keywords: Partially Supervised Multi-Task Learning, NexusFlow, invertible coupling layers, domain partition, autonomous driving<br /><br />Summary:  
Partially Supervised Multi-Task Learning (PS-MTL) addresses learning from multiple tasks with incomplete annotations. Existing methods predominantly focus on homogeneous, dense prediction tasks, lacking solutions for structurally diverse tasks. NexusFlow is introduced as a novel, lightweight, plug-and-play framework that effectively handles both homogeneous and heterogeneous task settings. Central to NexusFlow are surrogate networks with invertible coupling layers that align latent feature distributions across tasks into a unified representation. The coupling layers are bijective, preserving detailed information and preventing representational collapse while allowing alignment across structurally different tasks without compromising expressive capacity. NexusFlow is first evaluated on a domain-partitioned autonomous driving scenario involving dense map reconstruction and sparse multi-object tracking supervised in different geographic regions, which presents both structural and domain disparities. It achieves state-of-the-art performance on the nuScenes dataset, surpassing strong partially supervised baselines. To validate its general applicability, NexusFlow is also tested on the NYUv2 dataset with three homogeneous dense prediction tasks — segmentation, depth estimation, and surface normal prediction — confirming consistent task improvements. These results demonstrate NexusFlow’s broad utility and effectiveness in advancing PS-MTL across diverse real-world problems. <div>
arXiv:2512.06251v1 Announce Type: new 
Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-driven Fine-grained Retrieval</title>
<link>https://arxiv.org/abs/2512.06255</link>
<guid>https://arxiv.org/abs/2512.06255</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained image retrieval, language-driven supervision, large language models, vision-language models, attribute-level descriptions<br /><br />Summary: Existing fine-grained image retrieval (FGIR) techniques typically use one-hot labels based on category names for supervision, which are semantically sparse and limit the ability to model detailed comparisons across categories, reducing generalization to unseen classes. To overcome this, the paper proposes LaFG, a novel framework that leverages large language models (LLMs) and vision-language models (VLMs) to transform category names into rich attribute-level supervision. LaFG treats category names as semantic anchors and prompts an LLM to generate detailed, attribute-focused descriptions for each class. To address potential omissions in these descriptions, a frozen VLM maps them into a vision-aligned space where attributes are clustered into a comprehensive dataset-wide vocabulary, enriched by attributes mined from related categories. Using this vocabulary, LaFG applies a global prompt template to select relevant attributes and aggregate them into category-specific linguistic prototypes. These prototypes then act as supervision to guide the retrieval model, enabling it to better capture and compare fine-grained details across categories, ultimately improving its ability to generalize to unseen categories in fine-grained image retrieval tasks. <div>
arXiv:2512.06255v1 Announce Type: new 
Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs</title>
<link>https://arxiv.org/abs/2512.06258</link>
<guid>https://arxiv.org/abs/2512.06258</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, reasoning paths, path selection bias, Path-Select Optimization, Group Relative Policy Optimization<br /><br />Summary:  
This paper identifies a critical problem in Large Vision-Language Models (LVLMs) where models often reach correct answers through flawed or unstable reasoning paths, emphasizing that the issue stems from a path selection bias rather than knowledge gaps. Evidence for this comes from significant differences between Pass@K (large K) and Pass@1 performance, indicating misreasoning rather than ignorance is the core challenge. To address this, the authors propose a two-stage post-training framework called Path-Select Optimization (PSO). The first stage uses Group Relative Policy Optimization (GRPO) with template and answer-based rewards to encourage structured, stepwise reasoning. In the second stage, an online preference optimization process is introduced where the model self-evaluates multiple reasoning paths, aligning toward preferred trajectories and storing incorrect paths in a Negative Replay Memory (NRM) to avoid repeating mistakes. This continual refinement mechanism improves reasoning stability and accuracy. Extensive experiments confirm that PSO effectively filters out invalid reasoning trajectories, improves reasoning accuracy by an average of 7.4%, and results in more stable and consistent chains of thought. The authors will release their code publicly to facilitate further research and application. <div>
arXiv:2512.06258v1 Announce Type: new 
Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06269</link>
<guid>https://arxiv.org/abs/2512.06269</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, multi-view triangulation, geometry consistency, photorealistic rendering  

<br /><br />Summary:  
1. The paper addresses the limitations of 3D Gaussian Splatting in real-time novel view synthesis, particularly issues arising from relying solely on photometric loss which leads to "floater" artifacts and unstructured geometry.  
2. To overcome these challenges, the authors propose a novel method that enforces global geometry consistency by integrating constrained multi-view triangulation.  
3. Their approach establishes a consensus on the 3D representation by leveraging multiple estimated views to improve the physical accuracy of reconstructions.  
4. The optimization penalizes deviations of rendered 3D points from a robust consensus point, which is recalculated through self-supervised triangulation over neighboring views.  
5. Experimental results demonstrate state-of-the-art performance, with a notable mean Chamfer Distance of 0.50 mm on the DTU dataset, surpassing other explicit reconstruction methods.  
6. The authors plan to release their code as open-source to promote reproducibility and facilitate further research in the community. <div>
arXiv:2512.06269v1 Announce Type: new 
Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacePhys: State of the Heart Learning</title>
<link>https://arxiv.org/abs/2512.06275</link>
<guid>https://arxiv.org/abs/2512.06275</guid>
<content:encoded><![CDATA[
<div> Keywords: remote photoplethysmography, vital sign measurement, FacePhys, real-time inference, computational efficiency<br /><br />Summary:<br /><br />1. The paper presents FacePhys, a novel memory-efficient algorithm for remote photoplethysmography (rPPG), aimed at measuring vital signs using cameras for comfortable and ubiquitous health monitoring.<br />2. FacePhys addresses the key challenges of model scalability, cross-dataset generalization, and real-time operation through a temporal-spatial state space duality approach.<br />3. It leverages a transferable heart state to capture subtle periodic variations across video frames while maintaining minimal computational overhead, enabling training on extended video sequences and low-latency inference.<br />4. FacePhys achieves a new state-of-the-art performance with a 49% reduction in error compared to existing methods.<br />5. The solution supports real-time inference with a memory footprint of only 3.6 MB and per-frame latency of 9.46 milliseconds, outperforming current models by 83% to 99%, making it suitable for practical deployment.<br />6. A live demonstration of FacePhys is available for public access at https://www.facephys.com/, showcasing its potential for reliable and efficient vital sign monitoring through cameras. <div>
arXiv:2512.06275v1 Announce Type: new 
Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
<div> Referring Expression Comprehension, Multi-modal Large Language Model, RefBench-PRO, Dynamic IoU-based GRPO, Reinforcement Learning<br /><br />Summary:  
Referring Expression Comprehension (REC) involves localizing specific image regions based on textual descriptions. Existing benchmarks mainly focus on perceptual abilities and lack interpretable scoring metrics, limiting insights into the grounding capabilities of Multi-modal Large Language Models (MLLMs) across various cognitive skills. To overcome these limitations, this work introduces RefBench-PRO, a comprehensive REC benchmark that divides referring expressions into two main dimensions: perception and reasoning. These dimensions are further broken down into six progressively challenging tasks: attribute, position, interaction, commonsense, relation, and reject. The authors develop a fully automated data generation pipeline to create diverse referring expressions across these sub-dimensions, ensuring robust evaluation. Additionally, Ref-R1, a reinforcement learning-based method incorporating Dynamic IoU-based GRPO, is proposed to enhance localization accuracy, particularly under complex reasoning scenarios, thereby establishing a stronger baseline for REC. Extensive experiments demonstrate that RefBench-PRO facilitates interpretable and nuanced evaluation of MLLMs on referring expression comprehension tasks, exposing greater challenges in both perceptual and reasoning capabilities. This benchmark and methodology collectively push forward the assessment and improvement of grounding abilities in vision-language models. <div>
arXiv:2512.06276v1 Announce Type: new 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.06281</link>
<guid>https://arxiv.org/abs/2512.06281</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, modality imbalance, latent visual reconstruction, masked image modeling, visual attention  

<br /><br />Summary:  
Multimodal Large Language Models (MLLMs) have achieved strong performance on tasks involving multiple data types, but they often suffer from a modality imbalance issue. This issue arises because visual information is underutilized relative to textual information in the deeper network layers, leading to reduced visual performance and hallucinations. The root cause is the training paradigm centered around next-text-token prediction, which does not provide direct supervisory signals for visual data. To address this, the authors propose Latent Visual Reconstruction (LaVer), a novel training framework that introduces masked image modeling within the joint latent semantic space of LLMs. LaVer provides explicit visual activation, which encourages the model to allocate more attention to visual inputs and maintain more discriminative visual representations throughout its layers. Extensive experiments across multiple benchmarks demonstrate LaVer's effectiveness, especially in scenarios demanding dense visual reasoning. The framework enhances the visual capability of MLLMs and reduces visual hallucination. The authors have also made the code for LaVer publicly available, facilitating further research and application development in this area. <div>
arXiv:2512.06281v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sleep Monitoring System Based on Audio, Video and Depth Information</title>
<link>https://arxiv.org/abs/2512.06282</link>
<guid>https://arxiv.org/abs/2512.06282</guid>
<content:encoded><![CDATA[
<div> motion events, light-on/off events, noise events, infrared depth sensor, event detection algorithm<br /><br />Summary:<br /><br />This paper presents a noninvasive sleep monitoring system designed to quantitatively evaluate sleep disturbances using an event-based method. The study classifies sleep disturbances into three main event types: motion events, light-on/off events, and noise events, which are crucial for understanding the sleep environment. The monitoring device integrates an infrared depth sensor, an RGB camera, and a four-microphone array to capture multimodal data during sleep in low-light home settings. A background model leveraging depth signals is employed to quantify the magnitude of movements, while a separate background model using color images detects lighting changes, addressing the limitation of depth sensors in sensing light variations. An event detection algorithm processes the sensor data to detect the occurrences of the three classified events. Experiments conducted under actual sleep conditions demonstrate the system's reliability and effectiveness in monitoring and classifying sleep disturbances. This integrated approach offers a comprehensive and nonintrusive solution for sleep disturbance evaluation in real-world home environments, showing potential for enhancing sleep quality assessment. <div>
arXiv:2512.06282v1 Announce Type: new 
Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification</title>
<link>https://arxiv.org/abs/2512.06290</link>
<guid>https://arxiv.org/abs/2512.06290</guid>
<content:encoded><![CDATA[
<div> Keywords: Stroke classification, reference points, Inline Sequence Attention, Cross-Ellipse Query, online handwriting  

<br /><br />Summary:  
The paper addresses the challenge of stroke classification in online handwriting, which is complicated by variations in writing styles, ambiguous content, and dynamic writing positions. The key difficulty lies in modeling the semantic relationships between strokes, particularly because stroke interactions tend to be localized, and existing deep learning models struggle to capture these fine-grained relationships. To overcome this, the authors propose representing strokes through selected reference points combined with feature vectors, balancing detail and redundancy. They introduce StrokeNet, an architecture that sequentially encodes these reference points and uses an Inline Sequence Attention (ISA) module to build contextual stroke features. Additionally, a novel Cross-Ellipse Query (CEQ) mechanism clusters reference points to extract spatial features across multiple scales. The framework also incorporates a joint optimization strategy that simultaneously predicts stroke categories via reference point regression and models semantic transitions between adjacent strokes using an Auxiliary Branch. Experimental results across several public online handwriting datasets demonstrate state-of-the-art performance. Notably, on the CASIA-onDo dataset, the accuracy of stroke classification improves significantly from 93.81% to 95.54%, validating the method’s effectiveness and robustness in handling complex stroke interactions. <div>
arXiv:2512.06290v1 Announce Type: new 
Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06306</link>
<guid>https://arxiv.org/abs/2512.06306</guid>
<content:encoded><![CDATA[
<div> Event cameras, human pose estimation, point cloud, temporal modeling, edge enhancement<br /><br />Summary:<br /><br />This paper addresses human pose estimation using event cameras, which provide high temporal resolution and low latency beneficial for scenarios with challenging conditions. Traditional approaches usually convert event streams into dense event frames, which compromises temporal resolution and increases computational load. To overcome this, the authors propose a point cloud-based framework that directly exploits the spatiotemporal characteristics of event streams. The method introduces an Event Temporal Slicing Convolution module designed to capture short-term dependencies across event slices. Alongside this, the Event Slice Sequencing module is employed to enable structured temporal modeling of the event data. To enhance spatial details, especially in sparse event scenarios, an edge enhancement technique is applied within the point cloud-based event representation. Experimental validation on the DHP19 dataset demonstrates consistent performance improvements using the proposed approach across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer. The results confirm that leveraging event-based data in a point cloud format with temporal and edge enhancements can significantly improve human pose estimation models. <div>
arXiv:2512.06306v1 Announce Type: new 
Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06328</link>
<guid>https://arxiv.org/abs/2512.06328</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.06328v1

Keywords: ReCAD, reinforcement learning, parametric CAD, vision-language models, generative models<br /><br />Summary:<br /><br />1. The paper introduces ReCAD, a reinforcement learning (RL) framework designed to generate accurate parametric computer-aided design (CAD) models by leveraging pretrained large models (PLMs) and their generative capabilities from multimodal inputs.<br /><br />2. Unlike previous approaches relying mainly on supervised fine-tuning and limited editability, ReCAD accesses simple functional interfaces such as point coordinates to enable complex CAD operations like pattern replication and mirroring.<br /><br />3. The framework initiates by fine-tuning vision-language models (VLMs) on rewritten CAD scripts translated into parameterized code, which helps generate accurate textual supervision.<br /><br />4. A novel RL strategy incorporates parameterized code guidance, enhancing reasoning ability for complex CAD generation tasks, supported by a hierarchical primitive learning process that progressively builds structured, compositional skills under a unified reward function optimizing both geometric accuracy and semantic fidelity.<br /><br />5. Experimental results show that ReCAD achieves state-of-the-art performance on text-to-CAD and image-to-CAD tasks, significantly reducing mean Chamfer Distance for both in-distribution (73.47 to 29.61) and out-of-distribution (272.06 to 80.23) cases, markedly outperforming existing baselines. <div>
arXiv:2512.06328v1 Announce Type: new 
Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening</title>
<link>https://arxiv.org/abs/2512.06330</link>
<guid>https://arxiv.org/abs/2512.06330</guid>
<content:encoded><![CDATA[
<div> Keywords: Pansharpening, Discrete Wavelet Transform, Cross-modal Interaction, Multi-scale Fusion, Spectral-Spatial Disentanglement<br /><br />Summary:<br />1. The paper introduces S2WMamba, a novel method for pansharpening that fuses high-resolution panchromatic (PAN) images with low-resolution multispectral (LRMS) images to produce high-resolution multispectral (HRMS) outputs.<br />2. A key challenge addressed is the entanglement of spatial details and spectral fidelity when jointly processing PAN and MS data; S2WMamba explicitly disentangles frequency information to overcome this.<br />3. The method employs a 2D Haar Discrete Wavelet Transform (DWT) on PAN images to localize spatial edges and textures, while a channel-wise 1D Haar DWT processes multispectral pixels as 1D signals, separating low- and high-frequency components to minimize spectral distortion.<br />4. S2WMamba features two parallel branches: the Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectral information from the 1D wavelet pyramid.<br />5. Information exchange between branches is facilitated by a Mamba-based cross-modulation mechanism that models long-range dependencies with linear complexity.<br />6. An adaptive multi-scale dynamic gate, combining multiplicative and additive operations, is used to fuse the outputs of both branches effectively.<br />7. Experiments on WV3, GF2, and QB datasets show that S2WMamba matches or surpasses recent strong baselines, achieving up to 0.23 dB PSNR improvement and a high HQNR score of 0.956 on full-resolution WV3.<br />8. Ablation studies confirm the effectiveness of the 2D/1D DWT placement, parallel dual-branch architecture, and fusion gate design.<br />9. The authors provide the code publicly for reproducibility at the specified GitHub repository. <div>
arXiv:2512.06330v1 Announce Type: new 
Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks</title>
<link>https://arxiv.org/abs/2512.06332</link>
<guid>https://arxiv.org/abs/2512.06332</guid>
<content:encoded><![CDATA[
<div> Keywords: cryo-electron microscopy, compositional heterogeneity, transformer, hypernetwork, implicit neural representation<br /><br />Summary: Cryo-electron microscopy (cryo-EM) is a critical method for determining three-dimensional structures of dynamic biomolecular complexes, typically used for imaging a single molecular species. Current approaches mainly address conformational heterogeneity within one or a few structures, lacking the ability to resolve compositional heterogeneity arising from mixtures containing many distinct molecular species. To overcome this limitation, the paper introduces CryoHype, a novel transformer-based hypernetwork designed for cryo-EM reconstruction. CryoHype dynamically adjusts the weights of an implicit neural representation, enabling it to handle multiple molecular species simultaneously. The method achieves state-of-the-art performance on a challenging benchmark dataset consisting of 100 different biomolecular structures. Moreover, CryoHype demonstrates scalability by successfully reconstructing 1,000 distinct structures from unlabeled cryo-EM images in a fixed-pose scenario. This advancement paves the way for high-throughput, simultaneous structural determination of diverse molecular species from mixed cryo-EM data, addressing a key challenge in the field that existing methods could not effectively resolve. <div>
arXiv:2512.06332v1 Announce Type: new 
Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate</title>
<link>https://arxiv.org/abs/2512.06344</link>
<guid>https://arxiv.org/abs/2512.06344</guid>
<content:encoded><![CDATA[
<div> Generative Image Compression, Multimodal Guidance, Semantic Consistency, Task-Aware Semantic Compression, Diffusion Decoder<br /><br />Summary:<br /><br />This paper addresses the challenge of semantic deviations in generative image compression at ultra-low bitrates (below 0.05 bpp), which hampers the deployment of such methods in bandwidth-restricted 6G semantic communication. The authors propose the Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework, which leverages three different guidance modalities to improve semantic consistency: a concise text caption to capture global semantics, a highly compressed image (HCI) for low-level visual details, and Semantic Pseudo-Words (SPWs) that provide fine-grained, task-relevant semantic information. These SPWs are produced by a novel Task-Aware Semantic Compression Module (TASCM), which uses multi-head self-attention to focus on important semantics while filtering unnecessary data. The integration of these guidance signals into the image reconstruction relies on a Multimodal-Guided Diffusion Decoder (MGDD), which utilizes a dual-path cooperative mechanism combining cross-attention and ControlNet residuals to effectively inject the multimodal information into the diffusion process. Experimental results demonstrate that MTGC significantly enhances semantic consistency, perceptual quality, and pixel-level fidelity, achieving a notable 10.59% reduction in DISTS on the DIV2K dataset, validating its effectiveness under ultra-low bitrate conditions. <div>
arXiv:2512.06344v1 Announce Type: new 
Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLUENet: Cluster Attention Makes Neural Networks Have Eyes</title>
<link>https://arxiv.org/abs/2512.06345</link>
<guid>https://arxiv.org/abs/2512.06345</guid>
<content:encoded><![CDATA[
<div> Clustering, Attention, Visual Semantic Understanding, Interpretability, Deep Architecture<br /><br />Summary: Despite the widespread success of convolutional and attention-based models in vision tasks, these approaches face limitations due to their rigid receptive fields and complex structures, which reduce interpretability and hinder the modeling of irregular spatial patterns. To improve transparency and semantic modeling flexibility, clustering paradigms have been explored, but they typically suffer from issues such as limited accuracy, low efficiency, and gradient vanishing during training. Addressing these challenges, the paper introduces CLUster attEntion Network (CLUENet), a transparent deep learning architecture aimed at enhancing visual semantic understanding. CLUENet introduces three main innovations: (i) a Global Soft Aggregation and Hard Assignment mechanism using Temperature-Scaled Cosine Attention combined with gated residual connections for improved local feature modeling; (ii) inter-block Hard and Shared Feature Dispatching to optimize information flow; and (iii) an improved cluster pooling strategy that enhances performance. Experimental results on CIFAR-100 and Mini-ImageNet datasets demonstrate that CLUENet surpasses existing clustering-based methods and conventional visual models by achieving a remarkable balance between classification accuracy, computational efficiency, and model interpretability. This work offers a significant step toward transparent, effective visual models suitable for tasks demanding high levels of model explainability. <div>
arXiv:2512.06345v1 Announce Type: new 
Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search</title>
<link>https://arxiv.org/abs/2512.06353</link>
<guid>https://arxiv.org/abs/2512.06353</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, Quantization, Tree Structured Search, Environmental Noise Guidance, General Monarch Branch<br /><br />Summary: Diffusion Transformers (DiTs) have become a powerful model architecture for image generation, surpassing traditional U-Net designs in scalability and performance but facing deployment challenges due to high computational and memory requirements. This work introduces TreeQ, a novel unified framework specifically designed to address these challenges in DiT quantization. First, TreeQ implements Tree Structured Search (TSS), which exploits the linear structure of DiTs to efficiently explore the solution space in O(n) time while enhancing accuracy through comparison-based pruning. Second, it proposes Environmental Noise Guidance (ENG), a method that harmonizes the optimization objectives of Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) via a single hyperparameter, simplifying the quantization process. Third, to combat information loss in ultra-low-bit precision settings, TreeQ incorporates the General Monarch Branch (GMB), a structured sparse branch that preserves critical details in generated images. Comprehensive experiments show that TreeQ achieves state-of-the-art results on the DiT-XL/2 architecture under both W3A3 and W4A4 PTQ/PEFT configurations. Remarkably, this approach is the first to reach near-lossless 4-bit PTQ performance for DiT models, marking a significant step toward more practical deployment of DiTs. The code and trained models are publicly accessible at the provided GitHub repository. <div>
arXiv:2512.06353v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Latent Space for Generative Single-Image Reflection Removal</title>
<link>https://arxiv.org/abs/2512.06358</link>
<guid>https://arxiv.org/abs/2512.06358</guid>
<content:encoded><![CDATA[
<div> Reflection removal, latent diffusion, VAE, composite image, depth-guided sampling<br /><br />Summary: This paper addresses the challenging problem of single-image reflection removal, which is ill-posed due to the complexity of separating overlapping layers in a reflection-contaminated image. The authors identify a fundamental issue with existing semantic encoder latent spaces, which do not naturally represent composite images as a linear combination of layers, limiting recovery and generalization. To overcome this, the paper introduces a novel framework based on a reflection-equivariant variational autoencoder (VAE) designed to align the latent space with the physics of reflection formation, thereby enabling more meaningful layer disentanglement. Additionally, the method employs a learnable, task-specific text embedding that offers precise guidance while avoiding ambiguity inherent in natural language instructions. The approach also incorporates a depth-guided early-branching sampling technique to leverage generative stochasticity effectively, enhancing the quality and realism of the reconstructed images. Extensive experimental validation demonstrates that this method sets a new state-of-the-art (SOTA) performance across multiple benchmark datasets, showing strong generalization to complex real-world scenarios where reflections are difficult to remove. The combination of physics-aligned latent space, precise guidance, and innovative sampling strategy collectively yields robust and high-quality reflection removal results. <div>
arXiv:2512.06358v1 Announce Type: new 
Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection</title>
<link>https://arxiv.org/abs/2512.06363</link>
<guid>https://arxiv.org/abs/2512.06363</guid>
<content:encoded><![CDATA[
<div> Physical Presentation Attacks, Digital Forgery Attacks, Unified Attack Detection, Spoofing-aware Prompt Learning, CLIP-based Defense<br /><br />Summary:<br /><br />This paper addresses the vulnerability of face recognition systems to both physical presentation attacks (PAs) and digital forgery attacks (DFs), aiming to secure biometric data through a unified detection framework. Existing methods typically use CLIP with regularization to improve generalization across attack types but face challenges due to conflicting optimization objectives in the shared prompt space for physical and digital attacks. To resolve this, the authors propose the Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization by introducing parallel learnable prompt branches specific to physical and digital attacks. This is enhanced by an adaptive Spoofing Context Prompt Generation mechanism, allowing independent control for each attack category. Additionally, the Cues-awareness Augmentation technique leverages the dual-prompt design to create challenging sample mining tasks, which strengthen the model’s robustness against previously unseen attack techniques. Extensive experiments conducted on the large-scale UniAttackDataPlus dataset demonstrate that SPL-UAD significantly improves performance for unified attack detection tasks compared to prior approaches, confirming the effectiveness of decoupling prompt optimization and the benefits of cue-aware augmentation strategies. <div>
arXiv:2512.06363v1 Announce Type: new 
Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos</title>
<link>https://arxiv.org/abs/2512.06368</link>
<guid>https://arxiv.org/abs/2512.06368</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular dynamic video reconstruction, SMPL human body model, geometric priors, human boundary preservation, cross-attention fusion<br /><br />Summary:<br /><br />This article addresses the challenges in monocular dynamic video reconstruction for dynamic human scenes, focusing on issues like geometric inconsistencies and resolution degradation. Existing approaches often fail due to lack of 3D human structural understanding, resulting in distorted limb proportions and unnatural fusion between humans and objects. Moreover, downsampling to reduce memory usage causes human boundaries to drift toward background geometry, reducing accuracy. To overcome these problems, the authors propose a novel method that integrates hybrid geometric priors combining SMPL human body models with monocular depth estimation. This approach leverages structured human priors to maintain surface consistency while capturing fine-grained details in human regions. The proposed Human3R framework features a hierarchical pipeline: it first processes full-resolution images to reconstruct overall scene geometry, then strategically crops and applies cross-attention fusion to enhance human-specific details. The integration of SMPL priors is achieved through a Feature Fusion Module that ensures geometrically plausible reconstructions and preserves fine boundary details of humans. Experiments on TUM Dynamics and GTA-IM datasets demonstrate that Human3R outperforms previous methods in reconstructing dynamic humans accurately and with improved detail. <div>
arXiv:2512.06368v1 Announce Type: new 
Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06373</link>
<guid>https://arxiv.org/abs/2512.06373</guid>
<content:encoded><![CDATA[
arXiv:2512.06373v1 Announce Type: new 
Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</title>
<link>https://arxiv.org/abs/2512.06376</link>
<guid>https://arxiv.org/abs/2512.06376</guid>
<content:encoded><![CDATA[
arXiv:2512.06376v1 Announce Type: new 
Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System</title>
<link>https://arxiv.org/abs/2512.06377</link>
<guid>https://arxiv.org/abs/2512.06377</guid>
<content:encoded><![CDATA[
arXiv:2512.06377v1 Announce Type: new 
Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OCFER-Net: Recognizing Facial Expression in Online Learning System</title>
<link>https://arxiv.org/abs/2512.06379</link>
<guid>https://arxiv.org/abs/2512.06379</guid>
<content:encoded><![CDATA[
arXiv:2512.06379v1 Announce Type: new 
Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement</title>
<link>https://arxiv.org/abs/2512.06400</link>
<guid>https://arxiv.org/abs/2512.06400</guid>
<content:encoded><![CDATA[
arXiv:2512.06400v1 Announce Type: new 
Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[
arXiv:2512.06421v1 Announce Type: new 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Perception CNN for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2512.06422</link>
<guid>https://arxiv.org/abs/2512.06422</guid>
<content:encoded><![CDATA[
arXiv:2512.06422v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DragMesh: Interactive 3D Generation Made Easy</title>
<link>https://arxiv.org/abs/2512.06424</link>
<guid>https://arxiv.org/abs/2512.06424</guid>
<content:encoded><![CDATA[
arXiv:2512.06424v1 Announce Type: new 
Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition</title>
<link>https://arxiv.org/abs/2512.06426</link>
<guid>https://arxiv.org/abs/2512.06426</guid>
<content:encoded><![CDATA[
arXiv:2512.06426v1 Announce Type: new 
Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening</title>
<link>https://arxiv.org/abs/2512.06434</link>
<guid>https://arxiv.org/abs/2512.06434</guid>
<content:encoded><![CDATA[
arXiv:2512.06434v1 Announce Type: new 
Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</title>
<link>https://arxiv.org/abs/2512.06438</link>
<guid>https://arxiv.org/abs/2512.06438</guid>
<content:encoded><![CDATA[
arXiv:2512.06438v1 Announce Type: new 
Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Stable Cross-Domain Depression Recognition under Missing Modalities</title>
<link>https://arxiv.org/abs/2512.06447</link>
<guid>https://arxiv.org/abs/2512.06447</guid>
<content:encoded><![CDATA[
arXiv:2512.06447v1 Announce Type: new 
Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction</title>
<link>https://arxiv.org/abs/2512.06485</link>
<guid>https://arxiv.org/abs/2512.06485</guid>
<content:encoded><![CDATA[
arXiv:2512.06485v1 Announce Type: new 
Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion</title>
<link>https://arxiv.org/abs/2512.06504</link>
<guid>https://arxiv.org/abs/2512.06504</guid>
<content:encoded><![CDATA[
arXiv:2512.06504v1 Announce Type: new 
Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images</title>
<link>https://arxiv.org/abs/2512.06521</link>
<guid>https://arxiv.org/abs/2512.06521</guid>
<content:encoded><![CDATA[
arXiv:2512.06521v1 Announce Type: new 
Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization</title>
<link>https://arxiv.org/abs/2512.06530</link>
<guid>https://arxiv.org/abs/2512.06530</guid>
<content:encoded><![CDATA[
arXiv:2512.06530v1 Announce Type: new 
Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2512.06531</link>
<guid>https://arxiv.org/abs/2512.06531</guid>
<content:encoded><![CDATA[
arXiv:2512.06531v1 Announce Type: new 
Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging spatial awareness and global context in medical image segmentation</title>
<link>https://arxiv.org/abs/2512.06560</link>
<guid>https://arxiv.org/abs/2512.06560</guid>
<content:encoded><![CDATA[
arXiv:2512.06560v1 Announce Type: new 
Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities</title>
<link>https://arxiv.org/abs/2512.06562</link>
<guid>https://arxiv.org/abs/2512.06562</guid>
<content:encoded><![CDATA[
arXiv:2512.06562v1 Announce Type: new 
Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06565</link>
<guid>https://arxiv.org/abs/2512.06565</guid>
<content:encoded><![CDATA[
arXiv:2512.06565v1 Announce Type: new 
Abstract: We present GNC--Pose, a fully learning--free monocular 6D object pose estimation pipeline for textured objects that combines rendering--based initialization, geometry--aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D--3D correspondences obtained through feature matching and rendering--based alignment, our method builds upon the Graduated Non--Convexity (GNC) principle and introduces a geometry--aware, cluster--based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC--Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC--Pose achieves competitive accuracy compared with both learning-based and learning--free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules</title>
<link>https://arxiv.org/abs/2512.06575</link>
<guid>https://arxiv.org/abs/2512.06575</guid>
<content:encoded><![CDATA[
arXiv:2512.06575v1 Announce Type: new 
Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding</title>
<link>https://arxiv.org/abs/2512.06581</link>
<guid>https://arxiv.org/abs/2512.06581</guid>
<content:encoded><![CDATA[
arXiv:2512.06581v1 Announce Type: new 
Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain</title>
<link>https://arxiv.org/abs/2512.06598</link>
<guid>https://arxiv.org/abs/2512.06598</guid>
<content:encoded><![CDATA[
arXiv:2512.06598v1 Announce Type: new 
Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2512.06612</link>
<guid>https://arxiv.org/abs/2512.06612</guid>
<content:encoded><![CDATA[
arXiv:2512.06612v1 Announce Type: new 
Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</title>
<link>https://arxiv.org/abs/2512.06613</link>
<guid>https://arxiv.org/abs/2512.06613</guid>
<content:encoded><![CDATA[
arXiv:2512.06613v1 Announce Type: new 
Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</title>
<link>https://arxiv.org/abs/2512.06642</link>
<guid>https://arxiv.org/abs/2512.06642</guid>
<content:encoded><![CDATA[
arXiv:2512.06642v1 Announce Type: new 
Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextMamba: Scene Text Detector with Mamba</title>
<link>https://arxiv.org/abs/2512.06657</link>
<guid>https://arxiv.org/abs/2512.06657</guid>
<content:encoded><![CDATA[
arXiv:2512.06657v1 Announce Type: new 
Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Image Descriptions from Attention Sequences</title>
<link>https://arxiv.org/abs/2512.06662</link>
<guid>https://arxiv.org/abs/2512.06662</guid>
<content:encoded><![CDATA[
arXiv:2512.06662v1 Announce Type: new 
Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks</title>
<link>https://arxiv.org/abs/2512.06663</link>
<guid>https://arxiv.org/abs/2512.06663</guid>
<content:encoded><![CDATA[
arXiv:2512.06663v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning</title>
<link>https://arxiv.org/abs/2512.06673</link>
<guid>https://arxiv.org/abs/2512.06673</guid>
<content:encoded><![CDATA[
arXiv:2512.06673v1 Announce Type: new 
Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RunawayEvil: Jailbreaking the Image-to-Video Generative Models</title>
<link>https://arxiv.org/abs/2512.06674</link>
<guid>https://arxiv.org/abs/2512.06674</guid>
<content:encoded><![CDATA[
arXiv:2512.06674v1 Announce Type: new 
Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy</title>
<link>https://arxiv.org/abs/2512.06684</link>
<guid>https://arxiv.org/abs/2512.06684</guid>
<content:encoded><![CDATA[
arXiv:2512.06684v1 Announce Type: new 
Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation</title>
<link>https://arxiv.org/abs/2512.06689</link>
<guid>https://arxiv.org/abs/2512.06689</guid>
<content:encoded><![CDATA[
arXiv:2512.06689v1 Announce Type: new 
Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Entropy in Visual Grounding: Analysis and Optimization</title>
<link>https://arxiv.org/abs/2512.06726</link>
<guid>https://arxiv.org/abs/2512.06726</guid>
<content:encoded><![CDATA[
arXiv:2512.06726v1 Announce Type: new 
Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data</title>
<link>https://arxiv.org/abs/2512.06736</link>
<guid>https://arxiv.org/abs/2512.06736</guid>
<content:encoded><![CDATA[
arXiv:2512.06736v1 Announce Type: new 
Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.06738</link>
<guid>https://arxiv.org/abs/2512.06738</guid>
<content:encoded><![CDATA[
arXiv:2512.06738v1 Announce Type: new 
Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.06746</link>
<guid>https://arxiv.org/abs/2512.06746</guid>
<content:encoded><![CDATA[
arXiv:2512.06746v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement</title>
<link>https://arxiv.org/abs/2512.06750</link>
<guid>https://arxiv.org/abs/2512.06750</guid>
<content:encoded><![CDATA[
arXiv:2512.06750v1 Announce Type: new 
Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors</title>
<link>https://arxiv.org/abs/2512.06759</link>
<guid>https://arxiv.org/abs/2512.06759</guid>
<content:encoded><![CDATA[
arXiv:2512.06759v1 Announce Type: new 
Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms</title>
<link>https://arxiv.org/abs/2512.06763</link>
<guid>https://arxiv.org/abs/2512.06763</guid>
<content:encoded><![CDATA[
arXiv:2512.06763v1 Announce Type: new 
Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.06769</link>
<guid>https://arxiv.org/abs/2512.06769</guid>
<content:encoded><![CDATA[
arXiv:2512.06769v1 Announce Type: new 
Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06774</link>
<guid>https://arxiv.org/abs/2512.06774</guid>
<content:encoded><![CDATA[
arXiv:2512.06774v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</title>
<link>https://arxiv.org/abs/2512.06783</link>
<guid>https://arxiv.org/abs/2512.06783</guid>
<content:encoded><![CDATA[
arXiv:2512.06783v1 Announce Type: new 
Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Geometry Encoding Volume for Real-time Stereo Matching</title>
<link>https://arxiv.org/abs/2512.06793</link>
<guid>https://arxiv.org/abs/2512.06793</guid>
<content:encoded><![CDATA[
arXiv:2512.06793v1 Announce Type: new 
Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDOT: Efficient Unified Video Creation via Optimal Transport Distillation</title>
<link>https://arxiv.org/abs/2512.06802</link>
<guid>https://arxiv.org/abs/2512.06802</guid>
<content:encoded><![CDATA[
arXiv:2512.06802v1 Announce Type: new 
Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06810</link>
<guid>https://arxiv.org/abs/2512.06810</guid>
<content:encoded><![CDATA[
arXiv:2512.06810v1 Announce Type: new 
Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06811</link>
<guid>https://arxiv.org/abs/2512.06811</guid>
<content:encoded><![CDATA[
arXiv:2512.06811v1 Announce Type: new 
Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshSplatting: Differentiable Rendering with Opaque Meshes</title>
<link>https://arxiv.org/abs/2512.06818</link>
<guid>https://arxiv.org/abs/2512.06818</guid>
<content:encoded><![CDATA[
arXiv:2512.06818v1 Announce Type: new 
Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseCoop: Cooperative Perception with Kinematic-Grounded Queries</title>
<link>https://arxiv.org/abs/2512.06838</link>
<guid>https://arxiv.org/abs/2512.06838</guid>
<content:encoded><![CDATA[
arXiv:2512.06838v1 Announce Type: new 
Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</title>
<link>https://arxiv.org/abs/2512.06840</link>
<guid>https://arxiv.org/abs/2512.06840</guid>
<content:encoded><![CDATA[
arXiv:2512.06840v1 Announce Type: new 
Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.06845</link>
<guid>https://arxiv.org/abs/2512.06845</guid>
<content:encoded><![CDATA[
arXiv:2512.06845v1 Announce Type: new 
Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</title>
<link>https://arxiv.org/abs/2512.06849</link>
<guid>https://arxiv.org/abs/2512.06849</guid>
<content:encoded><![CDATA[
arXiv:2512.06849v1 Announce Type: new 
Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06862</link>
<guid>https://arxiv.org/abs/2512.06862</guid>
<content:encoded><![CDATA[
arXiv:2512.06862v1 Announce Type: new 
Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training</title>
<link>https://arxiv.org/abs/2512.06864</link>
<guid>https://arxiv.org/abs/2512.06864</guid>
<content:encoded><![CDATA[
arXiv:2512.06864v1 Announce Type: new 
Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Retrieval Augmented Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06865</link>
<guid>https://arxiv.org/abs/2512.06865</guid>
<content:encoded><![CDATA[
arXiv:2512.06865v1 Announce Type: new 
Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[
arXiv:2512.06866v1 Announce Type: new 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective</title>
<link>https://arxiv.org/abs/2512.06870</link>
<guid>https://arxiv.org/abs/2512.06870</guid>
<content:encoded><![CDATA[
arXiv:2512.06870v1 Announce Type: new 
Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification</title>
<link>https://arxiv.org/abs/2512.06877</link>
<guid>https://arxiv.org/abs/2512.06877</guid>
<content:encoded><![CDATA[
arXiv:2512.06877v1 Announce Type: new 
Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion</title>
<link>https://arxiv.org/abs/2512.06882</link>
<guid>https://arxiv.org/abs/2512.06882</guid>
<content:encoded><![CDATA[
arXiv:2512.06882v1 Announce Type: new 
Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoPano: Unified Panorama Generation via Joint Modeling</title>
<link>https://arxiv.org/abs/2512.06885</link>
<guid>https://arxiv.org/abs/2512.06885</guid>
<content:encoded><![CDATA[
arXiv:2512.06885v1 Announce Type: new 
Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Learning for Domain Adaptive Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06886</link>
<guid>https://arxiv.org/abs/2512.06886</guid>
<content:encoded><![CDATA[
arXiv:2512.06886v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation</title>
<link>https://arxiv.org/abs/2512.06888</link>
<guid>https://arxiv.org/abs/2512.06888</guid>
<content:encoded><![CDATA[
arXiv:2512.06888v1 Announce Type: new 
Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Zero-Shot Reference-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.06905</link>
<guid>https://arxiv.org/abs/2512.06905</guid>
<content:encoded><![CDATA[
arXiv:2512.06905v1 Announce Type: new 
Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</title>
<link>https://arxiv.org/abs/2512.06921</link>
<guid>https://arxiv.org/abs/2512.06921</guid>
<content:encoded><![CDATA[
arXiv:2512.06921v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology</title>
<link>https://arxiv.org/abs/2512.06949</link>
<guid>https://arxiv.org/abs/2512.06949</guid>
<content:encoded><![CDATA[
arXiv:2512.06949v1 Announce Type: new 
Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Masking based Self-Supervised Learning for Image Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06981</link>
<guid>https://arxiv.org/abs/2512.06981</guid>
<content:encoded><![CDATA[
arXiv:2512.06981v1 Announce Type: new 
Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues</title>
<link>https://arxiv.org/abs/2512.07034</link>
<guid>https://arxiv.org/abs/2512.07034</guid>
<content:encoded><![CDATA[
arXiv:2512.07034v1 Announce Type: new 
Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating and Preserving High-level Fidelity in Super-Resolution</title>
<link>https://arxiv.org/abs/2512.07037</link>
<guid>https://arxiv.org/abs/2512.07037</guid>
<content:encoded><![CDATA[
arXiv:2512.07037v1 Announce Type: new 
Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07051</link>
<guid>https://arxiv.org/abs/2512.07051</guid>
<content:encoded><![CDATA[
arXiv:2512.07051v1 Announce Type: new 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07052</link>
<guid>https://arxiv.org/abs/2512.07052</guid>
<content:encoded><![CDATA[
arXiv:2512.07052v1 Announce Type: new 
Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v1 Announce Type: new 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Homology-Guided Frequency Filtering for Image Compression</title>
<link>https://arxiv.org/abs/2512.07065</link>
<guid>https://arxiv.org/abs/2512.07065</guid>
<content:encoded><![CDATA[
arXiv:2512.07065v1 Announce Type: new 
Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-measure: Contextualizing Metric for Camouflage</title>
<link>https://arxiv.org/abs/2512.07076</link>
<guid>https://arxiv.org/abs/2512.07076</guid>
<content:encoded><![CDATA[
arXiv:2512.07076v1 Announce Type: new 
Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title>
<link>https://arxiv.org/abs/2512.07078</link>
<guid>https://arxiv.org/abs/2512.07078</guid>
<content:encoded><![CDATA[
arXiv:2512.07078v1 Announce Type: new 
Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</title>
<link>https://arxiv.org/abs/2512.07107</link>
<guid>https://arxiv.org/abs/2512.07107</guid>
<content:encoded><![CDATA[
arXiv:2512.07107v1 Announce Type: new 
Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection</title>
<link>https://arxiv.org/abs/2512.07110</link>
<guid>https://arxiv.org/abs/2512.07110</guid>
<content:encoded><![CDATA[
arXiv:2512.07110v1 Announce Type: new 
Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-free Clothing Region of Interest Self-correction for Virtual Try-On</title>
<link>https://arxiv.org/abs/2512.07126</link>
<guid>https://arxiv.org/abs/2512.07126</guid>
<content:encoded><![CDATA[
arXiv:2512.07126v1 Announce Type: new 
Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP</title>
<link>https://arxiv.org/abs/2512.07128</link>
<guid>https://arxiv.org/abs/2512.07128</guid>
<content:encoded><![CDATA[
arXiv:2512.07128v1 Announce Type: new 
Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07135</link>
<guid>https://arxiv.org/abs/2512.07135</guid>
<content:encoded><![CDATA[
arXiv:2512.07135v1 Announce Type: new 
Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2512.07136</link>
<guid>https://arxiv.org/abs/2512.07136</guid>
<content:encoded><![CDATA[
arXiv:2512.07136v1 Announce Type: new 
Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2512.07141</link>
<guid>https://arxiv.org/abs/2512.07141</guid>
<content:encoded><![CDATA[
arXiv:2512.07141v1 Announce Type: new 
Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</title>
<link>https://arxiv.org/abs/2512.07155</link>
<guid>https://arxiv.org/abs/2512.07155</guid>
<content:encoded><![CDATA[
arXiv:2512.07155v1 Announce Type: new 
Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</title>
<link>https://arxiv.org/abs/2512.07165</link>
<guid>https://arxiv.org/abs/2512.07165</guid>
<content:encoded><![CDATA[
arXiv:2512.07165v1 Announce Type: new 
Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing</title>
<link>https://arxiv.org/abs/2512.07166</link>
<guid>https://arxiv.org/abs/2512.07166</guid>
<content:encoded><![CDATA[
arXiv:2512.07166v1 Announce Type: new 
Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</title>
<link>https://arxiv.org/abs/2512.07170</link>
<guid>https://arxiv.org/abs/2512.07170</guid>
<content:encoded><![CDATA[
arXiv:2512.07170v1 Announce Type: new 
Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration</title>
<link>https://arxiv.org/abs/2512.07171</link>
<guid>https://arxiv.org/abs/2512.07171</guid>
<content:encoded><![CDATA[
arXiv:2512.07171v1 Announce Type: new 
Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>START: Spatial and Textual Learning for Chart Understanding</title>
<link>https://arxiv.org/abs/2512.07186</link>
<guid>https://arxiv.org/abs/2512.07186</guid>
<content:encoded><![CDATA[
arXiv:2512.07186v1 Announce Type: new 
Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification</title>
<link>https://arxiv.org/abs/2512.07190</link>
<guid>https://arxiv.org/abs/2512.07190</guid>
<content:encoded><![CDATA[
arXiv:2512.07190v1 Announce Type: new 
Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction</title>
<link>https://arxiv.org/abs/2512.07191</link>
<guid>https://arxiv.org/abs/2512.07191</guid>
<content:encoded><![CDATA[
arXiv:2512.07191v1 Announce Type: new 
Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression</title>
<link>https://arxiv.org/abs/2512.07192</link>
<guid>https://arxiv.org/abs/2512.07192</guid>
<content:encoded><![CDATA[
arXiv:2512.07192v1 Announce Type: new 
Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07197</link>
<guid>https://arxiv.org/abs/2512.07197</guid>
<content:encoded><![CDATA[
arXiv:2512.07197v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Storytelling Images with Rich Chains-of-Reasoning</title>
<link>https://arxiv.org/abs/2512.07198</link>
<guid>https://arxiv.org/abs/2512.07198</guid>
<content:encoded><![CDATA[
arXiv:2512.07198v1 Announce Type: new 
Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Diffusion Models via Code Execution</title>
<link>https://arxiv.org/abs/2512.07201</link>
<guid>https://arxiv.org/abs/2512.07201</guid>
<content:encoded><![CDATA[
arXiv:2512.07201v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning</title>
<link>https://arxiv.org/abs/2512.07203</link>
<guid>https://arxiv.org/abs/2512.07203</guid>
<content:encoded><![CDATA[
arXiv:2512.07203v1 Announce Type: new 
Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT</title>
<link>https://arxiv.org/abs/2512.07206</link>
<guid>https://arxiv.org/abs/2512.07206</guid>
<content:encoded><![CDATA[
arXiv:2512.07206v1 Announce Type: new 
Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds</title>
<link>https://arxiv.org/abs/2512.07211</link>
<guid>https://arxiv.org/abs/2512.07211</guid>
<content:encoded><![CDATA[
arXiv:2512.07211v1 Announce Type: new 
Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.07215</link>
<guid>https://arxiv.org/abs/2512.07215</guid>
<content:encoded><![CDATA[
arXiv:2512.07215v1 Announce Type: new 
Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title>
<link>https://arxiv.org/abs/2512.07228</link>
<guid>https://arxiv.org/abs/2512.07228</guid>
<content:encoded><![CDATA[
arXiv:2512.07228v1 Announce Type: new 
Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2512.07229</link>
<guid>https://arxiv.org/abs/2512.07229</guid>
<content:encoded><![CDATA[
arXiv:2512.07229v1 Announce Type: new 
Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STRinGS: Selective Text Refinement in Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07230</link>
<guid>https://arxiv.org/abs/2512.07230</guid>
<content:encoded><![CDATA[
arXiv:2512.07230v1 Announce Type: new 
Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07234</link>
<guid>https://arxiv.org/abs/2512.07234</guid>
<content:encoded><![CDATA[
arXiv:2512.07234v1 Announce Type: new 
Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Camera Positional Encoding for Controlled Video Generation</title>
<link>https://arxiv.org/abs/2512.07237</link>
<guid>https://arxiv.org/abs/2512.07237</guid>
<content:encoded><![CDATA[
arXiv:2512.07237v1 Announce Type: new 
Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture</title>
<link>https://arxiv.org/abs/2512.07241</link>
<guid>https://arxiv.org/abs/2512.07241</guid>
<content:encoded><![CDATA[
arXiv:2512.07241v1 Announce Type: new 
Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Textual Explanations via Translating Decision-Critical Features</title>
<link>https://arxiv.org/abs/2512.07245</link>
<guid>https://arxiv.org/abs/2512.07245</guid>
<content:encoded><![CDATA[
arXiv:2512.07245v1 Announce Type: new 
Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title>
<link>https://arxiv.org/abs/2512.07247</link>
<guid>https://arxiv.org/abs/2512.07247</guid>
<content:encoded><![CDATA[
arXiv:2512.07247v1 Announce Type: new 
Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement</title>
<link>https://arxiv.org/abs/2512.07251</link>
<guid>https://arxiv.org/abs/2512.07251</guid>
<content:encoded><![CDATA[
arXiv:2512.07251v1 Announce Type: new 
Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</title>
<link>https://arxiv.org/abs/2512.07253</link>
<guid>https://arxiv.org/abs/2512.07253</guid>
<content:encoded><![CDATA[
arXiv:2512.07253v1 Announce Type: new 
Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</title>
<link>https://arxiv.org/abs/2512.07269</link>
<guid>https://arxiv.org/abs/2512.07269</guid>
<content:encoded><![CDATA[
arXiv:2512.07269v1 Announce Type: new 
Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation</title>
<link>https://arxiv.org/abs/2512.07273</link>
<guid>https://arxiv.org/abs/2512.07273</guid>
<content:encoded><![CDATA[
arXiv:2512.07273v1 Announce Type: new 
Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</title>
<link>https://arxiv.org/abs/2512.07275</link>
<guid>https://arxiv.org/abs/2512.07275</guid>
<content:encoded><![CDATA[
arXiv:2512.07275v1 Announce Type: new 
Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery</title>
<link>https://arxiv.org/abs/2512.07276</link>
<guid>https://arxiv.org/abs/2512.07276</guid>
<content:encoded><![CDATA[
arXiv:2512.07276v1 Announce Type: new 
Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts</title>
<link>https://arxiv.org/abs/2512.07302</link>
<guid>https://arxiv.org/abs/2512.07302</guid>
<content:encoded><![CDATA[
arXiv:2512.07302v1 Announce Type: new 
Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset</title>
<link>https://arxiv.org/abs/2512.07305</link>
<guid>https://arxiv.org/abs/2512.07305</guid>
<content:encoded><![CDATA[
arXiv:2512.07305v1 Announce Type: new 
Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.07328</link>
<guid>https://arxiv.org/abs/2512.07328</guid>
<content:encoded><![CDATA[
arXiv:2512.07328v1 Announce Type: new 
Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers</title>
<link>https://arxiv.org/abs/2512.07331</link>
<guid>https://arxiv.org/abs/2512.07331</guid>
<content:encoded><![CDATA[
arXiv:2512.07331v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Referring Expression Segmentation on Aerial Photos</title>
<link>https://arxiv.org/abs/2512.07338</link>
<guid>https://arxiv.org/abs/2512.07338</guid>
<content:encoded><![CDATA[
arXiv:2512.07338v1 Announce Type: new 
Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07345</link>
<guid>https://arxiv.org/abs/2512.07345</guid>
<content:encoded><![CDATA[
arXiv:2512.07345v1 Announce Type: new 
Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition</title>
<link>https://arxiv.org/abs/2512.07348</link>
<guid>https://arxiv.org/abs/2512.07348</guid>
<content:encoded><![CDATA[
arXiv:2512.07348v1 Announce Type: new 
Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&amp;Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&amp;Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection</title>
<link>https://arxiv.org/abs/2512.07351</link>
<guid>https://arxiv.org/abs/2512.07351</guid>
<content:encoded><![CDATA[
arXiv:2512.07351v1 Announce Type: new 
Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.07360</link>
<guid>https://arxiv.org/abs/2512.07360</guid>
<content:encoded><![CDATA[
arXiv:2512.07360v1 Announce Type: new 
Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency</title>
<link>https://arxiv.org/abs/2512.07379</link>
<guid>https://arxiv.org/abs/2512.07379</guid>
<content:encoded><![CDATA[
arXiv:2512.07379v1 Announce Type: new 
Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</title>
<link>https://arxiv.org/abs/2512.07381</link>
<guid>https://arxiv.org/abs/2512.07381</guid>
<content:encoded><![CDATA[
arXiv:2512.07381v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicCBMs: Logic-Enhanced Concept-Based Learning</title>
<link>https://arxiv.org/abs/2512.07383</link>
<guid>https://arxiv.org/abs/2512.07383</guid>
<content:encoded><![CDATA[
arXiv:2512.07383v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline</title>
<link>https://arxiv.org/abs/2512.07385</link>
<guid>https://arxiv.org/abs/2512.07385</guid>
<content:encoded><![CDATA[
arXiv:2512.07385v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring</title>
<link>https://arxiv.org/abs/2512.07391</link>
<guid>https://arxiv.org/abs/2512.07391</guid>
<content:encoded><![CDATA[
arXiv:2512.07391v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing Objects along Hand Interaction Timelines in Egocentric Video</title>
<link>https://arxiv.org/abs/2512.07394</link>
<guid>https://arxiv.org/abs/2512.07394</guid>
<content:encoded><![CDATA[
arXiv:2512.07394v1 Announce Type: new 
Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs</title>
<link>https://arxiv.org/abs/2512.07410</link>
<guid>https://arxiv.org/abs/2512.07410</guid>
<content:encoded><![CDATA[
arXiv:2512.07410v1 Announce Type: new 
Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Exploration of Mobility Interaction Patterns</title>
<link>https://arxiv.org/abs/2512.07415</link>
<guid>https://arxiv.org/abs/2512.07415</guid>
<content:encoded><![CDATA[
arXiv:2512.07415v1 Announce Type: new 
Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When normalization hallucinates: unseen risks in AI-powered whole slide image processing</title>
<link>https://arxiv.org/abs/2512.07426</link>
<guid>https://arxiv.org/abs/2512.07426</guid>
<content:encoded><![CDATA[
arXiv:2512.07426v1 Announce Type: new 
Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Video Editing with Temporal Reasoner</title>
<link>https://arxiv.org/abs/2512.07469</link>
<guid>https://arxiv.org/abs/2512.07469</guid>
<content:encoded><![CDATA[
arXiv:2512.07469v1 Announce Type: new 
Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance</title>
<link>https://arxiv.org/abs/2512.07480</link>
<guid>https://arxiv.org/abs/2512.07480</guid>
<content:encoded><![CDATA[
arXiv:2512.07480v1 Announce Type: new 
Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior</title>
<link>https://arxiv.org/abs/2512.07498</link>
<guid>https://arxiv.org/abs/2512.07498</guid>
<content:encoded><![CDATA[
arXiv:2512.07498v1 Announce Type: new 
Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.07500</link>
<guid>https://arxiv.org/abs/2512.07500</guid>
<content:encoded><![CDATA[
arXiv:2512.07500v1 Announce Type: new 
Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.07503</link>
<guid>https://arxiv.org/abs/2512.07503</guid>
<content:encoded><![CDATA[
arXiv:2512.07503v1 Announce Type: new 
Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points</title>
<link>https://arxiv.org/abs/2512.07504</link>
<guid>https://arxiv.org/abs/2512.07504</guid>
<content:encoded><![CDATA[
arXiv:2512.07504v1 Announce Type: new 
Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshRipple: Structured Autoregressive Generation of Artist-Meshes</title>
<link>https://arxiv.org/abs/2512.07514</link>
<guid>https://arxiv.org/abs/2512.07514</guid>
<content:encoded><![CDATA[
arXiv:2512.07514v1 Announce Type: new 
Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images</title>
<link>https://arxiv.org/abs/2512.07527</link>
<guid>https://arxiv.org/abs/2512.07527</guid>
<content:encoded><![CDATA[
arXiv:2512.07527v1 Announce Type: new 
Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07564</link>
<guid>https://arxiv.org/abs/2512.07564</guid>
<content:encoded><![CDATA[
arXiv:2512.07564v1 Announce Type: new 
Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation</title>
<link>https://arxiv.org/abs/2512.07568</link>
<guid>https://arxiv.org/abs/2512.07568</guid>
<content:encoded><![CDATA[
arXiv:2512.07568v1 Announce Type: new 
Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\"ive fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs</title>
<link>https://arxiv.org/abs/2512.07580</link>
<guid>https://arxiv.org/abs/2512.07580</guid>
<content:encoded><![CDATA[
arXiv:2512.07580v1 Announce Type: new 
Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Image Technical Report</title>
<link>https://arxiv.org/abs/2512.07584</link>
<guid>https://arxiv.org/abs/2512.07584</guid>
<content:encoded><![CDATA[
arXiv:2512.07584v1 Announce Type: new 
Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07590</link>
<guid>https://arxiv.org/abs/2512.07590</guid>
<content:encoded><![CDATA[
arXiv:2512.07590v1 Announce Type: new 
Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery</title>
<link>https://arxiv.org/abs/2512.07596</link>
<guid>https://arxiv.org/abs/2512.07596</guid>
<content:encoded><![CDATA[
arXiv:2512.07596v1 Announce Type: new 
Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Segment Any 3D Thing as Instance Tracking</title>
<link>https://arxiv.org/abs/2512.07599</link>
<guid>https://arxiv.org/abs/2512.07599</guid>
<content:encoded><![CDATA[
arXiv:2512.07599v1 Announce Type: new 
Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposition Sampling for Efficient Region Annotations in Active Learning</title>
<link>https://arxiv.org/abs/2512.07606</link>
<guid>https://arxiv.org/abs/2512.07606</guid>
<content:encoded><![CDATA[
arXiv:2512.07606v1 Announce Type: new 
Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation</title>
<link>https://arxiv.org/abs/2512.07628</link>
<guid>https://arxiv.org/abs/2512.07628</guid>
<content:encoded><![CDATA[
arXiv:2512.07628v1 Announce Type: new 
Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method</title>
<link>https://arxiv.org/abs/2512.07651</link>
<guid>https://arxiv.org/abs/2512.07651</guid>
<content:encoded><![CDATA[
arXiv:2512.07651v1 Announce Type: new 
Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research</title>
<link>https://arxiv.org/abs/2512.07652</link>
<guid>https://arxiv.org/abs/2512.07652</guid>
<content:encoded><![CDATA[
arXiv:2512.07652v1 Announce Type: new 
Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization-Guided Diffusion for Interactive Scene Generation</title>
<link>https://arxiv.org/abs/2512.07661</link>
<guid>https://arxiv.org/abs/2512.07661</guid>
<content:encoded><![CDATA[
arXiv:2512.07661v1 Announce Type: new 
Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset</title>
<link>https://arxiv.org/abs/2512.07668</link>
<guid>https://arxiv.org/abs/2512.07668</guid>
<content:encoded><![CDATA[
arXiv:2512.07668v1 Announce Type: new 
Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations</title>
<link>https://arxiv.org/abs/2512.07674</link>
<guid>https://arxiv.org/abs/2512.07674</guid>
<content:encoded><![CDATA[
arXiv:2512.07674v1 Announce Type: new 
Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only</title>
<link>https://arxiv.org/abs/2512.07698</link>
<guid>https://arxiv.org/abs/2512.07698</guid>
<content:encoded><![CDATA[
arXiv:2512.07698v1 Announce Type: new 
Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment</title>
<link>https://arxiv.org/abs/2512.07702</link>
<guid>https://arxiv.org/abs/2512.07702</guid>
<content:encoded><![CDATA[
arXiv:2512.07702v1 Announce Type: new 
Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PVeRA: Probabilistic Vector-Based Random Matrix Adaptation</title>
<link>https://arxiv.org/abs/2512.07703</link>
<guid>https://arxiv.org/abs/2512.07703</guid>
<content:encoded><![CDATA[
arXiv:2512.07703v1 Announce Type: new 
Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnCageNet: Tracking and Pose Estimation of Caged Animal</title>
<link>https://arxiv.org/abs/2512.07712</link>
<guid>https://arxiv.org/abs/2512.07712</guid>
<content:encoded><![CDATA[
arXiv:2512.07712v1 Announce Type: new 
Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation</title>
<link>https://arxiv.org/abs/2512.07720</link>
<guid>https://arxiv.org/abs/2512.07720</guid>
<content:encoded><![CDATA[
arXiv:2512.07720v1 Announce Type: new 
Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving action classification with brain-inspired deep networks</title>
<link>https://arxiv.org/abs/2512.07729</link>
<guid>https://arxiv.org/abs/2512.07729</guid>
<content:encoded><![CDATA[
arXiv:2512.07729v1 Announce Type: new 
Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title>
<link>https://arxiv.org/abs/2512.07730</link>
<guid>https://arxiv.org/abs/2512.07730</guid>
<content:encoded><![CDATA[
arXiv:2512.07730v1 Announce Type: new 
Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</title>
<link>https://arxiv.org/abs/2512.07733</link>
<guid>https://arxiv.org/abs/2512.07733</guid>
<content:encoded><![CDATA[
arXiv:2512.07733v1 Announce Type: new 
Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLTCOE Evaluation Team at TREC 2025: VQA Track</title>
<link>https://arxiv.org/abs/2512.07738</link>
<guid>https://arxiv.org/abs/2512.07738</guid>
<content:encoded><![CDATA[
arXiv:2512.07738v1 Announce Type: new 
Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.07745</link>
<guid>https://arxiv.org/abs/2512.07745</guid>
<content:encoded><![CDATA[
arXiv:2512.07745v1 Announce Type: new 
Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.07747</link>
<guid>https://arxiv.org/abs/2512.07747</guid>
<content:encoded><![CDATA[
arXiv:2512.07747v1 Announce Type: new 
Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction</title>
<link>https://arxiv.org/abs/2512.07756</link>
<guid>https://arxiv.org/abs/2512.07756</guid>
<content:encoded><![CDATA[
arXiv:2512.07756v1 Announce Type: new 
Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.07760</link>
<guid>https://arxiv.org/abs/2512.07760</guid>
<content:encoded><![CDATA[
arXiv:2512.07760v1 Announce Type: new 
Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</title>
<link>https://arxiv.org/abs/2512.07776</link>
<guid>https://arxiv.org/abs/2512.07776</guid>
<content:encoded><![CDATA[
arXiv:2512.07776v1 Announce Type: new 
Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Matching Variational AutoEncoder</title>
<link>https://arxiv.org/abs/2512.07778</link>
<guid>https://arxiv.org/abs/2512.07778</guid>
<content:encoded><![CDATA[
arXiv:2512.07778v1 Announce Type: new 
Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory</title>
<link>https://arxiv.org/abs/2512.07802</link>
<guid>https://arxiv.org/abs/2512.07802</guid>
<content:encoded><![CDATA[
arXiv:2512.07802v1 Announce Type: new 
Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-view Pyramid Transformer: Look Coarser to See Broader</title>
<link>https://arxiv.org/abs/2512.07806</link>
<guid>https://arxiv.org/abs/2512.07806</guid>
<content:encoded><![CDATA[
arXiv:2512.07806v1 Announce Type: new 
Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes</title>
<link>https://arxiv.org/abs/2512.07807</link>
<guid>https://arxiv.org/abs/2512.07807</guid>
<content:encoded><![CDATA[
arXiv:2512.07807v1 Announce Type: new 
Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</title>
<link>https://arxiv.org/abs/2512.07821</link>
<guid>https://arxiv.org/abs/2512.07821</guid>
<content:encoded><![CDATA[
arXiv:2512.07821v1 Announce Type: new 
Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing</title>
<link>https://arxiv.org/abs/2512.07826</link>
<guid>https://arxiv.org/abs/2512.07826</guid>
<content:encoded><![CDATA[
arXiv:2512.07826v1 Announce Type: new 
Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</title>
<link>https://arxiv.org/abs/2512.07829</link>
<guid>https://arxiv.org/abs/2512.07829</guid>
<content:encoded><![CDATA[
arXiv:2512.07829v1 Announce Type: new 
Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</title>
<link>https://arxiv.org/abs/2512.07831</link>
<guid>https://arxiv.org/abs/2512.07831</guid>
<content:encoded><![CDATA[
arXiv:2512.07831v1 Announce Type: new 
Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Visual Similarity</title>
<link>https://arxiv.org/abs/2512.07833</link>
<guid>https://arxiv.org/abs/2512.07833</guid>
<content:encoded><![CDATA[
arXiv:2512.07833v1 Announce Type: new 
Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voxify3D: Pixel Art Meets Volumetric Rendering</title>
<link>https://arxiv.org/abs/2512.07834</link>
<guid>https://arxiv.org/abs/2512.07834</guid>
<content:encoded><![CDATA[
arXiv:2512.07834v1 Announce Type: new 
Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.05992</link>
<guid>https://arxiv.org/abs/2512.05992</guid>
<content:encoded><![CDATA[
arXiv:2512.05992v1 Announce Type: cross 
Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Temporal Single-photon LiDAR</title>
<link>https://arxiv.org/abs/2512.06008</link>
<guid>https://arxiv.org/abs/2512.06008</guid>
<content:encoded><![CDATA[
arXiv:2512.06008v1 Announce Type: cross 
Abstract: Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</title>
<link>https://arxiv.org/abs/2512.06147</link>
<guid>https://arxiv.org/abs/2512.06147</guid>
<content:encoded><![CDATA[
arXiv:2512.06147v1 Announce Type: cross 
Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&amp;M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</title>
<link>https://arxiv.org/abs/2512.06589</link>
<guid>https://arxiv.org/abs/2512.06589</guid>
<content:encoded><![CDATA[
arXiv:2512.06589v1 Announce Type: cross 
Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Quantization using Gaussian Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.06609</link>
<guid>https://arxiv.org/abs/2512.06609</guid>
<content:encoded><![CDATA[
arXiv:2512.06609v1 Announce Type: cross 
Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</title>
<link>https://arxiv.org/abs/2512.06628</link>
<guid>https://arxiv.org/abs/2512.06628</guid>
<content:encoded><![CDATA[
arXiv:2512.06628v1 Announce Type: cross 
Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2512.06648</link>
<guid>https://arxiv.org/abs/2512.06648</guid>
<content:encoded><![CDATA[
arXiv:2512.06648v1 Announce Type: cross 
Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning</title>
<link>https://arxiv.org/abs/2512.06649</link>
<guid>https://arxiv.org/abs/2512.06649</guid>
<content:encoded><![CDATA[
arXiv:2512.06649v1 Announce Type: cross 
Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2512.06665</link>
<guid>https://arxiv.org/abs/2512.06665</guid>
<content:encoded><![CDATA[
arXiv:2512.06665v1 Announce Type: cross 
Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data</title>
<link>https://arxiv.org/abs/2512.06730</link>
<guid>https://arxiv.org/abs/2512.06730</guid>
<content:encoded><![CDATA[
arXiv:2512.06730v1 Announce Type: cross 
Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v1 Announce Type: cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association</title>
<link>https://arxiv.org/abs/2512.06757</link>
<guid>https://arxiv.org/abs/2512.06757</guid>
<content:encoded><![CDATA[
arXiv:2512.06757v1 Announce Type: cross 
Abstract: This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both "heard" and "unheard" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices</title>
<link>https://arxiv.org/abs/2512.06848</link>
<guid>https://arxiv.org/abs/2512.06848</guid>
<content:encoded><![CDATA[
arXiv:2512.06848v1 Announce Type: cross 
Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Visual SLAM using a General 3D Prior</title>
<link>https://arxiv.org/abs/2512.06868</link>
<guid>https://arxiv.org/abs/2512.06868</guid>
<content:encoded><![CDATA[
arXiv:2512.06868v1 Announce Type: cross 
Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v1 Announce Type: cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</title>
<link>https://arxiv.org/abs/2512.06963</link>
<guid>https://arxiv.org/abs/2512.06963</guid>
<content:encoded><![CDATA[
arXiv:2512.06963v1 Announce Type: cross 
Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</title>
<link>https://arxiv.org/abs/2512.06990</link>
<guid>https://arxiv.org/abs/2512.06990</guid>
<content:encoded><![CDATA[
arXiv:2512.06990v1 Announce Type: cross 
Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</title>
<link>https://arxiv.org/abs/2512.07040</link>
<guid>https://arxiv.org/abs/2512.07040</guid>
<content:encoded><![CDATA[
arXiv:2512.07040v1 Announce Type: cross 
Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.07130</link>
<guid>https://arxiv.org/abs/2512.07130</guid>
<content:encoded><![CDATA[
arXiv:2512.07130v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.07132</link>
<guid>https://arxiv.org/abs/2512.07132</guid>
<content:encoded><![CDATA[
arXiv:2512.07132v1 Announce Type: cross 
Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</title>
<link>https://arxiv.org/abs/2512.07142</link>
<guid>https://arxiv.org/abs/2512.07142</guid>
<content:encoded><![CDATA[
arXiv:2512.07142v1 Announce Type: cross 
Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers</title>
<link>https://arxiv.org/abs/2512.07150</link>
<guid>https://arxiv.org/abs/2512.07150</guid>
<content:encoded><![CDATA[
arXiv:2512.07150v1 Announce Type: cross 
Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</title>
<link>https://arxiv.org/abs/2512.07224</link>
<guid>https://arxiv.org/abs/2512.07224</guid>
<content:encoded><![CDATA[
arXiv:2512.07224v1 Announce Type: cross 
Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affine Subspace Models and Clustering for Patch-Based Image Denoising</title>
<link>https://arxiv.org/abs/2512.07259</link>
<guid>https://arxiv.org/abs/2512.07259</guid>
<content:encoded><![CDATA[
arXiv:2512.07259v1 Announce Type: cross 
Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Unification of Concept Learning with Concept Cones</title>
<link>https://arxiv.org/abs/2512.07355</link>
<guid>https://arxiv.org/abs/2512.07355</guid>
<content:encoded><![CDATA[
arXiv:2512.07355v1 Announce Type: cross 
Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</title>
<link>https://arxiv.org/abs/2512.07390</link>
<guid>https://arxiv.org/abs/2512.07390</guid>
<content:encoded><![CDATA[
arXiv:2512.07390v1 Announce Type: cross 
Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models</title>
<link>https://arxiv.org/abs/2512.07419</link>
<guid>https://arxiv.org/abs/2512.07419</guid>
<content:encoded><![CDATA[
arXiv:2512.07419v1 Announce Type: cross 
Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</title>
<link>https://arxiv.org/abs/2512.07437</link>
<guid>https://arxiv.org/abs/2512.07437</guid>
<content:encoded><![CDATA[
arXiv:2512.07437v1 Announce Type: cross 
Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Geometry Distribution for 3D Animation Generation</title>
<link>https://arxiv.org/abs/2512.07459</link>
<guid>https://arxiv.org/abs/2512.07459</guid>
<content:encoded><![CDATA[
arXiv:2512.07459v1 Announce Type: cross 
Abstract: Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \times$ higher user study score), achieving the best results across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces</title>
<link>https://arxiv.org/abs/2512.07509</link>
<guid>https://arxiv.org/abs/2512.07509</guid>
<content:encoded><![CDATA[
arXiv:2512.07509v1 Announce Type: cross 
Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX: Reasoning with Latent Exploration for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07558</link>
<guid>https://arxiv.org/abs/2512.07558</guid>
<content:encoded><![CDATA[
arXiv:2512.07558v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework</title>
<link>https://arxiv.org/abs/2512.07574</link>
<guid>https://arxiv.org/abs/2512.07574</guid>
<content:encoded><![CDATA[
arXiv:2512.07574v1 Announce Type: cross 
Abstract: Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation</title>
<link>https://arxiv.org/abs/2512.07576</link>
<guid>https://arxiv.org/abs/2512.07576</guid>
<content:encoded><![CDATA[
arXiv:2512.07576v1 Announce Type: cross 
Abstract: Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</title>
<link>https://arxiv.org/abs/2512.07687</link>
<guid>https://arxiv.org/abs/2512.07687</guid>
<content:encoded><![CDATA[
arXiv:2512.07687v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep transfer learning for image classification: a survey</title>
<link>https://arxiv.org/abs/2205.09904</link>
<guid>https://arxiv.org/abs/2205.09904</guid>
<content:encoded><![CDATA[
arXiv:2205.09904v2 Announce Type: replace 
Abstract: Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for image classification is when large deep models can be trained on abundant labelled data. However there are many real world scenarios where the requirement for large amounts of training data to get the best performance cannot be met. In these scenarios transfer learning can help improve performance. To date there have been no surveys that comprehensively review deep transfer learning as it relates to image classification overall. However, several recent general surveys of deep transfer learning and ones that relate to particular specialised target image classification tasks have been published. We believe it is important for the future progress in the field that all current knowledge is collated and the overarching patterns analysed and discussed. In this survey we formally define deep transfer learning and the problem it attempts to solve in relation to image classification. We survey the current state of the field and identify where recent progress has been made. We show where the gaps in current knowledge are and make suggestions for how to progress the field to fill in these knowledge gaps. We present a new taxonomy of the applications of transfer learning for image classification. This taxonomy makes it easier to see overarching patterns of where transfer learning has been effective and, where it has failed to fulfill its potential. This also allows us to suggest where the problems lie and how it could be used more effectively. We show that under this new taxonomy, many of the applications where transfer learning has been shown to be ineffective or even hinder performance are to be expected when taking into account the source and target datasets and the techniques used.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping</title>
<link>https://arxiv.org/abs/2303.11228</link>
<guid>https://arxiv.org/abs/2303.11228</guid>
<content:encoded><![CDATA[
arXiv:2303.11228v3 Announce Type: replace 
Abstract: Object segmentation for robotic grasping under dynamic conditions often faces challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Deep Learning network that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders, one for each signal input and a spatial pyramidal pooling with atrous convolutions. Encoders capture rich contextual information by pooling the concatenated features at different resolutions while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The evaluation results show a 6-10\% segmentation accuracy improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The model code is available at https://github.com/sanket0707/Bimodal-SegNet.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2308.09388</link>
<guid>https://arxiv.org/abs/2308.09388</guid>
<content:encoded><![CDATA[
arXiv:2308.09388v3 Announce Type: replace 
Abstract: Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, "whether diffusion model can boost image restoration". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Roadside Monocular 3D Detection Prompted by 2D Detection</title>
<link>https://arxiv.org/abs/2404.01064</link>
<guid>https://arxiv.org/abs/2404.01064</guid>
<content:encoded><![CDATA[
arXiv:2404.01064v5 Announce Type: replace 
Abstract: Roadside monocular 3D detection requires detecting objects of predefined classes in an RGB frame and predicting their 3D attributes, such as bird's-eye-view (BEV) locations. It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To address this task, we introduce Promptable 3D Detector (Pro3D), a novel detector design that leverages 2D detections as prompts. We build our Pro3D upon two key insights. First, compared to a typical 3D detector, a 2D detector is ``easier'' to train due to fewer loss terms and performs significantly better at localizing objects w.r.t 2D metrics. Second, once 2D detections precisely locate objects in the image, a 3D detector can focus on lifting these detections into 3D BEV, especially when fixed camera pose or scene geometry provide an informative prior. To encode and incorporate 2D detections, we explore three methods: (a) concatenating features from both 2D and 3D detectors, (b) attentively fusing 2D and 3D detector features, and (c) encoding properties of predicted 2D bounding boxes \{$x$, $y$, width, height, label\} and attentively fusing them with the 3D detector feature. Interestingly, the third method significantly outperforms the others, underscoring the effectiveness of 2D detections as prompts that offer precise object targets and allow the 3D detector to focus on lifting them into 3D. Pro3D is adaptable for use with a wide range of 2D and 3D detectors with minimal modifications. Comprehensive experiments demonstrate that our Pro3D significantly enhances existing methods, achieving state-of-the-art results on two contemporary benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSP-GNN: Learning to Track via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2407.04308</link>
<guid>https://arxiv.org/abs/2407.04308</guid>
<content:encoded><![CDATA[
arXiv:2407.04308v3 Announce Type: replace 
Abstract: We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Model</title>
<link>https://arxiv.org/abs/2408.01627</link>
<guid>https://arxiv.org/abs/2408.01627</guid>
<content:encoded><![CDATA[
arXiv:2408.01627v3 Announce Type: replace 
Abstract: In recent years, the talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high-quality video. However, no single model has yet achieved equivalence across all quantitative and qualitative metrics. We introduce Jamba, a hybrid Transformer-Mamba model, to animate a 3D face. Mamba, a pioneering Structured State Space Model (SSM) architecture, was developed to overcome the limitations of conventional Transformer architectures, particularly in handling long sequences. This challenge has constrained traditional models. Jamba combines the advantages of both the Transformer and Mamba approaches, offering a comprehensive solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and lip sync through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Test Time Adaptation with Few-shot Guidance</title>
<link>https://arxiv.org/abs/2409.01341</link>
<guid>https://arxiv.org/abs/2409.01341</guid>
<content:encoded><![CDATA[
arXiv:2409.01341v4 Announce Type: replace 
Abstract: Deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. To address this issue, Test Time Adaptation (TTA) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. Although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a novel and practical setting that utilizes a few-shot support set on top of TTA. Adhering to the principle of few inputs, big gains, FS-TTA reduces blind exploration in unseen target domains. Furthermore, we propose a two-stage framework to tackle FS-TTA, including (i) fine-tuning the pre-trained source model with few-shot support set, along with using feature diversity augmentation module to avoid overfitting, (ii) implementing test time adaptation based on prototype memory bank guidance to produce high quality pseudo-label for model adaptation. Through extensive experiments on three cross-domain classification benchmarks, we demonstrate the superior performance and reliability of our FS-TTA and framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-Customized Image Generation</title>
<link>https://arxiv.org/abs/2410.02483</link>
<guid>https://arxiv.org/abs/2410.02483</guid>
<content:encoded><![CDATA[
arXiv:2410.02483v2 Announce Type: replace 
Abstract: Customized Image Generation, generating customized images with user-specified concepts, has raised significant attention due to its creativity and novelty. With impressive progress achieved in subject customization, some pioneer works further explored the customization of action and interaction beyond entity (i.e., human, animal, and object) appearance. However, these approaches only focus on basic actions and interactions between two entities, and their effects are limited by insufficient ''exactly same'' reference images. To extend customized image generation to more complex scenes for general real-world applications, we propose a new task: event-customized image generation. Given a single reference image, we define the ''event'' as all specific actions, poses, relations, or interactions between different entities in the scene. This task aims at accurately capturing the complex event and generating customized images with various target entities. To solve this task, we proposed a novel training-free event customization method: FreeEvent. Specifically, FreeEvent introduces two extra paths alongside the general diffusion denoising process: 1) Entity switching path: it applies cross-attention guidance and regulation for target entity generation. 2) Event transferring path: it injects the spatial feature and self-attention maps from the reference image to the target image for event generation. To further facilitate this new task, we collected two evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and ablations have demonstrated the effectiveness of FreeEvent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation</title>
<link>https://arxiv.org/abs/2410.06055</link>
<guid>https://arxiv.org/abs/2410.06055</guid>
<content:encoded><![CDATA[
arXiv:2410.06055v2 Announce Type: replace 
Abstract: While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution (HR) image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one. Instead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for HR image generation; however, existing methods often result in poor image quality and long inference time. We introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel training-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency. Extensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications. Codes: https://github.com/kmittle/RepLDM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation</title>
<link>https://arxiv.org/abs/2410.07618</link>
<guid>https://arxiv.org/abs/2410.07618</guid>
<content:encoded><![CDATA[
arXiv:2410.07618v2 Announce Type: replace 
Abstract: Although Chinese calligraphy generation has achieved style transfer, generating calligraphy by specifying the calligrapher, font, and character style remains challenging. To address this, we propose a new Chinese calligraphy generation model 'Moyun' , which replaces the Unet in the Diffusion model with Vision Mamba and introduces the TripleLabel control mechanism to achieve controllable calligraphy generation. The model was tested on our large-scale dataset 'Mobao' of over 1.9 million images, and the results demonstrate that 'Moyun' can effectively control the generation process and produce calligraphy in the specified style. Even for calligraphy the calligrapher has not written, 'Moyun' can generate calligraphy that matches the style of the calligrapher.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Motion: A Generative Approach for Scene Dynamics Compression</title>
<link>https://arxiv.org/abs/2410.09768</link>
<guid>https://arxiv.org/abs/2410.09768</guid>
<content:encoded><![CDATA[
arXiv:2410.09768v3 Announce Type: replace 
Abstract: This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-https://github.com/xyzysz/GNVDC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion</title>
<link>https://arxiv.org/abs/2411.10036</link>
<guid>https://arxiv.org/abs/2411.10036</guid>
<content:encoded><![CDATA[
arXiv:2411.10036v2 Announce Type: replace 
Abstract: Multimodal image fusion (MMIF) integrates information from different modalities to obtain a comprehensive image, aiding downstream tasks. However, existing research focuses on complementary information fusion and training strategies, overlooking the critical role of underlying architectural components like normalization and convolution kernels. We reevaluate the UNet architecture for end-to-end MMIF, identifying that widely used batch normalization limits performance by smoothing crucial sparse features. To address this, we propose a hybrid of instance and group normalization to maintain sample independence and reinforce intrinsic feature correlations. Crucially, this strategy facilitates richer feature maps, enabling large kernel convolution to fully leverage its receptive field, enhancing detail preservation. Furthermore, the proposed multi-path adaptive fusion module dynamically calibrates features from varying scales and receptive fields, ensuring effective information transfer. Our method achieves SOTA objective performance on MSRS, M$^3$FD, TNO, and Harvard datasets, producing visually clearer salient objects and lesion areas. Notably, it improves MSRS segmentation mIoU by 8.1\% over the infrared image. This performance stems from a synergistic design of normalization and convolution kernels, which preserves critical sparse features. The code is available at https://github.com/HeDan-11/LKC-FUNet.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyViM: Frequency Decoupling for Tiny Hybrid Vision Mamba</title>
<link>https://arxiv.org/abs/2411.17473</link>
<guid>https://arxiv.org/abs/2411.17473</guid>
<content:encoded><![CDATA[
arXiv:2411.17473v2 Announce Type: replace 
Abstract: Mamba has shown great potential for computer vision due to its linear complexity in modeling the global context with respect to the input length. However, existing lightweight Mamba-based backbones cannot demonstrate performance that matches Convolution or Transformer-based methods. By observing, we find that simply modifying the scanning path in the image domain is not conducive to fully exploiting the potential of vision Mamba. In this paper, we first perform comprehensive spectral and quantitative analyses, and verify that the Mamba block mainly models low-frequency information under Convolution-Mamba hybrid architecture. Based on the analyses, we introduce a novel Laplace mixer to decouple the features in terms of frequency and input only the low-frequency components into the Mamba block. In addition, considering the redundancy of the features and the different requirements for high-frequency details and low-frequency global information at different stages, we introduce a frequency ramp inception, i.e., gradually reduce the input dimensions of the high-frequency branches, so as to efficiently trade-off the high-frequency and low-frequency components at different layers. By integrating mobile-friendly convolution and efficient Laplace mixer, we build a series of tiny hybrid vision Mamba called TinyViM. The proposed TinyViM achieves impressive performance on several downstream tasks including image classification, semantic segmentation, object detection and instance segmentation. In particular, TinyViM outperforms Convolution, Transformer and Mamba-based models with similar scales, and the throughput is about 2-3 times higher than that of other Mamba-based models. Code is available at https://github.com/xwmaxwma/TinyViM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-ICE: An Inner Interpretable Framework for Image Classification via Bi-directional Interactions between Concept and Input Embeddings</title>
<link>https://arxiv.org/abs/2411.18645</link>
<guid>https://arxiv.org/abs/2411.18645</guid>
<content:encoded><![CDATA[
arXiv:2411.18645v2 Announce Type: replace 
Abstract: Inner interpretability is a promising field aiming to uncover the internal mechanisms of AI systems through scalable, automated methods. While significant research has been conducted on large language models, limited attention has been paid to applying inner interpretability to large-scale image tasks, focusing primarily on architectural and functional levels to visualize learned concepts. In this paper, we first present a conceptual framework that supports inner interpretability and multilevel analysis for large-scale image classification tasks. Specifically, we introduce the Bi-directional Interaction between Concept and Input Embeddings (Bi-ICE) module, which facilitates interpretability across the computational, algorithmic, and implementation levels. This module enhances transparency by generating predictions based on human-understandable concepts, quantifying their contributions, and localizing them within the inputs. Finally, we showcase enhanced transparency in image classification, measuring concept contributions, and pinpointing their locations within the inputs. Our approach highlights algorithmic interpretability by demonstrating the process of concept learning and its convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twisted Convolutional Networks (TCNs): Enhancing Feature Interactions for Non-Spatial Data Classification</title>
<link>https://arxiv.org/abs/2412.00238</link>
<guid>https://arxiv.org/abs/2412.00238</guid>
<content:encoded><![CDATA[
arXiv:2412.00238v2 Announce Type: replace 
Abstract: Twisted Convolutional Networks (TCNs) are proposed as a novel deep learning architecture for classifying one-dimensional data with arbitrary feature order and minimal spatial relationships. Unlike conventional Convolutional Neural Networks (CNNs) that rely on structured feature sequences, TCNs explicitly combine subsets of input features through theoretically grounded multiplicative and pairwise interaction mechanisms to create enriched representations. This feature combination strategy, formalized through polynomial feature expansions, captures high-order feature interactions that traditional convolutional approaches miss. We provide a comprehensive mathematical framework for TCNs, demonstrating how the twisted convolution operation generalizes standard convolutions while maintaining computational tractability. Through extensive experiments on five benchmark datasets from diverse domains (medical diagnostics, political science, synthetic data, chemometrics, and healthcare), we show that TCNs achieve statistically significant improvements over CNNs, Residual Networks (ResNet), Graph Neural Networks (GNNs), DeepSets, and Support Vector Machine (SVM). The performance gains are validated through statistical testing. TCNs also exhibit superior training stability and generalization capabilities, highlighting their robustness for non-spatial data classification tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Object Detectors via Collective Contribution of Pixels</title>
<link>https://arxiv.org/abs/2412.00666</link>
<guid>https://arxiv.org/abs/2412.00666</guid>
<content:encoded><![CDATA[
arXiv:2412.00666v2 Announce Type: replace 
Abstract: Visual explanations for object detectors are crucial for enhancing their reliability. Object detectors identify and localize instances by assessing multiple visual features collectively. When generating explanations, overlooking these collective influences in detections may lead to missing compositional cues or capturing spurious correlations. However, existing methods typically focus solely on individual pixel contributions, neglecting the collective contribution of multiple pixels. To address this limitation, we propose a game-theoretic method based on Shapley values and interactions to explicitly capture both individual and collective pixel contributions. Our method provides explanations for both bounding box localization and class determination, highlighting regions crucial for detection. Extensive experiments demonstrate that the proposed method identifies important regions more accurately than state-of-the-art methods. The code will be publicly available soon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation</title>
<link>https://arxiv.org/abs/2412.01254</link>
<guid>https://arxiv.org/abs/2412.01254</guid>
<content:encoded><![CDATA[
arXiv:2412.01254v3 Announce Type: replace 
Abstract: This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity: (i) fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait; (ii) even coarse-grained expression control can cause facial changes that compromise identity, since they all act on the face. These limitations remain unaddressed by previous generation methods, which primarily rely on coarse control signals or two-stage inference that integrates portrait animation. Here, we introduce EmojiDiff, the first end-to-end solution that enables simultaneous control of extremely detailed expression (RGB-level) and high-fidelity identity in portrait generation. To address the above challenges, EmojiDiff adopts a two-stage scheme involving decoupled training and fine-tuning. For decoupled training, we innovate ID-irrelevant Data Iteration (IDI) to synthesize cross-identity expression pairs by dividing and optimizing the processes of maintaining expression and altering identity, thereby ensuring stable and high-quality data generation. Training the model with this data, we effectively disentangle fine expression features in the expression template from other extraneous information (e.g., identity, skin). Subsequently, we present ID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves rapid reconstruction and joint supervision of identity and expression information, thus aligning identity representations of images with and without expression control. Experimental results demonstrate that our method remarkably outperforms counterparts, achieves precise expression control with highly maintained identity, and generalizes well to various diffusion models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAMCL: Empowering SAM to Continually Learn from Dynamic Domains with Extreme Storage Efficiency</title>
<link>https://arxiv.org/abs/2412.05012</link>
<guid>https://arxiv.org/abs/2412.05012</guid>
<content:encoded><![CDATA[
arXiv:2412.05012v2 Announce Type: replace 
Abstract: Segment Anything Model (SAM) struggles in open-world scenarios with diverse domains. In such settings, naive fine-tuning with a well-designed learning module is inadequate and often causes catastrophic forgetting issue when learning incrementally. To address this issue, we propose a novel continual learning (CL) method for SAM, termed SAMCL. Rather than relying on a fixed learning module, our method decomposes incremental knowledge into separate modules and trains a selector to choose the appropriate one during inference. However, this intuitive design introduces two key challenges: ensuring effective module learning and selection, and managing storage as tasks accumulate. To tackle these, we introduce two components: AugModule and Module Selector. AugModule reduces the storage of the popular LoRA learning module by sharing parameters across layers while maintaining accuracy. It also employs heatmaps-generated from point prompts-to further enhance domain adaptation with minimal additional cost. Module Selector leverages the observation that SAM's embeddings can effectively distinguish domains, enabling high selection accuracy by training on low-consumed embeddings instead of raw images. Experiments show that SAMCL outperforms state-of-the-art methods, achieving only 0.19% forgetting and at least 2.5% gain on unseen domains. Each AugModule requires just 0.233 MB, reducing storage by at least 24.3% over other fine-tuning approaches. The buffer storage for Module Selector is further reduced by up to 256$\times$.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unsupervised Domain Bridging via Image Degradation in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2412.10339</link>
<guid>https://arxiv.org/abs/2412.10339</guid>
<content:encoded><![CDATA[
arXiv:2412.10339v2 Announce Type: replace 
Abstract: Semantic segmentation suffers from significant performance degradation when the trained network is applied to a different domain. To address this issue, unsupervised domain adaptation (UDA) has been extensively studied. Despite the effectiveness of selftraining techniques in UDA, they still overlook the explicit modeling of domain-shared feature extraction. In this paper, we propose DiDA, an unsupervised domain bridging approach for semantic segmentation. DiDA consists of two key modules: (1) Degradation-based Intermediate Domain Construction, which creates continuous intermediate domains through simple image degradation operations to encourage learning domain-invariant features as domain differences gradually diminish; (2) Semantic Shift Compensation, which leverages a diffusion encoder to disentangle and compensate for semantic shift information with degraded timesteps, preserving discriminative representations in the intermediate domains. As a plug-and-play solution, DiDA supports various degradation operations and seamlessly integrates with existing UDA methods. Extensive experiments on multiple domain adaptive semantic segmentation benchmarks demonstrate that DiDA consistently achieves significant performance improvements across all settings. Code is available at https://github.com/Woof6/DiDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Interpretability for Adversarial Robustness: A Hybrid Generative Classification Approach</title>
<link>https://arxiv.org/abs/2412.20025</link>
<guid>https://arxiv.org/abs/2412.20025</guid>
<content:encoded><![CDATA[
arXiv:2412.20025v2 Announce Type: replace 
Abstract: Deep learning-based discriminative classifiers, despite their remarkable success, remain vulnerable to adversarial examples that can mislead model predictions. While adversarial training can enhance robustness, it fails to address the intrinsic vulnerability stemming from the opaque nature of these black-box models. We present a deep ensemble model that combines discriminative features with generative models to achieve both high accuracy and adversarial robustness. Our approach integrates a bottom-level pre-trained discriminative network for feature extraction with a top-level generative classification network that models adversarial input distributions through a deep latent variable model. Using variational Bayes, our model achieves superior robustness against white-box adversarial attacks without adversarial training. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate our model's superior adversarial robustness. Through evaluations using counterfactual metrics and feature interaction-based metrics, we establish correlations between model interpretability and adversarial robustness. Additionally, preliminary results on Tiny-ImageNet validate our approach's scalability to more complex datasets, offering a practical solution for developing robust image classification models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering</title>
<link>https://arxiv.org/abs/2501.01371</link>
<guid>https://arxiv.org/abs/2501.01371</guid>
<content:encoded><![CDATA[
arXiv:2501.01371v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) demonstrate remarkable capabilities in visual understanding and reasoning, such as in Visual Question Answering (VQA), where the model is asked a question related to a visual input. Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. CLIP-UP leverages CLIP-based similarity measures to extract question-image alignment information to detect unanswerability, requiring efficient training of only a few additional layers, while keeping the original VLMs' weights unchanged. Tested across several models, CLIP-UP achieves significant improvements on benchmarks assessing unanswerability in both multiple-choice and open-ended VQA, surpassing other methods, while preserving original performance on other tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expectation-Maximization as the Engine of Scalable Medical Intelligence</title>
<link>https://arxiv.org/abs/2501.03410</link>
<guid>https://arxiv.org/abs/2501.03410</guid>
<content:encoded><![CDATA[
arXiv:2501.03410v2 Announce Type: replace 
Abstract: Large, high-quality, annotated datasets are the foundation of medical AI research, but constructing even a small, moderate-quality, annotated dataset can take years of effort from multidisciplinary teams. Although active learning can prioritize what to annotate, scaling up still requires extensive manual efforts to revise the noisy annotations. We formulate this as a missing-data problem and develop ScaleMAI, a framework that unifies data annotation and model development co-evolution through an Expectation-Maximization (EM) process. In this iterative process, the AI model automatically identifies and corrects the mistakes in annotations (Expectation), while the refined annotated data retrain the model to improve accuracy (Maximization). In addition to the classical EM algorithm, ScaleMAI brings human experts into the loop to review annotations that cannot be adequately addressed by either Expectation or Maximization step (<5%). As a result, ScaleMAI progressively creates an annotated dataset of 47,315 CT scans (4.8x larger than the largest public dataset, PanTS) including 4,163,720 per-voxel annotations for benign/malignant tumors and 88 anatomical structures. ScaleMAI iteratively trains a model that exceeds human expert performance in tumor diagnosis (+7%), and outperforms models developed from smaller, moderate-quality datasets, with statistically significant gains in tumor detection (+10%) and segmentation (+14%) on two prestigious benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</title>
<link>https://arxiv.org/abs/2501.17792</link>
<guid>https://arxiv.org/abs/2501.17792</guid>
<content:encoded><![CDATA[
arXiv:2501.17792v3 Announce Type: replace 
Abstract: We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through the.se experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</title>
<link>https://arxiv.org/abs/2502.00843</link>
<guid>https://arxiv.org/abs/2502.00843</guid>
<content:encoded><![CDATA[
arXiv:2502.00843v2 Announce Type: replace 
Abstract: In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a "teacher" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from 20.11% to 35.16% across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPFFNet: Strip Perception and Feature Fusion Spatial Pyramid Pooling for Fabric Defect Detection</title>
<link>https://arxiv.org/abs/2502.01445</link>
<guid>https://arxiv.org/abs/2502.01445</guid>
<content:encoded><![CDATA[
arXiv:2502.01445v3 Announce Type: replace 
Abstract: Defect detection in fabrics is critical for quality control, yet existing methods often struggle with complex backgrounds and shape-specific defects. In this paper, we propose an improved fabric defect detection model based on YOLOv11. To enhance the detection of strip defects, we introduce a Strip Perception Module (SPM) that improves feature capture through multi-scale convolution. We further enhance the spatial pyramid pooling fast (SPPF) by integrating a squeeze-and-excitation mechanism, resulting in the SE-SPPF module, which better integrates spatial and channel information for more effective defect feature extraction. Additionally, we propose a novel focal enhanced complete intersection over union (FECIoU) metric with adaptive weights, addressing scale differences and class imbalance by adjusting the weights of hard-to-detect instances through focal loss. Experimental results demonstrate that our model achieves a 0.8-8.1% improvement in mean average precision (mAP) on the Tianchi dataset and a 1.6-13.2% improvement on our custom dataset, outperforming other state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation</title>
<link>https://arxiv.org/abs/2502.09274</link>
<guid>https://arxiv.org/abs/2502.09274</guid>
<content:encoded><![CDATA[
arXiv:2502.09274v2 Announce Type: replace 
Abstract: 3D scene understanding is a critical yet challenging task in autonomous driving due to the irregularity and sparsity of LiDAR data, as well as the computational demands of processing large-scale point clouds. Recent methods leverage range-view representations to enhance efficiency, but they often adopt higher azimuth resolutions to mitigate information loss during spherical projection, where only the closest point is retained for each 2D grid. However, processing wide panoramic range-view images remains inefficient and may introduce additional distortions. Our empirical analysis shows that training with multiple range images, obtained from splitting the full point cloud, improves both segmentation accuracy and computational efficiency. However, this approach also poses new challenges of exacerbated class imbalance and increase in projection artifacts. To address these, we introduce FLARES, a novel training paradigm that incorporates two tailored data augmentation techniques and a specialized post-processing method designed for multi-range settings. Extensive experiments demonstrate that FLARES is highly generalizable across different architectures, yielding 2.1%~7.9% mIoU improvements on SemanticKITTI and 1.8%~3.9% mIoU on nuScenes, while delivering over 40% speed-up in inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v4 Announce Type: replace 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D Large Multimodal Models (LMMs), yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D LMMs. These long-standing challenges include the failure to adapt to varying point cloud resolutions during inference and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the pre-trained encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the state-of-the-art model, PointLLM-PiSA-13B, achieving 57.91%, 61.0%, and 55.20% on the classification, captioning, and VQA tasks, respectively. Our results show that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thicker and Quicker: A Jumbo Token for Fast Plain Vision Transformers</title>
<link>https://arxiv.org/abs/2502.15021</link>
<guid>https://arxiv.org/abs/2502.15021</guid>
<content:encoded><![CDATA[
arXiv:2502.15021v3 Announce Type: replace 
Abstract: ViTs are general and accurate, and address many tasks, but ViTs are slow, and are not always practical when efficiency is key. Existing methods for faster ViTs design hybrid non-ViT architectures, losing generality, or shrink their tokens, sacrificing accuracy. While many non-ViT architectures are both fast and accurate, they cannot flexibly process other input shapes, pre-train by SOTA self-supervised learning, reduce computation by dropping tokens, and more like ViTs can. We make ViTs faster by reducing patch token width while increasing global token width by adding a new Jumbo token. Our wider Jumbo token is processed by its own wider FFN to increase model capacity. Yet our Jumbo FFN is efficient: it processes a single token, for speed, and its parameters are shared across all layers, for memory. Crucially, our Jumbo is attention-only and non-hierarchical, like a plain ViT, so it is simple, scalable, flexible, and compatible with ViT methods new and old. Jumbo improves over ViT baselines with Registers from Nano to Large scales while maintaining speed/throughput on ImageNet-1K (0.1-13%). Jumbo also improves MAE pre-training (4.9% linear probing on ImageNet-1K), test-time adaptation (5.2% on ImageNet-C), and time series modeling. Our Jumbo models even achieve better speed-accuracy trade-offs than specialized non-ViT compute-efficient models, while maintaining plain-ViT compatibility for practicality. Code and weights available: https://github.com/antofuller/jumbo
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Industrial Anomalies Synthesis</title>
<link>https://arxiv.org/abs/2502.16412</link>
<guid>https://arxiv.org/abs/2502.16412</guid>
<content:encoded><![CDATA[
arXiv:2502.16412v2 Announce Type: replace 
Abstract: This paper comprehensively reviews anomaly synthesis methodologies. Existing surveys focus on limited techniques, missing an overall field view and understanding method interconnections. In contrast, our study offers a unified review, covering about 40 representative methods across Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based synthesis. We introduce the first industrial anomaly synthesis (IAS) taxonomy. Prior works lack formal classification or use simplistic taxonomies, hampering structured comparisons and trend identification. Our taxonomy provides a fine-grained framework reflecting methodological progress and practical implications, grounding future research. Furthermore, we explore cross-modality synthesis and large-scale VLM. Previous surveys overlooked multimodal data and VLM in anomaly synthesis, limiting insights into their advantages. Our survey analyzes their integration, benefits, challenges, and prospects, offering a roadmap to boost IAS with multimodal learning. More resources are available at https://github.com/M-3LAB/awesome-anomaly-synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2503.06983</link>
<guid>https://arxiv.org/abs/2503.06983</guid>
<content:encoded><![CDATA[
arXiv:2503.06983v2 Announce Type: replace 
Abstract: While cooperative perception can overcome the limitations of single-vehicle systems, the practical implementation of vehicle-to-vehicle and vehicle-to-infrastructure systems is often impeded by significant economic barriers. Aerial-ground cooperation (AGC), which pairs ground vehicles with drones, presents a more economically viable and rapidly deployable alternative. However, this emerging field has been held back by a critical lack of high-quality public datasets and benchmarks. To bridge this gap, we present \textit{Griffin}, a comprehensive AGC 3D perception dataset, featuring over 250 dynamic scenes (37k+ frames). It incorporates varied drone altitudes (20-60m), diverse weather conditions, realistic drone dynamics via CARLA-AirSim co-simulation, and critical occlusion-aware 3D annotations. Accompanying the dataset is a unified benchmarking framework for cooperative detection and tracking, with protocols to evaluate communication efficiency, altitude adaptability, and robustness to communication latency, data loss and localization noise. By experiments through different cooperative paradigms, we demonstrate the effectiveness and limitations of current methods and provide crucial insights for future research. The dataset and codes are available at https://github.com/wang-jh18-SVM/Griffin.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>