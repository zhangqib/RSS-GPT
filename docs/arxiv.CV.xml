<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>What Happens When: Learning Temporal Orders of Events in Videos</title>
<link>https://arxiv.org/abs/2512.08979</link>
<guid>https://arxiv.org/abs/2512.08979</guid>
<content:encoded><![CDATA[
<div> Keywords: VLMMs, temporal order, VECTOR benchmark, MECOT, chain-of-thought  

<br /><br />Summary:  
Video Large Multimodal Models (VLMMs) have demonstrated strong video understanding capabilities but struggle to accurately capture the temporal order of multiple events. Through comprehensive experiments, the authors found that VLMMs often perform well even when video frames are scrambled, suggesting these models rely on prior knowledge of typical scenarios rather than genuine sequential processing. To explicitly measure temporal understanding, the paper introduces VECTOR, a new benchmark designed to test a model's ability to identify the correct temporal order of events. Various VLMMs tested on VECTOR frequently fail to properly understand event sequences, highlighting a gap in current models. To overcome this, the authors propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which enhances temporal awareness by training models on detailed, event-by-event video descriptions and employing chain-of-thought prompts during inference. MECOT achieves superior performance on VECTOR and also improves results on existing video benchmarks, demonstrating its effectiveness in fostering temporal understanding. The authors provide their code, model, and datasets publicly, enabling further research and development in temporal video understanding. <div>
arXiv:2512.08979v1 Announce Type: new 
Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multi-Image Vision Agents via End2End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08980</link>
<guid>https://arxiv.org/abs/2512.08980</guid>
<content:encoded><![CDATA[
<div> Multi-image QA, Vision-language model, Reinforcement learning, Tool-use, Visual reflection<br /><br />Summary: The paper introduces IMAgent, an open-source vision-language agent designed to handle complex multi-image question answering (QA) tasks, overcoming the limitation of most existing VLM-based agents that only process single images. IMAgent is trained end-to-end via reinforcement learning within a multi-agent framework that generates challenging and visually-rich multi-image QA pairs. To support training and evaluation, the authors present MIFG-QA, a manually verified dataset containing 10,000 samples focused on multi-image reasoning. Recognizing that deeper reasoning steps often cause models to neglect visual inputs, the authors develop two specialized tools for visual reflection and confirmation, enhancing the agent’s ability to reallocate attention to image content during inference. A novel action-trajectory two-level mask strategy is employed to stabilize tool use through pure reinforcement learning without reliance on costly supervised fine-tuning data. Extensive experiments show that IMAgent not only retains strong performance on standard single-image benchmarks but also demonstrates significant improvements on the multi-image QA task with MIFG-QA. The authors plan to release both the codes and dataset to benefit the wider research community, providing actionable insights into developing more effective multi-image vision-language agents. <div>
arXiv:2512.08980v1 Announce Type: new 
Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding</title>
<link>https://arxiv.org/abs/2512.08981</link>
<guid>https://arxiv.org/abs/2512.08981</guid>
<content:encoded><![CDATA[
<div> Face recognition, demographic bias, unified text-image embedding, vision-language models, fair verification<br /><br />Summary:<br /><br />This paper focuses on addressing demographic bias in face recognition (FR) systems, which often arises from the entanglement of demographic-specific information with identity features in facial embeddings. Such biases are particularly problematic in multicultural urban environments where biometric systems are widely used. To mitigate this, the authors propose a novel method called Unified Text-Image Embedding (UTIE) that induces demographic ambiguity by enriching face embeddings with textual demographic features from other groups. This enrichment encourages the embeddings to prioritize identity-relevant information and reduce demographic influence. UTIE leverages the zero-shot learning and cross-modal semantic alignment capabilities of Vision-Language Models (VLMs), specifically using CLIP, OpenCLIP, and SigLIP in their experiments. The method was evaluated on two standard benchmarks for bias assessment in face recognition, RFW and BFW. Results demonstrate that UTIE consistently reduces bias metrics across demographic groups while maintaining or even improving face verification accuracy. This approach thus offers a promising direction to promote fairer FR systems without compromising performance. <div>
arXiv:2512.08981v1 Announce Type: new 
Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement</title>
<link>https://arxiv.org/abs/2512.08982</link>
<guid>https://arxiv.org/abs/2512.08982</guid>
<content:encoded><![CDATA[
<div> Diffusion Models, Low-Light Enhancement, Consistency Models, Retinex Decomposition, One-Step Sampling  

<br /><br />Summary:  
This paper introduces Consist-Retinex, the first framework to adapt consistency modeling for Retinex-based low-light image enhancement, addressing the inefficiency of diffusion models requiring hundreds of sampling steps. Unlike unconditional synthesis, conditional enhancement demands different training dynamics since it relies on large-noise regimes connecting degraded inputs to enhanced outputs. The authors propose two main innovations: a dual-objective consistency loss combining temporal consistency and ground-truth alignment with randomized time sampling to ensure full-spectrum supervision and stable training convergence; and an adaptive noise-emphasized sampling strategy that focuses training on large-noise regions critical for effective one-step conditional enhancement. Evaluated on the VE-LOL-L dataset, Consist-Retinex achieves state-of-the-art results, attaining a PSNR of 25.51 and FID of 44.73, outperforming the previous Diff-Retinex++ model which scored 23.41 PSNR and 49.59 FID, all using only a single sampling step. Additionally, the training cost is significantly reduced, requiring just one-eighth of the budget compared to the traditional 1000-step Diff-Retinex baseline, demonstrating both efficiency and performance improvements in low-light image enhancement tasks. <div>
arXiv:2512.08982v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification</title>
<link>https://arxiv.org/abs/2512.08983</link>
<guid>https://arxiv.org/abs/2512.08983</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV identification, Radio Frequency Fingerprint Identification, model pruning, spectral clustering, deep learning  

<br /><br />Summary: With the growing use of Unmanned Aerial Vehicles (UAVs) and increasing complexity in low-altitude security threats, traditional UAV identification methods face challenges in extracting reliable signal features and achieving real-time performance. Deep learning-based Radio Frequency Fingerprint Identification (RFFI) methods have enhanced recognition accuracy but suffer from large model sizes and high computational requirements, complicating deployment on resource-constrained edge devices. To address this, the paper proposes HSCP (Hierarchical Spectral Clustering Pruning), a novel framework integrating layer and channel pruning to attain significant model compression, improved performance, and efficient inference. HSCP first applies spectral clustering guided by Centered Kernel Alignment (CKA) to remove redundant layers, followed by channel dimension pruning for fine-grained redundancy elimination. A noise-robust fine-tuning strategy is employed to maintain model robustness. Experiments on the UAV-M100 benchmark demonstrate HSCP’s effectiveness, achieving 86.39% parameter reduction and 84.44% FLOPs reduction on ResNet18, while improving accuracy by 1.49% over the unpruned baseline. Furthermore, HSCP maintains superior robustness in environments with low signal-to-noise ratios, confirming its practical utility for UAV identification in complex and resource-limited scenarios. <div>
arXiv:2512.08983v1 Announce Type: new 
Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.08984</link>
<guid>https://arxiv.org/abs/2512.08984</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Retrieval-Augmented Framework, Large Language Models, Prompt Optimization, Unseen Activity Recognition  

<br /><br />Summary:  
Human Activity Recognition (HAR) is crucial for applications such as healthcare, rehabilitation, fitness tracking, and smart environments. Traditional deep learning methods require extensive dataset-specific training, large labeled datasets, and high computational costs. The paper introduces RAG-HAR, a novel training-free retrieval-augmented framework that utilizes large language models (LLMs) for HAR. RAG-HAR works by computing lightweight statistical descriptors from input data and retrieving semantically similar samples from a pre-built vector database to provide contextual evidence for LLM-based activity identification. To enhance its performance, the authors integrate prompt optimization and develop an LLM-based activity descriptor that enriches the vector database with context-aware information. RAG-HAR achieves state-of-the-art results across six diverse HAR benchmarks without any training or fine-tuning of models, highlighting its robustness and practicality. Importantly, this approach extends beyond recognizing previously known activities, enabling the detection and meaningful labeling of multiple unseen human activities. As a result, RAG-HAR offers a scalable and efficient solution for accurate activity recognition without the traditional heavy training requirements. <div>
arXiv:2512.08984v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Test-Time Scaling Approach for Image Generation</title>
<link>https://arxiv.org/abs/2512.08985</link>
<guid>https://arxiv.org/abs/2512.08985</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, test-time compute, diffusion models, compute allocation, Verifier-Threshold method

<br /><br />Summary:  
1. The paper addresses the challenge of improving efficiency in image generation by optimizing the allocation of test-time computational resources during inference.  
2. It focuses on diffusion and flow models where searching across noise samples demands significant compute, and existing methods allocate compute budgets non-uniformly but rely on greedy algorithms that are inefficient.  
3. The authors propose a novel approach called the Verifier-Threshold method, which dynamically reallocates computational effort across denoising steps to improve inference efficiency.  
4. This new method shows substantial gains, achieving a 2-4 times reduction in computational time compared to the current state-of-the-art methods when evaluated on the GenEval benchmark, without sacrificing performance.  
5. The proposed solution advances the practicality of large generative image models by enabling faster and more resource-efficient inference, benefiting real-world applications requiring high-quality image synthesis. <div>
arXiv:2512.08985v1 Announce Type: new 
Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy</title>
<link>https://arxiv.org/abs/2512.08986</link>
<guid>https://arxiv.org/abs/2512.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: Diabetic Retinopathy, quality control, image annotation, explainable classifier, contrastive learning<br /><br />Summary:<br /><br />Diabetic Retinopathy (DR) is a severe complication affecting individuals with long-term diabetes, potentially leading to vision loss if not diagnosed early. Fundus photography is a critical tool for capturing retinal structures and identifying abnormalities corresponding to various stages of DR. Artificial Intelligence (AI) can aid clinicians in detecting lesions from these images, thereby reducing the manual workload. However, training effective AI models demands high-quality annotated datasets, which are challenging to obtain due to the complexity of retinal structures and possible errors during image acquisition and manual annotation. To address these challenges, the authors propose a quality-control framework that guarantees only high-quality data is used for AI training and evaluation. First, the framework deploys an explainable feature-based classifier to filter out inadequate images, employing features extracted through both traditional image processing techniques and advanced contrastive learning. Subsequently, enhanced images are presented for annotation with the assistance of deep learning tools to improve accuracy and efficiency. Finally, the framework calculates annotator agreement using derived formulas to evaluate the usability of the annotations. This systematic approach ensures improved reliability and effectiveness in AI models for DR diagnosis. <div>
arXiv:2512.08986v1 Announce Type: new 
Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization</title>
<link>https://arxiv.org/abs/2512.08987</link>
<guid>https://arxiv.org/abs/2512.08987</guid>
<content:encoded><![CDATA[
<div> 3D inverse design, latent representation, physics-aware optimization, gradient-guided diffusion, topology-preserving refinement<br /><br />Summary:<br /><br />The paper addresses the challenge of inverse design in 3D domains, where the design space is vast and traditional exhaustive search methods are impractical. Recent deep learning approaches have helped by introducing generative priors and surrogate models, but they often rely on 2D projections or modification of existing 3D shapes, limiting volumetric detail and design freedom. To overcome these limitations, the authors propose a novel 3D Inverse Design (3DID) framework that directly explores the 3D design space from scratch. The key innovation involves learning a unified latent representation that jointly encodes both geometric shape and related physical field data compactly. The optimization process occurs in two stages: first, a gradient-guided diffusion sampler globally explores the continuous latent manifold; second, an objective-driven, topology-preserving refinement step fine-tunes the candidates, sculpting them towards the desired physical or functional targets. This combination enables generation of high-fidelity 3D geometries with better solution quality and greater design versatility compared to existing methods. The framework represents a significant advance in enabling fully 3D, physics-aware design optimization, opening pathways to more complex and detailed inverse design tasks in engineering and scientific applications. <div>
arXiv:2512.08987v1 Announce Type: new 
Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration</title>
<link>https://arxiv.org/abs/2512.08989</link>
<guid>https://arxiv.org/abs/2512.08989</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral image classification, knowledge transfer, spectral variations, semantic inconsistencies, cross-scene integration<br /><br />Summary:  
This paper addresses the challenges of knowledge transfer in hyperspectral image (HSI) classification across different domains, focusing on two main issues: spectral variations caused by different sensors and semantic inconsistencies present in heterogeneous scenes. Existing approaches generally assume either homogeneous domains or heterogeneous ones with overlapping label spaces, limiting their effectiveness when label spaces do not coincide and neglecting target-private information. To overcome these limitations, the authors propose a novel Cross-scene Knowledge Integration (CKI) framework designed for fully heterogeneous transfer settings. CKI consists of three core components: (1) Alignment of Spectral Characteristics (ASC), which mitigates spectral discrepancies through a domain-agnostic projection technique; (2) Cross-scene Knowledge Sharing Preference (CKSP), incorporating a Source Similarity Mechanism (SSM) to resolve semantic mismatches between source and target domains; and (3) Complementary Information Integration (CII), which enhances performance by leveraging target-specific complementary cues. Extensive experiments demonstrate that CKI achieves state-of-the-art performance and exhibits robust stability across a variety of challenging cross-scene HSI scenarios, validating its effectiveness in enabling more generalized and accurate HSI classification under diverse and fully heterogeneous domain conditions. <div>
arXiv:2512.08989v1 Announce Type: new 
Abstract: Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic World Models for Verification of Closed-loop Vision-based Systems</title>
<link>https://arxiv.org/abs/2512.08991</link>
<guid>https://arxiv.org/abs/2512.08991</guid>
<content:encoded><![CDATA[
<div> Deterministic World Model, vision-based control, generative models, reachability analysis, conformal prediction<br /><br />Summary:<br /><br />1. The paper addresses the challenge of verifying closed-loop vision-based control systems, which is difficult due to image high dimensionality and complex visual environment modeling. <br />2. Conventional generative models used as camera surrogates introduce overapproximation errors caused by stochastic latent variables, limiting verification precision. <br />3. To overcome this, the authors propose a Deterministic World Model (DWM) that directly maps system states to images without latent variables, enabling precise input bounds. <br />4. The DWM is trained using a dual-objective loss combining pixel-level reconstruction accuracy with a control difference loss, ensuring that the model's behavior matches the real system. <br />5. Integrating DWM into a verification pipeline combined with Star-based reachability analysis (StarV) and conformal prediction yields rigorous statistical bounds on errors between the model and actual vision-based system trajectories. <br />6. Experimental results on benchmarks demonstrate that this approach produces significantly tighter reachable sets and improved verification performance compared to methods using latent-variable generative models. <div>
arXiv:2512.08991v1 Announce Type: new 
Abstract: Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Generative AI helps Radiotherapy Planning with User Preference</title>
<link>https://arxiv.org/abs/2512.08996</link>
<guid>https://arxiv.org/abs/2512.08996</guid>
<content:encoded><![CDATA[
<div> Keywords: radiotherapy planning, 3D dose prediction, generative model, user-defined preferences, treatment planning systems  

<br /><br />Summary:  
This article addresses the complexity and variability in radiotherapy planning across different institutions and planners. It highlights how current deep learning models for 3D dose prediction often depend on reference plans as ground truth, which can introduce biases reflecting specific planning styles or institutional preferences. To overcome this limitation, the authors propose a novel generative model that predicts 3D dose distributions based purely on customizable user-defined preference flavors. These preferences allow planners to emphasize particular trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), enabling more personalized and flexible treatment plans tailored to individual patient needs. The model is designed for easy integration into existing clinical treatment planning systems, supporting efficient generation of high-quality radiotherapy plans. Comparative evaluation results indicate that, in some scenarios, this approach can outperform the Varian RapidPlan model regarding both adaptability to user preferences and overall plan quality. This work represents a significant step towards more personalized, flexible, and potentially higher-quality radiotherapy treatment planning using machine learning techniques. <div>
arXiv:2512.08996v1 Announce Type: new 
Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction</title>
<link>https://arxiv.org/abs/2512.08999</link>
<guid>https://arxiv.org/abs/2512.08999</guid>
<content:encoded><![CDATA[
<div> Keywords: Computed Tomography, Metal Artifact Reduction, Diffusion Model, Implicit Neural Representation, Data Fidelity<br /><br />Summary:<br /><br />Computed tomography (CT) images often suffer from severe artifacts caused by the presence of metals, which degrade image quality and diagnostic accuracy. Existing supervised metal artifact reduction (MAR) methods rely heavily on paired metal-clean training data, leading to unstable performance and limited clinical usefulness. Unsupervised methods addressing MAR face two main challenges: firstly, they fail to effectively integrate CT physical geometry into the reduction process, risking lack of data fidelity; secondly, traditional heuristic regularization techniques do not fully leverage the rich prior knowledge available in the data. To address these issues, the authors propose a framework combining implicit neural representation with a diffusion model to regularize the MAR process. The implicit neural representation enforces physical constraints and ensures data fidelity by embedding the CT geometry. Meanwhile, the pre-trained diffusion model introduces powerful prior knowledge to guide artifact reduction. Experiments on both simulated and real clinical CT datasets show the proposed method improves artifact removal quality and generalizes well across data types. The results suggest that this novel approach has strong potential for practical clinical deployment, offering a more stable and informed solution to metal artifact reduction in CT imaging. <div>
arXiv:2512.08999v1 Announce Type: new 
Abstract: Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography</title>
<link>https://arxiv.org/abs/2512.09001</link>
<guid>https://arxiv.org/abs/2512.09001</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, lithography defects, dataset generation, pixel-level annotation, Mask R-CNN<br /><br />Summary:<br />1. The research addresses the critical limitation in AI applications for micro/nano manufacturing caused by the lack of accessible, high-quality, and physically grounded training data for lithography defect inspection.<br />2. A novel methodology is proposed for generating large-scale and physically valid defect datasets with pixel-accurate segmentation masks, facilitating precise defect delineation.<br />3. The dataset generation framework synthesizes defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to original design layouts.<br />4. The synthesized layouts and their defect-free counterparts are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography, followed by optical micrograph imaging.<br />5. By comparing defect sample images with defect-free references, consistent pixel-level annotations for four defect classes (bridge, burr, pinch, contamination) are produced.<br />6. The resulting dataset comprises 3,530 optical micrographs with 13,365 annotated defect instances, each with full contour and geometry preserved.<br />7. Evaluation using segmentation Mask R-CNN demonstrates superior detection performance (AP@0.5 near or above 0.96) compared to Faster R-CNN, yielding about 34% mean average precision improvement for bridge, burr, and pinch, and approximately 42% improvement for contamination.<br />8. This methodology successfully enables robust, AI-driven measurement and inspection workflows in semiconductor fabrication through physically valid, pixel-level annotated lithography defect datasets. <div>
arXiv:2512.09001v1 Announce Type: new 
Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques</title>
<link>https://arxiv.org/abs/2512.09005</link>
<guid>https://arxiv.org/abs/2512.09005</guid>
<content:encoded><![CDATA[
<div> Keywords: body motion, face motion, generative modeling, multi-modal learning, motion generation<br /><br />Summary:<br /><br />This survey addresses the generation of body and face motion, highlighting their critical role in human communication by conveying verbal and non-verbal cues as well as personality traits. It reviews fundamental concepts and representation techniques essential for modeling motion dynamics effectively. The paper discusses contemporary generative approaches and how multi-modal learning frameworks incorporate diverse signals such as speech, conversational context, and visual inputs to produce motion. A variety of datasets used in the domain are examined to understand their contributions and limitations. Evaluation metrics for assessing the quality, coherence, and expressiveness of generated motions are also surveyed. Importantly, this work is noted as the first comprehensive review that jointly covers both body and face motion generation, emphasizing their interplay in dyadic interactions. The survey concludes by outlining future research directions aimed at enhancing the realism, expressiveness, and coherence of avatar motion in interactive settings. To support further study, the authors provide an extensive list of related resources and materials, available online at their dedicated webpage. <div>
arXiv:2512.09005v1 Announce Type: new 
Abstract: Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Lossless Ultimate Vision Token Compression for VLMs</title>
<link>https://arxiv.org/abs/2512.09010</link>
<guid>https://arxiv.org/abs/2512.09010</guid>
<content:encoded><![CDATA[
<div> Keywords: visual language models, token compression, iterative merging, spectrum pruning, FlashAttention  

<br /><br />Summary:  
This article addresses computational efficiency and latency challenges in visual language models (VLMs) caused by redundancy in token representations of high-resolution images and videos. Existing attention or similarity-based compression algorithms suffer from position bias and class imbalance, leading to accuracy loss, and perform poorly on shallow large language model (LLM) layers that have weaker cross-modal interactions. To overcome these issues, the authors propose extending token compression into the visual encoder using an iterative merging scheme orthogonal in spatial axes, which accelerates computation across the entire VLM. Additionally, they introduce a spectrum pruning unit within the LLM that utilizes an attention/similarity-free low-pass filter to gradually prune redundant visual tokens, fully compatible with modern FlashAttention techniques. Combining these methods, the paper presents the Lossless Ultimate Vision tokens Compression (LUVC) framework, which systematically compresses visual tokens until their complete elimination at the final LLM layer. This enables the progressive fusion of high-dimensional visual features into multimodal queries. Experiments demonstrate that LUVC achieves a 2× speedup in inference time with negligible accuracy degradation, and its training-free design allows for immediate deployment across multiple visual language models. <div>
arXiv:2512.09010v1 Announce Type: new 
Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Approach for Detection of Entities in Dynamic Media Contents</title>
<link>https://arxiv.org/abs/2512.09011</link>
<guid>https://arxiv.org/abs/2512.09011</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, character detection, video sequence, supervised learning, national security<br /><br />Summary: This paper presents a novel approach to detecting specific entities, particularly characters, in video sequences using deep learning techniques based on artificial neural networks. The study addresses the complexity involved in identifying target individuals within videos due to the presence of multiple objects in the data. The proposed method leverages supervised learning algorithms structured to utilize simple features of the target character, achieving significant successes compared to existing state-of-the-art techniques. Results demonstrate that this approach efficiently locates desired individuals from both private and public image databases. The authors emphasize the practical application of their classifier in the context of Angola, where it has the potential to enhance national security systems. Specifically, it can be used to match database records of persons of interest—such as missing individuals or criminals—with video data from the Integrated Public Security Centre (CISP). This fusion of video analysis and deep learning could provide strong support for law enforcement and public safety efforts, marking a valuable advancement in computer vision applications for security. <div>
arXiv:2512.09011v1 Announce Type: new 
Abstract: The notion of learning underlies almost every evolution of Intelligent Agents. In this paper, we present an approach for searching and detecting a given entity in a video sequence. Specifically, we study how the deep learning technique by artificial neuralnetworks allows us to detect a character in a video sequence. The technique of detecting a character in a video is a complex field of study, considering the multitude of objects present in the data under analysis. From the results obtained, we highlight the following, compared to state of the art: In our approach, within the field of Computer Vision, the structuring of supervised learning algorithms allowed us to achieve several successes from simple characteristics of the target character. Our results demonstrate that is new approach allows us to locate, in an efficient way, wanted individuals from a private or public image base. For the case of Angola, the classifier we propose opens the possibility of reinforcing the national security system based on the database of target individuals (disappeared, criminals, etc.) and the video sequences of the Integrated Public Security Centre (CISP).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Remove Lens Flare in Event Camera</title>
<link>https://arxiv.org/abs/2512.09016</link>
<guid>https://arxiv.org/abs/2512.09016</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, lens flare, E-Deflare, restoration, benchmark<br /><br />Summary:<br /><br />1. Event cameras offer advantages like high temporal resolution and dynamic range but suffer from lens flare, an optical artifact causing significant image degradation in event data.<br /><br />2. Lens flare in event streams produces a complex spatio-temporal distortion that has not been adequately addressed in prior research.<br /><br />3. The authors introduce E-Deflare, the first comprehensive framework designed specifically to remove lens flare from event camera outputs.<br /><br />4. They derive a physics-based forward model explaining the nonlinear suppression mechanisms involved in lens flare, forming the theoretical foundation of their approach.<br /><br />5. Leveraging this foundation, the team creates the E-Deflare Benchmark, which includes a large-scale simulated training dataset called E-Flare-2.7K and the first paired real-world test set named E-Flare-R, collected using a novel optical system.<br /><br />6. Using these resources, they develop E-DeflareNet, a deep learning network that delivers state-of-the-art performance in restoring flare-corrupted event camera data.<br /><br />7. Extensive experiments confirm the effectiveness of E-DeflareNet and demonstrate significant improvements in subsequent vision tasks.<br /><br />8. The authors have made their code and datasets publicly accessible to facilitate further research and applications in this domain. <div>
arXiv:2512.09016v1 Announce Type: new 
Abstract: Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors</title>
<link>https://arxiv.org/abs/2512.09056</link>
<guid>https://arxiv.org/abs/2512.09056</guid>
<content:encoded><![CDATA[
<div> Keywords: Object pose estimation, vision-language model, zero-shot, 3D concept maps, 6DoF relative pose<br /><br />Summary:<br /><br />1. Object pose estimation is a crucial task in computer vision and robotics, but traditional methods typically require extensive training on specific datasets.  
2. ConceptPose is introduced as a novel framework that is both training-free and model-free, leveraging the zero-shot capabilities of large-scale vision-language models (VLMs).  
3. The framework creates open-vocabulary 3D concept maps by tagging each 3D point with a concept vector derived from saliency maps generated by the VLM.  
4. By establishing robust 3D-to-3D correspondences across these concept maps, ConceptPose enables precise estimation of the 6DoF (six degrees of freedom) relative pose between objects.  
5. Without any object-specific or dataset-specific training, ConceptPose surpasses existing methods on zero-shot relative pose estimation benchmarks, achieving state-of-the-art accuracy with over 62% improvement in ADD(-S) score compared to prior approaches, including those that require extensive training. <div>
arXiv:2512.09056v1 Announce Type: new 
Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding</title>
<link>https://arxiv.org/abs/2512.09062</link>
<guid>https://arxiv.org/abs/2512.09062</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, construction site, 3D perception, dataset, segmentation<br /><br />Summary:  
This paper introduces SIP (Site in Pieces), a novel dataset designed specifically for 3D scene interpretation in active construction environments. Unlike existing datasets that rely on densely fused LiDAR scans with uniform sampling and complete visibility, SIP captures the practical limitations of real-world construction site data acquisition, including isolated single-station LiDAR views, radial density decay, and fragmented geometry. The dataset encompasses both indoor and outdoor scenes, annotated at the point level using a taxonomy tailored to construction: Built Environment, Construction Operations, and Site Surroundings. SIP includes structural elements and temporary, slender objects such as scaffolding, MEP piping, and scissor lifts, which are challenging to segment due to occlusions and sparsity. The creators detail the scanning protocol, annotation workflow, and quality control processes to ensure consistency and reliability. SIP is openly accessible and supported by a Git repository offering adaptable class configurations, facilitating integration with modern 3D deep learning frameworks. By providing data that reflect realistic sensing conditions and material complexities in construction sites, SIP aims to enable more robust benchmarking and advance the development of construction-oriented 3D computer vision tasks such as progress monitoring, safety assessment, and digital twin creation. <div>
arXiv:2512.09062v1 Announce Type: new 
Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</title>
<link>https://arxiv.org/abs/2512.09069</link>
<guid>https://arxiv.org/abs/2512.09069</guid>
<content:encoded><![CDATA[
<div> Keywords: Age-related macular degeneration, Optical coherence tomography, Knowledge distillation, EfficientNet-B2, Deep learning compression<br /><br />Summary:<br /><br />1. Age-related macular degeneration (AMD) and choroidal neovascularization (CNV) are major causes of vision loss globally, with optical coherence tomography (OCT) playing a crucial role in early diagnosis and management.<br /><br />2. State-of-the-art deep learning models like ConvNeXtV2-Large offer high diagnostic accuracy but face challenges in real-time clinical deployment due to their heavy computational requirements.<br /><br />3. This study presents KD-OCT, a novel knowledge distillation framework designed to compress a high-performance ConvNeXtV2-Large teacher model into a lightweight EfficientNet-B2 student model while maintaining diagnostic performance.<br /><br />4. The teacher model is enhanced with advanced augmentations, stochastic weight averaging, and focal loss, while KD-OCT employs real-time distillation using a combined loss function balancing soft teacher knowledge transfer and hard ground-truth supervision.<br /><br />5. Evaluation on the Noor Eye Hospital (NEH) dataset with patient-level cross-validation demonstrates that KD-OCT achieves near-teacher-level accuracy with significant reductions in model size and inference time, outperforming comparable multi-scale or feature-fusion OCT classifiers.<br /><br />6. The compressed student model enables efficient edge deployment for AMD screening, overcoming previous computational bottlenecks in clinical environments.<br /><br />7. The code for KD-OCT is publicly available at https://github.com/erfan-nourbakhsh/KD-OCT. <div>
arXiv:2512.09069v1 Announce Type: new 
Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</title>
<link>https://arxiv.org/abs/2512.09071</link>
<guid>https://arxiv.org/abs/2512.09071</guid>
<content:encoded><![CDATA[
<div> Visual place recognition, threshold selection, Gaussian mixture model, image descriptors, robot navigation<br /><br />Summary:<br /><br />1. Visual Place Recognition (VPR) is crucial for camera-based mapping and navigation but is challenging due to varying conditions such as seasonal changes, illumination, structural changes, and transient objects like pedestrians or vehicles.<br /><br />2. Traditional VPR methods evaluate performance using metrics like recall@K and ROC curves, but in real-world robotic applications, deciding a suitable matching threshold is usually done manually.<br /><br />3. Manually setting a threshold is difficult because it may not generalize well across diverse visual scenarios encountered by robots.<br /><br />4. The paper proposes an automatic threshold selection approach using statistics derived from the 'negative' Gaussian mixture model, which represents image statistics from places that do not match the query location.<br /><br />5. Experimental results demonstrate that this method reliably selects effective thresholds for various image databases and descriptor types, improving robustness and practicality in real robotic VPR systems. <div>
arXiv:2512.09071v1 Announce Type: new 
Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.09081</link>
<guid>https://arxiv.org/abs/2512.09081</guid>
<content:encoded><![CDATA[
<div> Keywords: AgentComp, compositionality, text-to-image models, large language models, preference optimization  

<br /><br />Summary:  
This paper addresses the challenge of compositionality in text-to-image generative models, which struggle to accurately represent object relationships, attribute bindings, and fine-grained prompt details. The main limitation identified is that existing models are not explicitly trained to distinguish between compositionally similar prompts and images, leading to outputs that deviate subtly from intended descriptions. To overcome this, the authors introduce AgentComp, a novel framework designed to improve models' compositional reasoning abilities. AgentComp utilizes large language models equipped with image generation, editing, and visual question answering (VQA) tools to autonomously create diverse compositional datasets. These datasets enable an agentic preference optimization process during fine-tuning, helping text-to-image models better differentiate between similar compositional variations. Empirical results demonstrate that AgentComp achieves state-of-the-art performance on compositionality benchmarks like T2I-CompBench. Notably, this improved compositional understanding does not come at the cost of image quality, which has been a common issue in previous methods. Furthermore, the approach exhibits strong generalization capabilities to related tasks such as text rendering, even though these were not explicitly targeted during training, indicating the robustness of the proposed method. <div>
arXiv:2512.09081v1 Announce Type: new 
Abstract: Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters</title>
<link>https://arxiv.org/abs/2512.09092</link>
<guid>https://arxiv.org/abs/2512.09092</guid>
<content:encoded><![CDATA[
<div> Underground mining, disaster response, multimodal learning, vision-language models, image captioning<br /><br />Summary:  
This article introduces MDSE (Multimodal Disaster Situation Explainer), a novel vision-language framework designed to generate detailed textual descriptions of post-disaster underground mining environments. The framework addresses challenges such as darkness, dust, and collapses that impair visual clarity and situational awareness in underground mining disasters. MDSE incorporates three key innovations: (i) Context-Aware Cross-Attention that ensures robust alignment between visual and textual features despite severe image degradation; (ii) a Segmentation-aware dual pathway visual encoder that merges global and region-specific image embeddings to enhance understanding of critical scene regions; and (iii) a Resource-Efficient Transformer-Based Language Model that produces expressive captions with reduced computational cost. To facilitate the development and evaluation of the system, the authors introduce the Underground Mine Disaster (UMD) dataset, the first image-caption corpus featuring real underground disaster scenes. Experimental results on the UMD dataset and other related benchmarks demonstrate that MDSE markedly outperforms existing state-of-the-art image captioning models by generating more accurate, detailed, and contextually relevant descriptions. These improvements have the potential to significantly enhance situational awareness for underground emergency responders, supporting safer and more effective disaster response efforts. The source code is publicly available on GitHub. <div>
arXiv:2512.09092v1 Announce Type: new 
Abstract: Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Food Image Generation on Multi-Noun Categories</title>
<link>https://arxiv.org/abs/2512.09095</link>
<guid>https://arxiv.org/abs/2512.09095</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-noun food categories, image generation, text encoder, FoCULR, spatial layout refinement  

<br /><br />Summary: Generating realistic images for food categories with compound names involving multiple nouns is a difficult task because conventional generative models often misinterpret these multi-noun prompts. For example, the phrase "egg noodle" might lead to images showing distinct eggs and noodles separately, which is semantically incorrect. Such multi-noun categories are prevalent in real-world food datasets and benchmarks like UEC-256, making this issue widely relevant. The core problems arise from insufficient domain-specific knowledge in the text encoders and the inability to correctly understand or represent the relationships between nouns, resulting in flawed spatial arrangements of ingredients or objects. To address these challenges, the authors propose a new approach named FoCULR (Food Category Understanding and Layout Refinement). FoCULR integrates explicit food domain knowledge into the generation pipeline and introduces key food concepts early in the image synthesis process. This strategy helps the model better comprehend multi-noun food categories and generate more accurate images with proper spatial layouts. Experiments indicate that incorporating FoCULR significantly enhances the quality and semantic correctness of generated food images, demonstrating the effectiveness of domain knowledge infusion and layout refinement in food image synthesis tasks. <div>
arXiv:2512.09095v1 Announce Type: new 
Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GimbalDiffusion: Gravity-Aware Camera Control for Video Generation</title>
<link>https://arxiv.org/abs/2512.09112</link>
<guid>https://arxiv.org/abs/2512.09112</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video generation, camera control, GimbalDiffusion, gravity-aligned coordinates, null-pitch conditioning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of fine-grained control over camera motion and orientation in text-to-video generation, which has been limited by relative or ambiguous camera trajectory representations in existing methods. 2. GimbalDiffusion is introduced as a novel framework that grounds camera control in absolute physical-world coordinates, using gravity as a universal reference, enabling precise and interpretable manipulation of camera parameters. 3. Unlike prior approaches reliant on relative frame-to-frame motion, this method defines camera trajectories in an absolute coordinate system, removing the need for an initial reference frame. 4. The authors utilize panoramic 360-degree videos to create diverse camera trajectories that go beyond the common straight, forward-facing paths typical in standard video datasets. 5. To improve camera guidance, a null-pitch conditioning strategy is proposed to reduce conflicts between text-driven content and camera orientation (e.g., preventing unrealistic content like grass appearing when the camera points to the sky). 6. A new benchmark is established by rebalancing the SpatialVID-HQ dataset to evaluate text-to-video models under wide variations in camera pitch, enhancing assessment of camera-aware generation. Together, these contributions significantly improve the controllability and robustness of generative text-to-video frameworks by enabling precise, gravity-aligned camera manipulation. <div>
arXiv:2512.09112v1 Announce Type: new 
Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperF: Neural Implicit Fields for Multi-Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.09115</link>
<guid>https://arxiv.org/abs/2512.09115</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-image super-resolution, coordinate-based neural networks, implicit neural representation, sub-pixel alignment, test-time optimization<br /><br />Summary:  
1. The paper addresses the challenge of enhancing image resolution in scenarios affected by sensor limitations, atmospheric conditions, and cost, such as satellite remote sensing and smartphone cameras.  
2. Traditional single-image super-resolution methods rely on learned priors or auxiliary high-resolution guides but often introduce unrealistic "hallucinated" details.  
3. Multi-image super-resolution (MISR) improves resolution by leveraging multiple low-resolution images captured with sub-pixel shifts, offering a more physically grounded reconstruction.  
4. The authors propose SuperF, a novel MISR technique using coordinate-based neural networks (implicit neural representations) to represent continuous signals and optimize the reconstruction.  
5. SuperF uniquely shares a single implicit neural representation across multiple shifted frames and simultaneously optimizes frame alignment via parameterized affine transformations at sub-pixel precision.  
6. Optimization occurs through a super-sampled coordinate grid that matches the target high-resolution output, enhancing reconstruction fidelity.  
7. Experiments on simulated satellite bursts and handheld camera images demonstrate effective upsampling up to 8× without requiring any high-resolution training data, marking a significant advantage over learning-based approaches. <div>
arXiv:2512.09115v1 Announce Type: new 
Abstract: High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to "hallucinated" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task.
  The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation</title>
<link>https://arxiv.org/abs/2512.09134</link>
<guid>https://arxiv.org/abs/2512.09134</guid>
<content:encoded><![CDATA[
<div> Keywords: coronary angiography, fractional flow reserve, deep learning, quantitative flow ratio, virtual stenting<br /><br />Summary:<br /><br />1. Coronary angiography is the primary method for assessing coronary artery disease, but visual assessment of stenosis varies widely and correlates only moderately with ischemia.<br />2. Wire-based fractional flow reserve (FFR) improves lesion assessment but is not routinely used in clinical practice.<br />3. Angiography-derived indices such as quantitative flow ratio (QFR) provide a wire-free physiological evaluation, but existing tools are often workflow-intensive and lack integration with automated anatomical analysis and virtual percutaneous coronary intervention (PCI) planning.<br />4. The authors developed AngioAI-QFR, an end-to-end angiography-only pipeline combining deep learning-based stenosis detection, lumen segmentation, centerline and diameter extraction, per millimeter relative flow capacity (RFC) profiling, and virtual stenting with automatic recalculation of angiography-derived QFR.<br />5. When evaluated in 100 consecutive vessels against invasive FFR, AngioAI-QFR demonstrated excellent stenosis detection (precision 0.97), lumen segmentation (Dice 0.78), and strong correlation with FFR (r = 0.89, mean absolute error 0.045).<br />6. The diagnostic performance for detecting FFR ≤ 0.80 was high, with an area under the curve (AUC) of 0.93, sensitivity 0.88, and specificity 0.86.<br />7. The pipeline ran fully automatically in 93% of vessels, with a median processing time of 41 seconds.<br />8. RFC profiling enabled differentiation between focal and diffuse capacity loss, and virtual stenting predicted greater QFR improvement in focal lesions compared to diffuse disease.<br />9. Overall, AngioAI-QFR offers a practical, near real-time, integrated solution combining computer vision, functional flow profiling, and virtual PCI planning from angiographic images alone. <div>
arXiv:2512.09134v1 Announce Type: new 
Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars</title>
<link>https://arxiv.org/abs/2512.09162</link>
<guid>https://arxiv.org/abs/2512.09162</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, head avatars, UV texture mapping, relighting, monocular video<br /><br />Summary:  
This work presents a novel approach to photorealistic head avatar reconstruction by combining the precise accuracy of 2D Gaussian Splatting with the user-friendly properties of UV texture mapping. The proposed method embeds each canonical Gaussian primitive's local frame into UV space patches of a template mesh, allowing for continuous and editable material textures. Unlike traditional mesh-based methods, this approach enables intuitive editing without sacrificing fidelity. Importantly, reconstruction is performed from a single monocular video, making the process more accessible with conventional equipment. The method also integrates an efficient physically based reflectance model that supports advanced relighting and editing of intrinsic material maps, enhancing visual realism and flexibility. Extensive experimental comparisons with state-of-the-art techniques validate the accuracy and quality of the reconstructions and relighting results. Additionally, the approach provides intuitive controls for modifying an avatar's appearance and geometry through texture mapping, all achieved without requiring further optimization steps. This combination paves the way for practical applications in visual effects, videoconferencing, and virtual reality by overcoming previous limitations related to editability and ease of use. <div>
arXiv:2512.09162v1 Announce Type: new 
Abstract: Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WonderZoom: Multi-Scale 3D World Generation</title>
<link>https://arxiv.org/abs/2512.09164</link>
<guid>https://arxiv.org/abs/2512.09164</guid>
<content:encoded><![CDATA[
<div> Keywords: WonderZoom, multi-scale 3D scenes, scale-adaptive Gaussian surfels, progressive detail synthesizer, single image 3D generation<br /><br />Summary:<br /><br />1. WonderZoom is a novel method designed to generate 3D scenes with content spanning multiple spatial scales, all derived from a single input image. 2. Unlike existing 3D world generation models that are restricted to single-scale synthesis, WonderZoom enables coherent scene synthesis across varying levels of granularity. 3. The core challenge tackled by this approach is the absence of a scale-aware 3D representation capable of both generating and rendering extremely diverse spatial sizes within the same scene. 4. Two major innovations are introduced: first, the use of scale-adaptive Gaussian surfels that allow real-time generation and rendering of multi-scale 3D scenes, and second, a progressive detail synthesizer that iteratively adds finer-scale 3D content in a zoom-in manner. 5. This framework enables users to zoom into any 3D region and synthesize previously non-existent detailed features, ranging from broad landscapes down to microscopic elements. 6. Experimental results demonstrate that WonderZoom surpasses state-of-the-art video and 3D generation models in terms of output quality and spatial alignment. 7. The authors provide video demonstrations and an interactive online viewer showcasing the multi-scale 3D worlds generated by WonderZoom at https://wonderzoom.github.io/. <div>
arXiv:2512.09164v1 Announce Type: new 
Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Compositional Zero-Shot Learning, vision-language models, prompt-based learning, knowledge retention, multimodal fusion

<br /><br />Summary: This paper addresses the challenge of Continual Compositional Zero-Shot Learning (CCZSL), focusing on adapting vision-language models (VLMs) to recognize new attributes, objects, and their unique compositions, while avoiding forgetting previously learned knowledge. Unlike classical continual learning where classes are distinct and non-overlapping, CCZSL is more complex because attributes and objects may reoccur across learning sessions, though their compositions remain unique. The authors propose PromptCCZSL, a prompt-based framework built on a frozen VLM backbone that leverages recency-weighted multi-teacher distillation to retain prior knowledge effectively. The method uses session-aware compositional prompts to integrate multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain consistent global semantics. The Cosine Anchor Loss (CAL) further stabilizes knowledge retention. To improve adaptation during the current session, two additional losses are introduced: Orthogonal Projection Loss (OPL) to keep new attribute and object embeddings distinct from previous ones, and Intra-Session Diversity Loss (IDL) to encourage variety within current session embeddings for more discriminative representations. A new evaluation protocol is also introduced to jointly measure catastrophic forgetting and compositional generalization. Experiments on UT-Zappos and C-GQA datasets demonstrate that PromptCCZSL significantly outperforms previous VLM-based and non-VLM methods, setting a new benchmark for CCZSL in closed-world settings. <div>
arXiv:2512.09172v1 Announce Type: new 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation</title>
<link>https://arxiv.org/abs/2512.09185</link>
<guid>https://arxiv.org/abs/2512.09185</guid>
<content:encoded><![CDATA[
<div> Keywords: disease progression, flow matching, latent alignment, auto-encoders, MRI benchmarks<br /><br />Summary:<br /><br />1. The study addresses the challenge of understanding disease progression, which is crucial for early diagnosis and personalized treatment strategies. 2. Traditional generative models fail to capture the continuous and monotonic nature of disease dynamics, often producing latent representations that lack semantic coherence, while diffusion-based models disrupt continuity through random denoising. 3. The authors propose treating disease dynamics as a velocity field and apply Flow Matching (FM) to better align the temporal evolution of patient data, making the progression more interpretable compared to prior methods. 4. A key limitation addressed is the misalignment of latent spaces produced by Auto-Encoders, which neither ensure consistency across patients nor correlate well with clinical severity indicators like age or disease conditions. 5. To overcome this, the framework introduces patient-specific latent alignment that constrains patient trajectories along a defined axis, with progression magnitude increasing monotonically with disease severity, resulting in a consistent and semantically meaningful latent space. 6. The framework, named Δ-LFM, was evaluated on three longitudinal MRI benchmarks, showing strong empirical results and providing a novel approach for interpreting and visualizing disease dynamics over time. <div>
arXiv:2512.09185v1 Announce Type: new 
Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $\Delta$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $\Delta$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs</title>
<link>https://arxiv.org/abs/2512.09215</link>
<guid>https://arxiv.org/abs/2512.09215</guid>
<content:encoded><![CDATA[
<div> 3D visual grounding, zero-shot, vision-language models, scene graph, spatial reasoning<br /><br />Summary:<br /><br />1. The paper addresses 3D visual grounding (3DVG), which involves identifying objects in 3D scenes based on language descriptions.  
2. Existing zero-shot 3DVG approaches rely on 2D vision-language models (VLMs) by converting 3D spatial information into composite inputs like view renderings or videos, which creates entangled visual representations that complicate reasoning.  
3. The authors propose a new paradigm, VLM x SI, which externalizes 3D spatial information into a structured format, allowing the VLM to selectively retrieve relevant cues incrementally rather than processing all information at once.  
4. They introduce the View-on-Graph (VoG) method that organizes the 3D scene into a multi-modal, multi-layer scene graph, enabling the VLM to explore and reason over the scene graph actively and transparently.  
5. This structured approach reduces reasoning difficulty for the VLM and produces interpretable step-by-step reasoning traces.  
6. Extensive experiments demonstrate that VoG achieves state-of-the-art zero-shot performance in 3D visual grounding, validating structured scene exploration as an effective strategy in this domain. <div>
arXiv:2512.09215v1 Announce Type: new 
Abstract: 3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Next-Generation Consumer Experience with Feature Coding for Machines</title>
<link>https://arxiv.org/abs/2512.09232</link>
<guid>https://arxiv.org/abs/2512.09232</guid>
<content:encoded><![CDATA[
<div> Feature Coding for Machines, MPEG-AI, Neural Network Compression, Edge Computing, Remote Inference Efficiency<br /><br />Summary: This paper introduces the Feature Coding for Machines (FCM) standard, a recent development by the Moving Picture Experts Group (MPEG) under the MPEG-AI framework, designed to optimize data transfer for AI-driven applications. FCM enables the efficient extraction, compression, and transmission of intermediate neural network features, which facilitates the offloading of complex computational tasks from low-powered consumer devices to high-capacity base servers. This division of labor allows devices with limited resources to benefit from large deep learning models without the typical computational overhead. The study provides an overview of FCM’s methodology and implementation, emphasizing its role in supporting increasingly intelligent and interconnected consumer devices. Experimental results demonstrate that the FCM standard achieves comparable accuracy levels to traditional remote inference approaches while significantly reducing bitrate requirements by approximately 75.90%. These advances highlight the potential of FCM to enhance the efficiency of machine-to-machine communication in various AI applications, reducing latency and bandwidth consumption, and thereby supporting more scalable and practical deployment of deep learning models in resource-constrained environments. Overall, FCM represents a promising step forward in efficient neural feature coding for machine tasks at the network edge. <div>
arXiv:2512.09232v1 Announce Type: new 
Abstract: As consumer devices become increasingly intelligent and interconnected, efficient data transfer solutions for machine tasks have become essential. This paper presents an overview of the latest Feature Coding for Machines (FCM) standard, part of MPEG-AI and developed by the Moving Picture Experts Group (MPEG). FCM supports AI-driven applications by enabling the efficient extraction, compression, and transmission of intermediate neural network features. By offloading computationally intensive operations to base servers with high computing resources, FCM allows low-powered devices to leverage large deep learning models. Experimental results indicate that the FCM standard maintains the same level of accuracy while reducing bitrate requirements by 75.90% compared to remote inference.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Feature Compression for Machines with Global Statistics Preservation</title>
<link>https://arxiv.org/abs/2512.09235</link>
<guid>https://arxiv.org/abs/2512.09235</guid>
<content:encoded><![CDATA[
<div> Keywords: split-inference, feature compression, Z-score normalization, Feature Coding for Machines (FCM), bitrate reduction

<br /><br />Summary:  
The paper addresses the challenge of compressing intermediate feature data in split-inference AI models, where an AI model is divided into two parts requiring data transfer between them. The authors propose using Z-score normalization to enhance the recovery of compressed feature data at the decoder side, aiming to improve compression efficiency. This method is integrated into the emerging MPEG Feature Coding for Machines (FCM) codec standard, replacing the existing scaling method used in the current development phase. The proposed approach reduces overhead bits involved in transmission and simultaneously improves end-task accuracy, making it more effective than previous methods. To accommodate different scenarios and further reduce overhead, a simplified variant of the method is also introduced. Experimental results demonstrate that the new method achieves an average bitrate reduction of 17.09% across multiple AI tasks without loss of task accuracy. For specific tasks like object tracking, the bitrate reduction can be as high as 65.69%. This indicates significant compression improvements that do not compromise the performance of AI models relying on compressed intermediate feature data transfer. <div>
arXiv:2512.09235v1 Announce Type: new 
Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI</title>
<link>https://arxiv.org/abs/2512.09244</link>
<guid>https://arxiv.org/abs/2512.09244</guid>
<content:encoded><![CDATA[
<div> Chronic Kidney Disease, Deep Convolutional Neural Network, CT Kidney Images, SMOTE, Grad-CAM  

<br /><br />Summary:  
This study addresses the global health challenge posed by Chronic Kidney Disease (CKD), characterized by gradual renal function decline and systemic fluid imbalance. It emphasizes the need for early and reliable diagnostic methods to reduce morbidity and mortality associated with CKD. The authors introduce a deep convolutional neural network (CNN) framework designed specifically for early detection of CKD using CT kidney images. To overcome class imbalance inherent in medical imaging data, the Synthetic Minority Over-sampling Technique (SMOTE) was applied, enhancing the model's ability to generalize across minority classes. The dataset used comprised 12,446 CT images categorized into cyst, normal, stone, and tumor classes, ensuring a robust and comprehensive evaluation. Interpretability of the CNN was facilitated through Gradient-weighted Class Activation Mapping (Grad-CAM), enabling clinical practitioners to visually assess and verify the areas influencing the model’s decisions. The developed CNN demonstrated exceptional performance, achieving 100% accuracy in CKD detection. This highlights the model’s potential as a powerful tool for early CKD diagnosis, promising improvements in clinical workflows and patient outcomes by enabling timely interventions and appropriate medical management. <div>
arXiv:2512.09244v1 Announce Type: new 
Abstract: Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniPSD: Layered PSD Generation with Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.09247</link>
<guid>https://arxiv.org/abs/2512.09247</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, PSD generation, transparency, in-context learning, RGBA-VAE<br /><br />Summary: Recent progress in diffusion models has significantly enhanced image generation and editing, but generating or reconstructing layered PSD files with transparent alpha channels remains a complex challenge. This paper introduces OmniPSD, a unified diffusion framework developed within the Flux ecosystem that supports both text-to-PSD generation and image-to-PSD decomposition via in-context learning. For text-to-PSD generation, OmniPSD spatially arranges multiple target layers into a single canvas and leverages spatial attention to learn their compositional relationships, resulting in semantically coherent and hierarchically organized layers. In terms of image-to-PSD decomposition, the method employs iterative in-context editing that progressively extracts and erases textual and foreground elements, enabling the reconstruction of editable PSD layers from a single flattened image. A novel RGBA-VAE auxiliary module is integrated to maintain transparency information without compromising structural learning. Extensive experiments conducted on a newly introduced RGBA-layered dataset demonstrate that OmniPSD can achieve high fidelity in generation, structural consistency across layers, and strong transparency awareness. Overall, OmniPSD represents a new paradigm for layered design generation and decomposition, employing diffusion transformers to effectively handle complex PSD files with transparency and multi-layered content. <div>
arXiv:2512.09247v1 Announce Type: new 
Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.09251</link>
<guid>https://arxiv.org/abs/2512.09251</guid>
<content:encoded><![CDATA[
<div> Keywords: Glacial lake monitoring, segmentation, large language models, spatial reasoning, remote sensing  

<br /><br />Summary:  
1. The paper addresses the critical issue of glacial lake monitoring to reduce risks associated with Glacial Lake Outburst Floods (GLOFs).  
2. Existing segmentation techniques using CNNs and Vision Transformers (ViTs) are limited to pixel-level predictions and lack global context and human-interpretable reasoning.  
3. The authors propose GLACIA, a novel framework combining large language models with segmentation to produce both accurate masks and spatial reasoning outputs, enhancing interpretability.  
4. They introduce the GLake-Pos dataset pipeline, offering diverse, spatially grounded question-answer pairs to enable instance-aware positional reasoning, which was previously missing in remote sensing datasets.  
5. Comparative experiments demonstrate GLACIA's superiority with a mean Intersection over Union (mIoU) of 87.30, outperforming state-of-the-art CNN, ViT, geo-foundation, and reasoning-based segmentation methods.  
6. The integrated natural language interaction functionality allows for intuitive disaster preparedness and supports policy-making in rapidly changing glacial environments, making decision-making more efficient and interpretable.  
7. The code and resources are publicly available at the provided GitHub repository, promoting further research and application development in this area. <div>
arXiv:2512.09251v1 Announce Type: new 
Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROI-Packing: Efficient Region-Based Compression for Machine Vision</title>
<link>https://arxiv.org/abs/2512.09258</link>
<guid>https://arxiv.org/abs/2512.09258</guid>
<content:encoded><![CDATA[
<div> ROI Packing, image compression, machine vision, object detection, instance segmentation<br /><br />Summary:<br /><br />This paper presents ROI-Packing, an innovative image compression technique designed specifically for machine vision applications. The method focuses on identifying and prioritizing regions of interest (ROI) that are essential for maintaining end-task accuracy, such as object detection and instance segmentation, while efficiently packing these regions and discarding less relevant image data. Unlike prior approaches, ROI-Packing does not require retraining or fine-tuning the end-task models, making it readily deployable. The authors evaluate the method comprehensively across five different datasets and two common vision tasks. Results demonstrate that ROI-Packing can reduce the bitrate by up to 44.10% without any loss in accuracy on the tasks. Furthermore, when compared to the state-of-the-art Versatile Video Coding (VVC) codec, which is standardized by the Moving Picture Experts Group (MPEG), ROI-Packing achieves an 8.88% improvement in accuracy at equivalent bitrates. This work highlights the potential of targeted compression strategies that focus on task-relevant image content to enhance both compression efficiency and machine vision performance. <div>
arXiv:2512.09258v1 Announce Type: new 
Abstract: This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification</title>
<link>https://arxiv.org/abs/2512.09270</link>
<guid>https://arxiv.org/abs/2512.09270</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D Gaussian Splatting, long-range dynamic scenes, Anchor Relay-based Bidirectional Blending, temporal coherence, hierarchical densification<br /><br />Summary: Recent progress in 4D Gaussian Splatting (4DGS) extends the capabilities of 3D Gaussian Splatting by enabling real-time rendering of dynamic scenes through temporal modeling. However, modeling long-range motion dynamics poses challenges such as memory explosion, temporal flickering, and occlusion handling failures when using naive extensions. To overcome these, the paper introduces MoRel, a novel 4DGS framework that incorporates an Anchor Relay-based Bidirectional Blending (ARBB) mechanism. This method progressively constructs locally canonical anchor spaces at key-frame indices and models inter-frame deformations at the anchor level to enhance temporal consistency. MoRel learns bidirectional deformations between key-frame anchors and blends them adaptively with learnable opacity control, effectively reducing temporal discontinuities and flickering. Additionally, a Feature-variance-guided Hierarchical Densification (FHD) scheme is proposed to densify key-frame anchors selectively based on feature variance, balancing rendering quality and efficiency. To validate the approach, the authors present SelfCap\(_{\text{LR}}\), a new dataset featuring long-range 4D dynamic motions captured over wider spatial domains and larger motion magnitudes than existing datasets. MoRel achieves temporally coherent, flicker-free long-range 4D reconstructions while maintaining bounded memory usage, demonstrating scalability and efficiency in dynamic Gaussian-based scene representations. <div>
arXiv:2512.09270v1 Announce Type: new 
Abstract: Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations</title>
<link>https://arxiv.org/abs/2512.09271</link>
<guid>https://arxiv.org/abs/2512.09271</guid>
<content:encoded><![CDATA[
<div> Keywords: Long Text-to-Image, Alignment Evaluation, Graph-structured Annotations, Multi-modal Large Language Models, Chain-of-Thought

<br /><br />Summary:  
The paper addresses the challenge of evaluating image-text alignment in long Text-to-Image (T2I) generation scenarios, where current benchmarks largely cater to short prompts and offer only coarse-grained MOS or Likert scale annotations. To overcome this, the authors introduce LongT2IBench, a dataset consisting of 14,000 long text-image pairs enriched with detailed graph-structured human annotations that capture entities, attributes, and relations within prompts. They propose a Generate-Refine-Qualify protocol to convert complex long prompts into interpretable textual graph structures, enabling fine-grained alignment assessment. These graph annotations are then transformed into alignment scores and interpretations to support the development of more precise T2I evaluation models. Building on this dataset, the authors develop LongT2IExpert, an evaluator leveraging multi-modal large language models (MLLMs) fine-tuned via an instruction-tuning process that incorporates a Hierarchical Alignment Chain-of-Thought (CoT). This approach allows LongT2IExpert to deliver both quantitative alignment scores and rich structured interpretations. Extensive experiments demonstrate that LongT2IExpert outperforms existing methods in both evaluating and interpreting long T2I alignment. The dataset and code have been made publicly available, promoting further research in this domain. <div>
arXiv:2512.09271v1 Announce Type: new 
Abstract: The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis</title>
<link>https://arxiv.org/abs/2512.09276</link>
<guid>https://arxiv.org/abs/2512.09276</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's disease, hypomimia, facial expression analysis, CLIP architecture, LSTM classification<br /><br />Summary:  
Parkinson's disease (PD) is a common neurodegenerative disorder that impairs patients' daily life and social interactions. This study introduces a novel auxiliary diagnostic method for PD based on dynamic facial expression analysis. The focus is on hypomimia, a distinctive clinical symptom characterized by reduced facial expressivity and facial rigidity. The method captures these two manifestations by analyzing patients' facial expressions during various emotional performances. A multimodal facial expression analysis network is developed, which employs the CLIP architecture to effectively combine visual and textual features while maintaining the temporal dynamics of facial expressions. The extracted expression intensity features are then fed into an LSTM-based classification network designed for PD diagnosis. Experimental results demonstrate that this approach achieves a high diagnostic accuracy of 93.1%, surpassing the performance of other existing in-vitro PD diagnostic methods. The proposed technique provides a more accessible and convenient option for early screening and diagnosis of PD, potentially enhancing the overall diagnostic experience for patients by offering a less intrusive and faster detection method. <div>
arXiv:2512.09276v1 Announce Type: new 
Abstract: Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoGoColor: Local-Global 3D Colorization for 360{\deg} Scenes</title>
<link>https://arxiv.org/abs/2512.09278</link>
<guid>https://arxiv.org/abs/2512.09278</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D colorization, multi-view consistency, color diversity, diffusion model, 360° scenes

<br /><br />Summary: The paper addresses the challenge of 3D colorization in single-channel 3D reconstruction, which is essential for visualization in robotics and medical imaging. Traditional methods rely on 2D image colorization models but suffer from an inherent inconsistency that causes colors to be averaged during training, producing monotonous and oversimplified color results, especially in complex 360° scenes. To overcome this, the authors propose LoGoColor, a novel pipeline that preserves color diversity by generating a new set of consistently colorized training views, thus bypassing the averaging process. The main challenge with this approach is maintaining strict multi-view consistency across all colorized views. LoGoColor tackles this by adopting a Local-Global strategy, partitioning the scene into subscenes and addressing both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. Experimental results demonstrate that LoGoColor achieves more consistent and plausible 3D colorization in complex environments compared to existing methods. Additionally, the study introduces a new metric, the Color Diversity Index, to quantitatively validate the superior color diversity provided by their approach. <div>
arXiv:2512.09278v1 Announce Type: new 
Abstract: Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360{\deg} scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360{\deg} scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model</title>
<link>https://arxiv.org/abs/2512.09282</link>
<guid>https://arxiv.org/abs/2512.09282</guid>
<content:encoded><![CDATA[
<div> Keywords: image restoration, foundation model, data mixture proportions, diffusion-based model, Mixture-of-Experts (MoE)

<br /><br />Summary:  
This paper presents FoundIR-v2, a high-capacity diffusion-based image restoration foundation model that addresses the challenge of balancing diverse restoration tasks in training data. The authors highlight the critical role of data mixture proportions from different restoration tasks in determining the overall performance of all-in-one image restoration models. To optimize this, they introduce a data equilibrium scheduling paradigm that dynamically adjusts the mixture proportions of datasets from various tasks during training, ensuring balanced dataset composition. This approach leverages the data mixing law to achieve consistent generalization and comprehensive performance across tasks. Furthermore, the model incorporates a Mixture-of-Experts (MoE)-driven scheduler in generative pre-training, which allocates task-adaptive diffusion priors for each restoration task. This ensures that the model can handle the distinct degradation types and severity levels specific to each task effectively. Extensive experiments show that FoundIR-v2 can successfully address over 50 sub-tasks across a wide range of real-world image restoration scenarios and delivers competitive performance compared to state-of-the-art methods, demonstrating its flexibility and robustness in multi-task image restoration. <div>
arXiv:2512.09282v1 Announce Type: new 
Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MelanomaNet: Explainable Deep Learning for Skin Lesion Classification</title>
<link>https://arxiv.org/abs/2512.09289</link>
<guid>https://arxiv.org/abs/2512.09289</guid>
<content:encoded><![CDATA[
<div> Keywords: skin lesion classification, deep learning, interpretability, uncertainty quantification, melanoma

<br /><br />Summary:  
This article presents MelanomaNet, an explainable deep learning system designed for multi-class skin lesion classification, addressing the challenge of model interpretability that limits clinical adoption. The system integrates four key interpretability mechanisms: an EfficientNet V2 backbone for feature extraction, GradCAM++ for attention visualization, automated extraction of the ABCDE clinical criteria (Asymmetry, Border, Color, Diameter, Evolution), and Fast Concept Activation Vectors (FastCAV) for concept-based explanations. Additionally, MelanomaNet incorporates Monte Carlo Dropout to quantify uncertainty, separating epistemic (model) and aleatoric (data) uncertainties, which helps flag unreliable predictions for further clinical review. The model was evaluated on a large-scale dataset, ISIC 2019, containing 25,331 dermoscopic images classified into nine diagnostic categories. MelanomaNet achieved an accuracy of 85.61% and a weighted F1 score of 0.8564, demonstrating strong classification performance. Importantly, the interpretability features ensure that the model's attention aligns with established dermatological criteria, making the explanations clinically meaningful. The combination of high predictive performance, interpretability, and uncertainty estimation aims to increase trust and adoption of deep learning tools in dermatology workflows. The authors have made the source code publicly accessible for further research and clinical application development. <div>
arXiv:2512.09289v1 Announce Type: new 
Abstract: Automated skin lesion classification using deep learning has shown remarkable accuracy, yet clinical adoption remains limited due to the "black box" nature of these models. We present MelanomaNet, an explainable deep learning system for multi-class skin lesion classification that addresses this gap through four complementary interpretability mechanisms. Our approach combines an EfficientNet V2 backbone with GradCAM++ attention visualization, automated ABCDE clinical criterion extraction, Fast Concept Activation Vectors (FastCAV) for concept-based explanations, and Monte Carlo Dropout uncertainty quantification. We evaluate our system on the ISIC 2019 dataset containing 25,331 dermoscopic images across 9 diagnostic categories. Our model achieves 85.61% accuracy with a weighted F1 score of 0.8564, while providing clinically meaningful explanations that align model attention with established dermatological assessment criteria. The uncertainty quantification module decomposes prediction confidence into epistemic and aleatoric components, enabling automatic flagging of unreliable predictions for clinical review. Our results demonstrate that high classification performance can be achieved alongside comprehensive interpretability, potentially facilitating greater trust and adoption in clinical dermatology workflows. The source code is available at https://github.com/suxrobgm/explainable-melanoma
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.09296</link>
<guid>https://arxiv.org/abs/2512.09296</guid>
<content:encoded><![CDATA[
<div> Small Target Recognition, YOLOv8n-SPTS, Feature Extraction, Multi-scale Fusion, Triple-Stage Feature Pyramid  

<br /><br />Summary:  
This paper addresses the challenge of small target recognition in autonomous driving, focusing on improving detection performance under issues like missing small target information, scale imbalance, and occlusion. First, the authors optimize the feature extraction module by replacing four traditional convolution modules in the YOLOv8n backbone bottleneck structure with Space-to-Depth Convolution (SPD-Conv) modules, which preserve fine-grained spatial details and improve feature capturing for low-resolution small targets. Second, to enhance feature fusion, they introduce the Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module, which combines multi-scale feature extraction from Spatial Pyramid Pooling with the Cross Stage Partial Connection mechanism, boosting contextual understanding and multi-scale feature expression. Third, the model design is tailored for small target detection by proposing a Triple-Stage Feature Pyramid (TSFP) that adds a 160×160 resolution detection head dedicated to small targets while removing some large target heads to maintain computational balance. Evaluations on the VisDrone2019-DET dataset demonstrate that YOLOv8n-SPTS leads with precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Lastly, visualization confirms that the model notably reduces miss rates for small, occluded, and densely packed objects such as pedestrians and bicycles. <div>
arXiv:2512.09296v1 Announce Type: new 
Abstract: This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VABench: A Comprehensive Benchmark for Audio-Video Generation</title>
<link>https://arxiv.org/abs/2512.09299</link>
<guid>https://arxiv.org/abs/2512.09299</guid>
<content:encoded><![CDATA[
<div> audio-video synchronization, video generation benchmark, multi-dimensional evaluation, text-to-audio-video, lip-speech consistency

<br /><br />Summary:<br /><br />This paper introduces VABench, a novel benchmark framework designed to evaluate synchronous audio-video generation models comprehensively. The benchmark addresses the current lack of convincing evaluation metrics for synchronized audio and video outputs in existing video generation benchmarks. VABench covers three main task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It incorporates two major evaluation modules spanning 15 dimensions, which assess pairwise similarities such as text-video, text-audio, and video-audio, as well as critical aspects like audio-video synchronization and lip-speech consistency. Additionally, the framework includes carefully curated audio and video question-answering (QA) pairs to enhance the evaluation depth. The benchmark also spans seven major content categories, including animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds, ensuring diverse and comprehensive testing scenarios. Alongside proposing the benchmark, the authors provide systematic analysis and visualization of evaluation results, setting a new standard for assessing video generation models with synchronous audio capabilities. VABench aims to stimulate more thorough and multi-dimensional progress within the field of synchronous audio-video generation. <div>
arXiv:2512.09299v1 Announce Type: new 
Abstract: Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation</title>
<link>https://arxiv.org/abs/2512.09307</link>
<guid>https://arxiv.org/abs/2512.09307</guid>
<content:encoded><![CDATA[
<div> Polyp segmentation, colonoscopy, foundation models, knowledge distillation, lightweight models<br /><br />Summary:<br /><br />Accurate polyp segmentation in colonoscopy is crucial for early colorectal cancer detection but remains challenging due to variations in polyp size, shape, color, and their camouflaged appearance. Lightweight segmentation models like U-Net, U-Net++, and PraNet are computationally efficient and easy to deploy yet struggle to handle these challenges effectively. Large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former show strong generalization in natural images but are difficult to apply directly to medical imaging due to limited relevant data and domain-specific complexities. To address this, the authors propose Polyp-DiFoM, a novel distillation framework that transfers semantic knowledge from these foundation models into lightweight baseline architectures, improving accuracy while maintaining efficiency. Key innovations include integrating semantic priors from foundation models into canonical models like U-Net and U-Net++, and introducing frequency domain encoding to enhance distillation effectiveness. Experiments on five benchmark polyp segmentation datasets—Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300—demonstrate that Polyp-DiFoM significantly outperforms both baseline and state-of-the-art methods with about nine times less computational cost. The framework thus offers an effective solution for accurate, real-time polyp segmentation suitable for clinical implementation. The code is publicly available on GitHub. <div>
arXiv:2512.09307v1 Announce Type: new 
Abstract: Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance</title>
<link>https://arxiv.org/abs/2512.09311</link>
<guid>https://arxiv.org/abs/2512.09311</guid>
<content:encoded><![CDATA[
<div> Keywords: Suspiciousness estimation, USE50k dataset, DeepUSEvision, YOLOv12, multimodal fusion<br /><br />Summary:<br /><br />1. This work addresses the crucial task of suspiciousness estimation, which is vital for proactive threat detection and ensuring public safety in complex, uncontrolled environments.<br /><br />2. The authors introduce USE50k, a large-scale annotated dataset comprising 65,500 images collected from diverse public settings such as airports, railway stations, restaurants, and parks. The dataset captures various suspicious cues, including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures.<br /><br />3. Building upon this dataset, the paper presents DeepUSEvision, a computationally efficient and modular vision-based framework designed for real-time suspiciousness analysis.<br /><br />4. DeepUSEvision integrates three core components: an enhanced YOLOv12-based Suspicious Object Detector, dual deep convolutional neural networks (DCNN-I and DCNN-II) focused on facial expression and body language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses these multimodal outputs.<br /><br />5. Extensive experimental evaluations demonstrate that DeepUSEvision outperforms state-of-the-art methods in accuracy, robustness, and interpretability, providing an effective and scalable solution for intelligent surveillance and real-time risk assessment in safety-critical scenarios. <div>
arXiv:2512.09311v1 Announce Type: new 
Abstract: Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook</title>
<link>https://arxiv.org/abs/2512.09315</link>
<guid>https://arxiv.org/abs/2512.09315</guid>
<content:encoded><![CDATA[
<div> Label noise, medical imaging, robustness, benchmark, noisy labels<br /><br />Summary:<br /><br />Learning from noisy labels poses significant challenges in medical image analysis due to the expertise required for annotation and substantial inter-observer variability, which often causes inconsistent or erroneous labeling. Despite substantial work on learning with noisy labels (LNL), existing methods have not been systematically evaluated for robustness in medical imaging contexts. To fill this gap, the authors introduce LNMBench, a comprehensive benchmark designed specifically for evaluating label noise issues in medical imaging. LNMBench includes 10 representative LNL methods assessed across 7 diverse datasets, spanning 6 different imaging modalities and 3 distinct noise patterns. This setup provides a unified and reproducible framework for robustness evaluation under realistic noise conditions. Experiments on LNMBench demonstrate that current LNL methods experience substantial performance degradation when exposed to high or real-world noise, underscoring ongoing challenges related to class imbalance and domain variability typical of medical data. Motivated by these insights, the authors propose a straightforward yet effective improvement to enhance model robustness under such adverse conditions. To promote reproducibility, standardized evaluation, and further research, the LNMBench codebase is publicly released and accessible via GitHub at https://github.com/myyy777/LNMBench. This resource aims to drive advancements in noise-resilient algorithms for both academic research and practical medical applications. <div>
arXiv:2512.09315v1 Announce Type: new 
Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking</title>
<link>https://arxiv.org/abs/2512.09327</link>
<guid>https://arxiv.org/abs/2512.09327</guid>
<content:encoded><![CDATA[
arXiv:2512.09327v1 Announce Type: new 
Abstract: Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video</title>
<link>https://arxiv.org/abs/2512.09335</link>
<guid>https://arxiv.org/abs/2512.09335</guid>
<content:encoded><![CDATA[
arXiv:2512.09335v1 Announce Type: new 
Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment</title>
<link>https://arxiv.org/abs/2512.09350</link>
<guid>https://arxiv.org/abs/2512.09350</guid>
<content:encoded><![CDATA[
arXiv:2512.09350v1 Announce Type: new 
Abstract: Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding</title>
<link>https://arxiv.org/abs/2512.09354</link>
<guid>https://arxiv.org/abs/2512.09354</guid>
<content:encoded><![CDATA[
arXiv:2512.09354v1 Announce Type: new 
Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</title>
<link>https://arxiv.org/abs/2512.09363</link>
<guid>https://arxiv.org/abs/2512.09363</guid>
<content:encoded><![CDATA[
arXiv:2512.09363v1 Announce Type: new 
Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.09364</link>
<guid>https://arxiv.org/abs/2512.09364</guid>
<content:encoded><![CDATA[
arXiv:2512.09364v1 Announce Type: new 
Abstract: Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement</title>
<link>https://arxiv.org/abs/2512.09373</link>
<guid>https://arxiv.org/abs/2512.09373</guid>
<content:encoded><![CDATA[
arXiv:2512.09373v1 Announce Type: new 
Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log NeRF: Comparing Spaces for Learning Radiance Fields</title>
<link>https://arxiv.org/abs/2512.09375</link>
<guid>https://arxiv.org/abs/2512.09375</guid>
<content:encoded><![CDATA[
arXiv:2512.09375v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perception-Inspired Color Space Design for Photo White Balance Editing</title>
<link>https://arxiv.org/abs/2512.09383</link>
<guid>https://arxiv.org/abs/2512.09383</guid>
<content:encoded><![CDATA[
arXiv:2512.09383v1 Announce Type: new 
Abstract: White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.
  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.
  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography</title>
<link>https://arxiv.org/abs/2512.09393</link>
<guid>https://arxiv.org/abs/2512.09393</guid>
<content:encoded><![CDATA[
arXiv:2512.09393v1 Announce Type: new 
Abstract: Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.
  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.
  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.
  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein-Aligned Hyperbolic Multi-View Clustering</title>
<link>https://arxiv.org/abs/2512.09402</link>
<guid>https://arxiv.org/abs/2512.09402</guid>
<content:encoded><![CDATA[
arXiv:2512.09402v1 Announce Type: new 
Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Point Cloud Registration</title>
<link>https://arxiv.org/abs/2512.09407</link>
<guid>https://arxiv.org/abs/2512.09407</guid>
<content:encoded><![CDATA[
arXiv:2512.09407v1 Announce Type: new 
Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping</title>
<link>https://arxiv.org/abs/2512.09417</link>
<guid>https://arxiv.org/abs/2512.09417</guid>
<content:encoded><![CDATA[
arXiv:2512.09417v1 Announce Type: new 
Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis</title>
<link>https://arxiv.org/abs/2512.09418</link>
<guid>https://arxiv.org/abs/2512.09418</guid>
<content:encoded><![CDATA[
arXiv:2512.09418v1 Announce Type: new 
Abstract: Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography</title>
<link>https://arxiv.org/abs/2512.09422</link>
<guid>https://arxiv.org/abs/2512.09422</guid>
<content:encoded><![CDATA[
arXiv:2512.09422v1 Announce Type: new 
Abstract: Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \(69.38\%\) using only \(25\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds</title>
<link>https://arxiv.org/abs/2512.09423</link>
<guid>https://arxiv.org/abs/2512.09423</guid>
<content:encoded><![CDATA[
arXiv:2512.09423v1 Announce Type: new 
Abstract: Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents</title>
<link>https://arxiv.org/abs/2512.09435</link>
<guid>https://arxiv.org/abs/2512.09435</guid>
<content:encoded><![CDATA[
arXiv:2512.09435v1 Announce Type: new 
Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model</title>
<link>https://arxiv.org/abs/2512.09441</link>
<guid>https://arxiv.org/abs/2512.09441</guid>
<content:encoded><![CDATA[
arXiv:2512.09441v1 Announce Type: new 
Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation</title>
<link>https://arxiv.org/abs/2512.09446</link>
<guid>https://arxiv.org/abs/2512.09446</guid>
<content:encoded><![CDATA[
arXiv:2512.09446v1 Announce Type: new 
Abstract: Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework</title>
<link>https://arxiv.org/abs/2512.09461</link>
<guid>https://arxiv.org/abs/2512.09461</guid>
<content:encoded><![CDATA[
arXiv:2512.09461v1 Announce Type: new 
Abstract: Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing</title>
<link>https://arxiv.org/abs/2512.09463</link>
<guid>https://arxiv.org/abs/2512.09463</guid>
<content:encoded><![CDATA[
arXiv:2512.09463v1 Announce Type: new 
Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach</title>
<link>https://arxiv.org/abs/2512.09471</link>
<guid>https://arxiv.org/abs/2512.09471</guid>
<content:encoded><![CDATA[
arXiv:2512.09471v1 Announce Type: new 
Abstract: Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Color encoding in Latent Space of Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2512.09477</link>
<guid>https://arxiv.org/abs/2512.09477</guid>
<content:encoded><![CDATA[
arXiv:2512.09477v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images</title>
<link>https://arxiv.org/abs/2512.09489</link>
<guid>https://arxiv.org/abs/2512.09489</guid>
<content:encoded><![CDATA[
arXiv:2512.09489v1 Announce Type: new 
Abstract: Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio</title>
<link>https://arxiv.org/abs/2512.09492</link>
<guid>https://arxiv.org/abs/2512.09492</guid>
<content:encoded><![CDATA[
arXiv:2512.09492v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Guided Learning Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2512.09497</link>
<guid>https://arxiv.org/abs/2512.09497</guid>
<content:encoded><![CDATA[
arXiv:2512.09497v1 Announce Type: new 
Abstract: Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction</title>
<link>https://arxiv.org/abs/2512.09525</link>
<guid>https://arxiv.org/abs/2512.09525</guid>
<content:encoded><![CDATA[
arXiv:2512.09525v1 Announce Type: new 
Abstract: Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.09546</link>
<guid>https://arxiv.org/abs/2512.09546</guid>
<content:encoded><![CDATA[
arXiv:2512.09546v1 Announce Type: new 
Abstract: This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.09555</link>
<guid>https://arxiv.org/abs/2512.09555</guid>
<content:encoded><![CDATA[
arXiv:2512.09555v1 Announce Type: new 
Abstract: Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection</title>
<link>https://arxiv.org/abs/2512.09565</link>
<guid>https://arxiv.org/abs/2512.09565</guid>
<content:encoded><![CDATA[
arXiv:2512.09565v1 Announce Type: new 
Abstract: Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.09573</link>
<guid>https://arxiv.org/abs/2512.09573</guid>
<content:encoded><![CDATA[
arXiv:2512.09573v1 Announce Type: new 
Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis</title>
<link>https://arxiv.org/abs/2512.09576</link>
<guid>https://arxiv.org/abs/2512.09576</guid>
<content:encoded><![CDATA[
arXiv:2512.09576v1 Announce Type: new 
Abstract: Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title>
<link>https://arxiv.org/abs/2512.09579</link>
<guid>https://arxiv.org/abs/2512.09579</guid>
<content:encoded><![CDATA[
arXiv:2512.09579v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation</title>
<link>https://arxiv.org/abs/2512.09580</link>
<guid>https://arxiv.org/abs/2512.09580</guid>
<content:encoded><![CDATA[
arXiv:2512.09580v1 Announce Type: new 
Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision</title>
<link>https://arxiv.org/abs/2512.09583</link>
<guid>https://arxiv.org/abs/2512.09583</guid>
<content:encoded><![CDATA[
arXiv:2512.09583v1 Announce Type: new 
Abstract: Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CS3D: An Efficient Facial Expression Recognition via Event Vision</title>
<link>https://arxiv.org/abs/2512.09592</link>
<guid>https://arxiv.org/abs/2512.09592</guid>
<content:encoded><![CDATA[
arXiv:2512.09592v1 Announce Type: new 
Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Chain-of-Thought Reasoning for Videos</title>
<link>https://arxiv.org/abs/2512.09616</link>
<guid>https://arxiv.org/abs/2512.09616</guid>
<content:encoded><![CDATA[
arXiv:2512.09616v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation</title>
<link>https://arxiv.org/abs/2512.09617</link>
<guid>https://arxiv.org/abs/2512.09617</guid>
<content:encoded><![CDATA[
arXiv:2512.09617v1 Announce Type: new 
Abstract: Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder</title>
<link>https://arxiv.org/abs/2512.09626</link>
<guid>https://arxiv.org/abs/2512.09626</guid>
<content:encoded><![CDATA[
arXiv:2512.09626v1 Announce Type: new 
Abstract: Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking SAM2-based Trackers on FMOX</title>
<link>https://arxiv.org/abs/2512.09633</link>
<guid>https://arxiv.org/abs/2512.09633</guid>
<content:encoded><![CDATA[
arXiv:2512.09633v1 Announce Type: new 
Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments</title>
<link>https://arxiv.org/abs/2512.09644</link>
<guid>https://arxiv.org/abs/2512.09644</guid>
<content:encoded><![CDATA[
arXiv:2512.09644v1 Announce Type: new 
Abstract: Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification</title>
<link>https://arxiv.org/abs/2512.09646</link>
<guid>https://arxiv.org/abs/2512.09646</guid>
<content:encoded><![CDATA[
arXiv:2512.09646v1 Announce Type: new 
Abstract: Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</title>
<link>https://arxiv.org/abs/2512.09663</link>
<guid>https://arxiv.org/abs/2512.09663</guid>
<content:encoded><![CDATA[
arXiv:2512.09663v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OxEnsemble: Fair Ensembles for Low-Data Classification</title>
<link>https://arxiv.org/abs/2512.09665</link>
<guid>https://arxiv.org/abs/2512.09665</guid>
<content:encoded><![CDATA[
arXiv:2512.09665v1 Announce Type: new 
Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence</title>
<link>https://arxiv.org/abs/2512.09670</link>
<guid>https://arxiv.org/abs/2512.09670</guid>
<content:encoded><![CDATA[
arXiv:2512.09670v1 Announce Type: new 
Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized</title>
<link>https://arxiv.org/abs/2512.09687</link>
<guid>https://arxiv.org/abs/2512.09687</guid>
<content:encoded><![CDATA[
arXiv:2512.09687v1 Announce Type: new 
Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.09700</link>
<guid>https://arxiv.org/abs/2512.09700</guid>
<content:encoded><![CDATA[
arXiv:2512.09700v1 Announce Type: new 
Abstract: Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stylized Meta-Album: Group-bias injection with style transfer to study robustness against distribution shifts</title>
<link>https://arxiv.org/abs/2512.09773</link>
<guid>https://arxiv.org/abs/2512.09773</guid>
<content:encoded><![CDATA[
arXiv:2512.09773v1 Announce Type: new 
Abstract: We introduce Stylized Meta-Album (SMA), a new image classification meta-dataset comprising 24 datasets (12 content datasets, and 12 stylized datasets), designed to advance studies on out-of-distribution (OOD) generalization and related topics. Created using style transfer techniques from 12 subject classification datasets, SMA provides a diverse and extensive set of 4800 groups, combining various subjects (objects, plants, animals, human actions, textures) with multiple styles. SMA enables flexible control over groups and classes, allowing us to configure datasets to reflect diverse benchmark scenarios. While ideally, data collection would capture extensive group diversity, practical constraints often make this infeasible. SMA addresses this by enabling large and configurable group structures through flexible control over styles, subject classes, and domains-allowing datasets to reflect a wide range of real-world benchmark scenarios. This design not only expands group and class diversity, but also opens new methodological directions for evaluating model performance across diverse group and domain configurations-including scenarios with many minority groups, varying group imbalance, and complex domain shifts-and for studying fairness, robustness, and adaptation under a broader range of realistic conditions. To demonstrate SMA's effectiveness, we implemented two benchmarks: (1) a novel OOD generalization and group fairness benchmark leveraging SMA's domain, class, and group diversity to evaluate existing benchmarks. Our findings reveal that while simple balancing and algorithms utilizing group information remain competitive as claimed in previous benchmarks, increasing group diversity significantly impacts fairness, altering the superiority and relative rankings of algorithms. We also propose to use \textit{Top-M worst group accuracy} as a new hyperparameter tuning metric, demonstrating broader fairness during optimization and delivering better final worst-group accuracy for larger group diversity. (2) An unsupervised domain adaptation (UDA) benchmark utilizing SMA's group diversity to evaluate UDA algorithms across more scenarios, offering a more comprehensive benchmark with lower error bars (reduced by 73\% and 28\% in closed-set setting and UniDA setting, respectively) compared to existing efforts. These use cases highlight SMA's potential to significantly impact the outcomes of conventional benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation</title>
<link>https://arxiv.org/abs/2512.09792</link>
<guid>https://arxiv.org/abs/2512.09792</guid>
<content:encoded><![CDATA[
arXiv:2512.09792v1 Announce Type: new 
Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of "apparent rotation", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation</title>
<link>https://arxiv.org/abs/2512.09801</link>
<guid>https://arxiv.org/abs/2512.09801</guid>
<content:encoded><![CDATA[
arXiv:2512.09801v1 Announce Type: new 
Abstract: Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing</title>
<link>https://arxiv.org/abs/2512.09806</link>
<guid>https://arxiv.org/abs/2512.09806</guid>
<content:encoded><![CDATA[
arXiv:2512.09806v1 Announce Type: new 
Abstract: U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.09814</link>
<guid>https://arxiv.org/abs/2512.09814</guid>
<content:encoded><![CDATA[
arXiv:2512.09814v1 Announce Type: new 
Abstract: Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composing Concepts from Images and Videos via Concept-prompt Binding</title>
<link>https://arxiv.org/abs/2512.09824</link>
<guid>https://arxiv.org/abs/2512.09824</guid>
<content:encoded><![CDATA[
arXiv:2512.09824v1 Announce Type: new 
Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities</title>
<link>https://arxiv.org/abs/2512.09847</link>
<guid>https://arxiv.org/abs/2512.09847</guid>
<content:encoded><![CDATA[
arXiv:2512.09847v1 Announce Type: new 
Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.09864</link>
<guid>https://arxiv.org/abs/2512.09864</guid>
<content:encoded><![CDATA[
arXiv:2512.09864v1 Announce Type: new 
Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI</title>
<link>https://arxiv.org/abs/2512.09867</link>
<guid>https://arxiv.org/abs/2512.09867</guid>
<content:encoded><![CDATA[
arXiv:2512.09867v1 Announce Type: new 
Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling</title>
<link>https://arxiv.org/abs/2512.09871</link>
<guid>https://arxiv.org/abs/2512.09871</guid>
<content:encoded><![CDATA[
arXiv:2512.09871v1 Announce Type: new 
Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs</title>
<link>https://arxiv.org/abs/2512.09874</link>
<guid>https://arxiv.org/abs/2512.09874</guid>
<content:encoded><![CDATA[
arXiv:2512.09874v1 Announce Type: new 
Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisualActBench: Can VLMs See and Act like a Human?</title>
<link>https://arxiv.org/abs/2512.09907</link>
<guid>https://arxiv.org/abs/2512.09907</guid>
<content:encoded><![CDATA[
arXiv:2512.09907v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway</title>
<link>https://arxiv.org/abs/2512.09913</link>
<guid>https://arxiv.org/abs/2512.09913</guid>
<content:encoded><![CDATA[
arXiv:2512.09913v1 Announce Type: new 
Abstract: We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splatent: Splatting Diffusion Latents for Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.09923</link>
<guid>https://arxiv.org/abs/2512.09923</guid>
<content:encoded><![CDATA[
arXiv:2512.09923v1 Announce Type: new 
Abstract: Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning</title>
<link>https://arxiv.org/abs/2512.09924</link>
<guid>https://arxiv.org/abs/2512.09924</guid>
<content:encoded><![CDATA[
arXiv:2512.09924v1 Announce Type: new 
Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures</title>
<link>https://arxiv.org/abs/2512.09925</link>
<guid>https://arxiv.org/abs/2512.09925</guid>
<content:encoded><![CDATA[
arXiv:2512.09925v1 Announce Type: new 
Abstract: Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agreement Disagreement Guided Knowledge Transfer for Cross-Scene Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2512.08990</link>
<guid>https://arxiv.org/abs/2512.08990</guid>
<content:encoded><![CDATA[
arXiv:2512.08990v1 Announce Type: cross 
Abstract: Knowledge transfer plays a crucial role in cross-scene hyperspectral imaging (HSI). However, existing studies often overlook the challenges of gradient conflicts and dominant gradients that arise during the optimization of shared parameters. Moreover, many current approaches fail to simultaneously capture both agreement and disagreement information, relying only on a limited shared subset of target features and consequently missing the rich, diverse patterns present in the target scene. To address these issues, we propose an Agreement Disagreement Guided Knowledge Transfer (ADGKT) framework that integrates both mechanisms to enhance cross-scene transfer. The agreement component includes GradVac, which aligns gradient directions to mitigate conflicts between source and target domains, and LogitNorm, which regulates logit magnitudes to prevent domination by a single gradient source. The disagreement component consists of a Disagreement Restriction (DiR) and an ensemble strategy, which capture diverse predictive target features and mitigate the loss of critical target information. Extensive experiments demonstrate the effectiveness and superiority of the proposed method in achieving robust and balanced knowledge transfer across heterogeneous HSI scenes.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning</title>
<link>https://arxiv.org/abs/2512.08992</link>
<guid>https://arxiv.org/abs/2512.08992</guid>
<content:encoded><![CDATA[
arXiv:2512.08992v1 Announce Type: cross 
Abstract: The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant</title>
<link>https://arxiv.org/abs/2512.08998</link>
<guid>https://arxiv.org/abs/2512.08998</guid>
<content:encoded><![CDATA[
arXiv:2512.08998v1 Announce Type: cross 
Abstract: Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Attribution of Model Performance Gaps in Medical Imaging Under Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.09094</link>
<guid>https://arxiv.org/abs/2512.09094</guid>
<content:encoded><![CDATA[
arXiv:2512.09094v1 Announce Type: cross 
Abstract: Deep learning models for medical image segmentation suffer significant performance drops due to distribution shifts, but the causal mechanisms behind these drops remain poorly understood. We extend causal attribution frameworks to high-dimensional segmentation tasks, quantifying how acquisition protocols and annotation variability independently contribute to performance degradation. We model the data-generating process through a causal graph and employ Shapley values to fairly attribute performance changes to individual mechanisms. Our framework addresses unique challenges in medical imaging: high-dimensional outputs, limited samples, and complex mechanism interactions. Validation on multiple sclerosis (MS) lesion segmentation across 4 centers and 7 annotators reveals context-dependent failure modes: annotation protocol shifts dominate when crossing annotators (7.4% $\pm$ 8.9% DSC attribution), while acquisition shifts dominate when crossing imaging centers (6.5% $\pm$ 9.1%). This mechanism-specific quantification enables practitioners to prioritize targeted interventions based on deployment context.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual Primitive Fitting of 3D Shapes with SuperFrusta</title>
<link>https://arxiv.org/abs/2512.09201</link>
<guid>https://arxiv.org/abs/2512.09201</guid>
<content:encoded><![CDATA[
arXiv:2512.09201v1 Announce Type: cross 
Abstract: We introduce a framework for converting 3D shapes into compact and editable assemblies of analytic primitives, directly addressing the persistent trade-off between reconstruction fidelity and parsimony. Our approach combines two key contributions: a novel primitive, termed SuperFrustum, and an iterative fiting algorithm, Residual Primitive Fitting (ResFit). SuperFrustum is an analytical primitive that is simultaneously (1) expressive, being able to model various common solids such as cylinders, spheres, cones & their tapered and bent forms, (2) editable, being compactly parameterized with 8 parameters, and (3) optimizable, with a sign distance field differentiable w.r.t. its parameters almost everywhere. ResFit is an unsupervised procedure that interleaves global shape analysis with local optimization, iteratively fitting primitives to the unexplained residual of a shape to discover a parsimonious yet accurate decompositions for each input shape. On diverse 3D benchmarks, our method achieves state-of-the-art results, improving IoU by over 9 points while using nearly half as many primitives as prior work. The resulting assemblies bridge the gap between dense 3D data and human-controllable design, producing high-fidelity and editable shape programs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge</title>
<link>https://arxiv.org/abs/2512.09309</link>
<guid>https://arxiv.org/abs/2512.09309</guid>
<content:encoded><![CDATA[
arXiv:2512.09309v1 Announce Type: cross 
Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration</title>
<link>https://arxiv.org/abs/2512.09340</link>
<guid>https://arxiv.org/abs/2512.09340</guid>
<content:encoded><![CDATA[
arXiv:2512.09340v1 Announce Type: cross 
Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane</title>
<link>https://arxiv.org/abs/2512.09343</link>
<guid>https://arxiv.org/abs/2512.09343</guid>
<content:encoded><![CDATA[
arXiv:2512.09343v1 Announce Type: cross 
Abstract: QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rates and architectures for learning geometrically non-trivial operators</title>
<link>https://arxiv.org/abs/2512.09376</link>
<guid>https://arxiv.org/abs/2512.09376</guid>
<content:encoded><![CDATA[
arXiv:2512.09376v1 Announce Type: cross 
Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos</title>
<link>https://arxiv.org/abs/2512.09406</link>
<guid>https://arxiv.org/abs/2512.09406</guid>
<content:encoded><![CDATA[
arXiv:2512.09406v1 Announce Type: cross 
Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments</title>
<link>https://arxiv.org/abs/2512.09447</link>
<guid>https://arxiv.org/abs/2512.09447</guid>
<content:encoded><![CDATA[
arXiv:2512.09447v1 Announce Type: cross 
Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiePrune: Lie Group and Quantum Geometric Dual Representation for One-Shot Structured Pruning of Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2512.09469</link>
<guid>https://arxiv.org/abs/2512.09469</guid>
<content:encoded><![CDATA[
arXiv:2512.09469v1 Announce Type: cross 
Abstract: Quantum neural networks (QNNs) and parameterized quantum circuits (PQCs) are key building blocks for near-term quantum machine learning. However, their scalability is constrained by excessive parameters, barren plateaus, and hardware limitations. We propose LiePrune, the first mathematically grounded one-shot structured pruning framework for QNNs that leverages Lie group structure and quantum geometric information. Each gate is jointly represented in a Lie group--Lie algebra dual space and a quantum geometric feature space, enabling principled redundancy detection and aggressive compression. Experiments on quantum classification (MNIST, FashionMNIST), quantum generative modeling (Bars-and-Stripes), and quantum chemistry (LiH VQE) show that LiePrune achieves over $10\times$ compression with negligible or even improved task performance, while providing provable guarantees on redundancy detection, functional approximation, and computational complexity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics</title>
<link>https://arxiv.org/abs/2512.09510</link>
<guid>https://arxiv.org/abs/2512.09510</guid>
<content:encoded><![CDATA[
arXiv:2512.09510v1 Announce Type: cross 
Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</title>
<link>https://arxiv.org/abs/2512.09607</link>
<guid>https://arxiv.org/abs/2512.09607</guid>
<content:encoded><![CDATA[
arXiv:2512.09607v1 Announce Type: cross 
Abstract: Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation</title>
<link>https://arxiv.org/abs/2512.09610</link>
<guid>https://arxiv.org/abs/2512.09610</guid>
<content:encoded><![CDATA[
arXiv:2512.09610v1 Announce Type: cross 
Abstract: People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthPix: A lightspeed PIV images generator</title>
<link>https://arxiv.org/abs/2512.09664</link>
<guid>https://arxiv.org/abs/2512.09664</guid>
<content:encoded><![CDATA[
arXiv:2512.09664v1 Announce Type: cross 
Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation</title>
<link>https://arxiv.org/abs/2512.09779</link>
<guid>https://arxiv.org/abs/2512.09779</guid>
<content:encoded><![CDATA[
arXiv:2512.09779v1 Announce Type: cross 
Abstract: Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&amp;Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronusOmni: Improving Time Awareness of Omni Large Language Models</title>
<link>https://arxiv.org/abs/2512.09841</link>
<guid>https://arxiv.org/abs/2512.09841</guid>
<content:encoded><![CDATA[
arXiv:2512.09841v1 Announce Type: cross 
Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</title>
<link>https://arxiv.org/abs/2512.09851</link>
<guid>https://arxiv.org/abs/2512.09851</guid>
<content:encoded><![CDATA[
arXiv:2512.09851v1 Announce Type: cross 
Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Heading Prediction for Autonomous Aerial Vehicles</title>
<link>https://arxiv.org/abs/2512.09898</link>
<guid>https://arxiv.org/abs/2512.09898</guid>
<content:encoded><![CDATA[
arXiv:2512.09898v1 Announce Type: cross 
Abstract: The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506{\deg} and a root mean squared error of 0.1957{\deg}, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</title>
<link>https://arxiv.org/abs/2512.09903</link>
<guid>https://arxiv.org/abs/2512.09903</guid>
<content:encoded><![CDATA[
arXiv:2512.09903v1 Announce Type: cross 
Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating</title>
<link>https://arxiv.org/abs/2512.09920</link>
<guid>https://arxiv.org/abs/2512.09920</guid>
<content:encoded><![CDATA[
arXiv:2512.09920v1 Announce Type: cross 
Abstract: Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Causal Principles for Improving Visual Dialog</title>
<link>https://arxiv.org/abs/1911.10496</link>
<guid>https://arxiv.org/abs/1911.10496</guid>
<content:encoded><![CDATA[
arXiv:1911.10496v3 Announce Type: replace 
Abstract: This paper unravels the design tricks adopted by us, the champion team MReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for improving Visual Dialog (VisDial). By "improving", we mean that they can promote almost every existing VisDial model to the state-of-the-art performance on the leader-board. Such a major improvement is only due to our careful inspection on the causality behind the model and data, finding that the community has overlooked two causalities in VisDial. Intuitively, Principle 1 suggests: we should remove the direct input of the dialog history to the answer model, otherwise a harmful shortcut bias will be introduced; Principle 2 says: there is an unobserved confounder for history, question, and answer, leading to spurious correlations from training data. In particular, to remove the confounder suggested in Principle 2, we propose several causal intervention algorithms, which make the training fundamentally different from the traditional likelihood estimation. Note that the two principles are model-agnostic, so they are applicable in any VisDial model. The code is available at https://github.com/simpleshinobu/visdial-principles.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Infrared Small Target Detection: A Feature-Enhanced and Sensitivity-Tunable Framework</title>
<link>https://arxiv.org/abs/2407.20090</link>
<guid>https://arxiv.org/abs/2407.20090</guid>
<content:encoded><![CDATA[
arXiv:2407.20090v3 Announce Type: replace 
Abstract: Recently, single-frame infrared small target (SIRST) detection technology has attracted widespread attention. Different from most existing deep learning-based methods that focus on improving network architectures, we propose a feature-enhanced and sensitivity-tunable (FEST) framework, which is compatible with existing SIRST detection networks and further enhances their detection performance. The FEST framework improves the model's robustness from two aspects: feature enhancement and target confidence regulation. For feature enhancement, we employ a multi-scale fusion strategy to improve the model's perception to multi-scale features of multi-size targets, and design an edge enhancement difficulty mining (EEDM) loss to guide the network to continuously focus on challenging target regions and edge features during training. For target confidence regulation, an adjustable sensitivity (AS) strategy is proposed for network post-processing. This strategy enhances the model's adaptability in complex scenarios and significantly improves the detection rate of infrared small targets while maintaining segmentation accuracy. Extensive experimental results show that our FEST framework can effectively enhance the performance of existing SIRST detection networks. The code is available at https://github.com/YuChuang1205/FEST-Framework
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial-Robustness-Guided Graph Pruning</title>
<link>https://arxiv.org/abs/2411.12331</link>
<guid>https://arxiv.org/abs/2411.12331</guid>
<content:encoded><![CDATA[
arXiv:2411.12331v2 Announce Type: replace 
Abstract: Graph learning plays a central role in many data mining and machine learning tasks, such as manifold learning, data representation and analysis, dimensionality reduction, clustering, and visualization. In this work, we propose a highly scalable, adversarial-robustness-guided graph pruning framework for learning graph topologies from data. By performing a spectral adversarial robustness evaluation, our method aims to learn sparse, undirected graphs that help the underlying algorithms resist noise and adversarial perturbations. In particular, we explicitly identify and prune edges that are most vulnerable to adversarial attacks. We use spectral clustering, one of the most representative graph-based machine learning algorithms, to evaluate the proposed framework. Compared with prior state-of-the-art graph learning approaches, the proposed method is more scalable and significantly improves both the computational efficiency and the solution quality of spectral clustering.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations</title>
<link>https://arxiv.org/abs/2412.01826</link>
<guid>https://arxiv.org/abs/2412.01826</guid>
<content:encoded><![CDATA[
arXiv:2412.01826v2 Announce Type: replace 
Abstract: We present RELOCATE, a simple training-free baseline designed to perform the challenging task of visual query localization in long videos. To eliminate the need for task-specific training and efficiently handle long videos, RELOCATE leverages a region-based representation derived from pretrained vision models. At a high level, it follows the classic object localization approach: (1) identify all objects in each video frame, (2) compare the objects with the given query and select the most similar ones, and (3) perform bidirectional tracking to get a spatio-temporal response. However, we propose some key enhancements to handle small objects, cluttered scenes, partial visibility, and varying appearances. Notably, we refine the selected objects for accurate localization and generate additional visual queries to capture visual variations. We evaluate RELOCATE on the challenging Ego4D Visual Query 2D Localization dataset, establishing a new baseline that outperforms prior task-specific methods by 49% (relative improvement) in spatio-temporal average precision.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence models for continuous cell cycle stage prediction from brightfield images</title>
<link>https://arxiv.org/abs/2502.02182</link>
<guid>https://arxiv.org/abs/2502.02182</guid>
<content:encoded><![CDATA[
arXiv:2502.02182v2 Announce Type: replace 
Abstract: Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the Fucci system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous Fucci signals using non-fluorescence brightfield imaging, a widely available label-free modality. To that end, we generated a large dataset of 1.3 M images of dividing RPE1 cells with full cell cycle trajectories to quantitatively compare the predictive performance of distinct model categories including single time-frame models, causal state space models and bidirectional transformer models. We show that both causal and transformer-based models significantly outperform single- and fixed frame approaches, enabling the prediction of visually imperceptible transitions like G1/S within 1h resolution. Our findings underscore the importance of sequence models for accurate predictions of cell cycle dynamics and highlight their potential for label-free imaging.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization</title>
<link>https://arxiv.org/abs/2502.05593</link>
<guid>https://arxiv.org/abs/2502.05593</guid>
<content:encoded><![CDATA[
arXiv:2502.05593v2 Announce Type: replace 
Abstract: Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight Space Representation Learning on Diverse NeRF Architectures</title>
<link>https://arxiv.org/abs/2502.09623</link>
<guid>https://arxiv.org/abs/2502.09623</guid>
<content:encoded><![CDATA[
arXiv:2502.09623v3 Announce Type: replace 
Abstract: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network. Recent studies have demonstrated that these weights can be used as input for frameworks designed to address deep learning tasks; however, such frameworks require NeRFs to adhere to a specific, predefined architecture. In this paper, we introduce the first framework capable of processing NeRFs with diverse architectures and performing inference on architectures unseen at training time. We achieve this by training a Graph Meta-Network within an unsupervised representation learning framework, and show that a contrastive objective is conducive to obtaining an architecture-agnostic latent space. In experiments conducted across 13 NeRF architectures belonging to three families (MLPs, tri-planes, and, for the first time, hash tables), our approach demonstrates robust performance in classification, retrieval, and language tasks involving multiple architectures, even unseen at training time, while also matching or exceeding the results of existing frameworks limited to single architectures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v5 Announce Type: replace 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Motion Unlearning</title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
arXiv:2503.18674v3 Announce Type: replace 
Abstract: We introduce Human Motion Unlearning and motivate it through the concrete task of preventing violent 3D motion synthesis, an important safety requirement given that popular text-to-motion datasets (HumanML3D and Motion-X) contain from 7\% to 15\% violent sequences spanning both atomic gestures (e.g., a single punch) and highly compositional actions (e.g., loading and swinging a leg to kick). By focusing on violence unlearning, we demonstrate how removing a challenging, multifaceted concept can serve as a proxy for the broader capability of motion "forgetting." To enable systematic evaluation of Human Motion Unlearning, we establish the first motion unlearning benchmark by automatically filtering HumanML3D and Motion-X datasets to create distinct forget sets (violent motions) and retain sets (safe motions). We introduce evaluation metrics tailored to sequential unlearning, measuring both suppression efficacy and the preservation of realism and smooth transitions. We adapt two state-of-the-art, training-free image unlearning methods (UCE and RECE) to leading text-to-motion architectures (MoMask and BAMM), and propose Latent Code Replacement (LCR), a novel, training-free approach that identifies violent codes in a discrete codebook representation and substitutes them with safe alternatives. Our experiments show that unlearning violent motions is indeed feasible and that acting on latent codes strikes the best trade-off between violence suppression and preserving overall motion quality. This work establishes a foundation for advancing safe motion synthesis across diverse applications. Website: https://www.pinlab.org/hmu.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset</title>
<link>https://arxiv.org/abs/2503.19804</link>
<guid>https://arxiv.org/abs/2503.19804</guid>
<content:encoded><![CDATA[
arXiv:2503.19804v2 Announce Type: replace 
Abstract: Low-light image enhancement is crucial for a myriad of applications, from night vision and surveillance, to autonomous driving. However, due to the inherent limitations that come in hand with capturing images in low-illumination environments, the task of enhancing such scenes still presents a formidable challenge. To advance research in this field, we introduce our Low Exposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure benchmark dataset for low-light image enhancement comprising of over 230K frames showcasing 24K real-world indoor and outdoor, with-and without human, scenes. Captured using 3 different camera sensors, LENVIZ offers a wide range of lighting conditions, noise levels, and scene complexities, making it the largest publicly available up-to 4K resolution benchmark in the field. LENVIZ includes high quality human-generated ground truth, for which each multi-exposure low-light scene has been meticulously curated and edited by expert photographers to ensure optimal image quality. Furthermore, we also conduct a comprehensive analysis of current state-of-the-art low-light image enhancement techniques on our dataset and highlight potential areas of improvement.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation</title>
<link>https://arxiv.org/abs/2504.03166</link>
<guid>https://arxiv.org/abs/2504.03166</guid>
<content:encoded><![CDATA[
arXiv:2504.03166v2 Announce Type: replace 
Abstract: The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Polarization Multiplexing: Single-Shot Invisible Shape and Reflectance Recovery</title>
<link>https://arxiv.org/abs/2504.13177</link>
<guid>https://arxiv.org/abs/2504.13177</guid>
<content:encoded><![CDATA[
arXiv:2504.13177v2 Announce Type: replace 
Abstract: We propose spatial polarization multiplexing (SPM) for joint sensing of shape and reflectance of a static or dynamic deformable object, which is also invisible to the naked eye. Past structured-light methods are limited to shape acquisition and cannot recover reflectance as they alter scene appearance. Our key idea is to spatially multiplex a polarization pattern to encode the incident ray and also densely sample the reflected light. We derive a quantized polarized light pattern that can be robustly and uniquely decoded from the reflected Angle of Linear Polarization (AoLP) values. It also enables single-shot disentanglement of polarimetric diffuse and specular reflections for accurate BRDF estimation. We achieve this spatial polarization multiplexing (SPM) with a constrained de Bruijn sequence. We validate this novel invisible single-shot shape and reflectance method with real static and dynamic objects. The results demonstrate the effectiveness of SPM for accurate shape and BRDF measurement which opens new avenues of application for 3D sensing thanks to its invisibility and ability to jointly recover the radiometric properties.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing</title>
<link>https://arxiv.org/abs/2505.19148</link>
<guid>https://arxiv.org/abs/2505.19148</guid>
<content:encoded><![CDATA[
arXiv:2505.19148v2 Announce Type: replace 
Abstract: Resolving closely-spaced small targets in dense clusters presents a significant challenge in infrared imaging, as the overlapping signals hinder precise determination of their quantity, sub-pixel positions, and radiation intensities. While deep learning has advanced the field of infrared small target detection, its application to closely-spaced infrared small targets has not yet been explored. This gap exists primarily due to the complexity of separating superimposed characteristics and the lack of an open-source infrastructure. In this work, we propose the Dynamic Iterative Shrinkage Thresholding Network (DISTA-Net), which reconceptualizes traditional sparse reconstruction within a dynamic framework. DISTA-Net adaptively generates convolution weights and thresholding parameters to tailor the reconstruction process in real time. To the best of our knowledge, DISTA-Net is the first deep learning model designed specifically for the unmixing of closely-spaced infrared small targets, achieving superior sub-pixel detection accuracy. Moreover, we have established the first open-source ecosystem to foster further research in this field. This ecosystem comprises three key components: (1) CSIST-100K, a publicly available benchmark dataset; (2) CSO-mAP, a custom evaluation metric for sub-pixel detection; and (3) GrokCSO, an open-source toolkit featuring DISTA-Net and other models. Our code and dataset are available at https://github.com/GrokCV/GrokCSO.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Infer Parameterized Representations of Plants from 3D Scans</title>
<link>https://arxiv.org/abs/2505.22337</link>
<guid>https://arxiv.org/abs/2505.22337</guid>
<content:encoded><![CDATA[
arXiv:2505.22337v2 Announce Type: replace 
Abstract: Plants frequently contain numerous organs, organized in 3D branching systems defining the plant's architecture. Reconstructing the architecture of plants from unstructured observations is challenging because of self-occlusion and spatial proximity between organs, which are often thin structures. To achieve the challenging task, we propose an approach that allows to infer a parameterized representation of the plant's architecture from a given 3D scan of a plant. In addition to the plant's branching structure, this representation contains parametric information for each plant organ, and can therefore be used directly in a variety of tasks. In this data-driven approach, we train a recursive neural network with virtual plants generated using a procedural model. After training, the network allows to infer a parametric tree-like representation based on an input 3D point cloud. Our method is applicable to any plant that can be represented as binary axial tree. We quantitatively evaluate our approach on Chenopodium Album plants on reconstruction, segmentation and skeletonization, which are important problems in plant phenotyping. In addition to carrying out several tasks at once, our method achieves results on-par with strong baselines for each task. We apply our method, trained exclusively on synthetic data, to 3D scans and show that it generalizes well.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2506.02022</link>
<guid>https://arxiv.org/abs/2506.02022</guid>
<content:encoded><![CDATA[
arXiv:2506.02022v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) show reasoning promise, yet their visual perception is a critical bottleneck. Strikingly, MLLMs can produce correct answers even while misinterpreting crucial visual elements, masking these underlying failures. Our preliminary study on a joint perception-reasoning dataset revealed that for one leading MLLM, 29% of its correct answers to reasoning questions still exhibited visual perception errors. To systematically address this, we introduce "Do You See Me", a scalable benchmark with 1,758 images and 2,612 questions. It spans seven human-psychology inspired subtasks in 2D and 3D, featuring controllable complexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading closed-source and 5 major open-source models reveal a stark deficit: humans achieve 96.49% accuracy, while top MLLMs average below 50%. This performance gap widens rapidly with increased task complexity (e.g., from 12% to 45% in the visual form constancy subtask). Further analysis into the root causes suggests that failures stem from challenges like misallocated visual attention and the instability of internal representations for fine-grained details, especially at or below encoder patch resolution. This underscores an urgent need for MLLMs with truly robust visual perception. The benchmark dataset, source code and evaluation scripts are available at https://github.com/microsoft/Do-You-See-Me.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models</title>
<link>https://arxiv.org/abs/2506.11436</link>
<guid>https://arxiv.org/abs/2506.11436</guid>
<content:encoded><![CDATA[
arXiv:2506.11436v2 Announce Type: replace 
Abstract: Audio-Visual Segmentation (AVS) faces a fundamental challenge of effectively aligning audio and visual modalities. While recent approaches leverage foundation models to address data scarcity, they often rely on single-modality knowledge or combine foundation models in an off-the-shelf manner, failing to address the cross-modal alignment challenge. In this paper, we present TAViS, a novel framework that \textbf{couples} the knowledge of multimodal foundation models (ImageBind) for cross-modal alignment and a segmentation foundation model (SAM2) for precise segmentation. However, effectively combining these models poses two key challenges: the difficulty in transferring the knowledge between SAM2 and ImageBind due to their different feature spaces, and the insufficiency of using only segmentation loss for supervision. To address these challenges, we introduce a text-bridged design with two key components: (1) a text-bridged hybrid prompting mechanism where pseudo text provides class prototype information while retaining modality-specific details from both audio and visual inputs, and (2) an alignment supervision strategy that leverages text as a bridge to align shared semantic concepts within audio-visual modalities. Our approach achieves superior performance on single-source, multi-source, semantic datasets, and excels in zero-shot settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DELTAv2: Accelerating Dense 3D Tracking</title>
<link>https://arxiv.org/abs/2508.01170</link>
<guid>https://arxiv.org/abs/2508.01170</guid>
<content:encoded><![CDATA[
arXiv:2508.01170v2 Announce Type: replace 
Abstract: We propose a novel algorithm for accelerating dense long-term 3D point tracking in videos. Through analysis of existing state-of-the-art methods, we identify two major computational bottlenecks. First, transformer-based iterative tracking becomes expensive when handling a large number of trajectories. To address this, we introduce a coarse-to-fine strategy that begins tracking with a small subset of points and progressively expands the set of tracked trajectories. The newly added trajectories are initialized using a learnable interpolation module, which is trained end-to-end alongside the tracking network. Second, we propose an optimization that significantly reduces the cost of correlation feature computation, another key bottleneck in prior methods. Together, these improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURORA:Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation</title>
<link>https://arxiv.org/abs/2508.02149</link>
<guid>https://arxiv.org/abs/2508.02149</guid>
<content:encoded><![CDATA[
arXiv:2508.02149v2 Announce Type: replace 
Abstract: Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to precisely locate sounding objects by integrating visual, auditory, and textual cues. Existing methods often lack genuine semantic understanding, tending to memorize fixed reasoning patterns. Furthermore, jointly training for reasoning and segmentation can compromise pixel-level precision. To address these issues, we introduce AURORA, a novel framework designed to enhance genuine reasoning and language comprehension in reference audio-visual segmentation. We employ a structured Chain-of-Thought (CoT) prompting mechanism to guide the model through a step-by-step reasoning process and introduce a novel segmentation feature distillation loss to effectively integrate these reasoning abilities without sacrificing segmentation performance. To further cultivate the model's genuine reasoning capabilities, we devise a further two-stage training strategy: first, a ``corrective reflective-style training" stage utilizes self-correction to enhance the quality of reasoning paths, followed by reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster robustness in challenging scenarios. Experiments demonstrate that AURORA achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes effectively to unreferenced segmentation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v2 Announce Type: replace 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</title>
<link>https://arxiv.org/abs/2508.06982</link>
<guid>https://arxiv.org/abs/2508.06982</guid>
<content:encoded><![CDATA[
arXiv:2508.06982v4 Announce Type: replace 
Abstract: We present WeatherDiffusion, a diffusion-based framework for controllable weather editing in intrinsic space. Our framework includes two components based on diffusion priors: an inverse renderer that estimates material properties, scene geometry, and lighting as intrinsic maps from an input image, and a forward renderer that utilizes these geometry and material maps along with a text prompt that describes specific weather conditions to generate a final image. The intrinsic maps enhance controllability compared to traditional pixel-space editing approaches. We propose an intrinsic map-aware attention mechanism that improves spatial correspondence and decomposition quality in large outdoor scenes. For forward rendering, we leverage CLIP-space interpolation of weather prompts to achieve fine-grained weather control. We also introduce a synthetic and a real-world dataset, containing 38k and 18k images under various weather conditions, each with intrinsic map annotations. WeatherDiffusion outperforms state-of-the-art pixel-space editing approaches, weather restoration methods, and rendering-based methods, showing promise for downstream tasks such as autonomous driving, enhancing the robustness of detection and segmentation in challenging weather scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugLift: Uncertainty Aware Depth Descriptors for Robust 2D to 3D Pose Lifting</title>
<link>https://arxiv.org/abs/2508.07112</link>
<guid>https://arxiv.org/abs/2508.07112</guid>
<content:encoded><![CDATA[
arXiv:2508.07112v3 Announce Type: replace 
Abstract: Lifting based 3D human pose estimators infer 3D joints from 2D keypoints, but often struggle to generalize to real world settings with noisy 2D detections. We revisit the input to lifting and propose AugLift, a simple augmentation of standard lifting that enriches each 2D keypoint (x, y) with an Uncertainty Aware Depth Descriptor (UADD). We run a single off the shelf monocular depth estimator to obtain a depth map, and for every keypoint with detector confidence c we extract depth statistics from its confidence scaled neighborhood, forming a compact, interpretable UADD (c, d, d_min, d_max) that captures both local geometry and reliability. AugLift is modular, requires no new sensors or architectural changes, and integrates by expanding the input layer of existing lifting models.
  Across four datasets and four lifting architectures, AugLift boosts cross dataset (out of distribution) performance on unseen data by an average of 10.1 percent, while also improving in distribution performance by 4.0 percent as measured by MPJPE. A post hoc analysis clarifies when and why it helps: gains are largest on novel poses and significantly occluded joints, where depth statistics resolve front back ambiguities while confidence calibrates the spatial neighborhoods from which they are drawn. We also study interaction with recent image feature lifting methods and find the signals are complementary: adding UADD to image conditioned lifting yields both ID and OOD gains. A learned depth feature extension (AugLiftV2) improves performance further while trading off interpretability. Together, these results indicate that lightweight, confidence aware depth cues are a powerful plug in for robust 2D to 3D pose lifting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring</title>
<link>https://arxiv.org/abs/2508.11482</link>
<guid>https://arxiv.org/abs/2508.11482</guid>
<content:encoded><![CDATA[
arXiv:2508.11482v2 Announce Type: replace 
Abstract: The construction industry increasingly relies on visual data to support Artificial Intelligence (AI) and Machine Learning (ML) applications for site monitoring. High-quality, domain-specific datasets, comprising images, videos, and point clouds, capture site geometry and spatiotemporal dynamics, including the location and interaction of objects, workers, and materials. However, despite growing interest in leveraging visual datasets, existing resources vary widely in sizes, data modalities, annotation quality, and representativeness of real-world construction conditions. A systematic review to categorize their data characteristics and application contexts is still lacking, limiting the community's ability to fully understand the dataset landscape, identify critical gaps, and guide future directions toward more effective, reliable, and scalable AI applications in construction. To address this gap, this study conducts an extensive search of academic databases and open-data platforms, yielding 51 publicly available visual datasets that span the 2005-2024 period. These datasets are categorized using a structured data schema covering (i) data fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv) downstream application domains (e.g., progress tracking). This study synthesizes these findings into an open-source catalog, OpenConstruction, supporting data-driven method development. Furthermore, the study discusses several critical limitations in the existing construction dataset landscape and presents a roadmap for future data infrastructure anchored in the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles. By reviewing the current landscape and outlining strategic priorities, this study supports the advancement of data-centric solutions in the construction sector.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix-game 2.0: An open-source real-time and streaming interactive world model</title>
<link>https://arxiv.org/abs/2508.13009</link>
<guid>https://arxiv.org/abs/2508.13009</guid>
<content:encoded><![CDATA[
arXiv:2508.13009v3 Announce Type: replace 
Abstract: Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seedream 4.0: Toward Next-generation Multimodal Image Generation</title>
<link>https://arxiv.org/abs/2509.20427</link>
<guid>https://arxiv.org/abs/2509.20427</guid>
<content:encoded><![CDATA[
arXiv:2509.20427v3 Announce Type: replace 
Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. We further scale our model and data as Seedream 4.5. Seedream 4.0 and Seedream 4.5 are accessible on Volcano Engine https://www.volcengine.com/experience/ark?launch=seedream.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification</title>
<link>https://arxiv.org/abs/2510.08269</link>
<guid>https://arxiv.org/abs/2510.08269</guid>
<content:encoded><![CDATA[
arXiv:2510.08269v2 Announce Type: replace 
Abstract: Multi-label classification (MLC) offers a more comprehensive semantic understanding of Remote Sensing (RS) imagery compared to traditional single-label classification (SLC). However, obtaining complete annotations for MLC is particularly challenging due to the complexity and high cost of the labeling process. As a practical alternative, single-positive multi-label learning (SPML) has emerged, where each image is annotated with only one relevant label, and the model is expected to recover the full set of labels. While scalable, SPML introduces significant supervision ambiguity, demanding specialized solutions for model training. Although various SPML methods have been proposed in the computer vision domain, research in the RS context remains limited. To bridge this gap, we propose Adaptive Gradient Calibration (AdaGC), a novel and generalizable SPML framework tailored to RS imagery. AdaGC adopts a gradient calibration (GC) mechanism with a dual exponential moving average (EMA) module for robust pseudo-label generation. We introduce a theoretically grounded, training-dynamics-based indicator to adaptively trigger GC, which ensures GC's effectiveness by preventing it from being affected by model underfitting or overfitting to label noise. Extensive experiments on two benchmark RS datasets under two distinct label noise types demonstrate that AdaGC achieves state-of-the-art (SOTA) performance while maintaining strong robustness across diverse settings. The codes and data will be released at https://github.com/rslab-unitrento/AdaGC.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</title>
<link>https://arxiv.org/abs/2510.08352</link>
<guid>https://arxiv.org/abs/2510.08352</guid>
<content:encoded><![CDATA[
arXiv:2510.08352v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not "shortsighted", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation</title>
<link>https://arxiv.org/abs/2510.11173</link>
<guid>https://arxiv.org/abs/2510.11173</guid>
<content:encoded><![CDATA[
arXiv:2510.11173v2 Announce Type: replace 
Abstract: Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above the prior state of the art across both validation and test partitions. Extensive experiments demonstrate a strong positive correlation among the CoT trajectory, the generated heatmap, and the decoded mask, supporting an interpretable alignment between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and in more precise mask prediction. Code, checkpoints and logs are released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foveation Improves Payload Capacity in Steganography</title>
<link>https://arxiv.org/abs/2510.13151</link>
<guid>https://arxiv.org/abs/2510.13151</guid>
<content:encoded><![CDATA[
arXiv:2510.13151v2 Announce Type: replace 
Abstract: Steganography finds its use in visual medium such as providing metadata and watermarking. With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits, while achieving better accuracy of up to 1 failure bit out of 2000, at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in creating multi-modal latent representations in steganography.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression</title>
<link>https://arxiv.org/abs/2511.09702</link>
<guid>https://arxiv.org/abs/2511.09702</guid>
<content:encoded><![CDATA[
arXiv:2511.09702v2 Announce Type: replace 
Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation</title>
<link>https://arxiv.org/abs/2511.14184</link>
<guid>https://arxiv.org/abs/2511.14184</guid>
<content:encoded><![CDATA[
arXiv:2511.14184v3 Announce Type: replace 
Abstract: Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoD: A Diffusion Foundation Model for Image Compression</title>
<link>https://arxiv.org/abs/2511.18706</link>
<guid>https://arxiv.org/abs/2511.18706</guid>
<content:encoded><![CDATA[
arXiv:2511.18706v2 Announce Type: replace 
Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion</title>
<link>https://arxiv.org/abs/2511.22048</link>
<guid>https://arxiv.org/abs/2511.22048</guid>
<content:encoded><![CDATA[
arXiv:2511.22048v2 Announce Type: replace 
Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22787</link>
<guid>https://arxiv.org/abs/2511.22787</guid>
<content:encoded><![CDATA[
arXiv:2511.22787v2 Announce Type: replace 
Abstract: In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2512.00368</link>
<guid>https://arxiv.org/abs/2512.00368</guid>
<content:encoded><![CDATA[
arXiv:2512.00368v2 Announce Type: replace 
Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFM-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images</title>
<link>https://arxiv.org/abs/2512.00718</link>
<guid>https://arxiv.org/abs/2512.00718</guid>
<content:encoded><![CDATA[
arXiv:2512.00718v2 Announce Type: replace 
Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary accuracy. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios. The codes are available at https://github.com/wondelyan/VFM-ISRefiner .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalised Medical Phrase Grounding</title>
<link>https://arxiv.org/abs/2512.01085</link>
<guid>https://arxiv.org/abs/2512.01085</guid>
<content:encoded><![CDATA[
arXiv:2512.01085v2 Announce Type: replace 
Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach for Precise Wall Segmentation</title>
<link>https://arxiv.org/abs/2512.02413</link>
<guid>https://arxiv.org/abs/2512.02413</guid>
<content:encoded><![CDATA[
arXiv:2512.02413v2 Announce Type: replace 
Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans necessitates high-precision semantic segmentation of structural elements, particularly walls. However, existing methods often struggle with detecting thin structures and maintaining geometric precision. This study introduces MitUNet, a hybrid neural network combining a Mix-Transformer encoder and a U-Net decoder enhanced with spatial and channel attention blocks. Our approach, optimized with the Tversky loss function, achieves a balance between precision and recall, ensuring accurate boundary recovery. Experiments on the CubiCasa5k dataset and a proprietary regional dataset demonstrate MitUNet's superiority in generating structurally correct masks with high boundary accuracy, outperforming standard models. This tool provides a robust foundation for automated 3D reconstruction pipelines. To ensure reproducibility and facilitate future research, the source code and the proprietary regional dataset are publicly available at https://github.com/aliasstudio/mitunet and https://doi.org/10.5281/zenodo.17871079 respectively.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance</title>
<link>https://arxiv.org/abs/2512.02685</link>
<guid>https://arxiv.org/abs/2512.02685</guid>
<content:encoded><![CDATA[
arXiv:2512.02685v2 Announce Type: replace 
Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Informed Weighting Channel Normalizing Flow for Deep Generative Models</title>
<link>https://arxiv.org/abs/2407.04958</link>
<guid>https://arxiv.org/abs/2407.04958</guid>
<content:encoded><![CDATA[
arXiv:2407.04958v2 Announce Type: replace-cross 
Abstract: Normalizing Flows (NFs) are widely used in deep generative models for their exact likelihood estimation and efficient sampling.
  However, they require substantial memory since the latent space matches the input dimension.
  Multi-scale architectures address this by progressively reducing latent dimensions while preserving reversibility.
  Existing multi-scale architectures use simple, static channel-wise splitting, limiting expressiveness. To improve this, we introduce a regularized, feature-dependent $\mathtt{Shuffle}$ operation and integrate it into vanilla multi-scale architecture.
  This operation adaptively generates channel-wise weights and shuffles latent variables before splitting them.
  We observe that such operation guides the variables to evolve in the direction of entropy increase, hence we refer to NFs with the $\mathtt{Shuffle}$ operation as \emph{Entropy-Informed Weighting Channel Normalizing Flow} (EIW-Flow).
  Extensive experiments on CIFAR-10, CelebA, ImageNet, and LSUN demonstrate that EIW-Flow achieves state-of-the-art density estimation and competitive sample quality for deep generative modeling, with minimal computational overhead.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INRetouch: Context Aware Implicit Neural Representation for Photography Retouching</title>
<link>https://arxiv.org/abs/2412.03848</link>
<guid>https://arxiv.org/abs/2412.03848</guid>
<content:encoded><![CDATA[
arXiv:2412.03848v4 Announce Type: replace-cross 
Abstract: Professional photo editing remains challenging, requiring extensive knowledge of imaging pipelines and significant expertise. While recent deep learning approaches, particularly style transfer methods, have attempted to automate this process, they often struggle with output fidelity, editing control, and complex retouching capabilities. We propose a novel retouch transfer approach that learns from professional edits through before-after image pairs, enabling precise replication of complex editing operations. We develop a context-aware Implicit Neural Representation that learns to apply edits adaptively based on image content and context, and is capable of learning from a single example. Our method extracts implicit transformations from reference edits and adaptively applies them to new images. To facilitate this research direction, we introduce a comprehensive Photo Retouching Dataset comprising 100,000 high-quality images edited using over 170 professional Adobe Lightroom presets. Through extensive evaluation, we demonstrate that our approach not only surpasses existing methods in photo retouching but also enhances performance in related image reconstruction tasks like Gamut Mapping and Raw Reconstruction. By bridging the gap between professional editing capabilities and automated solutions, our work presents a significant step toward making sophisticated photo editing more accessible while maintaining high-fidelity results. The source code and the dataset are publicly available at https://omaralezaby.github.io/inretouch .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment</title>
<link>https://arxiv.org/abs/2503.10287</link>
<guid>https://arxiv.org/abs/2503.10287</guid>
<content:encoded><![CDATA[
arXiv:2503.10287v3 Announce Type: replace-cross 
Abstract: Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-modal task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, we propose a method called MACS to conduct multi-source audio-to-image generation. To our best knowledge, this is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, effective image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 out of the 21 evaluation indexes on all tasks and delivers superior visual quality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Pretrained Transformers May Be Universally Robust In-Context Learners</title>
<link>https://arxiv.org/abs/2505.14042</link>
<guid>https://arxiv.org/abs/2505.14042</guid>
<content:encoded><![CDATA[
arXiv:2505.14042v2 Announce Type: replace-cross 
Abstract: Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we present the first theoretical analysis suggesting that adversarially pretrained transformers can serve as universally robust foundation models -- models that can robustly adapt to diverse downstream tasks with only lightweight tuning. Specifically, we demonstrate that single-layer linear transformers, after adversarial pretraining across a variety of classification tasks, can robustly generalize to unseen classification tasks through in-context learning from clean demonstrations (i.e., without requiring additional adversarial training or examples). This universal robustness stems from the model's ability to adaptively focus on robust features within given tasks. We also show the two open challenges for attaining robustness: accuracy--robustness trade-off and sample-hungry training. This study initiates the discussion on the utility of universally robust foundation models. While their training is expensive, the investment would prove worthwhile as downstream tasks can enjoy free adversarial robustness. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.23589</link>
<guid>https://arxiv.org/abs/2509.23589</guid>
<content:encoded><![CDATA[
arXiv:2509.23589v2 Announce Type: replace-cross 
Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 7.72% over prior arts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</title>
<link>https://arxiv.org/abs/2509.25270</link>
<guid>https://arxiv.org/abs/2509.25270</guid>
<content:encoded><![CDATA[
arXiv:2509.25270v3 Announce Type: replace-cross 
Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</title>
<link>https://arxiv.org/abs/2510.09660</link>
<guid>https://arxiv.org/abs/2510.09660</guid>
<content:encoded><![CDATA[
arXiv:2510.09660v4 Announce Type: replace-cross 
Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Failures: Rethinking Foundation Models in Pathology</title>
<link>https://arxiv.org/abs/2510.23807</link>
<guid>https://arxiv.org/abs/2510.23807</guid>
<content:encoded><![CDATA[
arXiv:2510.23807v4 Announce Type: replace-cross 
Abstract: Despite their successes in vision and language, foundation models have stumbled in pathology, revealing low accuracy, instability, and heavy computational demands. These shortcomings stem not from tuning problems but from deeper conceptual mismatches: dense embeddings cannot represent the combinatorial richness of tissue, and current architectures inherit flaws in self-supervision, patch design, and noise-fragile pretraining. Biological complexity and limited domain innovation further widen the gap. The evidence is clear-pathology requires models explicitly designed for biological images rather than adaptations of large-scale natural-image methods whose assumptions do not hold for tissue.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporality for Sketch Representation Learning</title>
<link>https://arxiv.org/abs/2512.04007</link>
<guid>https://arxiv.org/abs/2512.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: sketches, temporal aspect, positional encodings, sequence modeling, decoders<br /><br />Summary:<br /><br />This work explores the role of temporal information in sketch representation learning, evaluating whether sketches should be treated as sequences. The research confirms that traditional positional encodings are valid for modeling sketches, with absolute coordinate encodings performing better than relative ones. Additionally, the study compares decoder architectures, finding that non-autoregressive decoders consistently outperform autoregressive decoders in representing sketches. Importantly, the study reveals that the significance of temporality in sketches is not universal but depends heavily on the specific stroke order used and the task being performed. Overall, the findings provide insight into how internal stroke order and temporal structure affect the quality of sketch representations, informing better model designs for various applications. <div>
arXiv:2512.04007v2 Announce Type: replace 
Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Cyberbullying in GIF using AI</title>
<link>https://arxiv.org/abs/2512.07838</link>
<guid>https://arxiv.org/abs/2512.07838</guid>
<content:encoded><![CDATA[
<div> Keywords: Cyberbullying, GIFs, Deep learning, VGG16, Twitter  

<br /><br />Summary:  
This study addresses the escalating issue of cyberbullying on social media platforms, with an emphasis on detecting cyberbullying in GIFs and stickers, an area with limited research compared to text and images. The researchers collected a diverse dataset of over 4100 GIFs from Twitter by extracting hashtags related to cyberbullying and using the GIPHY API to download relevant files. The dataset includes both cyberbullying and non-cyberbullying GIFs to facilitate effective training and evaluation. For detection, the study employed the pre-trained deep learning model VGG16, known for its robust performance in image processing tasks. The model achieved a high accuracy of 97% in classifying GIFs as cyberbullying or non-cyberbullying content. This highlights the potential of deep learning techniques for multimedia cyberbullying detection beyond text-based methods. Additionally, the authors provide the constructed GIF dataset to the research community, enabling further studies and improvements in this under-explored area. The work contributes to broadening the scope of cyberbullying detection by incorporating dynamic media formats like GIFs, thereby enhancing online safety measures on social networking platforms. <div>
arXiv:2512.07838v1 Announce Type: new 
Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-real time fires detection using satellite imagery in Sudan conflict</title>
<link>https://arxiv.org/abs/2512.07925</link>
<guid>https://arxiv.org/abs/2512.07925</guid>
<content:encoded><![CDATA[
<div> Keywords: Sudan conflict, deep learning, satellite imagery, fire damage monitoring, Planet Labs<br /><br />Summary:<br />1. The ongoing war in Sudan necessitates rapid and efficient monitoring methods to assess conflict-related damage.<br />2. The study leverages advances in deep learning alongside readily accessible satellite remote sensing data to achieve near real-time conflict monitoring.<br />3. Specifically, the authors utilize 4-band imagery from Planet Labs combined with a deep learning model to detect and assess fire damage caused by armed conflicts.<br />4. Five case studies in Sudan demonstrate the approach's effectiveness, showing improved accuracy in identifying active fires and charred areas compared to baseline methods.<br />5. The research finds that incorporating higher dimensional data such as 8-band imagery or time series imagery provides only marginal improvements over the 4-band imagery model.<br />6. These results underscore the potential for automated systems using readily available satellite imagery to provide timely insights into conflict zones, supporting humanitarian and strategic decision-making efforts. <div>
arXiv:2512.07925v1 Announce Type: new 
Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality</title>
<link>https://arxiv.org/abs/2512.07951</link>
<guid>https://arxiv.org/abs/2512.07951</guid>
<content:encoded><![CDATA[
<div> Video face swapping, temporal consistency, reference-guided editing, keyframe conditioning, Face2Face dataset<br /><br />Summary: Video face swapping plays an essential role in film and entertainment production but faces challenges in maintaining high fidelity and temporal consistency across long and complex sequences. This work, called LivingSwap, introduces the first video reference-guided face swapping model that leverages rich visual attributes from source videos to improve both fidelity and temporal coherence. The approach uses keyframes as conditioning signals to incorporate the target identity flexibly and controllably, combining them with video reference guidance to perform temporal stitching. This ensures stable identity preservation and high-quality reconstruction throughout lengthy video sequences. To overcome the lack of data for reference-guided training, the authors create a paired face-swapping dataset named Face2Face and augment it by reversing the data pairs to provide reliable ground-truth supervision. Extensive experiments show that LivingSwap delivers state-of-the-art results by seamlessly integrating target identities with the source video's expressions, lighting, and motions. Additionally, the method significantly reduces manual labor in video production workflows. The project webpage provides further resources and details: https://aim-uofa.github.io/LivingSwap <div>
arXiv:2512.07951v1 Announce Type: new 
Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection</title>
<link>https://arxiv.org/abs/2512.07984</link>
<guid>https://arxiv.org/abs/2512.07984</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical semantic segmentation, dental imaging, anatomical hierarchy, TL-pano dataset, feature-wise linear modulation<br /><br />Summary:<br />1. The paper addresses the challenge of accurately understanding anatomical structures in dental disease staging by incorporating explicit anatomical hierarchies into semantic segmentation models.<br />2. The authors propose a general framework that integrates a recurrent, level-wise prediction scheme, restrictive output heads, and top-down feature conditioning to embed anatomical hierarchy explicitly.<br />3. At each hierarchical level, the backbone model processes the original image combined with logits from the previous level; child class features are modulated using Feature-wise Linear Modulation (FiLM) of parent class probabilities to enhance finer detection.<br />4. A probabilistic composition rule is introduced to ensure consistency between parent and descendant class predictions, while a hierarchical loss combining class-weighted Dice, cross-entropy, and consistency terms ensures that parent predictions reflect the sum of their children.<br />5. The approach is validated on the newly proposed TL-pano dataset, consisting of 194 panoramic radiographs annotated for tooth layers and alveolar bone, using UNet and HRNet backbones in a 5-fold cross-validation.<br />6. Hierarchical variants significantly improve intersection-over-union (IoU), Dice scores, and recall, especially for fine-grained anatomical features, producing more anatomically coherent segmentation masks.<br />7. However, the hierarchical models show increased recall with a tendency for more false positives, indicating a trade-off between sensitivity and precision.<br />8. Overall, explicitly incorporating anatomical hierarchies improves segmentation performance and clinical plausibility, particularly under limited data conditions in dental imaging. <div>
arXiv:2512.07984v1 Announce Type: new 
Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08016</link>
<guid>https://arxiv.org/abs/2512.08016</guid>
<content:encoded><![CDATA[
<div> Cartographic reasoning, spatial relations, large vision language models, FRIEDA benchmark, map visual question answering<br /><br />Summary:<br /><br />1. Cartographic reasoning involves interpreting geographic relationships by aligning legends, scales, compass directions, text, and geometries across one or more map images, critical for tasks like disaster response and urban planning but remains under-evaluated.<br /><br />2. Existing large vision language models (LVLMs) often treat maps as a subset of charts, which overlooks the unique complexity of map visual question answering (VQA) that requires understanding layered symbology and spatial relations related to orientation, distance, and multi-map interactions.<br /><br />3. To address this, the authors introduce FRIEDA, a benchmark designed to test complex, open-ended cartographic reasoning in LVLMs using real-world map images from documents and reports across various domains and geographies.<br /><br />4. FRIEDA focuses on three categories of spatial relations from GIS literature: topological (e.g., border, within), metric (distance), and directional (orientation), with questions demanding multi-step inference and often cross-map grounding.<br /><br />5. Evaluation of eleven state-of-the-art LVLMs under direct and contextual settings reveals a significant performance gap, with the best models reaching only around 38% accuracy compared to human performance at nearly 85%, highlighting the challenge and establishing FRIEDA as a benchmark to advance spatial intelligence in LVLMs. <div>
arXiv:2512.08016v1 Announce Type: new 
Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification</title>
<link>https://arxiv.org/abs/2512.08038</link>
<guid>https://arxiv.org/abs/2512.08038</guid>
<content:encoded><![CDATA[
<div> Keywords: Retinopathy of Prematurity, Explainable AI, Sparse and Smooth Explainer, ADMM, Fundus Image Classification<br /><br />Summary:<br />1. The paper addresses the challenge of interpreting neural network models used for diagnosing Retinopathy of Prematurity (ROP) from fundus images, acknowledging the need for reliable model explainers due to their black-box nature.<br />2. It introduces a novel explainer method called Sparse and Smooth Explainer (SSplain), which generates pixel-wise explanations by enforcing smoothness and sparsity to better preserve the input image's structure.<br />3. SSplain relies on solving an optimization problem with combinatorial constraints via the Alternating Direction Method of Multipliers (ADMM), enabling the generation of realistic and interpretable explanations.<br />4. Experimental results demonstrate that SSplain outperforms existing explanation methods both in terms of post-hoc accuracy and maintaining smoothness in the output, thus providing more meaningful and consistent explanations aligned with clinical understanding.<br />5. The method's generalizability is validated by applying SSplain to additional publicly available datasets, and the source code has been made publicly available to facilitate further research and application in medical image explanation. <div>
arXiv:2512.08038v1 Announce Type: new 
Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment</title>
<link>https://arxiv.org/abs/2512.08040</link>
<guid>https://arxiv.org/abs/2512.08040</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language understanding, sign language translation, sign-subtitle alignment, multilingual pretraining, zero-shot generalisation<br /><br />Summary: This work introduces a unified model for sign language understanding that integrates two key tasks: sign language translation (SLT), which converts continuous signing videos into spoken language text, and sign-subtitle alignment (SSA), which temporally aligns signing video frames with subtitles. The model architecture comprises three components: a lightweight visual backbone extracting both manual and non-manual cues from human keypoints and lip-region images with privacy considerations; a Sliding Perceiver mapping network that aggregates visual features into word-level embeddings bridging the visual and textual modalities; and a multi-task scalable training strategy that jointly optimizes SLT and SSA to improve both linguistic and temporal alignment. To ensure cross-linguistic generalization, the model is pretrained on large-scale datasets featuring British Sign Language (BSL) and American Sign Language (ASL), specifically BOBSL and YouTube-SL-25 corpora. This multilingual pretraining coupled with robust model design achieves state-of-the-art performance on the BOBSL dataset for both SLT and SSA tasks. Additionally, the model shows strong zero-shot generalization capabilities and enhanced fine-tuned SLT performance on the How2Sign (ASL) dataset, demonstrating its potential for scalable, cross-lingual sign language translation applications. <div>
arXiv:2512.08040v1 Announce Type: new 
Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</title>
<link>https://arxiv.org/abs/2512.08042</link>
<guid>https://arxiv.org/abs/2512.08042</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, frequency-domain masking, generalization, model pruning, Green AI  

<br /><br />Summary: Universal deepfake detection seeks to identify AI-generated images from a wide range of generative models, including those not seen during training. The main challenge lies in achieving robust generalization to new and evolving deepfake techniques while minimizing computational costs to enable large-scale deployment, aligning with the goals of Green AI. This work proposes a novel training strategy based on frequency-domain masking, contrasted with traditional reliance on spatial features or large pretrained models. The method involves random masking and geometric transformations, with an emphasis on frequency masking due to its better generalization capabilities. Experiments demonstrate that frequency masking improves detection accuracy across various GAN and diffusion-based image generators. Additionally, the approach maintains strong performance even after substantial structured pruning, indicating its scalability and efficiency. The proposed method sets a new state-of-the-art for universal deepfake detection, combining strong generalization with resource-conscious operation. This makes it a promising solution for sustainable and practical deployment in large-scale deepfake screening scenarios. The authors have made the code and trained models publicly available to facilitate further research and application. <div>
arXiv:2512.08042v1 Announce Type: new 
Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning</title>
<link>https://arxiv.org/abs/2512.08048</link>
<guid>https://arxiv.org/abs/2512.08048</guid>
<content:encoded><![CDATA[
<div> Masking, Test-time adaptation, Distribution shift, Image corruption, Consistency loss<br /><br />Summary:<br /><br />1. The paper addresses the problem of distribution shifts at test time that degrade the performance of image classifiers, focusing on continual test-time adaptation (CTTA) methods.<br /><br />2. Existing CTTA approaches often rely on masking mechanisms guided by calibrated uncertainty estimates or stable attention scores, which introduces additional complexity.<br /><br />3. The authors question whether custom masking designs are necessary or if simple random masking can suffice under strong corruptions.<br /><br />4. They propose a novel method called Mask to Adapt (M2A), which uses a short sequence of randomly masked views—either spatial or frequency-based—and adapts the model using two objectives: a mask consistency loss to align predictions across views and an entropy minimization loss to encourage confident outputs.<br /><br />5. The study explores two types of masking: spatial masking (patch vs. pixel) and frequency masking (all vs. low vs. high frequencies), motivated by masked image modeling techniques.<br /><br />6. Experimental results on CIFAR10C, CIFAR100C, and ImageNetC datasets at high corruption severity demonstrate that M2A with spatial masking achieves significant improvements, outperforming or matching strong CTTA baselines, whereas frequency masking underperforms.<br /><br />7. Ablation studies confirm that simple random masking is both effective and robust, indicating that complex uncertainty- or attention-based masking schemes are not necessary.<br /><br />8. The findings suggest that combining random masking with consistency and entropy-based losses provides a straightforward yet powerful approach to test-time adaptation for image classifiers under distribution shifts. <div>
arXiv:2512.08048v1 Announce Type: new 
Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models</title>
<link>https://arxiv.org/abs/2512.08075</link>
<guid>https://arxiv.org/abs/2512.08075</guid>
<content:encoded><![CDATA[
<div> Keywords: Amazon Rainforest, deforestation detection, machine learning, satellite imagery, self-attention mechanisms<br /><br />Summary:<br /><br />1. The Amazon Rainforest's preservation is critical for global climate change mitigation, biodiversity protection, and indigenous culture safeguarding.<br />2. The PRODES project by INPE annually monitors deforestation in the Brazilian Amazon and other biomes using satellite data.<br />3. Recent machine learning models utilize PRODES data for deforestation detection by analyzing multitemporal satellite images as a change detection problem.<br />4. Current methods face challenges including suboptimal performance, limited use of modern architectures like self-attention-based networks, and lack of standardized evaluation protocols for direct comparison.<br />5. This work evaluates multiple change detection models on a unified dataset, comparing fully convolutional networks and Transformer-based self-attention architectures.<br />6. The study explores the effects of various pre- and post-processing techniques such as connected component filtering, texture replacement, and image enhancement, demonstrating significant improvement in model effectiveness.<br />7. Combining models through different fusion strategies yields better performance than individual models, achieving an F1-score of 80.41%, comparable to recent state-of-the-art results.<br /><br />This comprehensive evaluation addresses existing gaps, provides methodological standardization, and highlights the potential of advanced deep learning methods and processing techniques for improved deforestation monitoring in the Amazon using satellite imagery. <div>
arXiv:2512.08075v1 Announce Type: new 
Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2512.08135</link>
<guid>https://arxiv.org/abs/2512.08135</guid>
<content:encoded><![CDATA[
<div> Keywords: central-peripheral vision, multimodal model, spatial reasoning, 3D scene understanding, allocentric grid<br /><br />Summary:<br /><br />This paper introduces a novel central-peripheral vision-inspired framework (CVP) designed to enhance spatial reasoning in 3D scene understanding tasks. The approach is motivated by the human visual system, which uses central vision to focus on important details and peripheral vision to capture broader contextual information. Existing models typically rely on unstructured data representations such as point clouds or voxels, with scene context added implicitly via coordinate embeddings, limiting their ability to reason spatially in a structured manner. To overcome this, CVP incorporates two key components: the target-affinity token, serving as an analog to central vision by directing attention to query-relevant objects, and the allocentric grid, which acts like peripheral vision by encoding the global spatial layout and context of the scene. These elements operate together within a Large Multimodal Model framework to provide an explicit, structured understanding of complex 3D environments. Experimental results demonstrate that CVP outperforms existing state-of-the-art methods across several benchmarks for 3D scene understanding, validating the effectiveness of combining central and peripheral vision-inspired representations for improved spatial reasoning. <div>
arXiv:2512.08135v1 Announce Type: new 
Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing</title>
<link>https://arxiv.org/abs/2512.08161</link>
<guid>https://arxiv.org/abs/2512.08161</guid>
<content:encoded><![CDATA[
<div> Image dehazing, Fourier-RWKV, Multi-State Perception, Deformable Quad-directional Token Shift, Semantic Bridge Module<br /><br />Summary: The paper addresses the challenge of image dehazing under complex, non-uniform haze conditions that degrade visual perception. It introduces Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel framework employing a Multi-State Perception paradigm to model haze degradation comprehensively with linear computational complexity. The framework integrates three key perceptual states: (1) Spatial-form Perception, achieved through the Deformable Quad-directional Token Shift (DQ-Shift), which dynamically adjusts receptive fields to adapt locally to haze variations; (2) Frequency-domain Perception, realized in the Fourier Mix block by extending the RWKV-based WKV attention mechanism into the Fourier domain to maintain long-range dependencies while reducing spatial attenuation; and (3) Semantic-relation Perception, enabled by the Semantic Bridge Module (SBM) that uses Dynamic Semantic Kernel Fusion (DSK-Fusion) to align encoder-decoder features effectively and suppress reconstruction artifacts. Extensive experiments on various benchmarks show that Fourier-RWKV achieves state-of-the-art dehazing results across diverse haze scenarios. Notably, the method significantly lowers computational costs compared to typical Transformer-based approaches, striking a favorable balance between high restoration quality and real-time efficiency. The authors also provide code publicly at the linked repository for reproducibility and further research. <div>
arXiv:2512.08161v1 Announce Type: new 
Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators</title>
<link>https://arxiv.org/abs/2512.08163</link>
<guid>https://arxiv.org/abs/2512.08163</guid>
<content:encoded><![CDATA[
<div> Keywords: Monocular depth estimation, human perception, deep neural networks, model accuracy, KITTI dataset  

<br /><br />Summary:  
1. This study focuses on monocular depth estimation, which is crucial for applications like autonomous driving and robotics.  
2. Although deep neural networks have achieved superhuman accuracy on sensor-based benchmarks, aligning their representations with human perception remains a challenge.  
3. Prior research in object recognition has found a trade-off between accuracy and human-like behavior, prompting an investigation into whether a similar pattern exists for depth estimation, especially in outdoor natural scenes.  
4. The authors evaluated 69 monocular depth estimation models using the KITTI dataset and dissected error patterns through affine fitting to create interpretable components of prediction errors.  
5. Results showed that while humans and DNNs share some estimation biases reflected in positive error correlations, improvements in accuracy do not necessarily make model behavior more human-like.  
6. This divergence underlines the importance of developing new, multifaceted evaluation methods that focus on human-centric metrics beyond traditional accuracy measures for robust and interpretable monocular depth estimation models. <div>
arXiv:2512.08163v1 Announce Type: new 
Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoLoom: High-quality Geometric Diagram Generation from Textual Input</title>
<link>https://arxiv.org/abs/2512.08180</link>
<guid>https://arxiv.org/abs/2512.08180</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric diagram generation, autoformalization, GeoLingua, Monte Carlo optimization, constraint-based evaluation  

<br /><br />Summary: High-quality geometric diagram generation requires precise spatial accuracy and benefits from well-defined constraints, presenting unique challenges and opportunities. To address this, the paper introduces GeoLoom, a novel framework designed for generating geometric diagrams from natural language descriptions. GeoLoom consists of two main components: an autoformalization module that converts natural language into GeoLingua, a custom formal language tailored for diagram generation, and a coordinate solver that uses Monte Carlo optimization to compute exact diagram coordinates from formal constraints. To facilitate development and evaluation, the authors present GeoNF, a dataset that pairs natural language geometric problem statements with corresponding GeoLingua formalizations. Additionally, the work proposes a new constraint-based evaluation metric to mathematically quantify structural deviations in generated diagrams, enabling more principled and interpretable assessment and iterative refinement. Empirical evaluations demonstrate that GeoLoom significantly outperforms existing baseline methods in maintaining structural fidelity, offering a scalable and interpretable approach to text-to-diagram generation in geometric domains. These contributions collectively provide a strong foundation for future research bridging natural language understanding, formal reasoning, and precise geometric visualization. <div>
arXiv:2512.08180v1 Announce Type: new 
Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animal Re-Identification on Microcontrollers</title>
<link>https://arxiv.org/abs/2512.08198</link>
<guid>https://arxiv.org/abs/2512.08198</guid>
<content:encoded><![CDATA[
<div> Keywords: Animal Re-Identification, Microcontroller Units, MobileNetV2, On-device Inference, Wildlife Monitoring  

<br /><br />Summary:  
This work addresses the challenge of performing Animal Re-Identification (Animal Re-ID) directly on low-power, memory-constrained devices such as collar tags and microcontroller units (MCUs), essential for wildlife monitoring and precision livestock management in remote environments with limited connectivity. First, the authors analyze the performance gap between current state-of-the-art Animal Re-ID models and the capabilities of MCU-class hardware, revealing that traditional knowledge distillation from large teacher models offers limited improvements under strict constraints on memory and input resolution. Guided by these insights, they design a new, high-accuracy Animal Re-ID model by systematically scaling a MobileNetV2 backbone tailored for low-resolution inputs characteristic of MCU devices. The framework is validated on six public Animal Re-ID datasets, where the compact model achieves competitive retrieval accuracy while reducing the model size by more than 100 times compared to larger models. Additionally, a data-efficient fine-tuning strategy is proposed, enabling rapid adaptation to new environments with only three images per animal identity. On a self-collected cattle dataset, the system performs fully on-device inference with minimal accuracy loss and unchanged Top-1 accuracy compared to the cluster-based model. This demonstrates the practical feasibility of deploying adaptable, scalable Animal Re-ID solutions on MCU-class devices for real field applications. <div>
arXiv:2512.08198v1 Announce Type: new 
Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement</title>
<link>https://arxiv.org/abs/2512.08215</link>
<guid>https://arxiv.org/abs/2512.08215</guid>
<content:encoded><![CDATA[
<div> Keywords: human avatars, neural rendering, diffusion models, novel-view synthesis, 3D consistency<br /><br />Summary: The paper presents Blur2Sharp, an innovative framework designed to create lifelike human avatars with realistic pose variation and viewpoint flexibility. Traditional methods often produce images that lack geometric consistency across multiple views or suffer from low photorealism, leading to blurred outputs during complex motions and diverse viewing angles. Blur2Sharp addresses these challenges by integrating 3D-aware neural rendering via a Human NeRF model, which generates geometrically coherent multi-view images based on a single reference image, providing explicit 3D structural guidance. Following this, a diffusion model refines these renderings, enhancing image sharpness and detail preservation to maintain structural fidelity. The method further improves visual quality through hierarchical feature fusion that incorporates texture, normal vectors, and semantic priors extracted from parametric SMPL models. This combined approach enhances both global coherence and local detail accuracy in the synthesized images. Extensive experimental results show that Blur2Sharp outperforms existing state-of-the-art techniques in novel pose and novel view generation tasks. It is particularly robust in challenging situations that involve loose clothing and occlusions, demonstrating its efficacy and potential for advanced human avatar synthesis in computer vision and graphics applications. <div>
arXiv:2512.08215v1 Announce Type: new 
Abstract: The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisKnow: Constructing Visual Knowledge Base for Object Understanding</title>
<link>https://arxiv.org/abs/2512.08221</link>
<guid>https://arxiv.org/abs/2512.08221</guid>
<content:encoded><![CDATA[
<div> Visual Knowledge Base, Multi-modal data, Object understanding, AnimalKB, Knowledge graph completion<br /><br />Summary:<br /><br />1. The paper addresses the challenge of achieving in-depth object understanding in computer vision, moving beyond simple object recognition to comprehensively perceive object categories including components, appearance, relationships, and context.<br /><br />2. It emphasizes the need for systematic multi-modal data combining visual annotations (parts, attributes, co-occurrences) and textual knowledge to support advanced reasoning and question-answering tasks.<br /><br />3. The authors propose a Visual Knowledge Base structured as graphs, along with a construction framework named VisKnow, which extracts and integrates multi-modal, object-level knowledge from images and text, leveraging expert design and large-scale model applications.<br /><br />4. As a case study, they build AnimalKB, a visual knowledge base containing 406 animal categories, 22K textual knowledge triplets from encyclopedic texts, 420K images, and detailed region annotations for objects and parts.<br /><br />5. Experimental results demonstrate AnimalKB’s utility for improving zero-shot recognition, fine-grained visual question answering (VQA), and serving as a benchmark for knowledge graph completion and part segmentation, highlighting the potential of automated visual knowledge bases for enhancing visual understanding and practical applications. <div>
arXiv:2512.08221v1 Announce Type: new 
Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.08223</link>
<guid>https://arxiv.org/abs/2512.08223</guid>
<content:encoded><![CDATA[
<div> 3D object detection, prompt tuning, large language models, transfer learning, Scene-Oriented Prompt Pool (SOP²)  

<br /><br />Summary:  
This paper explores the application of prompt tuning methods, typically used in Natural Language Processing (NLP), within the field of 3D object detection. It investigates if a foundation model trained on a large-scale dataset like Waymo can be effectively adapted to various 3D detection scenarios using minimal parameter adjustments. The study specifically analyzes the impact of prompt tokens and prompt generators on model performance in 3D object detection tasks. Furthermore, the authors introduce a novel concept called the Scene-Oriented Prompt Pool (SOP²), designed to enhance the adaptability and effectiveness of prompt pools within 3D environments. The proposed SOP² demonstrates promising results, showing improvements in model tuning without requiring extensive retraining. By bridging techniques from language model tuning to 3D detection, the work highlights the strong potential for prompt-based methods to advance performance in this field. Ultimately, the paper aims to inspire further research into prompt tuning strategies tailored for complex 3D data, encouraging deeper exploration of how prompts can serve as a powerful tool for efficient and flexible adaptation of large-scale 3D detection models. <div>
arXiv:2512.08223v1 Announce Type: new 
Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New VVC profiles targeting Feature Coding for Machines</title>
<link>https://arxiv.org/abs/2512.08227</link>
<guid>https://arxiv.org/abs/2512.08227</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.08227v1  

Keywords: Versatile Video Coding, Feature Compression, MPEG-AI, Split Inference, BD-Rate  

<br /><br />Summary:  
1. Traditional video codecs are optimized for preserving perceptual quality based on human visual system models, but these approaches are not suitable for compressing intermediate neural network features in split inference systems.  
2. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity metrics irrelevant for compression evaluation.  
3. The paper investigates the application of Versatile Video Coding (VVC) to compress intermediate features under the MPEG-AI Feature Coding for Machines (FCM) standard.  
4. A detailed tool-level analysis is conducted to assess how individual VVC coding components affect compression efficiency and the accuracy of downstream vision tasks.  
5. Leveraging these insights, the authors propose three lightweight VVC profiles—Fast, Faster, and Fastest—tailored for feature compression:  
   - Fast profile yields a 2.96% BD-Rate improvement and decreases encoding time by 21.8%.  
   - Faster profile achieves a 1.85% BD-Rate improvement with a 51.5% speedup in encoding.  
   - Fastest profile reduces encoding time drastically by 95.6%, incurring only a 1.71% loss in BD-Rate.  
This work provides practical and efficient coding profiles for encoding neural network intermediate features effectively in machine-centric video coding scenarios. <div>
arXiv:2512.08227v1 Announce Type: new 
Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models</title>
<link>https://arxiv.org/abs/2512.08228</link>
<guid>https://arxiv.org/abs/2512.08228</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought, visual grounding, logical coherence, multimodal models, benchmark<br /><br />Summary:<br /><br />1. The paper introduces MM-CoT, a new diagnostic benchmark designed to evaluate Chain-of-Thought (CoT) reasoning in multimodal models (MMs) by focusing on both visual grounding and logical coherence rather than just generative capabilities.<br /><br />2. MM-CoT requires models to select a single event chain that meets two orthogonal constraints: visual consistency, ensuring each reasoning step is based on observable evidence, and logical coherence, ensuring causal and commonsense validity.<br /><br />3. The benchmark incorporates adversarial distractors that violate either visual consistency or logical coherence, enabling precise identification of reasoning failures in vision-language models.<br /><br />4. Evaluation of leading vision-language models on MM-CoT reveals that even state-of-the-art systems struggle, highlighting a significant gap between generative fluency and actual reasoning fidelity.<br /><br />5. MM-CoT exhibits low correlation with existing benchmarks, underscoring its unique ability to measure the combined challenge of visual grounding and logical reasoning, thereby providing a foundation for developing more faithful and coherent multimodal reasoning systems. <div>
arXiv:2512.08228v1 Announce Type: new 
Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems</title>
<link>https://arxiv.org/abs/2512.08229</link>
<guid>https://arxiv.org/abs/2512.08229</guid>
<content:encoded><![CDATA[
<div> Keywords: depth completion, RGB-D sensors, sparse depth sampling, surface normals, geometry-aware sampling<br /><br />Summary:<br />1. Accurate 3D perception is crucial for industrial robotic tasks such as manipulation, inspection, and navigation, yet existing RGB-D and stereo sensors often produce noisy, incomplete, or biased depth maps. <br />2. Depth completion techniques aim to generate dense and reliable depth maps by combining sparse depth data with RGB images; however, current pipelines typically use unrealistic sparse depth sampling methods that assume uniform randomness rather than reflecting real sensor behaviors. <br />3. This work introduces a novel normal-guided sparse depth sampling strategy that uses PCA-based surface normal estimation from the RGB-D point cloud to create a per-pixel depth reliability measure. <br />4. Using this geometry-aware reliability distribution, the sparse depth samples are drawn more realistically, better replicating sensor-specific reliability patterns influenced by surface geometry. <br />5. The proposed method is integrated with the Marigold-DC diffusion-based depth completion model and evaluated on the NYU Depth v2 dataset, demonstrating improved accuracy, reduction of artifacts near edges and discontinuities, and more realistic training conditions that reflect actual sensor behavior. <div>
arXiv:2512.08229v1 Announce Type: new 
Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastBEV++: Fast by Algorithm, Deployable by Design</title>
<link>https://arxiv.org/abs/2512.08237</link>
<guid>https://arxiv.org/abs/2512.08237</guid>
<content:encoded><![CDATA[
<div> FastBEV++, Bird's-Eye-View, view transformation, depth-aware fusion, real-time performance<br /><br />Summary:<br /><br />1. FastBEV++ addresses the challenge in camera-only Bird's-Eye-View (BEV) perception, balancing high performance with ease of deployment on vehicles by overcoming reliance on costly view transformations and specialized platform-specific kernels.<br /><br />2. The framework introduces a novel "Deployable by Design" view transformation method that breaks down complex projections into a standard Index-Gather-Reshape pipeline, executed solely with basic native operators like gather and matrix multiplication.<br /><br />3. This approach removes the requirement for custom CUDA kernels, enabling full compatibility with TensorRT and making the system highly portable across platforms.<br /><br />4. On the algorithmic side, FastBEV++ integrates a depth-aware fusion mechanism that is jointly learned, supported by temporal aggregation and robust data augmentation, enhancing the geometric accuracy of BEV representations.<br /><br />5. Validations on the nuScenes benchmark demonstrate that FastBEV++ achieves a new state-of-the-art NDS score of 0.359 and real-time speeds exceeding 134 FPS on automotive-grade hardware such as Tesla T4, making it a mature, scalable, and efficient solution for production-level autonomous systems. <div>
arXiv:2512.08237v1 Announce Type: new 
Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridToken-VLM: Hybrid Token Compression for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08240</link>
<guid>https://arxiv.org/abs/2512.08240</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, hybrid compression, semantic disentanglement, MGVQ quantization, voco token<br /><br />Summary:<br /><br />1. Vision-language models (VLMs) face computational challenges due to the quadratic cost of processing hundreds of visual patch tokens when integrated with large language models (LLMs), resulting in memory strain and limited context windows.<br /><br />2. Existing compression strategies either use continuous compression, which reduces semantic clarity like object identities, or discrete quantization, which loses fine-grained visual details such as textures.<br /><br />3. The proposed HTC-VLM framework introduces a hybrid dual-channel approach that separates semantic information and appearance: a continuous pathway processes fine-grained ViT patches, while a discrete pathway uses multi-granularity vector quantization (MGVQ) to produce symbolic anchors represented by four tokens.<br /><br />4. These dual representations combine into a 580-token hybrid sequence and are further compressed into a single voco token, enabled by a disentanglement attention mask and bottleneck mechanism, which balances efficiency and grounded semantic representation.<br /><br />5. HTC-VLM demonstrates superior performance with an average retention of 87.2% across seven vision-language benchmarks, outperforming continuous baseline methods at 81.0%, while achieving a 580-to-1 compression ratio. Attention studies confirm the voco token favors discrete semantic anchors, validating the hybrid framework’s effectiveness in resolving the trade-off between efficiency and fidelity for scalable VLMs. <div>
arXiv:2512.08240v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI</title>
<link>https://arxiv.org/abs/2512.08243</link>
<guid>https://arxiv.org/abs/2512.08243</guid>
<content:encoded><![CDATA[
<div> Keywords: Residual-SwinCA-Net, breast lesion segmentation, Swin Transformer, Multi-Scale Channel Attention, BUSI dataset<br /><br />Summary:  
This study proposes a novel deep hybrid segmentation framework called Residual-SwinCA-Net designed for breast lesion segmentation in ultrasound images. The framework integrates locally correlated feature extraction through residual CNN modules and global dependency learning via customized Swin Transformer blocks with internal residual pathways, which improve gradient stability and facilitate global feature fusion. To enhance tissue continuity, suppress ultrasound noise, and emphasize fine structural transitions, a Laplacian-of-Gaussian regional operator is applied, along with a boundary-oriented operator to maintain the morphological integrity of malignant lesion contours. The model employs a stage-wise contraction strategy by progressively reducing feature maps to capture scale invariance and improve robustness to structural variability. Each decoder level incorporates a Multi-Scale Channel Attention and Squeezing (MSCAS) module that selectively emphasizes salient encoder features, retains discriminative global and complementary local information, and suppresses redundant activations efficiently. Finally, a Pixel-Attention module adaptively weighs malignant lesion pixels to encode class-relevant spatial cues while reducing background noise interference. The Residual-SwinCA-Net was tested on the publicly available BUSI dataset, outperforming existing CNN and ViT models with a mean accuracy of 99.29%, IoU of 98.74%, and Dice score of 0.9041. The framework significantly enhances lesion diagnostic performance, aiding timely and accurate clinical decision-making. <div>
arXiv:2512.08243v1 Announce Type: new 
Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.08247</link>
<guid>https://arxiv.org/abs/2512.08247</guid>
<content:encoded><![CDATA[
<div> Future Temporal Knowledge Distillation, 3D Object Detection, Autonomous Driving, Knowledge Distillation, NuScenes Dataset<br /><br />Summary: This paper addresses the challenge of improving online camera-based temporal 3D object detection for autonomous driving by transferring knowledge from offline models that utilize future frames. Existing knowledge distillation (KD) methods mainly focus on spatial features or temporal relations but fail to effectively incorporate future frame information due to strict frame alignment requirements. To overcome this, the authors propose Future Temporal Knowledge Distillation (FTKD), a sparse query-based framework that allows an online student model to learn from an offline teacher model’s future frame features without needing strict alignment. FTKD incorporates a future-aware feature reconstruction strategy that encourages capturing future information and introduces future-guided logit distillation to exploit the teacher model’s stable foreground and background context. The method is compatible with two state-of-the-art 3D object detection baselines and demonstrates significant improvements, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset. Additionally, FTKD improves velocity estimation accuracy while maintaining the same inference cost, making it a practical solution for real-time autonomous driving systems. <div>
arXiv:2512.08247v1 Announce Type: new 
Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.08253</link>
<guid>https://arxiv.org/abs/2512.08253</guid>
<content:encoded><![CDATA[
<div> Few-shot learning, 3D point cloud, semantic segmentation, prototype learning, contrastive loss<br /><br />Summary:<br /><br />Few-shot 3D point cloud semantic segmentation (FS-3DSeg) focuses on segmenting new classes using only a few labeled examples. Existing prototype learning methods typically generate class prototypes solely from the support set, which risks prototype bias due to insufficient consideration of the query data distribution. This bias causes prototypes to overfit support set features and reduces segmentation accuracy under distribution shifts between support and query sets. To address this, the paper introduces a Query-aware Hub Prototype (QHP) learning method that models semantic correlations between support and query samples explicitly. The method includes a Hub Prototype Generation (HPG) module that constructs a bipartite graph linking support and query points, identifies frequently connected support hubs, and produces prototypes aligned with query semantics. Additionally, the Prototype Distribution Optimization (PDO) module refines prototypes by leveraging a purity-reweighted contrastive loss, which pulls ambiguous prototypes and poor hubs closer to class centers, mitigating noisy or outlier influences. Experimental validation on the S3DIS and ScanNet datasets demonstrates that QHP significantly outperforms state-of-the-art methods, effectively reducing the semantic gap between support-based prototypes and the query distribution, thereby enhancing few-shot segmentation performance in 3D point clouds. <div>
arXiv:2512.08253v1 Announce Type: new 
Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFP: Real-World Scene Recovery Using Spatial and Frequency Priors</title>
<link>https://arxiv.org/abs/2512.08254</link>
<guid>https://arxiv.org/abs/2512.08254</guid>
<content:encoded><![CDATA[
<div> Spatial prior, Frequency prior, Scene recovery, Image degradation, Weighted fusion<br /><br />Summary: This paper addresses the challenge of real-world scene recovery by proposing a novel method called Spatial and Frequency Priors (SFP). First, it identifies limitations in existing approaches that either use a single prior—insufficient for handling multiple degradations—or rely on complex networks trained on synthetic data that struggle to generalize to diverse real scenarios. Second, in the spatial domain, the authors discover that the inverse of a degraded image projects along its spectral direction in a manner resembling scene transmission. Leveraging this spatial prior, they estimate the transmission map to recover scenes affected by scattering degradation. Third, in the frequency domain, a mask is constructed for adaptive frequency enhancement, guided by two novel priors: one assumes that the mean intensity of the DC components across color channels in the degraded image approximates that of the clear image, while the other observes that low radial frequencies below 0.001 typically make up about 1% of the total spectral magnitude in clear images. Fourth, the method integrates spatial restoration, frequency enhancement, and salient input features through a weighted fusion strategy to produce the final recovered image. Lastly, extensive experiments demonstrate that SFP consistently outperforms existing techniques under various degradation conditions, proving its effectiveness and generalizability in real-world scene recovery tasks. <div>
arXiv:2512.08254v1 Announce Type: new 
Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera</title>
<link>https://arxiv.org/abs/2512.08262</link>
<guid>https://arxiv.org/abs/2512.08262</guid>
<content:encoded><![CDATA[
<div> Keywords: extrinsic calibration, LiDAR, RADAR, camera sensors, deep learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurate extrinsic calibration of LiDAR, RADAR, and camera sensors, which is crucial for reliable perception in autonomous vehicles.<br />2. It identifies that current calibration methods struggle due to mechanical vibrations and cumulative sensor drift in dynamic environments.<br />3. The authors propose RLCNet, a novel end-to-end trainable deep learning framework designed for simultaneous online calibration of multimodal sensors.<br />4. RLCNet is validated on real-world datasets and is optimized for practical deployment, showing robust performance across diverse environments.<br />5. To support real-time operation, the paper introduces an online calibration framework featuring a weighted moving average and outlier rejection to dynamically adjust calibration parameters.<br />6. This framework reduces prediction noise and improves resilience against sensor drift.<br />7. An ablation study is performed to demonstrate the importance of the architectural choices in the proposed network.<br />8. The results indicate that RLCNet outperforms existing methods in terms of accuracy and robustness, making it a superior approach for multimodal sensor calibration in autonomous vehicles. <div>
arXiv:2512.08262v1 Announce Type: new 
Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoX: Egocentric Video Generation from a Single Exocentric Video</title>
<link>https://arxiv.org/abs/2512.08269</link>
<guid>https://arxiv.org/abs/2512.08269</guid>
<content:encoded><![CDATA[
<div> Egocentric perception, exocentric video, video diffusion models, geometry-guided self-attention, egocentric video generation  

<br /><br />Summary:  
This paper introduces EgoX, a novel framework designed to convert exocentric (third-person) videos into egocentric (first-person) videos, addressing the challenges of extreme camera pose variations and minimal view overlap. EgoX builds on pretrained large-scale spatio-temporal video diffusion models, adapting them using lightweight LoRA techniques to effectively generate egocentric views from a single exocentric input. The framework employs a unified conditioning strategy that merges exocentric and egocentric priors through width and channel-wise concatenation, facilitating a more coherent transformation between perspectives. To maintain geometric consistency and enhance visual fidelity, EgoX incorporates a geometry-guided self-attention mechanism that focuses attention on spatially relevant regions of the scene. Experimental results demonstrate that EgoX produces visually realistic and geometrically coherent egocentric videos even when applied to unseen and in-the-wild videos, highlighting its robustness and scalability. This work advances immersive video understanding by enabling faithful content preservation and plausible synthesis of unseen areas, opening new avenues for egocentric video generation from exocentric inputs. <div>
arXiv:2512.08269v1 Announce Type: new 
Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAVAS: Physics-Aware Video-to-Audio Synthesis</title>
<link>https://arxiv.org/abs/2512.08282</link>
<guid>https://arxiv.org/abs/2512.08282</guid>
<content:encoded><![CDATA[
<div> Keywords: Video-to-Audio synthesis, physical reasoning, latent diffusion, object interaction, Audio-Physics Correlation Coefficient (APCC)  

<br /><br />Summary:  
This paper introduces Physics-Aware Video-to-Audio Synthesis (PAVAS), a novel approach that integrates physical reasoning into video-to-audio (V2A) generation. Unlike previous models that rely mainly on appearance-driven correlations, PAVAS incorporates physical parameters to generate more realistic sounds. The method employs a Physics-Driven Audio Adapter (Phy-Adapter) that utilizes object-level physical information, such as mass and velocity, estimated by the Physical Parameter Estimator (PPE). The PPE leverages a Vision-Language Model (VLM) to infer object mass and combines this with a segmentation-based dynamic 3D reconstruction module to recover motion trajectories and compute velocity. These physical inputs help the model produce audio that better reflects real-world sound generation processes. To evaluate physical realism, the authors create VGG-Impact, a new benchmark dataset focused on object-object interactions, and propose the Audio-Physics Correlation Coefficient (APCC) metric to quantify the alignment between physical parameters and generated audio. Extensive experiments demonstrate that PAVAS outperforms current state-of-the-art V2A methods, achieving superior physical plausibility and perceptual coherence in the synthesized sounds. Additional demo videos and resources are available on the project website. <div>
arXiv:2512.08282v1 Announce Type: new 
Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</title>
<link>https://arxiv.org/abs/2512.08294</link>
<guid>https://arxiv.org/abs/2512.08294</guid>
<content:encoded><![CDATA[
<div> Keywords: subject-driven generation, identity fidelity, video-derived dataset, vision-language models, image manipulation<br /><br />Summary:<br /><br />1. The paper addresses challenges in subject-driven image generation where current models often fail to maintain accurate reference identities and struggle with scenes containing multiple subjects.<br />2. To overcome these limitations, the authors introduce OpenSubject, a large-scale video-derived dataset featuring 2.5 million samples and 4.35 million images specifically designed for subject-driven generation and manipulation tasks.<br />3. The dataset construction involves a four-stage pipeline: (i) Video Curation through resolution and aesthetic filtering to select high-quality clips, (ii) Cross-Frame Subject Mining and Pairing using vision-language models (VLMs) to ensure category consensus and diversity-aware pairing of image pairs, (iii) Identity-Preserving Reference Image Synthesis deploying segmentation map-guided outpainting and box-guided inpainting with geometry-aware augmentations to retain subject identity, and (iv) Verification and Captioning where a VLM validates the synthesized samples, re-synthesizes failed ones, and generates descriptive captions.<br />4. The authors also propose a benchmark that evaluates multiple aspects including identity fidelity, prompt adherence, manipulation consistency, and background consistency, all assessed with a VLM-based judge.<br />5. Extensive experiments demonstrate that training models with OpenSubject significantly improves the performance of subject-driven generation and manipulation, especially in complex multi-subject scenes. <div>
arXiv:2512.08294v1 Announce Type: new 
Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation</title>
<link>https://arxiv.org/abs/2512.08309</link>
<guid>https://arxiv.org/abs/2512.08309</guid>
<content:encoded><![CDATA[
<div> Procedural Noise, Diffusion Models, Infinite Generation, Terrain Synthesis, Planetary Scale  

<br /><br />Summary: This article introduces Terrain Diffusion, a novel AI-driven approach that advances traditional procedural noise functions like Perlin noise by leveraging diffusion models for enhanced realism and large-scale coherence in terrain generation. Terrain Diffusion preserves the critical advantages of procedural noise—seamless infinite extent, seed-consistency, and constant-time random access—while addressing its limitations in detail and fidelity. At the heart of the method is InfiniteDiffusion, an innovative algorithm enabling real-time, seamless, and infinite generation of landscapes. The approach uses a hierarchical arrangement of diffusion models to integrate broad planetary context with intricate local terrain features. A compact Laplacian encoding system is employed to maintain output stability across Earth-scale dynamic ranges, ensuring consistent quality at varying scales. Additionally, an open-source infinite-tensor framework supports efficient manipulation of boundless data structures using constant memory, facilitating practical implementation. Few-step consistency distillation further enhances generation efficiency, reducing computational overhead. Collectively, these advancements position diffusion models as a practical and scalable foundation for procedural world generation, capable of coherently and controllably synthesizing entire planets without spatial limitations, making it a significant step forward for applications in gaming, simulation, and virtual world creation. <div>
arXiv:2512.08309v1 Announce Type: new 
Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDM: Geometry-aware Distribution Matching for Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.08317</link>
<guid>https://arxiv.org/abs/2512.08317</guid>
<content:encoded><![CDATA[
<div> Dataset Distillation, Geometry-aware, Distribution Matching, Manifold Learning, Optimal Transport<br /><br />Summary:<br /><br />1. The paper addresses dataset distillation, which aims to create a small synthetic subset of data that enables training models with performance comparable to those trained on the full dataset. 2. Existing distribution-matching methods focus on Euclidean spaces, thus only capturing linear data structures and ignoring important intrinsic geometries such as curvature that real-world high-dimensional data often exhibit. 3. The authors propose GeoDM, a novel geometry-aware distribution-matching framework that models data in a Cartesian product of Euclidean, hyperbolic, and spherical manifolds, capturing flat, hierarchical, and cyclical data structures within a unified framework. 4. GeoDM introduces learnable curvature and weighting parameters for each geometry type, allowing the model to adaptively fit the underlying data manifold geometry during distillation. 5. An optimal transport loss is designed to improve the fidelity of the matched distributions, and theoretical analysis demonstrates that geometry-aware matching across product spaces can achieve tighter generalization error bounds compared to purely Euclidean approaches. 6. Extensive experiments on standard benchmarks verify that GeoDM outperforms state-of-the-art dataset distillation methods and is robust across different distribution-matching strategies and geometric settings. <div>
arXiv:2512.08317v1 Announce Type: new 
Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge</title>
<link>https://arxiv.org/abs/2512.08323</link>
<guid>https://arxiv.org/abs/2512.08323</guid>
<content:encoded><![CDATA[
<div> Teeth landmark detection, 3D intraoral scans, deep learning, orthodontics, MICCAI challenge<br /><br />Summary:  
Teeth landmark detection plays a crucial role in clinical orthodontics by enabling accurate diagnostics, personalized treatment planning, and effective monitoring of dental treatments. The task is complicated by the complex geometry of individual teeth and significant inter-personal variation, making it challenging to achieve precise identification. To overcome these difficulties, the use of advanced techniques such as deep learning is essential for reliable detection of 3D tooth landmarks. Addressing this need, the 3DTeethLand challenge was organized in conjunction with the MICCAI 2024 conference. This challenge aimed to foster the development of algorithms capable of accurately detecting teeth landmarks from intraoral 3D scans. A notable contribution of the challenge is the introduction of the first publicly available dataset dedicated to 3D teeth landmark detection. This dataset provides a valuable benchmark for evaluating the effectiveness of state-of-the-art methods in this domain. Furthermore, the challenge serves to encourage the research community to contribute innovative methodologies that can help address this clinically significant problem, ultimately improving outcomes in dental care through enhanced diagnostic and treatment capabilities. <div>
arXiv:2512.08323v1 Announce Type: new 
Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification</title>
<link>https://arxiv.org/abs/2512.08325</link>
<guid>https://arxiv.org/abs/2512.08325</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Motion Magnification, Diffusion Model, Optical Flow, Noise Suppression, Motion Amplification<br /><br />Summary:<br /><br />1. The paper introduces GeoDiffMM, a novel diffusion-based Lagrangian framework for Video Motion Magnification (VMM) that uses optical flow as a geometric cue to enable structurally consistent motion amplification.<br />2. Existing Eulerian VMM approaches face challenges in separating photon noise from true micro-motions when displacements are very small; GeoDiffMM addresses these challenges by explicitly integrating geometry-aware optical flow.<br />3. The authors propose a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise, providing supervision that improves geometry accuracy and generalization of the optical flow estimation.<br />4. GeoDiffMM employs a Diffusion Motion Magnifier, which conditions the denoising process on optical flow as a geometry prior and a learnable magnification factor, selectively amplifying motion components aligned with scene semantics while suppressing irrelevant noise.<br />5. Finally, the framework includes a Flow-based Video Synthesis module that accurately maps the amplified motion back to the image domain, producing high-fidelity outputs.<br />Extensive experiments on both real and synthetic datasets demonstrate that GeoDiffMM outperforms state-of-the-art VMM methods by significantly enhancing motion magnification quality, especially in low-displacement and noise-prone scenarios. <div>
arXiv:2512.08325v1 Announce Type: new 
Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low Rank Support Quaternion Matrix Machine</title>
<link>https://arxiv.org/abs/2512.08327</link>
<guid>https://arxiv.org/abs/2512.08327</guid>
<content:encoded><![CDATA[
<div> Keywords: Quaternion, Low-rank, Support Quaternion Matrix Machine, Color image classification, ADMM<br /><br />Summary:<br /><br />This article introduces a novel method for color image classification named Low-rank Support Quaternion Matrix Machine (LSQMM), which leverages quaternion algebra to treat RGB channels as pure quaternions, thereby preserving intrinsic inter-channel relationships. Unlike conventional approaches that represent input features as vectors, matrices, or third-order tensors, LSQMM models color images in the quaternion domain to effectively utilize channel coupling. To promote low-rank structures arising from strongly correlated color channels, the authors incorporate a quaternion nuclear norm regularization term, extending the classical matrix nuclear norm concept to quaternions. The classification model is formulated as a hinge loss optimization problem augmented by this quaternion nuclear norm regularizer. To solve the resulting optimization, an iterative algorithm based on the Alternating Direction Method of Multipliers (ADMM) is proposed, tailored for quaternion optimization challenges. The method is evaluated on multiple color image classification datasets, demonstrating superior performance in classification accuracy, robustness against noise and variations, and computational efficiency. Comparative experiments against state-of-the-art classifiers—including support vector machines (SVMs), support matrix machines (SMMs), and support tensor machines (STMs)—highlight the advantages of LSQMM, suggesting its potential as an effective tool for practical color image classification tasks. <div>
arXiv:2512.08327v1 Announce Type: new 
Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08329</link>
<guid>https://arxiv.org/abs/2512.08329</guid>
<content:encoded><![CDATA[
<div> Keywords: image protection, adversarial perturbations, feature-space analysis, frequency-domain, generative models<br /><br />Summary:<br /><br />This paper investigates recent image protection techniques like Glaze and Nightshade, which add subtle adversarial perturbations to thwart text-to-image generative models. While these methods are empirically effective, the study aims to understand their internal structure, detectability, and how they affect image representations. Using a combined white-box and black-box analytical framework, the authors perform latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization. They find that these image protection perturbations are structured, low-entropy signals closely tied to the original image content across representational, spatial, and spectral dimensions. Rather than causing a broad shift in image representation, protected images maintain their content-driven feature organization with distinct protection-specific substructures. Detectability of these perturbations depends on entropy, spatial placement, and frequency alignment, and stacking protections increases the detectable structure. Frequency-domain analysis reveals that Glaze and Nightshade reshape energy along image-aligned frequency axes instead of adding indiscriminate noise. Overall, the study demonstrates that image protection operates via structured feature-level changes instead of overt semantic shifts, explaining how these protective signals remain visually imperceptible yet systematically identifiable. This work enhances the interpretability of adversarial image protections and guides future development of defenses and detection methods for generative AI systems. <div>
arXiv:2512.08329v1 Announce Type: new 
Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08330</link>
<guid>https://arxiv.org/abs/2512.08330</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D representation learning, diffusion model, contrastive learning, self-supervised learning, PointDico  

<br /><br />Summary:  
1. The paper addresses the challenges of self-supervised 3D representation learning caused by the unordered nature and uneven density of 3D data, which limits the effectiveness of existing contrastive and generative methods.  
2. It identifies that contrastive models are prone to overfitting, while 3D Masked Autoencoders face difficulties in processing unordered point clouds.  
3. To overcome these issues, the authors propose PointDico, a novel framework that integrates diffusion-based generative modeling and contrastive learning via knowledge distillation, where the diffusion model guides the contrastive model.  
4. A hierarchical pyramid conditional generator is introduced in PointDico to extract geometric features at multiple scales, capturing detailed and structural information effectively.  
5. The model uses a dual-channel design to merge local and global contextual features, enhancing the overall representation quality.  
6. Experimentally, PointDico sets a new state-of-the-art in 3D representation learning benchmarks with 94.32% accuracy on ScanObjectNN and 86.5% instance mIoU on ShapeNetPart, demonstrating its superior capability in understanding 3D point cloud data. <div>
arXiv:2512.08330v1 Announce Type: new 
Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening</title>
<link>https://arxiv.org/abs/2512.08331</link>
<guid>https://arxiv.org/abs/2512.08331</guid>
<content:encoded><![CDATA[
<div> Pansharpening, adaptive convolution, Bi^2MAC, remote sensing, computational efficiency<br /><br />Summary:<br /><br />Pansharpening is a process that fuses a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral (HRMS) image. Traditional deep learning methods for this task struggle to adapt effectively to regional heterogeneity within feature representations. Existing adaptive convolution approaches try to address this but often result in high computational costs and limited effectiveness in capturing heterogeneous regions. To tackle these issues, the authors propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which leverages different region types while optimally allocating computational resources. Bi^2MAC includes a lightweight module that generates both soft and hard masks; soft masks preliminarily modulate input features, while hard masks guide separated processing branches for different region types. Less important features flow through a compact branch for efficient global processing, whereas complex heterogeneous features are handled by a focused branch that applies more computational power for detailed modeling. Extensive experiments across multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art performance, significantly reduces training time and parameter counts, and maintains the lowest computational cost among adaptive convolution models for pansharpening tasks. <div>
arXiv:2512.08331v1 Announce Type: new 
Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</title>
<link>https://arxiv.org/abs/2512.08334</link>
<guid>https://arxiv.org/abs/2512.08334</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid Splatting, Gaussian primitives, reflection baking, rendering speed, memory optimization<br /><br />Summary: This paper introduces HybridSplat, a novel Hybrid Splatting mechanism designed to improve the rendering of complex reflections in real-world scenes using 3D Gaussian splatting. The core innovation is the reflection-baked Gaussian tracing technique, which integrates view-dependent reflections directly within each Gaussian primitive. This approach utilizes tile-based Gaussian splatting for efficient reflection rendering. HybridSplat unifies reflective Gaussian primitives with base primitives into a hybrid framework, enabling high-fidelity scene reconstruction. Additionally, the authors present a pipeline-level acceleration method along with reflection-sensitive Gaussian pruning to significantly reduce model size. These strategies collectively enhance rendering speed and decrease memory consumption without compromising reflection quality. Extensive evaluations demonstrate that HybridSplat achieves approximately a 7x increase in rendering speed on complex reflective scenes from datasets like Ref-NeRF and NeRF-Casting. Furthermore, it requires four times fewer Gaussian primitives compared to similar ray-tracing-based Gaussian splatting techniques. Overall, HybridSplat establishes a new state-of-the-art performance benchmark in photorealistic novel view synthesis for challenging reflective environments by balancing efficiency, memory usage, and visual fidelity. <div>
arXiv:2512.08334v1 Announce Type: new 
Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation</title>
<link>https://arxiv.org/abs/2512.08337</link>
<guid>https://arxiv.org/abs/2512.08337</guid>
<content:encoded><![CDATA[
<div> Keywords: BOLD image generation, T1-weighted MRI, DINOv3, multi-slice attention, self-supervised learning<br /><br />Summary:  
The study addresses the challenge of generating BOLD (Blood Oxygen Level Dependent) images from T1-weighted (T1w) MRI scans, which is valuable for recovering missing or corrupted BOLD data and facilitating subsequent analysis. The authors introduce DINO-BOLDNet, an innovative framework guided by DINOv3, a self-supervised transformer model. This framework combines a frozen DINOv3 encoder that captures detailed within-slice structural features with a lightweight, trainable decoder. A dedicated slice-attention module integrates contextual information across adjacent slices to enhance spatial consistency. Subsequently, a multi-scale decoder restores detailed functional contrasts in the generated images. The model employs a perceptual loss based on DINO features to ensure structural and textural fidelity between the predicted and ground-truth BOLD images in the transformer feature space. Experiments conducted on a clinical dataset of 248 subjects demonstrate that DINO-BOLDNet outperforms a conditional GAN baseline in terms of PSNR (Peak Signal-to-Noise Ratio) and MS-SSIM (Multi-Scale Structural Similarity Index). This work represents the first successful attempt to directly generate mean BOLD images from T1w images, showcasing the promise of self-supervised transformer guidance in bridging structural to functional MRI representations. <div>
arXiv:2512.08337v1 Announce Type: new 
Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels</title>
<link>https://arxiv.org/abs/2512.08358</link>
<guid>https://arxiv.org/abs/2512.08358</guid>
<content:encoded><![CDATA[
<div> Monocular 3D tracking, dense 3D tracking, world-centric coordinate system, tracking upsampler, camera pose estimation<br /><br />Summary:<br /><br />This paper addresses the challenges in monocular 3D tracking, which aims to capture long-term 3D pixel motion from single-camera videos. Existing methods struggle to separate camera movement from foreground dynamic objects and cannot effectively track newly appearing dynamic subjects densely. To overcome these limitations, the authors propose TrackingWorld, a novel framework that enables dense 3D tracking of nearly all pixels within a world-centric coordinate system. The approach introduces a tracking upsampler that converts sparse 2D tracks into dense 2D tracks efficiently. Furthermore, to handle newly emerging objects, the upsampler is applied across all frames, with redundant tracks in overlapping areas removed to maintain compactness. Finally, the system employs an optimization-based framework to convert the dense 2D tracks into 3D trajectories by jointly estimating camera poses and 3D point coordinates. Evaluations on synthetic and real-world datasets demonstrate the method achieves accurate and dense 3D tracking within a consistent world-centric frame, outperforming existing approaches in handling dynamic scenes and camera motion separation. <div>
arXiv:2512.08358v1 Announce Type: new 
Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation</title>
<link>https://arxiv.org/abs/2512.08362</link>
<guid>https://arxiv.org/abs/2512.08362</guid>
<content:encoded><![CDATA[
<div> Keywords: fire detection, SCU-CGAN, U-Net, CBAM, dataset augmentation

<br /><br />Summary: This paper addresses the challenge of limited fire image datasets for improving household fire detection systems. The authors propose SCU-CGAN, a novel generative adversarial network combining U-Net architecture, Convolutional Block Attention Module (CBAM), and an additional discriminator, designed to generate realistic fire images from non-fire images. The model's performance is evaluated and shown to outperform existing generative models, specifically achieving a 41.5% improvement in the Kernel Inception Distance (KID) score compared to CycleGAN, indicating superior image quality. The augmented dataset produced by SCU-CGAN is then utilized to enhance fire detection model accuracy without modifying their structures. Experiments with the YOLOv5 nano fire detection model demonstrate significant gains, particularly a 56.5% increase in mean average precision (mAP@0.5:0.95), underscoring the effectiveness of the augmented data. This approach highlights the potential of GAN-based data augmentation in overcoming dataset scarcity, ultimately helping to improve early fire detection capabilities in IoT-enabled household environments. <div>
arXiv:2512.08362v1 Announce Type: new 
Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss</title>
<link>https://arxiv.org/abs/2512.08374</link>
<guid>https://arxiv.org/abs/2512.08374</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Pre-Norm architecture, norm disparity, asymmetric update dynamic, LayerNorm correction<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) integrate pre-trained vision encoders with language models, enabling advanced multimodal understanding. 2. The common Pre-Norm architecture in these models causes a significant norm disparity where visual tokens have much higher norms compared to textual tokens. 3. This disparity leads to an asymmetric update dynamic, wherein high-norm visual tokens experience “representational inertia,” updating and adapting more slowly than text tokens during training. 4. This imbalance hampers effective cross-modal feature fusion, reducing the overall performance of MLLMs. 5. The authors provide a formal theoretical analysis of this phenomenon and empirically verify that norm disparity and asymmetric updates persist across various mainstream MLLMs. 6. To address the issue, they propose a simple fix: adding a carefully initialized LayerNorm layer immediately after the visual projector to align the norms of visual and text tokens. 7. Experiments on the LLaVA-1.5 architecture demonstrate that this intervention significantly improves performance on diverse multimodal benchmarks and also boosts text-only tasks like MMLU. 8. The results suggest that correcting the architectural norm imbalance leads to a more balanced and capable multimodal language model overall. <div>
arXiv:2512.08374v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions</title>
<link>https://arxiv.org/abs/2512.08378</link>
<guid>https://arxiv.org/abs/2512.08378</guid>
<content:encoded><![CDATA[
<div> Keywords: image enhancement, noise suppression, gradient-domain weighted guided filter, Retinex model, multi-exposure fusion<br /><br />Summary:<br /><br />This paper addresses the problem of image degradation under challenging lighting conditions, which negatively affects vision-based applications. The authors propose a novel framework that performs simultaneous image enhancement and noise suppression in complex illumination scenarios. The first step involves using a gradient-domain weighted guided filter (GDWGIF) to accurately estimate illumination and improve overall image quality. Subsequently, the Retinex model is applied to decompose the image into illumination and reflection components. These components are processed in parallel: the illumination layer is corrected to enhance lighting conditions, while the reflection layer is enhanced to improve details and visual quality. Finally, the framework optimizes the image dynamic range through a combination of multi-exposure fusion and linear stretching techniques. Experiments conducted on real-world datasets from practical applications demonstrate that this method outperforms state-of-the-art approaches in both contrast enhancement and noise suppression. The proposed solution effectively balances noise reduction and detail preservation, making it suitable for diverse and complex lighting conditions. <div>
arXiv:2512.08378v1 Announce Type: new 
Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Digital Facial Retouching utilizing Face Beauty Information</title>
<link>https://arxiv.org/abs/2512.08397</link>
<guid>https://arxiv.org/abs/2512.08397</guid>
<content:encoded><![CDATA[
<div> Facial Retouching, Face Recognition, Beauty Assessment, Retouching Detection, Artificial Intelligence<br /><br />Summary:<br /><br />1. Facial retouching is commonly used in social media, advertisements, and professional photo studios to enhance appearance by making individuals look younger and removing skin imperfections.<br />2. While retouching serves aesthetic purposes, it poses challenges when retouched images are used as biometric samples in face recognition systems.<br />3. Prior research has demonstrated that facial retouching can reduce the accuracy of face recognition, necessitating effective detection methods for retouching.<br />4. This work analyzes how facial retouching impacts beauty assessment algorithms and explores the use of various artificial intelligence-based feature extraction techniques to improve detection of retouched images.<br />5. The study also investigates whether leveraging face beauty metrics can enhance the performance of retouching detection algorithms.<br />6. In scenarios where the specific retouching algorithm used in an attack is unknown, the proposed approach achieved a Detection Equal Error Rate (D-EER) of 1.1% on single-image detection, indicating high effectiveness in identifying retouched biometric samples. <div>
arXiv:2512.08397v1 Announce Type: new 
Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</title>
<link>https://arxiv.org/abs/2512.08400</link>
<guid>https://arxiv.org/abs/2512.08400</guid>
<content:encoded><![CDATA[
<div> Fish re-identification, Electronic Monitoring, Vision Transformer, Hard triplet mining, AutoFish dataset  

<br /><br />Summary:  
This paper addresses the challenge of automating fish re-identification (Re-ID) to support fisheries management, as Electronic Monitoring (EM) systems produce vast amounts of video data that cannot be manually reviewed at scale. The authors introduce the AutoFish dataset, which simulates an EM system setup featuring conveyor belts and six visually similar fish species for Re-ID evaluation. They develop an optimized deep learning pipeline that significantly improves key identification metrics such as Rank-1 accuracy (R1) and mean average precision at k (mAP@k) by combining hard triplet mining with a specialized image transformation pipeline tailored to the dataset, including custom normalization. Their experiments show that the Vision Transformer-based Swin-T architecture consistently outperforms the traditional CNN-based ResNet-50 architecture, achieving a peak 41.65% mAP@k and 90.43% Rank-1 accuracy. A detailed error analysis highlights that the main difficulty in the task is distinguishing between visually similar individual fish within the same species, where variations in viewpoint have a greater negative impact than partial occlusion. The authors provide their source code and documentation openly to facilitate further research and application. <div>
arXiv:2512.08400v1 Announce Type: new 
Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos</title>
<link>https://arxiv.org/abs/2512.08406</link>
<guid>https://arxiv.org/abs/2512.08406</guid>
<content:encoded><![CDATA[
<div> Human Mesh Recovery, temporal consistency, occlusion robustness, video segmentation, multi-human inference<br /><br />Summary:<br /><br />1. The paper addresses key limitations of existing Human Mesh Recovery (HMR) models, particularly their temporal inconsistency and performance degradation under occlusion when applied to video sequences.<br /><br />2. The authors introduce SAM-Body4D, a training-free framework designed to enhance temporal stability and robustness for 3D human pose and shape reconstruction specifically from videos.<br /><br />3. SAM-Body4D leverages identity-consistent masklets generated by a promptable video segmentation model to utilize the inherent continuity of human motion across frames.<br /><br />4. An Occlusion-Aware module is employed to refine these masklets by recovering missing regions caused by occlusions, improving the accuracy of the reconstructed meshes.<br /><br />5. The refined masklets guide the SAM 3D Body model to produce full-body mesh trajectories that are consistent over time, and a padding-based parallel strategy is introduced to efficiently handle multi-human scenarios in videos.<br /><br />6. Experimental evaluations demonstrate that SAM-Body4D improves temporal stability and occlusion robustness on challenging real-world video data without necessitating any additional training or model retraining.<br /><br />7. The authors have made their code and demo publicly accessible, facilitating further research and practical application of the method. <div>
arXiv:2512.08406v1 Announce Type: new 
Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval</title>
<link>https://arxiv.org/abs/2512.08410</link>
<guid>https://arxiv.org/abs/2512.08410</guid>
<content:encoded><![CDATA[
<div> Video understanding, Multimodal Large Language Models, OneClip-RAG, video chunking, long-video benchmarks<br /><br />Summary:<br /><br />1. Most Multimodal Large Language Models (MLLMs) face challenges processing long videos due to excessive memory overhead, limiting them to only a few frames. 2. The paper introduces One-shot video-Clip based Retrieval Augmentation (OneClip-RAG), an effective and efficient paradigm that leverages video clips to enhance video understanding by maintaining knowledge integrity and semantic coherence. 3. OneClip-RAG incorporates a novel query-guided video chunking algorithm that unifies clip chunking and cross-modal retrieval into a single step, reducing redundant computations. 4. To improve instruction-following capability, the authors propose the SynLongVideo dataset and design a progressive training scheme for OneClip-RAG. 5. OneClip-RAG was integrated with five recent MLLMs and evaluated on long-video benchmarks, demonstrating significant performance improvements—such as boosting InternLV2 8B and Qwen2-VL 7B to GPT-4o-level performance on MLVU—and superior efficiency, enabling LLaVA-Video to understand up to one hour of video in under 2.2 minutes on a single NVIDIA RTX 4090 GPU. <div>
arXiv:2512.08410v1 Announce Type: new 
Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking</title>
<link>https://arxiv.org/abs/2512.08430</link>
<guid>https://arxiv.org/abs/2512.08430</guid>
<content:encoded><![CDATA[
<div> 6D pose estimation, multi-view depth, sparse transformer, TSDF, bin picking<br /><br />Summary:<br /><br />This paper addresses the challenge of accurate 6D pose estimation in densely packed, cluttered industrial bin-picking environments, where occlusions, reflections, and textureless parts pose significant difficulties. The authors introduce a depth-only approach that fuses multi-view depth maps into either a detailed 3D point cloud or a sparse Truncated Signed Distance Field (TSDF) for better scene representation. Central to the framework is a staged heatmap mechanism that generates scene-adaptive attention priors at multiple resolutions, focusing computational resources on foreground regions while managing memory consumption efficiently. A novel density-aware sparse transformer block is proposed to dynamically handle self-occlusions and the uneven spatial distribution of 3D data, enhancing the model’s ability to process close-range robotic perception tasks. By leveraging fully sparse operations, the framework supports high-resolution volumetric representations that capture fine geometric details critical for precise pose estimation in clutter. The method processes entire scenes integrally and uses a per-voxel voting strategy to predict simultaneous 6D poses for multiple target objects, improving efficiency and scalability. Validation on the IPD and MV-YCB multi-view datasets demonstrates competitive performance in challenging industrial and household bin-picking scenarios, confirming the effectiveness of the approach. <div>
arXiv:2512.08430v1 Announce Type: new 
Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training</title>
<link>https://arxiv.org/abs/2512.08439</link>
<guid>https://arxiv.org/abs/2512.08439</guid>
<content:encoded><![CDATA[
arXiv:2512.08439v1 Announce Type: new 
Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multispectral Sensors for Color Correction in Mobile Cameras</title>
<link>https://arxiv.org/abs/2512.08441</link>
<guid>https://arxiv.org/abs/2512.08441</guid>
<content:encoded><![CDATA[
arXiv:2512.08441v1 Announce Type: new 
Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.08445</link>
<guid>https://arxiv.org/abs/2512.08445</guid>
<content:encoded><![CDATA[
arXiv:2512.08445v1 Announce Type: new 
Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery</title>
<link>https://arxiv.org/abs/2512.08467</link>
<guid>https://arxiv.org/abs/2512.08467</guid>
<content:encoded><![CDATA[
arXiv:2512.08467v1 Announce Type: new 
Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention</title>
<link>https://arxiv.org/abs/2512.08477</link>
<guid>https://arxiv.org/abs/2512.08477</guid>
<content:encoded><![CDATA[
arXiv:2512.08477v1 Announce Type: new 
Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</title>
<link>https://arxiv.org/abs/2512.08478</link>
<guid>https://arxiv.org/abs/2512.08478</guid>
<content:encoded><![CDATA[
arXiv:2512.08478v1 Announce Type: new 
Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions</title>
<link>https://arxiv.org/abs/2512.08486</link>
<guid>https://arxiv.org/abs/2512.08486</guid>
<content:encoded><![CDATA[
arXiv:2512.08486v1 Announce Type: new 
Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs</title>
<link>https://arxiv.org/abs/2512.08498</link>
<guid>https://arxiv.org/abs/2512.08498</guid>
<content:encoded><![CDATA[
arXiv:2512.08498v1 Announce Type: new 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2512.08503</link>
<guid>https://arxiv.org/abs/2512.08503</guid>
<content:encoded><![CDATA[
arXiv:2512.08503v1 Announce Type: new 
Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08505</link>
<guid>https://arxiv.org/abs/2512.08505</guid>
<content:encoded><![CDATA[
arXiv:2512.08505v1 Announce Type: new 
Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds</title>
<link>https://arxiv.org/abs/2512.08506</link>
<guid>https://arxiv.org/abs/2512.08506</guid>
<content:encoded><![CDATA[
arXiv:2512.08506v1 Announce Type: new 
Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Images via Self-Calling Agent</title>
<link>https://arxiv.org/abs/2512.08511</link>
<guid>https://arxiv.org/abs/2512.08511</guid>
<content:encoded><![CDATA[
arXiv:2512.08511v1 Announce Type: new 
Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Real Weights: Hypercomplex Representations for Stable Quantization</title>
<link>https://arxiv.org/abs/2512.08524</link>
<guid>https://arxiv.org/abs/2512.08524</guid>
<content:encoded><![CDATA[
arXiv:2512.08524v1 Announce Type: new 
Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVP: Multiple View Prediction Improves GUI Grounding</title>
<link>https://arxiv.org/abs/2512.08529</link>
<guid>https://arxiv.org/abs/2512.08529</guid>
<content:encoded><![CDATA[
arXiv:2512.08529v1 Announce Type: new 
Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation</title>
<link>https://arxiv.org/abs/2512.08534</link>
<guid>https://arxiv.org/abs/2512.08534</guid>
<content:encoded><![CDATA[
arXiv:2512.08534v1 Announce Type: new 
Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement</title>
<link>https://arxiv.org/abs/2512.08535</link>
<guid>https://arxiv.org/abs/2512.08535</guid>
<content:encoded><![CDATA[
arXiv:2512.08535v1 Announce Type: new 
Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.08537</link>
<guid>https://arxiv.org/abs/2512.08537</guid>
<content:encoded><![CDATA[
arXiv:2512.08537v1 Announce Type: new 
Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation</title>
<link>https://arxiv.org/abs/2512.08542</link>
<guid>https://arxiv.org/abs/2512.08542</guid>
<content:encoded><![CDATA[
arXiv:2512.08542v1 Announce Type: new 
Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Iteration-Free Fixed-Point Estimator for Diffusion Inversion</title>
<link>https://arxiv.org/abs/2512.08547</link>
<guid>https://arxiv.org/abs/2512.08547</guid>
<content:encoded><![CDATA[
arXiv:2512.08547v1 Announce Type: new 
Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2512.08557</link>
<guid>https://arxiv.org/abs/2512.08557</guid>
<content:encoded><![CDATA[
arXiv:2512.08557v1 Announce Type: new 
Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain</title>
<link>https://arxiv.org/abs/2512.08560</link>
<guid>https://arxiv.org/abs/2512.08560</guid>
<content:encoded><![CDATA[
arXiv:2512.08560v1 Announce Type: new 
Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Neural Image Signal Processing</title>
<link>https://arxiv.org/abs/2512.08564</link>
<guid>https://arxiv.org/abs/2512.08564</guid>
<content:encoded><![CDATA[
arXiv:2512.08564v1 Announce Type: new 
Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Aware Test-Time Segmentation for Continual Domain Shifts</title>
<link>https://arxiv.org/abs/2512.08569</link>
<guid>https://arxiv.org/abs/2512.08569</guid>
<content:encoded><![CDATA[
arXiv:2512.08569v1 Announce Type: new 
Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis</title>
<link>https://arxiv.org/abs/2512.08572</link>
<guid>https://arxiv.org/abs/2512.08572</guid>
<content:encoded><![CDATA[
arXiv:2512.08572v1 Announce Type: new 
Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery</title>
<link>https://arxiv.org/abs/2512.08577</link>
<guid>https://arxiv.org/abs/2512.08577</guid>
<content:encoded><![CDATA[
arXiv:2512.08577v1 Announce Type: new 
Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Pollen Recognition in Optical and Holographic Microscopy Images</title>
<link>https://arxiv.org/abs/2512.08589</link>
<guid>https://arxiv.org/abs/2512.08589</guid>
<content:encoded><![CDATA[
arXiv:2512.08589v1 Announce Type: new 
Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning</title>
<link>https://arxiv.org/abs/2512.08606</link>
<guid>https://arxiv.org/abs/2512.08606</guid>
<content:encoded><![CDATA[
arXiv:2512.08606v1 Announce Type: new 
Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</title>
<link>https://arxiv.org/abs/2512.08625</link>
<guid>https://arxiv.org/abs/2512.08625</guid>
<content:encoded><![CDATA[
arXiv:2512.08625v1 Announce Type: new 
Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Densification and Depth from Perspective-based Blur</title>
<link>https://arxiv.org/abs/2512.08627</link>
<guid>https://arxiv.org/abs/2512.08627</guid>
<content:encoded><![CDATA[
arXiv:2512.08627v1 Announce Type: new 
Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</title>
<link>https://arxiv.org/abs/2512.08639</link>
<guid>https://arxiv.org/abs/2512.08639</guid>
<content:encoded><![CDATA[
arXiv:2512.08639v1 Announce Type: new 
Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2512.08645</link>
<guid>https://arxiv.org/abs/2512.08645</guid>
<content:encoded><![CDATA[
arXiv:2512.08645v1 Announce Type: new 
Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition</title>
<link>https://arxiv.org/abs/2512.08647</link>
<guid>https://arxiv.org/abs/2512.08647</guid>
<content:encoded><![CDATA[
arXiv:2512.08647v1 Announce Type: new 
Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank</title>
<link>https://arxiv.org/abs/2512.08648</link>
<guid>https://arxiv.org/abs/2512.08648</guid>
<content:encoded><![CDATA[
arXiv:2512.08648v1 Announce Type: new 
Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds</title>
<link>https://arxiv.org/abs/2512.08673</link>
<guid>https://arxiv.org/abs/2512.08673</guid>
<content:encoded><![CDATA[
arXiv:2512.08673v1 Announce Type: new 
Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance</title>
<link>https://arxiv.org/abs/2512.08697</link>
<guid>https://arxiv.org/abs/2512.08697</guid>
<content:encoded><![CDATA[
arXiv:2512.08697v1 Announce Type: new 
Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth</title>
<link>https://arxiv.org/abs/2512.08700</link>
<guid>https://arxiv.org/abs/2512.08700</guid>
<content:encoded><![CDATA[
arXiv:2512.08700v1 Announce Type: new 
Abstract: Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images</title>
<link>https://arxiv.org/abs/2512.08730</link>
<guid>https://arxiv.org/abs/2512.08730</guid>
<content:encoded><![CDATA[
arXiv:2512.08730v1 Announce Type: new 
Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting</title>
<link>https://arxiv.org/abs/2512.08733</link>
<guid>https://arxiv.org/abs/2512.08733</guid>
<content:encoded><![CDATA[
arXiv:2512.08733v1 Announce Type: new 
Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture</title>
<link>https://arxiv.org/abs/2512.08738</link>
<guid>https://arxiv.org/abs/2512.08738</guid>
<content:encoded><![CDATA[
arXiv:2512.08738v1 Announce Type: new 
Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation</title>
<link>https://arxiv.org/abs/2512.08747</link>
<guid>https://arxiv.org/abs/2512.08747</guid>
<content:encoded><![CDATA[
arXiv:2512.08747v1 Announce Type: new 
Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices</title>
<link>https://arxiv.org/abs/2512.08751</link>
<guid>https://arxiv.org/abs/2512.08751</guid>
<content:encoded><![CDATA[
arXiv:2512.08751v1 Announce Type: new 
Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</title>
<link>https://arxiv.org/abs/2512.08765</link>
<guid>https://arxiv.org/abs/2512.08765</guid>
<content:encoded><![CDATA[
arXiv:2512.08765v1 Announce Type: new 
Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps</title>
<link>https://arxiv.org/abs/2512.08774</link>
<guid>https://arxiv.org/abs/2512.08774</guid>
<content:encoded><![CDATA[
arXiv:2512.08774v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fr\'echet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models</title>
<link>https://arxiv.org/abs/2512.08785</link>
<guid>https://arxiv.org/abs/2512.08785</guid>
<content:encoded><![CDATA[
arXiv:2512.08785v1 Announce Type: new 
Abstract: Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance</title>
<link>https://arxiv.org/abs/2512.08789</link>
<guid>https://arxiv.org/abs/2512.08789</guid>
<content:encoded><![CDATA[
arXiv:2512.08789v1 Announce Type: new 
Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning</title>
<link>https://arxiv.org/abs/2512.08820</link>
<guid>https://arxiv.org/abs/2512.08820</guid>
<content:encoded><![CDATA[
arXiv:2512.08820v1 Announce Type: new 
Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincar\'e ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08829</link>
<guid>https://arxiv.org/abs/2512.08829</guid>
<content:encoded><![CDATA[
arXiv:2512.08829v1 Announce Type: new 
Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v1 Announce Type: new 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference</title>
<link>https://arxiv.org/abs/2512.08860</link>
<guid>https://arxiv.org/abs/2512.08860</guid>
<content:encoded><![CDATA[
arXiv:2512.08860v1 Announce Type: new 
Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning</title>
<link>https://arxiv.org/abs/2512.08873</link>
<guid>https://arxiv.org/abs/2512.08873</guid>
<content:encoded><![CDATA[
arXiv:2512.08873v1 Announce Type: new 
Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing</title>
<link>https://arxiv.org/abs/2512.08881</link>
<guid>https://arxiv.org/abs/2512.08881</guid>
<content:encoded><![CDATA[
arXiv:2512.08881v1 Announce Type: new 
Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Rotation-Invariant Convolution for UAV Image Segmentation</title>
<link>https://arxiv.org/abs/2512.08888</link>
<guid>https://arxiv.org/abs/2512.08888</guid>
<content:encoded><![CDATA[
arXiv:2512.08888v1 Announce Type: new 
Abstract: Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers</title>
<link>https://arxiv.org/abs/2512.08889</link>
<guid>https://arxiv.org/abs/2512.08889</guid>
<content:encoded><![CDATA[
arXiv:2512.08889v1 Announce Type: new 
Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation</title>
<link>https://arxiv.org/abs/2512.08897</link>
<guid>https://arxiv.org/abs/2512.08897</guid>
<content:encoded><![CDATA[
arXiv:2512.08897v1 Announce Type: new 
Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Evolving 3D Scene Generation from a Single Image</title>
<link>https://arxiv.org/abs/2512.08905</link>
<guid>https://arxiv.org/abs/2512.08905</guid>
<content:encoded><![CDATA[
arXiv:2512.08905v1 Announce Type: new 
Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception</title>
<link>https://arxiv.org/abs/2512.08912</link>
<guid>https://arxiv.org/abs/2512.08912</guid>
<content:encoded><![CDATA[
arXiv:2512.08912v1 Announce Type: new 
Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration</title>
<link>https://arxiv.org/abs/2512.08922</link>
<guid>https://arxiv.org/abs/2512.08922</guid>
<content:encoded><![CDATA[
arXiv:2512.08922v1 Announce Type: new 
Abstract: Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiently Reconstructing Dynamic Scenes One D4RT at a Time</title>
<link>https://arxiv.org/abs/2512.08924</link>
<guid>https://arxiv.org/abs/2512.08924</guid>
<content:encoded><![CDATA[
arXiv:2512.08924v1 Announce Type: new 
Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment</title>
<link>https://arxiv.org/abs/2512.08930</link>
<guid>https://arxiv.org/abs/2512.08930</guid>
<content:encoded><![CDATA[
arXiv:2512.08930v1 Announce Type: new 
Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
arXiv:2512.08931v1 Announce Type: new 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm</title>
<link>https://arxiv.org/abs/2512.05791</link>
<guid>https://arxiv.org/abs/2512.05791</guid>
<content:encoded><![CDATA[
arXiv:2512.05791v1 Announce Type: cross 
Abstract: Purpose: The Unadjusted Langevin Algorithm (ULA) in combination with diffusion models can generate high quality MRI reconstructions with uncertainty estimation from highly undersampled k-space data. However, sampling methods such as diffusion posterior sampling or likelihood annealing suffer from long reconstruction times and the need for parameter tuning. The purpose of this work is to develop a robust sampling algorithm with fast convergence.
  Theory and Methods: In the reverse diffusion process used for sampling the posterior, the exact likelihood is multiplied with the diffused prior at all noise scales. To overcome the issue of slow convergence, preconditioning is used. The method is trained on fastMRI data and tested on retrospectively undersampled brain data of a healthy volunteer.
  Results: For posterior sampling in Cartesian and non-Cartesian accelerated MRI the new approach outperforms annealed sampling in terms of reconstruction speed and sample quality.
  Conclusion: The proposed exact likelihood with preconditioning enables rapid and reliable posterior sampling across various MRI reconstruction tasks without the need for parameter tuning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model</title>
<link>https://arxiv.org/abs/2512.07855</link>
<guid>https://arxiv.org/abs/2512.07855</guid>
<content:encoded><![CDATA[
arXiv:2512.07855v1 Announce Type: cross 
Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSPN-2: Efficient Parallel Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.07884</link>
<guid>https://arxiv.org/abs/2512.07884</guid>
<content:encoded><![CDATA[
arXiv:2512.07884v1 Announce Type: cross 
Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization</title>
<link>https://arxiv.org/abs/2512.07969</link>
<guid>https://arxiv.org/abs/2512.07969</guid>
<content:encoded><![CDATA[
arXiv:2512.07969v1 Announce Type: cross 
Abstract: Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \textbf{2$\times$--35$\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLD: Visual Language Goal Distance for Reinforcement Learning Navigation</title>
<link>https://arxiv.org/abs/2512.07976</link>
<guid>https://arxiv.org/abs/2512.07976</guid>
<content:encoded><![CDATA[
arXiv:2512.07976v1 Announce Type: cross 
Abstract: Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIP-Net: Continual Interpretable Prototype-based Network</title>
<link>https://arxiv.org/abs/2512.07981</link>
<guid>https://arxiv.org/abs/2512.07981</guid>
<content:encoded><![CDATA[
arXiv:2512.07981v1 Announce Type: cross 
Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIJIT: A Robotic Head for an Active Observer</title>
<link>https://arxiv.org/abs/2512.07998</link>
<guid>https://arxiv.org/abs/2512.07998</guid>
<content:encoded><![CDATA[
arXiv:2512.07998v1 Announce Type: cross 
Abstract: We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</title>
<link>https://arxiv.org/abs/2512.08029</link>
<guid>https://arxiv.org/abs/2512.08029</guid>
<content:encoded><![CDATA[
arXiv:2512.08029v1 Announce Type: cross 
Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizations of the Normalized Radon Cumulative Distribution Transform for Limited Data Recognition</title>
<link>https://arxiv.org/abs/2512.08099</link>
<guid>https://arxiv.org/abs/2512.08099</guid>
<content:encoded><![CDATA[
arXiv:2512.08099v1 Announce Type: cross 
Abstract: The Radon cumulative distribution transform (R-CDT) exploits one-dimensional Wasserstein transport and the Radon transform to represent prominent features in images. It is closely related to the sliced Wasserstein distance and facilitates classification tasks, especially in the small data regime, like the recognition of watermarks in filigranology. Here, a typical issue is that the given data may be subject to affine transformations caused by the measuring process. To make the R-CDT invariant under arbitrary affine transformations, a two-step normalization of the R-CDT has been proposed in our earlier works. The aim of this paper is twofold. First, we propose a family of generalized normalizations to enhance flexibility for applications. Second, we study multi-dimensional and non-Euclidean settings by making use of generalized Radon transforms. We prove that our novel feature representations are invariant under certain transformations and allow for linear separation in feature space. Our theoretical results are supported by numerical experiments based on 2d images, 3d shapes and 3d rotation matrices, showing near perfect classification accuracies and clustering results.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowSteer: Conditioning Flow Field for Consistent Image Restoration</title>
<link>https://arxiv.org/abs/2512.08125</link>
<guid>https://arxiv.org/abs/2512.08125</guid>
<content:encoded><![CDATA[
arXiv:2512.08125v1 Announce Type: cross 
Abstract: Flow-based text-to-image (T2I) models excel at prompt-driven image generation, but falter on Image Restoration (IR), often "drifting away" from being faithful to the measurement. Prior work mitigate this drift with data-specific flows or task-specific adapters that are computationally heavy and not scalable across tasks. This raises the question "Can't we efficiently manipulate the existing generative capabilities of a flow model?" To this end, we introduce FlowSteer (FS), an operator-aware conditioning scheme that injects measurement priors along the sampling path,coupling a frozed flow's implicit guidance with explicit measurement constraints. Across super-resolution, deblurring, denoising, and colorization, FS improves measurement consistency and identity preservation in a strictly zero-shot setting-no retrained models, no adapters. We show how the nature of flow models and their sensitivities to noise inform the design of such a scheduler. FlowSteer, although simple, achieves a higher fidelity of reconstructed images, while leveraging the rich generative priors of flow models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08153</link>
<guid>https://arxiv.org/abs/2512.08153</guid>
<content:encoded><![CDATA[
arXiv:2512.08153v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features</title>
<link>https://arxiv.org/abs/2512.08170</link>
<guid>https://arxiv.org/abs/2512.08170</guid>
<content:encoded><![CDATA[
arXiv:2512.08170v1 Announce Type: cross 
Abstract: In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model</title>
<link>https://arxiv.org/abs/2512.08188</link>
<guid>https://arxiv.org/abs/2512.08188</guid>
<content:encoded><![CDATA[
arXiv:2512.08188v1 Announce Type: cross 
Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title>
<link>https://arxiv.org/abs/2512.08216</link>
<guid>https://arxiv.org/abs/2512.08216</guid>
<content:encoded><![CDATA[
arXiv:2512.08216v1 Announce Type: cross 
Abstract: Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation</title>
<link>https://arxiv.org/abs/2512.08271</link>
<guid>https://arxiv.org/abs/2512.08271</guid>
<content:encoded><![CDATA[
arXiv:2512.08271v1 Announce Type: cross 
Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Reinforced Deep Priors for Reparameterized Full Waveform Inversion</title>
<link>https://arxiv.org/abs/2512.08284</link>
<guid>https://arxiv.org/abs/2512.08284</guid>
<content:encoded><![CDATA[
arXiv:2512.08284v1 Announce Type: cross 
Abstract: Full waveform inversion (FWI) has become a widely adopted technique for high-resolution subsurface imaging. However, its inherent strong nonlinearity often results in convergence toward local minima. Recently, deep image prior-based reparameterized FWI (DIP-FWI) has been proposed to alleviate the dependence on massive training data. By exploiting the spectral bias and implicit regularization in the neural network architecture, DIP-FWI can effectively avoid local minima and reconstruct more geologically plausible velocity models. Nevertheless, existing DIP-FWI typically use a fixed random input throughout the inversion process, which fails to utilize the mapping and correlation between the input and output of the network. Moreover, under complex geological conditions, the lack of informative prior in the input can exacerbate the ill-posedness of the inverse problem, leading to artifacts and unstable reconstructions. To address these limitations, we propose a self-reinforced DIP-FWI (SRDIP-FWI) framework, in which a steering algorithm alternately updates both the network parameters and the input at each iteration using feedback from the current network output. This design allows adaptive structural enhancement and improved regularization, thereby effectively mitigating the ill-posedness in FWI. Additionally, we analyze the spectral bias of the network in SRDIP-FWI and quantify its role in multiscale velocity model building. Synthetic tests and field land data application demonstrate that SRDIP-FWI achieves superior resolution, improved accuracy and greater depth penetration compared to multiscale FWI. More importantly, SRDIP-FWI eliminates the need for manual frequency-band selection and time-window picking, substantially simplifying the inversion workflow. Overall, the proposed method provides a novel, adaptive and robust framework for accurate subsurface velocity model reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2512.08360</link>
<guid>https://arxiv.org/abs/2512.08360</guid>
<content:encoded><![CDATA[
arXiv:2512.08360v1 Announce Type: cross 
Abstract: Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions</title>
<link>https://arxiv.org/abs/2512.08500</link>
<guid>https://arxiv.org/abs/2512.08500</guid>
<content:encoded><![CDATA[
arXiv:2512.08500v1 Announce Type: cross 
Abstract: Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks</title>
<link>https://arxiv.org/abs/2512.08545</link>
<guid>https://arxiv.org/abs/2512.08545</guid>
<content:encoded><![CDATA[
arXiv:2512.08545v1 Announce Type: cross 
Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm</title>
<link>https://arxiv.org/abs/2512.08629</link>
<guid>https://arxiv.org/abs/2512.08629</guid>
<content:encoded><![CDATA[
arXiv:2512.08629v1 Announce Type: cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-domain performance analysis with scores tailored to user preferences</title>
<link>https://arxiv.org/abs/2512.08715</link>
<guid>https://arxiv.org/abs/2512.08715</guid>
<content:encoded><![CDATA[
arXiv:2512.08715v1 Announce Type: cross 
Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</title>
<link>https://arxiv.org/abs/2310.02239</link>
<guid>https://arxiv.org/abs/2310.02239</guid>
<content:encoded><![CDATA[
arXiv:2310.02239v4 Announce Type: replace 
Abstract: The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spike-EVPR: Deep Spiking Residual Networks with SNN-Tailored Representations for Event-Based Visual Place Recognition</title>
<link>https://arxiv.org/abs/2402.10476</link>
<guid>https://arxiv.org/abs/2402.10476</guid>
<content:encoded><![CDATA[
arXiv:2402.10476v2 Announce Type: replace 
Abstract: Event cameras are ideal for visual place recognition (VPR) in challenging environments due to their high temporal resolution and high dynamic range. However, existing methods convert sparse events into dense frame-like representations for Artificial Neural Networks (ANNs), ignoring event sparsity and incurring high computational cost. Spiking Neural Networks (SNNs) complement event data through discrete spike signals to enable energy-efficient VPR, but their application is hindered by the lack of effective spike-compatible representations and deep architectures capable of learning discriminative global descriptors. To address these limitations, we propose Spike-EVPR, a directly trained, end-to-end SNN framework tailored for event-based VPR. First, we introduce two complementary event representations, MCS-Tensor and TSS-Tensor, designed to reduce temporal redundancy while preserving essential spatio-temporal cues. Furthermore, we propose a deep spiking residual architecture that effectively aggregates these features to generate robust place descriptors. Extensive experiments on the Brisbane-Event-VPR and DDD20 datasets demonstrate that Spike-EVPR achieves state-of-the-art performance, improving Recall@1 by 7.61% and 13.20%, respectively, while significantly reducing energy consumption.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning effective pruning at initialization from iterative pruning</title>
<link>https://arxiv.org/abs/2408.14757</link>
<guid>https://arxiv.org/abs/2408.14757</guid>
<content:encoded><![CDATA[
arXiv:2408.14757v2 Announce Type: replace 
Abstract: Pruning at initialization (PaI) reduces training costs by removing weights before training, which becomes increasingly crucial with the growing network size. However, current PaI methods still have a large accuracy gap with iterative pruning, especially at high sparsity levels. This raises an intriguing question: can we get inspiration from iterative pruning to improve the PaI performance? In the lottery ticket hypothesis, the iterative rewind pruning (IRP) finds subnetworks retroactively by rewinding the parameter to the original initialization in every pruning iteration, which means all the subnetworks are based on the initial state. Here, we hypothesise the surviving subnetworks are more important and bridge the initial feature and their surviving score as the PaI criterion. We employ an end-to-end neural network (\textbf{AutoS}parse) to learn this correlation, input the model's initial features, output their score and then prune the lowest score parameters before training. To validate the accuracy and generalization of our method, we performed PaI across various models. Results show that our approach outperforms existing methods in high-sparsity settings. Notably, as the underlying logic of model pruning is consistent in different models, only one-time IRP on one model is needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to VGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural network-based PaI method, we conduct extensive experiments to validate the factors influencing this approach. These results reveal the learning tendencies of neural networks and provide new insights into our understanding and research of PaI from a practical perspective. Our code is available at: https://github.com/ChengYaofeng/AutoSparse.git.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond accuracy: quantifying the reliability of Multiple Instance Learning for Whole Slide Image classification</title>
<link>https://arxiv.org/abs/2409.11110</link>
<guid>https://arxiv.org/abs/2409.11110</guid>
<content:encoded><![CDATA[
arXiv:2409.11110v3 Announce Type: replace 
Abstract: Machine learning models have become integral to many fields, but their reliability, defined as producing dependable, trustworthy, and domain-consistent predictions, remains a critical concern. Multiple Instance Learning (MIL) models designed for Whole Slide Image (WSI) classification in computational pathology are rarely evaluated in terms of reliability, leaving a key gap in understanding their suitability for high-stakes applications like clinical decision-making. In this paper, we address this gap by introducing three quantitative metrics for reliability assessment and applying them to several widely used MIL architectures across three region-wise annotated pathology datasets. Our findings indicate that the mean pooling instance (MEAN-POOL-INS)model demonstrates superior reliability compared to other networks, despite its simple architectural design and computational efficiency. These findings underscore the need of reliability evaluation alongside predictive performance in MIL models and establish MEAN-POOL-INS as a strong, trustworthy baseline for future research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning, Machine Learning -- Digital Signal and Image Processing: From Theory to Application</title>
<link>https://arxiv.org/abs/2410.20304</link>
<guid>https://arxiv.org/abs/2410.20304</guid>
<content:encoded><![CDATA[
arXiv:2410.20304v2 Announce Type: replace 
Abstract: Digital Signal Processing (DSP) and Digital Image Processing (DIP) with Machine Learning (ML) and Deep Learning (DL) are popular research areas in Computer Vision and related fields. We highlight transformative applications in image enhancement, filtering techniques, and pattern recognition. By integrating frameworks like the Discrete Fourier Transform (DFT), Z-Transform, and Fourier Transform methods, we enable robust data manipulation and feature extraction essential for AI-driven tasks. Using Python, we implement algorithms that optimize real-time data processing, forming a foundation for scalable, high-performance solutions in computer vision. This work illustrates the potential of ML and DL to advance DSP and DIP methodologies, contributing to artificial intelligence, automated feature extraction, and applications across diverse domains.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Radiance Fields for the Real World: A Survey</title>
<link>https://arxiv.org/abs/2501.13104</link>
<guid>https://arxiv.org/abs/2501.13104</guid>
<content:encoded><![CDATA[
arXiv:2501.13104v2 Announce Type: replace 
Abstract: Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since release. NeRFs can effectively reconstruct complex 3D scenes from 2D images, advancing different fields and applications such as scene understanding, 3D content generation, and robotics. Despite significant research progress, a thorough review of recent innovations, applications, and challenges is lacking. This survey compiles key theoretical advancements and alternative representations and investigates emerging challenges. It further explores applications on reconstruction, highlights NeRFs' impact on computer vision and robotics, and reviews essential datasets and toolkits. By identifying gaps in the literature, this survey discusses open challenges and offers directions for future research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape and Texture Recognition in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.23062</link>
<guid>https://arxiv.org/abs/2503.23062</guid>
<content:encoded><![CDATA[
arXiv:2503.23062v5 Announce Type: replace 
Abstract: Shapes and textures are the basic building blocks of visual perception. The ability to identify shapes regardless of orientation, texture, or context, and to recognize textures and materials independently of their associated objects, is essential for a general visual understanding of the world. This work introduces the Large Shapes and Textures dataset (LAS&amp;T), a giant collection of highly diverse shapes and textures, created by unsupervised extraction of patterns from natural images. This dataset is used to benchmark how effectively leading Large Vision-Language Models (LVLM/VLM) recognize and represent shapes, textures, and materials in 2D and 3D scenes. For shape recognition, we test the models' ability to match images of identical shapes that differ in orientation, texture, color, or environment. Our results show that the shape-recognition capabilities of LVLMs remain well below human performance, especially when multiple transformations are applied. LVLMs rely predominantly on high-level and semantic features and struggle with abstract shapes lacking class associations. For texture and material recognition, we evaluated the models' ability to identify images with identical textures and materials across different objects and environments. Interestingly, leading LVLMs approach human-level performance in recognizing materials in 3D scenes, yet substantially underperform humans when identifying simpler, more abstract 2D textures and shapes. These results are consistent across a wide range of leading LVLMs (GPT/Gemini/LLama/Qwen) and foundation vision models (DINO/CLIP), exposing major deficiencies in the ability of VLMs to extract low-level visual features. In contrast, humans and simple nets trained directly for these tasks achieve high accuracy. The LAS&amp;T dataset, featuring over 700,000 images for 2D/3D shape and textures recognition and retrieval, is freely available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2504.02316</link>
<guid>https://arxiv.org/abs/2504.02316</guid>
<content:encoded><![CDATA[
arXiv:2504.02316v2 Announce Type: replace 
Abstract: Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent prior view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel method that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise view control; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer can be seamlessly integrated into various 3D representations and score distillation paradigms, effectively mitigating the multi-face Janus problem.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Dataset Condensation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06670</link>
<guid>https://arxiv.org/abs/2505.06670</guid>
<content:encoded><![CDATA[
arXiv:2505.06670v2 Announce Type: replace 
Abstract: In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance, particularly in the video domain. In this paper, we focus on video dataset distillation. We begin by employing a video diffusion model to generate synthetic videos. Since the videos are generated only once, this significantly reduces computational costs. Next, we introduce the Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MELLM: A Flow-Guided Large Language Model for Micro-Expression Understanding</title>
<link>https://arxiv.org/abs/2505.07007</link>
<guid>https://arxiv.org/abs/2505.07007</guid>
<content:encoded><![CDATA[
arXiv:2505.07007v3 Announce Type: replace 
Abstract: Micro-expressions (MEs), brief and low-intensity facial movements revealing concealed emotions, are crucial for affective computing. Despite notable progress in ME recognition, existing methods are largely confined to discrete emotion classification, lacking the capacity for comprehensive ME Understanding (MEU), particularly in interpreting subtle facial dynamics and underlying emotional cues. While Multimodal Large Language Models (MLLMs) offer potential for MEU with their advanced reasoning abilities, they still struggle to perceive such subtle facial affective behaviors. To bridge this gap, we propose a ME Large Language Model (MELLM) that integrates optical flow-based sensitivity to subtle facial motions with the powerful inference ability of LLMs. Specifically, an iterative, warping-based optical-flow estimator, named MEFlowNet, is introduced to precisely capture facial micro-movements. For its training and evaluation, we construct MEFlowDataset, a large-scale optical-flow dataset with 54,611 onset-apex image pairs spanning diverse identities and subtle facial motions. Subsequently, we design a Flow-Guided Micro-Expression Understanding paradigm. Under this framework, the optical flow signals extracted by MEFlowNet are leveraged to build MEU-Instruct, an instruction-tuning dataset for MEU. MELLM is then fine-tuned on MEU-Instruct, enabling it to translate subtle motion patterns into human-readable descriptions and generate corresponding emotional inferences. Experiments demonstrate that MEFlowNet significantly outperforms existing optical flow methods in facial and ME-flow estimation, while MELLM achieves state-of-the-art accuracy and generalization across multiple ME benchmarks. To the best of our knowledge, this work presents two key contributions: MEFlowNet as the first dedicated ME flow estimator, and MELLM as the first LLM tailored for MEU.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multi-Modal Information to Enhance Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.08605</link>
<guid>https://arxiv.org/abs/2505.08605</guid>
<content:encoded><![CDATA[
arXiv:2505.08605v3 Announce Type: replace 
Abstract: Dataset distillation aims to create a small and highly representative synthetic dataset that preserves the essential information of a larger real dataset. Beyond reducing storage and computational costs, related approaches offer a promising avenue for privacy preservation in computer vision by eliminating the need to store or share sensitive real-world images. Existing methods focus solely on optimizing visual representations, overlooking the potential of multi-modal information. In this work, we propose a multi-modal dataset distillation framework that incorporates two key enhancements: caption-guided supervision and object-centric masking. To leverage textual information, we introduce two strategies: caption concatenation, which fuses caption embeddings with visual features during classification, and caption matching, which enforces semantic alignment between real and synthetic data through a caption-based loss. To improve data utility and reduce unnecessary background noise, we employ segmentation masks to isolate target objects and introduce two novel losses: masked feature alignment and masked gradient matching, both aimed at promoting object-centric learning. Extensive evaluations demonstrate that our approach improves downstream performance while promoting privacy protection by minimizing exposure to real data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make LVLMs Focus: Context-Aware Attention Modulation for Better Multimodal In-Context Learning</title>
<link>https://arxiv.org/abs/2505.17097</link>
<guid>https://arxiv.org/abs/2505.17097</guid>
<content:encoded><![CDATA[
arXiv:2505.17097v3 Announce Type: replace 
Abstract: Multimodal in-context learning (ICL) is becoming a key capability that allows large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, which expands their usefulness in many real-world applications. However, ICL performance remains unstable even when the in-context demonstrations (ICDs) are well matched, showing that LVLMs still struggle to make full use of the provided context. While existing work mainly focuses on prompt engineering or post-hoc logit calibration, we study the attention mechanisms inside LVLMs to address their inherent limitations. We identify two important weaknesses in their self-attention that hinder effective ICL. To address these weaknesses, we propose \textbf{Context-Aware Modulated Attention} (CAMA), a training-free and plug-and-play method that dynamically adjusts attention logits based on the input in-context sequence. CAMA uses a two-stage modulation process that strengthens attention to semantically important tokens, especially visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, showing clear effectiveness and generalization. It can also activate the intended benefits of prompt engineering methods and remains robust across different sequence configurations. Therefore, CAMA opens up new directions for improving multimodal reasoning through a deeper understanding of attention dynamics.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Missing Point in Vision Transformers for Universal Image Segmentation</title>
<link>https://arxiv.org/abs/2505.19795</link>
<guid>https://arxiv.org/abs/2505.19795</guid>
<content:encoded><![CDATA[
arXiv:2505.19795v2 Announce Type: replace 
Abstract: Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>50 Years of Automated Face Recognition</title>
<link>https://arxiv.org/abs/2505.24247</link>
<guid>https://arxiv.org/abs/2505.24247</guid>
<content:encoded><![CDATA[
arXiv:2505.24247v3 Announce Type: replace 
Abstract: Over the past five decades, automated face recognition (FR) has progressed from handcrafted geometric and statistical approaches to advanced deep learning architectures that now approach, and in many cases exceed, human performance. This paper traces the historical and technological evolution of FR, encompassing early algorithmic paradigms through to contemporary neural systems trained on extensive real and synthetically generated datasets. We examine pivotal innovations that have driven this progression, including advances in dataset construction, loss function formulation, network architecture design, and feature fusion strategies. Furthermore, we analyze the relationship between data scale, diversity, and model generalization, highlighting how dataset expansion correlates with benchmark performance gains. Recent systems have achieved near-perfect large-scale identification accuracy, with the leading algorithm in the latest NIST FRTE 1:N benchmark reporting a FNIR of 0.15 percent at FPIR of 0.001 on a gallery of over 10 million identities. We delineate key open problems and emerging directions, including scalable training, multi-modal fusion, synthetic data, and interpretable recognition frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection</title>
<link>https://arxiv.org/abs/2506.05872</link>
<guid>https://arxiv.org/abs/2506.05872</guid>
<content:encoded><![CDATA[
arXiv:2506.05872v2 Announce Type: replace 
Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation</title>
<link>https://arxiv.org/abs/2506.17159</link>
<guid>https://arxiv.org/abs/2506.17159</guid>
<content:encoded><![CDATA[
arXiv:2506.17159v2 Announce Type: replace 
Abstract: Medical image analysis is critical yet challenged by the need of jointly segmenting organs or tissues, and numerous instances for anatomical structures and tumor microenvironment analysis. Existing studies typically formulated different segmentation tasks in isolation, which overlooks the fundamental interdependencies between these tasks, leading to suboptimal segmentation performance and insufficient medical image understanding. To address this issue, we propose a Co-Seg++ framework for versatile medical segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing semantic and instance segmentation tasks to mutually enhance each other. We first devise a spatio-sequential prompt encoder (SSP-Encoder) to capture long-range spatial and sequential relationships between segmentation regions and image embeddings as prior spatial constraints. Moreover, we devise a multi-task collaborative decoder (MTC-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, jointly computing semantic and instance segmentation masks. Extensive experiments on diverse CT and histopathology datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts in the semantic, instance, and panoptic segmentation of dental anatomical structures, histopathology tissues, and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg-Plus.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards</title>
<link>https://arxiv.org/abs/2506.18331</link>
<guid>https://arxiv.org/abs/2506.18331</guid>
<content:encoded><![CDATA[
arXiv:2506.18331v4 Announce Type: replace 
Abstract: While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches. Our implementation code is publicly available at: https://github.com/AHHHZ975/Differentiable-Texture-Learning
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning an Ensemble Token from Task-driven Priors in Facial Analysis</title>
<link>https://arxiv.org/abs/2507.01290</link>
<guid>https://arxiv.org/abs/2507.01290</guid>
<content:encoded><![CDATA[
arXiv:2507.01290v3 Announce Type: replace 
Abstract: Facial analysis exhibits task-specific feature variations. While Convolutional Neural Networks (CNNs) have enabled the fine-grained representation of spatial information, Vision Transformers (ViTs) have facilitated the representation of semantic information at the patch level. While advances in backbone architectures have improved over the past decade, combining high-fidelity models often incurs computational costs on feature representation perspective. In this work, we introduce KT-Adapter, a novel methodology for learning knowledge token which enables the integration of high-fidelity feature representation in computationally efficient manner. Specifically, we propose a robust prior unification learning method that generates a knowledge token within a self-attention mechanism, sharing the mutual information across the pre-trained encoders. This knowledge token approach offers high efficiency with negligible computational cost. Our results show improved performance across facial analysis, with statistically significant enhancements observed in the feature representations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.04049</link>
<guid>https://arxiv.org/abs/2507.04049</guid>
<content:encoded><![CDATA[
arXiv:2507.04049v3 Announce Type: replace 
Abstract: Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding</title>
<link>https://arxiv.org/abs/2507.21888</link>
<guid>https://arxiv.org/abs/2507.21888</guid>
<content:encoded><![CDATA[
arXiv:2507.21888v3 Announce Type: replace 
Abstract: We address Embodied Reference Understanding, the task of predicting the object a person in the scene refers to through pointing gesture and language. This requires multimodal reasoning over text, visual pointing cues, and scene context, yet existing methods often fail to fully exploit visual disambiguation signals. We also observe that while the referent often aligns with the head-to-fingertip direction, in many cases it aligns more closely with the wrist-to-fingertip direction, making a single-line assumption overly limiting. To address this, we propose a dual-model framework, where one model learns from the head-to-fingertip direction and the other from the wrist-to-fingertip direction. We introduce a Gaussian ray heatmap representation of these lines and use them as input to provide a strong supervisory signal that encourages the model to better attend to pointing cues. To fuse their complementary strengths, we present the CLIP-Aware Pointing Ensemble module, which performs a hybrid ensemble guided by CLIP features. We further incorporate an auxiliary object center prediction head to enhance referent localization. We validate our approach on YouRefIt, achieving 75.0 mAP at 0.25 IoU, alongside state-of-the-art CLIP and C_D scores, and demonstrate its generality on unseen CAESAR and ISL Pointing, showing robust performance across benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
<link>https://arxiv.org/abs/2508.00518</link>
<guid>https://arxiv.org/abs/2508.00518</guid>
<content:encoded><![CDATA[
arXiv:2508.00518v2 Announce Type: replace 
Abstract: Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</title>
<link>https://arxiv.org/abs/2508.07871</link>
<guid>https://arxiv.org/abs/2508.07871</guid>
<content:encoded><![CDATA[
arXiv:2508.07871v2 Announce Type: replace 
Abstract: Modern large vision-language models (LVLMs) convert each input image into a large set of tokens that far outnumber the text tokens. Although this improves visual perception, it also introduces severe image token redundancy. Because image tokens contain sparse information, many contribute little to reasoning but greatly increase inference cost. Recent image token pruning methods address this issue by identifying important tokens and removing the rest. These methods improve efficiency with only small performance drops. However, most of them focus on single-image tasks and overlook multimodal in-context learning (ICL), where redundancy is higher and efficiency is more important. Redundant tokens weaken the advantage of multimodal ICL for rapid domain adaptation and lead to unstable performance. When existing pruning methods are applied in this setting, they cause large accuracy drops, which exposes a clear gap and the need for new approaches. To address this, we propose Contextually Adaptive Token Pruning (CATP), a training-free pruning method designed for multimodal ICL. CATP uses two stages of progressive pruning that fully reflect the complex cross-modal interactions in the input sequence. After removing 77.8% of the image tokens, CATP achieves an average performance gain of 0.6% over the vanilla model on four LVLMs and eight benchmarks, clearly outperforming all baselines. At the same time, it improves efficiency by reducing inference latency by an average of 10.78%. CATP strengthens the practical value of multimodal ICL and lays the foundation for future progress in interleaved image-text settings.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training</title>
<link>https://arxiv.org/abs/2508.12522</link>
<guid>https://arxiv.org/abs/2508.12522</guid>
<content:encoded><![CDATA[
arXiv:2508.12522v2 Announce Type: replace 
Abstract: Personalized expression recognition (ER) involves adapting a machine learning model to subject-specific data for improved recognition of expressions with considerable interpersonal variability. Subject-specific ER can benefit significantly from multi-source domain adaptation (MSDA) methods, where each domain corresponds to a specific subject to improve model accuracy and robustness. Despite promising results, state-of-the-art MSDA approaches often overlook multimodal information or blend sources into a single domain, limiting subject diversity and failing to explicitly capture unique subject-specific characteristics. To address these limitations, we introduce MuSACo, a multimodal subject-specific selection and adaptation method for ER based on co-training. It leverages complementary information across multiple modalities and multiple source domains for subject-specific adaptation. This makes MuSACo particularly relevant for affective computing applications in digital health, such as patient-specific assessment for stress or pain, where subject-level nuances are crucial. MuSACo selects source subjects relevant to the target and generates pseudo-labels using the dominant modality for class-aware learning, in conjunction with a class-agnostic loss to learn from less confident target samples. Finally, source features from each modality are aligned, while only confident target features are combined. Experimental results on challenging multimodal ER datasets: BioVid, StressID, and BAH show that MuSACo outperforms UDA (blending) and state-of-the-art MSDA methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples</title>
<link>https://arxiv.org/abs/2508.13309</link>
<guid>https://arxiv.org/abs/2508.13309</guid>
<content:encoded><![CDATA[
arXiv:2508.13309v2 Announce Type: replace 
Abstract: Numerous techniques have been proposed for generating adversarial examples in white-box settings under strict Lp-norm constraints. However, such norm-bounded examples often fail to align well with human perception, and only recently have a few methods begun specifically exploring perceptually aligned adversarial examples. Moreover, it remains unclear whether insights from Lp-constrained attacks can be effectively leveraged to improve perceptual efficacy. In this paper, we introduce DAASH, a fully differentiable meta-attack framework that generates effective and perceptually aligned adversarial examples by strategically composing existing Lp-based attack methods. DAASH operates in a multi-stage fashion: at each stage, it aggregates candidate adversarial examples from multiple base attacks using learned, adaptive weights and propagates the result to the next stage. A novel meta-loss function guides this process by jointly minimizing misclassification loss and perceptual distortion, enabling the framework to dynamically modulate the contribution of each base attack throughout the stages. We evaluate DAASH on adversarially trained models across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on Lp-constrained based methods, DAASH significantly outperforms state-of-the-art perceptual attacks such as AdvAD -- achieving higher attack success rates (e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM, LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively). Furthermore, DAASH generalizes well to unseen defenses, making it a practical and strong baseline for evaluating robustness without requiring handcrafted adaptive attacks for each new defense.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
arXiv:2508.15769v2 Announce Type: replace 
Abstract: 3D content generation has recently attracted significant research interest, driven by its critical applications in VR/AR and embodied AI. In this work, we tackle the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for extra optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architecture yields improved generation performance when multiple images are provided; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robustness of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Object Grounding for Time-Sensitive Video Understanding</title>
<link>https://arxiv.org/abs/2509.06335</link>
<guid>https://arxiv.org/abs/2509.06335</guid>
<content:encoded><![CDATA[
arXiv:2509.06335v2 Announce Type: replace 
Abstract: We propose to improve the time-sensitive video understanding (TSV) capability of video large language models (Video-LLMs) with grounded objects (GO). We hypothesize that TSV tasks can benefit from GO within frames, which is supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM for reasoning temporal localization. While augmenting prompts with textual descriptions of these object annotations improves the performance of LITA, it also introduces extra token length and susceptibility to the noise in object-level information. To address this, we propose GO-Tokenizer, a lightweight add-on module for Video-LLMs leveraging off-the-shelf object detectors to encode compact object information on the fly. Experimental results demonstrate that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its counterpart, utilizing textual descriptions of objects in the prompt. The gain generalizes across different models, datasets, and video understanding tasks, such as reasoning temporal localization and dense captioning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zo3T: Zero-Shot 3D-Aware Trajectory-Guided Image-to-Video Generation via Test-Time Training</title>
<link>https://arxiv.org/abs/2509.06723</link>
<guid>https://arxiv.org/abs/2509.06723</guid>
<content:encoded><![CDATA[
arXiv:2509.06723v3 Announce Type: replace 
Abstract: Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data-driven Typology of Vision Models from Integrated Representational Metrics</title>
<link>https://arxiv.org/abs/2509.21628</link>
<guid>https://arxiv.org/abs/2509.21628</guid>
<content:encoded><![CDATA[
arXiv:2509.21628v2 Announce Type: replace 
Abstract: Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity metrics, each capturing a different facet-geometry, unit tuning, or linear decodability-and assess family separability using multiple complementary measures. Metrics preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family discrimination, whereas flexible mappings such as Linear Predictivity show weaker separation. These findings indicate that geometry and tuning carry family-specific signatures, while linearly decodable information is more broadly shared. To integrate these complementary facets, we adapt Similarity Network Fusion (SNF), a method inspired by multi-omics integration. SNF achieves substantially sharper family separation than any individual metric and produces robust composite signatures. Clustering of the fused similarity matrix recovers both expected and surprising patterns: supervised ResNets and ViTs form distinct clusters, yet all self-supervised models group together across architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with masked autoencoders, suggesting convergence between architectural modernization and reconstruction-based training. This biology-inspired framework provides a principled typology of vision models, showing that emergent computational strategies-shaped jointly by architecture and training objective-define representational structure beyond surface design categories.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation</title>
<link>https://arxiv.org/abs/2510.11000</link>
<guid>https://arxiv.org/abs/2510.11000</guid>
<content:encoded><![CDATA[
arXiv:2510.11000v2 Announce Type: replace 
Abstract: Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction</title>
<link>https://arxiv.org/abs/2510.14885</link>
<guid>https://arxiv.org/abs/2510.14885</guid>
<content:encoded><![CDATA[
arXiv:2510.14885v2 Announce Type: replace 
Abstract: Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception</title>
<link>https://arxiv.org/abs/2510.17568</link>
<guid>https://arxiv.org/abs/2510.17568</guid>
<content:encoded><![CDATA[
arXiv:2510.17568v3 Announce Type: replace 
Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://arxiv.org/abs/2510.20812</link>
<guid>https://arxiv.org/abs/2510.20812</guid>
<content:encoded><![CDATA[
arXiv:2510.20812v3 Announce Type: replace 
Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery</title>
<link>https://arxiv.org/abs/2511.16887</link>
<guid>https://arxiv.org/abs/2511.16887</guid>
<content:encoded><![CDATA[
arXiv:2511.16887v2 Announce Type: replace 
Abstract: Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models</title>
<link>https://arxiv.org/abs/2511.22425</link>
<guid>https://arxiv.org/abs/2511.22425</guid>
<content:encoded><![CDATA[
arXiv:2511.22425v2 Announce Type: replace 
Abstract: We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IRPO: Boosting Image Restoration via Post-training GRPO</title>
<link>https://arxiv.org/abs/2512.00814</link>
<guid>https://arxiv.org/abs/2512.00814</guid>
<content:encoded><![CDATA[
arXiv:2512.00814v2 Announce Type: replace 
Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball</title>
<link>https://arxiv.org/abs/2512.01478</link>
<guid>https://arxiv.org/abs/2512.01478</guid>
<content:encoded><![CDATA[
arXiv:2512.01478v2 Announce Type: replace 
Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</title>
<link>https://arxiv.org/abs/2405.17537</link>
<guid>https://arxiv.org/abs/2405.17537</guid>
<content:encoded><![CDATA[
arXiv:2405.17537v5 Announce Type: replace-cross 
Abstract: Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery</title>
<link>https://arxiv.org/abs/2501.06039</link>
<guid>https://arxiv.org/abs/2501.06039</guid>
<content:encoded><![CDATA[
arXiv:2501.06039v2 Announce Type: replace-cross 
Abstract: Spatial proteomics technologies have transformed our understanding of complex tissue architecture in cancer but present unique challenges for computational analysis. Each study uses a different marker panel and protocol, and most methods are tailored to single cohorts, which limits knowledge transfer and robust biomarker discovery. Here we present Virtual Tissues (VirTues), a general-purpose foundation model for spatial proteomics that learns marker-aware, multi-scale representations of proteins, cells, niches and tissues directly from multiplex imaging data. From a single pretrained backbone, VirTues supports marker reconstruction, cell typing and niche annotation, spatial biomarker discovery, and patient stratification, including zero-shot annotation across heterogeneous panels and datasets. In triple-negative breast cancer, VirTues-derived biomarkers predict anti-PD-L1 chemo-immunotherapy response and stratify disease-free survival in an independent cohort, outperforming state-of-the-art biomarkers derived from the same datasets and current clinical stratification schemes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bezier Splatting for Fast and Differentiable Vector Graphics Rendering</title>
<link>https://arxiv.org/abs/2503.16424</link>
<guid>https://arxiv.org/abs/2503.16424</guid>
<content:encoded><![CDATA[
arXiv:2503.16424v4 Announce Type: replace-cross 
Abstract: Differentiable vector graphics (VGs) are widely used in image vectorization and vector synthesis, while existing representations are costly to optimize and struggle to achieve high-quality rendering results for high-resolution images. This work introduces a new differentiable VG representation, dubbed B\'ezier Splatting, that enables fast yet high-fidelity VG rasterization. B\'ezier Splatting samples 2D Gaussians along B\'ezier curves, which naturally provide positional gradients at object boundaries. Thanks to the efficient splatting-based differentiable rasterizer, B\'ezier Splatting achieves 30x and 150x faster per forward and backward rasterization step for open curves compared to DiffVG. Additionally, we introduce an adaptive pruning and densification strategy that dynamically adjusts the spatial distribution of curves to escape local minima, further improving VG quality. Furthermore, our new VG representation supports conversion to standard XML-based SVG format, enhancing interoperability with existing VG tools and pipelines. Experimental results show that B\'ezier Splatting significantly outperforms existing methods with better visual fidelity and significant optimization speedup.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases</title>
<link>https://arxiv.org/abs/2504.07606</link>
<guid>https://arxiv.org/abs/2504.07606</guid>
<content:encoded><![CDATA[
arXiv:2504.07606v3 Announce Type: replace-cross 
Abstract: Heart diseases remain the leading cause of mortality worldwide, implying approximately 18 million deaths according to the WHO. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid, and effective prediction. This work presents an automatic system based on a novel framework which combines Modal Decomposition and Masked Autoencoders (MAE) to extend the application from heart disease classification to the more challenging and specific task of heart failure time prediction, not previously addressed to the best of authors' knowledge. This system comprises two stages. The first one transforms the data from a database of echocardiography video sequences into a large collection of annotated images compatible with the training phase of machine learning-based frameworks and deep learning-based ones. This stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage builds and trains a Vision Transformer (ViT). MAEs based on a combined scheme of self-supervised (SSL) and supervised learning, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the ViT from scratch, even with scarce databases. The designed neural network analyses in real-time images from echocardiography sequences to estimate the time of happening a heart failure. This approach demonstrates to improve prediction accuracy from scarce databases and to be superior to several established ViT and Convolutional Neural Network (CNN) architectures. The source code will be incorporated into the next version release of the ModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion</title>
<link>https://arxiv.org/abs/2504.08937</link>
<guid>https://arxiv.org/abs/2504.08937</guid>
<content:encoded><![CDATA[
arXiv:2504.08937v4 Announce Type: replace-cross 
Abstract: In image fusion tasks, the absence of real fused images as priors forces most deep learning approaches to rely on large-scale paired datasets to extract global weighting features or to generate pseudo-supervised images through algorithmic constructions. Unlike previous methods, this work re-examines prior-guided learning under few-shot conditions by introducing rough set theory. We regard the traditional algorithm as a prior generator, while the network re-inferrs and adaptively optimizes the prior through a dynamic loss function, reducing the inference burden of the network and enabling effective few-shot learning.To provide the prior, we propose the Granular Ball Pixel Computation (GBPC) algorithm. GBPC models pixel pairs in a luminance subspace using meta-granular balls and mines intra-ball information at multiple granular levels. At the fine-grained level, sliding granular balls assign adaptive weights to individual pixels to produce pixel-level prior fusion. At the coarse-grained level, the algorithm performs split computation within a single image to estimate positive and boundary domain distributions, enabling modality awareness and prior confidence estimation, which dynamically guide the loss weighting.The network and the algorithmic prior are coupled through the loss function to form an integrated framework. Thanks to the dynamic weighting mechanism, the network can adaptively adjust to different priors during training, enhancing its perception and fusion capability across modalities. We name this framework GBFF (Granular Ball Fusion Framework). Experiments on four fusion tasks demonstrate that even with only ten training image pairs per task, GBFF achieves superior performance in both visual quality and model compactness. Code is available at: https://github.com/DMinjie/GBFF
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Task-Oriented Flying: Framework, Infrastructure, and Principles</title>
<link>https://arxiv.org/abs/2504.15129</link>
<guid>https://arxiv.org/abs/2504.15129</guid>
<content:encoded><![CDATA[
arXiv:2504.15129v2 Announce Type: replace-cross 
Abstract: Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</title>
<link>https://arxiv.org/abs/2505.11394</link>
<guid>https://arxiv.org/abs/2505.11394</guid>
<content:encoded><![CDATA[
arXiv:2505.11394v2 Announce Type: replace-cross 
Abstract: Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced, a method that is label-free and allows subsequent staining of sections after 3D-PLI measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established in the same section. However, inevitable distortions introduced during the staining process make a costly nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of such samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. We use a supervised setting, building on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method can predict a Cresyl violet staining from 3D-PLI, resulting in a virtual staining that exhibits plausible patterns of cell organization in gray matter, with larger cell bodies being localized at their expected positions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title>
<link>https://arxiv.org/abs/2505.12332</link>
<guid>https://arxiv.org/abs/2505.12332</guid>
<content:encoded><![CDATA[
arXiv:2505.12332v5 Announce Type: replace-cross 
Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PET Image Reconstruction Using Deep Diffusion Image Prior</title>
<link>https://arxiv.org/abs/2507.15078</link>
<guid>https://arxiv.org/abs/2507.15078</guid>
<content:encoded><![CDATA[
arXiv:2507.15078v2 Announce Type: replace-cross 
Abstract: Diffusion models have shown great promise in medical image denoising and reconstruction, but their application to Positron Emission Tomography (PET) imaging remains limited by tracer-specific contrast variability and high computational demands. In this work, we proposed an anatomical prior-guided PET image reconstruction method based on diffusion models, inspired by the deep diffusion image prior (DDIP) framework. The proposed method alternated between diffusion sampling and model fine-tuning guided by the PET sinogram, enabling the reconstruction of high-quality images from various PET tracers using a score function pretrained on a dataset of another tracer. To improve computational efficiency, the half-quadratic splitting (HQS) algorithm was adopted to decouple network optimization from iterative PET reconstruction. The proposed method was evaluated using one simulation and two clinical datasets. For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested on [$^{18}$F]FDG data and amyloid-negative PET data to assess out-of-distribution (OOD) performance. For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one [$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from another tracer. Experiment results show that the proposed PET reconstruction method can generalize robustly across tracer distributions and scanner types, providing an efficient and versatile reconstruction framework for low-dose PET imaging.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding WaveMamba with Frequency Maps for Image Debanding</title>
<link>https://arxiv.org/abs/2508.11331</link>
<guid>https://arxiv.org/abs/2508.11331</guid>
<content:encoded><![CDATA[
arXiv:2508.11331v2 Announce Type: replace-cross 
Abstract: Compression at low bitrates in modern codecs often introduces banding artifacts, especially in smooth regions such as skies. These artifacts degrade visual quality and are common in user-generated content due to repeated transcoding. We propose a banding restoration method that employs the Wavelet State Space Model and a frequency masking map to preserve high-frequency details. Furthermore, we provide a benchmark of open-source banding restoration methods and evaluate their performance on two public banding image datasets. Experimentation on the available datasets suggests that the proposed post-processing approach effectively suppresses banding compared to the state-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving image textures. Visual inspections of the results confirm this. Code and supplementary material are available at: https://github.com/xinyiW915/Debanding-PCS2025.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
<link>https://arxiv.org/abs/2508.19112</link>
<guid>https://arxiv.org/abs/2508.19112</guid>
<content:encoded><![CDATA[
arXiv:2508.19112v3 Announce Type: replace-cross 
Abstract: Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained transformer encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution decoder, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Contexts for Long Video Generation</title>
<link>https://arxiv.org/abs/2508.21058</link>
<guid>https://arxiv.org/abs/2508.21058</guid>
<content:encoded><![CDATA[
arXiv:2508.21058v3 Announce Type: replace-cross 
Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects</title>
<link>https://arxiv.org/abs/2510.02069</link>
<guid>https://arxiv.org/abs/2510.02069</guid>
<content:encoded><![CDATA[
arXiv:2510.02069v2 Announce Type: replace-cross 
Abstract: Accurate reconstruction and relighting of glossy objects remains a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restrict faithful material recovery and limit relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine environment map optimization accelerates convergence, and negative-only environment map clipping preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title>
<link>https://arxiv.org/abs/2510.24411</link>
<guid>https://arxiv.org/abs/2510.24411</guid>
<content:encoded><![CDATA[
arXiv:2510.24411v2 Announce Type: replace-cross 
Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Our code and data are available at https://github.com/OS-Copilot/OS-Sentinel.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.09907</link>
<guid>https://arxiv.org/abs/2511.09907</guid>
<content:encoded><![CDATA[
arXiv:2511.09907v2 Announce Type: replace-cross 
Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</title>
<link>https://arxiv.org/abs/2512.00387</link>
<guid>https://arxiv.org/abs/2512.00387</guid>
<content:encoded><![CDATA[
<div> Keywords: WiseEdit, image editing, cognitive creativity, knowledge-intensive benchmark, evaluation

<br /><br />Summary:  
The paper introduces WiseEdit, a novel knowledge-intensive benchmark designed to comprehensively evaluate advanced image editing models that incorporate cognition and creativity. Existing benchmarks are criticized for their narrow scope and inability to fully assess these sophisticated abilities. WiseEdit addresses this gap by structuring image editing tasks into three cascaded cognitive steps: Awareness, Interpretation, and Imagination, each representing distinct challenges for model evaluation. The benchmark additionally includes complex tasks that require the integrated use of these three steps, pushing models beyond simple editing capabilities. WiseEdit incorporates three fundamental types of knowledge—Declarative, Procedural, and Metacognitive—providing broad knowledge coverage to test models' reasoning and creativity. In total, the benchmark contains 1,220 test cases, enabling a detailed and objective assessment of state-of-the-art image editing models’ limitations in cognitive reasoning and creative composition. The benchmark, along with evaluation code and example generated images from various models, will be made publicly available soon. This work aims to push forward research in intelligent image editing by offering a deep and broad evaluation framework inspired by human cognitive processes in creativity. Further details and resources are accessible on the project webpage. <div>
arXiv:2512.00387v2 Announce Type: replace 
Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</title>
<link>https://arxiv.org/abs/2512.00960</link>
<guid>https://arxiv.org/abs/2512.00960</guid>
<content:encoded><![CDATA[
<div> 4D HOI reconstruction, human-object interaction, monocular videos, human-in-the-loop, dataset

<br /><br />Summary: This paper addresses the challenge of reconstructing accurate and scalable 4D human-object interaction (HOI) data from diverse and large-scale monocular internet videos, which are abundant but difficult to analyze. The authors propose 4DHOISolver, an efficient optimization framework that integrates sparse human-in-the-loop contact point annotations to constrain the ill-posed 4D HOI reconstruction problem, ensuring high spatio-temporal coherence and physical plausibility. Using this method, they introduce Open4DHOI, a large-scale dataset featuring 144 object types and 103 different actions, offering a rich variety for learning generalized robot interactions. The effectiveness of the reconstructed 4D interactions is validated by enabling a reinforcement learning agent to imitate the recovered human-object manipulation motions. Despite these advances, the paper highlights that current 3D foundation models fall short in automatically predicting precise human-object contact correspondences, making this an open problem for the community. This limitation justifies the human-in-the-loop annotation strategy employed. Data and code for 4DHOISolver and Open4DHOI will be made publicly available, advancing research in robotics and HOI understanding from in-the-wild videos. <div>
arXiv:2512.00960v2 Announce Type: replace 
Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-ACT: Learn from Multimodal Parallel Generation to Act</title>
<link>https://arxiv.org/abs/2512.00975</link>
<guid>https://arxiv.org/abs/2512.00975</guid>
<content:encoded><![CDATA[
<div> Keywords: generalist robotic policy, Vision-Language-Action model, multimodal learning, cross-modal learning, robotic task performance<br /><br />Summary:<br /><br />This paper introduces MM-ACT, a unified Vision-Language-Action (VLA) model designed for generalist robotic policies that require semantic understanding and predictive interaction capabilities. The model integrates text, image, and action data within a shared token space and supports generation across these three modalities. To enhance generation efficiency, MM-ACT utilizes a re-mask parallel decoding strategy for text and image outputs and a one-step parallel decoding strategy for action generation. The novel training method, called Context-Shared Multimodal Learning, supervises generation across modalities from a unified contextual representation, boosting the quality of action generation through cross-modal information exchange. The model’s performance was validated on three benchmarks: the LIBERO simulation, real-world Franka robot tasks, and RoboTwin2.0 for bimanual task execution. MM-ACT achieved impressive success rates of 96.3% on LIBERO, 72.0% on three Franka robot tasks, and 52.38% on eight RoboTwin2.0 tasks, demonstrating strong in-domain and out-of-domain capabilities. Notably, the cross-modal learning paradigm contributed an additional 9.25% improvement in task success rates. The authors have made their codes, models, and datasets publicly available at the provided GitHub repository to facilitate further research and development in robotic generalist policies. <div>
arXiv:2512.00975v2 Announce Type: replace 
Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</title>
<link>https://arxiv.org/abs/2512.01302</link>
<guid>https://arxiv.org/abs/2512.01302</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, Multi-Modal Diffusion Transformers, divide-and-conquer, attention masks, noise initialization<br /><br />Summary:<br /><br />This paper addresses the challenge of generating accurate long or multiple texts in text-to-image models, which typically suffer from diluted global attention. The authors propose DCText, a training-free method based on a divide-and-conquer approach that leverages Multi-Modal Diffusion Transformers known for reliable short-text generation. DCText first decomposes complex prompts by extracting and dividing the target text into segments, each assigned to specific regions in the image. To ensure each text segment is accurately rendered and the overall image remains coherent, two novel attention masks, Text-Focus and Context-Expansion, are introduced and applied sequentially during the denoising process. Furthermore, the method employs Localized Noise Initialization to enhance text accuracy and alignment of regions without increasing computational overhead. Extensive experiments on both single- and multi-sentence benchmarks demonstrate that DCText achieves superior text accuracy compared to existing approaches, while maintaining high image quality. Additionally, DCText outperforms others in generation speed, offering the lowest latency. This technique provides an effective and efficient solution to improve text rendering in text-to-image synthesis without requiring additional training. <div>
arXiv:2512.01302v2 Announce Type: replace 
Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians</title>
<link>https://arxiv.org/abs/2512.01306</link>
<guid>https://arxiv.org/abs/2512.01306</guid>
<content:encoded><![CDATA[
<div> Gaussian Swaying, aerodynamic simulation, 3D Gaussians, surface-based framework, realistic motion<br /><br />Summary:<br /><br />This paper introduces Gaussian Swaying, a novel surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike traditional mesh-based methods that require complex and computationally expensive meshing, or particle-based techniques which depend on discrete positional data, this approach models surfaces continuously with 3D Gaussian patches. This continuous representation enables efficient and fine-grained aerodynamic interactions that enhance realism in simulating natural motions such as branches swaying, flags rippling, and boats rocking. A key advantage of the method is its unification of simulation and rendering on the same representation, where Gaussian patches facilitate both force computations for dynamics and normal calculations for lightweight shading simultaneously. The framework is tested across synthetic and real-world datasets and evaluates multiple performance metrics to demonstrate its effectiveness. Results show that Gaussian Swaying achieves state-of-the-art performance and computational efficiency, making it a scalable solution for realistic aerodynamic scene simulation. The research highlights how this method advances the realism and efficiency of aerodynamic effects critical for applications in vision and graphics. <div>
arXiv:2512.01306v2 Announce Type: replace 
Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title>
<link>https://arxiv.org/abs/2512.01821</link>
<guid>https://arxiv.org/abs/2512.01821</guid>
<content:encoded><![CDATA[
<div> Spatial reasoning, Multimodal Large Language Models, Implicit spatial modeling, Relative Positional Encoding, Geometry-aware dataset<br /><br />Summary:<br /><br />This paper addresses the challenge of spatial reasoning in Multimodal Large Language Models (MLLMs), highlighting the limitation of current methods that rely solely on verbal descriptive tuning, leading to visual illiteracy. To overcome this, the authors propose MILO, an implicit spatial world modeling paradigm that enables human-like spatial imagination by integrating a visual generator to provide geometry-aware feedback. This integration grounds symbolic reasoning in perceptual visual experience rather than purely textual symbols. Alongside MILO, the study introduces RePE (Relative Positional Encoding), a novel encoding method that captures relative camera-pose transformations, which proves more effective than absolute coordinate systems for spatial understanding. To facilitate training, the authors develop GeoGen, a large-scale dataset featuring around 2,241 videos and 67,827 observation-action-outcome triplets, designed to be geometry-aware and generative. Experimental results across multiple baselines and benchmarks demonstrate that this combined approach significantly improves the spatial reasoning capabilities of MLLMs, enabling a more comprehensive and holistic understanding of 3D spaces. This work represents an important advancement in bridging symbolic spatial concepts and perceptual visual grounding in multimodal AI systems. <div>
arXiv:2512.01821v2 Announce Type: replace 
Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTArena: A Benchmark for Agentic PowerPoint Editing</title>
<link>https://arxiv.org/abs/2512.03042</link>
<guid>https://arxiv.org/abs/2512.03042</guid>
<content:encoded><![CDATA[
<div> PowerPoint editing, benchmark, PPTArena, PPTPilot, visual fidelity<br /><br />Summary:<br /><br />1. The paper introduces PPTArena, a new benchmark designed to evaluate reliable in-place editing of real PowerPoint slides guided by natural-language instructions. <br /><br />2. Unlike previous approaches focusing on image-PDF renderings or text-to-slide generation, PPTArena emphasizes detailed edits within existing slides covering text, charts, tables, animations, and master-level styles. It comprises 100 presentation decks, 2125 slides, and over 800 targeted edits. <br /><br />3. Each task in PPTArena includes a ground-truth deck along with a fully specified target outcome and utilizes a dual vision-language model (VLM) judge pipeline that separately assesses instruction adherence and visual quality using structural differences and slide image comparisons. <br /><br />4. The authors present PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences and dynamically routes between high-level programmatic tools and deterministic XML operations for precise modifications. It verifies results through an iterative plan-edit-check loop against task constraints. <br /><br />5. Experimental results demonstrate that PPTPilot outperforms strong proprietary and state-of-the-art VLM-based systems by over 10 percentage points, particularly excelling in visual fidelity, layout-sensitive edits, and deck-wide consistency. Nonetheless, long-horizon, document-scale editing tasks in PPTArena remain challenging for all existing agents, indicating significant room for future improvement in reliable automated PPT editing. <div>
arXiv:2512.03042v2 Announce Type: replace 
Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices</title>
<link>https://arxiv.org/abs/2512.05969</link>
<guid>https://arxiv.org/abs/2512.05969</guid>
<content:encoded><![CDATA[
<div> Keywords: video generation models, reasoning, Task Pair design, automated evaluation, reinforcement learning<br /><br />Summary:<br /><br />This paper demonstrates that video generation models have advanced to a stage where they can perform reasoning tasks. The models were tested on diverse reasoning challenges such as chess, maze navigation, Sudoku, mental rotation, and Raven’s Matrices, with leading models like Sora-2 achieving success rates around sixty percent. To facilitate systematic evaluation, the authors introduce a robust experimental paradigm called the "Task Pair" design, which pairs related tasks to test reasoning ability. They developed a code framework supporting 39 pre-integrated models and allowing easy addition of new models and tasks, ensuring scalability and extensibility. The study validates that their automated evaluation metrics strongly correlate with human judgment, indicating that this evaluation approach is reliable and scalable. With this foundation, the paper suggests that reinforcement learning can be applied to further improve the reasoning capabilities of video generation models. The authors provide open access to their raw results via a public website and share their entire evaluation codebase, VMEvalKit, on GitHub, encouraging community involvement and further research in this area. <div>
arXiv:2512.05969v1 Announce Type: new 
Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Dataset Quantization: A New Direction for Dataset Pruning</title>
<link>https://arxiv.org/abs/2512.05987</link>
<guid>https://arxiv.org/abs/2512.05987</guid>
<content:encoded><![CDATA[
<div> Dataset Quantization, Intra-sample Redundancy, Adaptive Quantization, Edge Devices, Compression

<br /><br />Summary:  
This paper proposes a novel dataset quantization approach aimed at reducing storage and communication costs for large-scale datasets, particularly in resource-constrained edge devices. Unlike traditional methods that address inter-sample redundancy, the presented technique focuses on compressing each image by reducing intra-sample redundancy, i.e., redundant or less informative content within individual samples while preserving essential features. The method first uses linear symmetric quantization to determine an initial quantization range and scale for each sample. Following this, an adaptive quantization allocation algorithm is employed to assign different quantization ratios according to samples' precision requirements, ensuring a constant total compression ratio across the dataset. Key contributions include being the first to represent datasets with limited bits for significant storage reduction, introducing a dataset-level quantization algorithm with adaptive ratio allocation, and performing extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. The results demonstrate that this approach maintains model training performance while achieving substantial compression, outperforming conventional quantization and dataset pruning techniques at equivalent compression ratios. This work highlights the potential for effective dataset compression strategies tailored to edge computing scenarios without sacrificing training accuracy. <div>
arXiv:2512.05987v1 Announce Type: new 
Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG3T: Visual Geometry Grounded Gaussian Transformer</title>
<link>https://arxiv.org/abs/2512.05988</link>
<guid>https://arxiv.org/abs/2512.05988</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic occupancy, multi-view fusion, Gaussian representation, Grid-Based Sampling, Positional Refinement<br /><br />Summary:<br /><br />1. The paper addresses the challenge of creating coherent 3D scene representations from multiple images taken at different viewpoints, which is a fundamental problem in 3D reconstruction and scene understanding.<br />2. Existing methods often fail due to fragmented and inconsistent 3D representations caused by processing each view independently, limiting the overall quality and coherence of the 3D output.<br />3. The authors propose VG3T, a novel multi-view feed-forward neural network that predicts a 3D semantic occupancy by directly modeling a set of semantically labeled 3D Gaussian primitives, integrating information from all views jointly rather than view-by-view.<br />4. Two key innovations, Grid-Based Sampling and Positional Refinement, are introduced to reduce the typical distance-dependent density bias arising from pixel-aligned initializations of Gaussian primitives, improving geometric and semantic accuracy.<br />5. VG3T demonstrates superior performance on the challenging nuScenes dataset by achieving a 1.7% absolute improvement in mean Intersection over Union (mIoU) while using 46% fewer Gaussian primitives than the previous state-of-the-art, indicating enhanced efficiency and representational power. <div>
arXiv:2512.05988v1 Announce Type: new 
Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head</title>
<link>https://arxiv.org/abs/2512.05991</link>
<guid>https://arxiv.org/abs/2512.05991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, emotional expression, Emotion-aware Gaussian Diffusion, action unit prompt, text-to-AU emotion controller<br /><br />Summary:<br /><br />This paper addresses the limitations of existing photo-realistic 3D talking head models based on 3D Gaussian Splatting, particularly in manipulating emotional expressions with fine granularity and expansive dynamics via multimodal control. The authors propose EmoDiffTalk, an editable 3D Gaussian talking head framework that introduces a novel Emotion-aware Gaussian Diffusion mechanism. This mechanism incorporates an action unit (AU) prompt Gaussian diffusion process designed for detailed facial animation, enabling precise control over subtle emotional expressions. Furthermore, EmoDiffTalk integrates an accurate text-to-AU emotion controller, allowing expressive and dynamic emotional editing driven by text input. The system has been evaluated on public datasets EmoTalk3D and RenderMe-360, where it demonstrates superior performance in emotional subtlety, lip-sync accuracy, and overall controllability compared to prior works. EmoDiffTalk sets a new standard for high-quality, diffusion-based multimodal 3D talking head synthesis. Notably, this framework is among the first to leverage 3D Gaussian Splatting for talking-head generation that supports continuous and multimodal emotional editing within the AU-based expression space, paving the way for advanced, editable 3D facial animation technologies. <div>
arXiv:2512.05991v1 Announce Type: new 
Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology</title>
<link>https://arxiv.org/abs/2512.05993</link>
<guid>https://arxiv.org/abs/2512.05993</guid>
<content:encoded><![CDATA[
<div> Neuropathology, foundation models, neurodegenerative diseases, whole-slide images, domain-specific AI  

<br /><br />Summary:  
1. Foundation models have revolutionized computational pathology by learning from large histology datasets but are mainly trained on surgical pathology data, which does not represent nervous tissue well.  
2. Neuropathology involves distinct tissue types and pathological features, such as neurons, glia, neurofibrillary tangles, amyloid plaques, Lewy bodies, and unique neurodegeneration patterns, which are not adequately captured by general-purpose models.  
3. To bridge this domain gap, the authors developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue covering various neurodegenerative diseases.  
4. NeuroFM outperforms general foundation models in neuropathology-specific tasks including mixed dementia classification, segmentation of the hippocampal region, and identification of neurodegenerative ataxias such as cerebellar essential tremor and spinocerebellar ataxia subtypes.  
5. This study demonstrates that domain-specialized foundation models tailored to brain pathology provide more accurate and reliable analyses for diagnosing and researching neurodegenerative diseases, setting a precedent for future development of specialized AI models in digital pathology. <div>
arXiv:2512.05993v1 Announce Type: new 
Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</title>
<link>https://arxiv.org/abs/2512.05996</link>
<guid>https://arxiv.org/abs/2512.05996</guid>
<content:encoded><![CDATA[
<div> Keywords: Fish detection, Underwater imagery, Weak supervision, Reinforcement learning, Marine ecology

<br /><br />Summary: Analyzing underwater fish imagery presents challenges due to visual degradation and the high cost of detailed annotations. The paper introduces FishDetector-R1, a novel multi-modal large model (MLLM)-based framework designed to detect, segment, and count fish using weak supervision. Evaluated on the DeepFish dataset, FishDetector-R1 shows significant performance improvements, achieving a 20% increase in Average Precision (AP), 10% higher mean Intersection over Union (mIoU), a 30% reduction in Mean Absolute Error (MAE), and a 35% decrease in the Grid Average Mean Absolute Error (GAME). These gains are attributed to two core innovations: a detect-to-count prompt that ensures spatially consistent fish detections and counts, and a Reinforcement Learning from Verifiable Reward (RLVR) method that uses sparse point labels within a scalable reward framework. Ablation studies confirm the effectiveness of the designed reward system. Additionally, FishDetector-R1 demonstrates strong cross-domain generalization capabilities on other underwater datasets, highlighting its robustness. Overall, this approach offers a reliable and scalable solution to improve marine visual ecological monitoring through weakly supervised learning, reducing annotation costs while maintaining high accuracy. More details and resources are available at the project page: https://umfieldrobotics.github.io/FishDetector-R1. <div>
arXiv:2512.05996v1 Announce Type: new 
Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrunedCaps: A Case For Primary Capsules Discrimination</title>
<link>https://arxiv.org/abs/2512.06003</link>
<guid>https://arxiv.org/abs/2512.06003</guid>
<content:encoded><![CDATA[
<div> Capsule Networks, Pruning, Primary Capsules, Dynamic Routing, Computational Efficiency<br /><br />Summary:<br /><br />This paper addresses the inefficiency of Capsule Networks (CapsNets) caused by the large number of Primary Capsules (PCs), which results in slow training and testing phases. The authors propose pruning the Primary Capsules to enhance the resource efficiency of CapsNets. They conduct experiments on four datasets: MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN. The results demonstrate that pruning up to 95 percent of the Capsules significantly accelerates the CapsNet, achieving up to 9.90 times faster performance compared to the original architecture without any drop in accuracy. Additionally, the pruned CapsNet reduces floating-point operations in the dynamic routing stage by more than 95.36 percent, highlighting substantial computational savings. The study further explores the varying impact of pruning on different datasets, providing insights into why some datasets benefit more from pruning than others. Overall, this research offers a practical approach to make CapsNets more resource-efficient while maintaining their advantages, contributing to faster and less resource-intensive deep learning models suitable for image classification tasks. <div>
arXiv:2512.06003v1 Announce Type: new 
Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization</title>
<link>https://arxiv.org/abs/2512.06006</link>
<guid>https://arxiv.org/abs/2512.06006</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, code adaptation, biomedical imaging, agent design, production pipeline<br /><br />Summary:<br /><br />1. The article addresses the challenge of adapting production-level computer vision tools to specialized scientific datasets, which currently faces significant bottlenecks due to the need for large annotated datasets for fine-tuning or labor-intensive manual code changes.  
2. It proposes the use of AI agents to automate the manual coding process required for adapting these tools, aiming to streamline and accelerate the adaptation to bespoke scientific datasets.  
3. A systematic evaluation framework for agentic code optimization is introduced, enabling objective assessment of different AI agent designs on this task.  
4. The framework is applied to three biomedical imaging pipelines, demonstrating that a simple AI agent architecture can consistently generate adaptation code that outperforms solutions produced by human experts.  
5. The study finds that more complex agent architectures do not always offer advantages, offering practical insights and a roadmap for optimal agent design.  
6. The framework and approach have been open-sourced, and the authors validate their method by successfully deploying agent-generated functions into a real production pipeline, illustrating clear potential for real-world scientific impact. <div>
arXiv:2512.06006v1 Announce Type: new 
Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Flexible Robustness Certificates for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06010</link>
<guid>https://arxiv.org/abs/2512.06010</guid>
<content:encoded><![CDATA[
<div> Keywords: certifiable robustness, semantic segmentation, Lipschitz constraints, adversarial attacks, randomized smoothing<br /><br />Summary: This paper addresses the vulnerability of deep neural networks to small adversarial perturbations, focusing on semantic segmentation tasks rather than classification. The authors introduce a new class of semantic segmentation networks incorporating built-in Lipschitz constraints to ensure certifiable robustness. These networks are efficiently trainable and achieve competitive pixel accuracy on challenging datasets like Cityscapes. A novel framework is proposed to generalize robustness certificates for semantic segmentation, demonstrating both computational efficiency and flexibility of Lipschitz networks in providing robustness guarantees. The approach enables real-time compatible certifiably robust semantic segmentation for the first time, which is a significant advancement in practical deployment. The method also allows for the computation of worst-case performance under \(\ell_2\) adversarial attacks within a specified radius \(\epsilon\), covering a variety of performance metrics. A major highlight is the certification process's speed, which is shown to be around 600 times faster than randomized smoothing methods at inference on an NVIDIA A100 GPU while maintaining comparable certificate quality. Finally, the authors validate the tightness of their worst-case robustness certificates by benchmarking them against state-of-the-art adversarial attacks, confirming the robustness and reliability of the proposed method. <div>
arXiv:2512.06010v1 Announce Type: new 
Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $\epsilon$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2512.06012</link>
<guid>https://arxiv.org/abs/2512.06012</guid>
<content:encoded><![CDATA[
<div> Keywords: Selective Laser Melting, powder morphology, machine learning, clustering, Fourier descriptors  

<br /><br />Summary:  
1. Selective Laser Melting (SLM) is an additive manufacturing technique whose part quality is highly dependent on the morphology of the metallic powder feedstock.  
2. Traditional powder characterization methods are low-throughput and qualitative, often failing to represent the heterogeneity found in industrial-scale powder batches.  
3. The authors propose an automated, machine learning-based framework that integrates high-throughput imaging with shape extraction and clustering to profile powder morphology at scale.  
4. Three clustering pipelines are developed and evaluated: an autoencoder-based pipeline, a shape-descriptor-based pipeline, and a functional-data-based pipeline.  
5. Using a dataset of approximately 126,000 powder particle images ranging from 0.5 to 102 micrometers, internal validity metrics such as the Davies-Bouldin index and Calinski-Harabasz score identify the Fourier-descriptor combined with k-means clustering as the most effective approach.  
6. This Fourier-descriptor + k-means pipeline achieves a very low Davies-Bouldin index and high Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop, demonstrating both effectiveness and efficiency.  
7. Although this study focuses on establishing a robust morphological clustering framework, the identified shape groups provide a foundation for future investigations into how particle shape affects powder flowability, packing density, and final SLM part quality.  
8. The unsupervised learning framework enables rapid, automated morphological assessment and allows monitoring of shape changes across powder reuse cycles, offering a path toward real-time feedstock monitoring within SLM manufacturing workflows. <div>
arXiv:2512.06012v1 Announce Type: new 
Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAT: Vision Action Transformer by Unlocking Full Representation of ViT</title>
<link>https://arxiv.org/abs/2512.06013</link>
<guid>https://arxiv.org/abs/2512.06013</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, robot learning, action tokens, imitation learning, manipulation tasks<br /><br />Summary:<br /><br />1. The paper addresses a limitation in current robot learning methods that use Vision Transformers (ViTs) by relying solely on features from the final layer, which discards valuable information.  
2. It proposes a novel architecture called Vision Action Transformer (VAT), which extends ViT by incorporating action tokens processed with visual features across all transformer layers.  
3. This design enables a deep and progressive fusion of perception and action generation, utilizing the full hierarchical representation of ViT rather than only the last layer's output.  
4. Experimental evaluation on simulated manipulation tasks across four LIBERO benchmarks shows that VAT achieves a 98.15% average success rate, outperforming previous state-of-the-art approaches such as OpenVLA-OFT.  
5. The work highlights the critical importance of leveraging the entire "representation trajectory" in vision models to enhance robotic policy learning and provides publicly available code on GitHub for reproducibility and further research. <div>
arXiv:2512.06013v1 Announce Type: new 
Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets</title>
<link>https://arxiv.org/abs/2512.06014</link>
<guid>https://arxiv.org/abs/2512.06014</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, chest X-ray, medical imaging, embedding models, model benchmarking

<br /><br />Summary:  
This study benchmarks two large-scale chest X-ray embedding models, CXR-Foundation (ELIXR v2.0) and MedImageInsight, using public datasets MIMIC-CR and NIH ChestX-ray14. Both models were evaluated through a unified preprocessing pipeline alongside fixed downstream LightGBM classifiers to ensure reproducibility. Embeddings were extracted directly from pre-trained encoders for multiple disease label prediction tasks. Performance metrics including mean AUROC and F1-score with 95% confidence intervals were reported. Results showed MedImageInsight slightly outperformed CXR-Foundation across most tasks, while CXR-Foundation demonstrated superior cross-dataset stability. Additionally, unsupervised clustering of MedImageInsight embeddings revealed well-defined disease-specific grouping consistent with quantitative performance. The study emphasizes the importance of standardizing evaluations for medical foundation models to enable fair and reproducible comparisons. Finally, it establishes reproducible baseline performances to facilitate future research, particularly in multimodal data integration and clinical applications. <div>
arXiv:2512.06014v1 Announce Type: new 
Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation</title>
<link>https://arxiv.org/abs/2512.06020</link>
<guid>https://arxiv.org/abs/2512.06020</guid>
<content:encoded><![CDATA[
<div> Preference-conditioned image generation, multimodal large language models, visual question answering, inter-user discrimination, diffusion-based image generation<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting image generation models to individual user preferences, aiming to produce images that reflect personal aesthetics beyond the textual prompt. The authors propose a novel multimodal framework that employs multimodal large language models (MLLMs) to extract detailed user representations. These representations are introduced into diffusion-based image generators to guide output personalization. To refine preference extraction, the MLLM is trained on a preference-oriented visual question answering task, enabling it to capture subtle semantic cues. The framework includes two probing tasks: inter-user discrimination, which differentiates between users, and intra-user discrimination, which distinguishes between content liked or disliked by a user. To ensure that the user embeddings align with the diffusion model’s text encoders despite modality differences, a maximum mean discrepancy-based alignment loss is designed, preserving multimodal structure and compatibility. The resulting embeddings effectively condition the image generator, allowing it to honor both the user's preferences and the given text prompts. Extensive experiments validate the approach, showing significant improvements over existing baselines in image quality and preference alignment, demonstrating the strength of enhanced representation extraction and modality alignment for personalized image generation. <div>
arXiv:2512.06020v1 Announce Type: new 
Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing</title>
<link>https://arxiv.org/abs/2512.06024</link>
<guid>https://arxiv.org/abs/2512.06024</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, wave free surface, velocity fields, attention-augmented neural network, ocean waves<br /><br />Summary:<br /><br />1. The paper presents a novel neural network designed for precise three-dimensional reconstruction of ocean wave free surfaces and associated velocity fields, tackling the challenge of long-term ocean wave observations.<br /><br />2. The network architecture is an attention-augmented pyramid tailored to handle the multi-scale and temporally continuous nature of wave motions, improving robustness especially under visual occlusions.<br /><br />3. Physics-based constraints are integrated into the model to enable time-resolved reconstruction of nonlinear 3D velocity fields derived from the evolving free-surface boundary.<br /><br />4. Experimental results from real-sea conditions show millimetre-level accuracy in wave elevation prediction, dominant-frequency errors below 0.01 Hz, and precise estimation of high-frequency spectral power laws.<br /><br />5. The model achieves high-fidelity 3D reconstruction of nonlinear velocity fields efficiently, capable of dense reconstruction of two million points in just 1.35 seconds, outperforming conventional visual reconstruction methods and demonstrating strong generalization even with occlusions, due to global multi-scale attention and learned wave propagation dynamics. <div>
arXiv:2512.06024v1 Announce Type: new 
Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06032</link>
<guid>https://arxiv.org/abs/2512.06032</guid>
<content:encoded><![CDATA[
<div> Segment Anything Models, SAM2, SAM3, prompt-based segmentation, concept-driven segmentation<br /><br />Summary:<br /><br />This paper highlights the fundamental discontinuity between Segment Anything Model versions SAM2 and SAM3. (1) Conceptual Break: SAM2 relies on spatial prompts like points, boxes, and masks for geometric and temporal segmentation, whereas SAM3 utilizes a multimodal fusion framework incorporating vision and language for semantic, open-vocabulary, and concept-driven segmentation. (2) Architectural Divergence: SAM2 is purely vision-temporal with spatial prompt inputs, while SAM3 integrates vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and Mixture-of-Experts to manage ambiguity. (3) Dataset and Annotation Differences: SAM2 is trained on SA-V video masks focused on geometric delineation, whereas SAM3 leverages multimodal corpora annotated with rich semantic concepts. (4) Training and Hyperparameter Distinctions: SAM2’s optimization strategies are unsuitable for SAM3 due to the latter’s complex multimodal objectives and architecture. (5) Evaluation and Failure Modes: SAM2 uses geometric Intersection-over-Union (IoU) metrics, while SAM3 requires semantic, open-vocabulary evaluation to capture concept-level performance. Overall, the analysis positions SAM3 as initiating a new class of segmentation foundation models centered on concept-driven understanding, marking a shift from traditional prompt-based segmentation and indicating future research directions in this domain. <div>
arXiv:2512.06032v1 Announce Type: new 
Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Learning for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.06058</link>
<guid>https://arxiv.org/abs/2512.06058</guid>
<content:encoded><![CDATA[
<div> 3D data, point cloud, supervised learning, self-supervised learning, transfer learning<br /><br />Summary:<br /><br />This dissertation addresses the growing importance of 3D data acquisition and utilization across multiple fields such as computer vision, robotics, and geospatial analysis. It highlights the diverse methods of capturing 3D data, including 3D scanners, LiDARs, and RGB-D cameras, which provide detailed geometric, shape, and scale information. The work emphasizes the benefits of combining 3D data with 2D images to enhance machine understanding of environments, which is crucial for applications like autonomous driving, robotics, remote sensing, and medical treatment. The core focus of the research lies in three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning techniques, and transfer learning by leveraging 2D pre-trained models to improve 3D network training. Importantly, the approach does not rely on simply transforming 2D data into 3D but integrates 2D knowledge to boost 3D understanding effectively. The dissertation includes extensive experiments demonstrating the effectiveness and potential of the proposed methods to advance point cloud representation learning through the integration of 2D and 3D data. <div>
arXiv:2512.06058v1 Announce Type: new 
Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</title>
<link>https://arxiv.org/abs/2512.06065</link>
<guid>https://arxiv.org/abs/2512.06065</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric video editing, instruction-guided, real-time inference, hand-object interactions, dataset  

<br /><br />Summary:  
This work focuses on instruction-guided editing of egocentric videos, targeting interactive augmented reality (AR) applications. Unlike existing AI video editors that perform well on third-person footage, egocentric videos present unique challenges such as rapid egomotion and frequent hand-object interactions, creating a considerable domain gap that complicates editing tasks. Traditional offline editing methods also suffer from high latency, restricting their usability for real-time interactions. To overcome these challenges, the authors introduce a comprehensive ecosystem for egocentric video editing consisting of three key components. First, they create EgoEditData, a manually curated dataset tailored to egocentric editing scenarios which emphasizes preserving hand and object interactions explicitly. Second, they develop EgoEdit, an instruction-following video editor optimized for egocentric footage that supports real-time streaming inference on a single GPU, enabling interactive latency. Third, they propose EgoEditBench, an evaluation suite designed to assess instruction faithfulness, preservation of hands and interactions, and temporal stability despite rapid egomotion. Experimental results demonstrate that EgoEdit delivers temporally stable and instruction-faithful editing outcomes, outperforming existing methods on egocentric editing benchmarks while maintaining competitive performance on general video editing tasks. EgoEditData and EgoEditBench will be publicly released to facilitate further research. <div>
arXiv:2512.06065v1 Announce Type: new 
Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light</title>
<link>https://arxiv.org/abs/2512.06080</link>
<guid>https://arxiv.org/abs/2512.06080</guid>
<content:encoded><![CDATA[
<div> Keywords: single-photon lidar, 3D scene reconstruction, multi-bounce light, multiplexed illumination, deep learning<br /><br />Summary:<br />1. This work tackles the difficult problem of reconstructing 3D scenes, especially in the presence of occluded regions and specular materials such as mirrors, using single-photon lidar technology. <br />2. Single-photon lidars estimate depth by detecting light emitted into the scene and reflected back; uniquely, they also capture multi-bounce light, which carries rich information about occluded geometry and material properties but is challenging to interpret. <br />3. Previous efforts have only succeeded when the laser illuminates one point at a time, while this paper addresses the more practical and complex scenario of simultaneous illumination of multiple scene points, inducing complicated light transport due to multiplexing, two-bounce reflections, shadows, and specularity. <br />4. Analytical inversion of this complex light transport proves impractical, so the authors develop a data-driven approach using deep learning to decompose multi-bounce signals and recover constituent contributions from each laser spot. <br />5. To train their model, they create and release a large simulated dataset with around 100,000 lidar transients from indoor environments, and experimentally validate that their method successfully infers 3D geometry in challenging scenes from a single measurement, demonstrating improved reconstruction in occluded and mirror-containing scenes. <div>
arXiv:2512.06080v1 Announce Type: new 
Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06096</link>
<guid>https://arxiv.org/abs/2512.06096</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Multimodal Language Models, 360-degree BEV, autonomous driving, spatial reasoning  

<br /><br />Summary:  
The paper introduces BeLLA, an innovative end-to-end architecture designed for autonomous driving that integrates unified 360-degree Bird's Eye View (BEV) spatial representations with a large language model to enhance question answering capabilities. Traditional approaches often rely on single-view encoders or aggregated multi-view features, which either fail to fully utilize multi-camera spatial structures or lack a unified spatial framework, limiting effective reasoning about ego-centric directions and object relationships. BeLLA addresses these limitations by providing a consistent and comprehensive spatial representation, enabling more accurate understanding of relative object positions and contextual behavioral patterns crucial for autonomous navigation. The model is rigorously evaluated on two benchmarks—NuScenes-QA and DriveLM—where it demonstrates substantial improvements, especially in tasks demanding complex spatial reasoning, achieving up to a 9.3% absolute gain over prior methods. Additionally, BeLLA remains competitive across a broad spectrum of question types, showcasing its versatility and robustness. Overall, this work advances the integration of vision and language models in driving contexts, pushing forward the interpretability and contextual awareness of autonomous vehicle systems. <div>
arXiv:2512.06096v1 Announce Type: new 
Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360{\deg} BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2512.06103</link>
<guid>https://arxiv.org/abs/2512.06103</guid>
<content:encoded><![CDATA[
<div> Keywords: Iris recognition, Presentation Attack Detection, Multispectral imaging, Vision Transformer, Spoofing detection

<br /><br />Summary: Iris recognition is a highly accurate biometric modality but is vulnerable to presentation attacks (PAs), making effective presentation attack detection (PAD) essential for security. Traditional iris systems typically use near-infrared (NIR) imaging, but multispectral imaging across multiple NIR bands can enhance PAD generalizability by providing complementary reflectance information. This work introduces SpectraIrisPAD, a novel deep learning framework based on a DINOv2 Vision Transformer backbone, which incorporates learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features for robust PAD. To support this, the authors developed MSIrPAD, a new comprehensive multispectral iris PAD dataset containing 18,848 images across eight diverse attack types, including textured contact lenses, print attacks, and display-based attacks, captured at five NIR wavelengths (800 nm, 830 nm, 850 nm, 870 nm, and 980 nm) using a custom multispectral sensor. Extensive experiments under unseen attack scenarios demonstrate that SpectraIrisPAD consistently outperforms state-of-the-art methods on multiple metrics, showcasing its superior robustness and generalization capabilities in detecting a wide variety of iris presentation attacks. <div>
arXiv:2512.06103v1 Announce Type: new 
Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation</title>
<link>https://arxiv.org/abs/2512.06105</link>
<guid>https://arxiv.org/abs/2512.06105</guid>
<content:encoded><![CDATA[
<div> Keywords: melanoma classification, interpretability, contrastive learning, Vision Transformer, clinical trust

<br /><br />Summary: This paper addresses the challenge of model opacity in deep learning-based melanoma classification, which hinders clinical adoption despite high accuracy. The authors propose a Cross-modal Explainable Framework for Melanoma (CEFM) that integrates clinical diagnostic criteria—Asymmetry, Border, and Color (ABC)—directly into the model's embedding space using contrastive learning. By employing dual projection heads within a Vision Transformer architecture, CEFM aligns visual features with clinical semantics, fostering interpretability. These aligned features are then converted into structured textual explanations through natural language generation, creating a transparent connection between raw images and clinical insights. Experimental results on public melanoma datasets show that CEFM achieves a classification accuracy of 92.79% and an AUC of 0.961. The framework also demonstrates significant improvements in interpretability metrics compared to baseline methods. Qualitative analyses further reveal that the spatial arrangement of learned embeddings corresponds well with clinicians’ use of the ABC rule, effectively enhancing trust and understanding of the model's decisions. Overall, CEFM successfully bridges the gap between black-box high-performance melanoma classification models and clinically reliable, interpretable tools for dermatology practice. <div>
arXiv:2512.06105v1 Announce Type: new 
Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation</title>
<link>https://arxiv.org/abs/2512.06158</link>
<guid>https://arxiv.org/abs/2512.06158</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D generation, multi-view video diffusion, point tracking, Gaussian Splatting, dynamic objects  

<br /><br />Summary:  
Generating dynamic 4D objects from sparse multi-view inputs is challenging due to the need for preserving both appearance and motion consistency across views and over time, while avoiding artifacts and temporal drift. The paper identifies an important limitation in existing methods, namely that supervision often relies on pixel- or latent-space video diffusion losses, which do not provide explicit temporally aware, feature-level tracking guidance. To address this, the authors propose Track4DGen, a two-stage framework that integrates a multi-view video diffusion model with a foundational point tracker and a hybrid 4D Gaussian Splatting reconstructor. In the first stage, dense, feature-level point correspondences derived from the tracker are enforced within the diffusion generator to produce temporally consistent features that reduce appearance drift and improve coherence across views. In the second stage, a dynamic 4D Gaussian Splatting model is reconstructed by combining diffusion features carrying the Stage One tracking priors with Hex-plane features, augmented with 4D Spherical Harmonics to better model complex dynamics. Experimentally, Track4DGen outperforms baseline methods on benchmarks for both multi-view video generation and 4D object generation, producing temporally stable and text-editable 4D assets. Additionally, the authors contribute Sketchfab28, a new high-quality, object-centric 4D generation dataset to support future research in this domain. <div>
arXiv:2512.06158v1 Announce Type: new 
Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</title>
<link>https://arxiv.org/abs/2512.06171</link>
<guid>https://arxiv.org/abs/2512.06171</guid>
<content:encoded><![CDATA[
<div> Shearography, defect detection, deep learning, annotation automation, segmentation<br /><br />Summary:<br /><br />1. Shearography is an interferometric method that measures surface displacement gradients, making it highly sensitive for identifying subsurface defects in safety-critical components. 2. A significant barrier to deploying shearography in industrial environments is the absence of extensive, high-quality annotated datasets. Manual annotation is not only time-consuming but also subjective and difficult to standardize across different operators. 3. To address this challenge, the authors propose an automated workflow that leverages deep learning techniques to generate defect annotations directly from shearography data. 4. This workflow produces detailed, high-resolution segmentation masks and bounding-box labels that correspond to defects detected in shearography images. 5. Validation against expert-labeled datasets demonstrates that the automated annotation achieves a level of accuracy adequate for use in weakly supervised training schemes, minimizing the dependency on manual labeling. Overall, this approach facilitates scalable and standardized dataset creation, which in turn supports more robust and efficient defect detection in industrial shearography applications. <div>
arXiv:2512.06171v1 Announce Type: new 
Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction</title>
<link>https://arxiv.org/abs/2512.06174</link>
<guid>https://arxiv.org/abs/2512.06174</guid>
<content:encoded><![CDATA[
<div> Keywords: shadow generation, physical modeling, deep learning, 3D geometry, illumination<br /><br />Summary:  
This paper addresses shadow generation with a focus on producing photorealistic shadows consistent with object geometry and scene illumination. It identifies a gap in existing deep-learning shadow generation methods that rarely incorporate explicit physical modeling based on the physics of shadow formation. The authors propose a novel framework embedding explicit physical modeling of both geometry and illumination into a deep learning pipeline. Starting from a single RGB image, their method estimates approximate 3D geometry represented as dense point maps and predicts a dominant light direction. Using these signals, the framework computes an initial shadow location and shape that adhere to physical principles of shadow casting. This physics-based shadow estimate is then refined within a diffusion model, enhancing realism and ensuring the final shadow maintains coherence with scene geometry and lighting. The model is trained on the DESOBAV2 dataset and demonstrates improved results over prior approaches, notably in challenging scenarios involving complex geometry or ambiguous lighting conditions. Overall, the approach successfully integrates physical insights with data-driven techniques to enhance shadow generation quality and physical correctness. <div>
arXiv:2512.06174v1 Announce Type: new 
Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction</title>
<link>https://arxiv.org/abs/2512.06179</link>
<guid>https://arxiv.org/abs/2512.06179</guid>
<content:encoded><![CDATA[
<div> Keywords: attached shadows, cast shadows, shadow detection, light estimation, geometry-illumination reasoning  

<br /><br />Summary:  
Attached shadows occur on surfaces where light is blocked due to self-occlusion, playing a vital role in perceiving 3D structure and improving scene understanding. Despite their importance, current shadow detection research primarily focuses on cast shadows, lacking specialized datasets and models for attached shadows. To bridge this gap, the authors propose a novel framework that jointly detects both cast and attached shadows by leveraging the interplay between shadows, scene illumination, and geometry. The system comprises a shadow detection module that separately predicts cast and attached shadows, alongside a light estimation module that deduces the light direction from shadow cues. Using the estimated light direction with surface normals, the framework generates a geometry-consistent partial map, highlighting regions prone to self-occlusion. This map is fed back to refine shadow predictions, creating an iterative closed-loop process that enhances both shadow segmentation and light estimation simultaneously. The authors also constructed a dedicated dataset containing 1,458 images with distinct annotations for cast and attached shadows to facilitate robust training and evaluation. Experimental results demonstrate that this geometry-illumination iterative reasoning approach significantly improves attached shadow detection performance, achieving at least a 33% reduction in Balanced Error Rate (BER) while preserving accuracy in detecting cast and full shadows. <div>
arXiv:2512.06179v1 Announce Type: new 
Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling</title>
<link>https://arxiv.org/abs/2512.06185</link>
<guid>https://arxiv.org/abs/2512.06185</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, fooling images, black-box attack, vision transformers, model robustness<br /><br />Summary:<br /><br />1. This study revisits the concept of "fooling images," initially proposed by Nguyen et al. (2015), which are inputs that cause deep neural networks (DNNs) to produce high-confidence incorrect classifications despite bearing no resemblance to natural images. <br /><br />2. The authors re-implement evolutionary fooling attacks using both CPPN-based and direct-encoding-based approaches on modern architectures, including convolutional neural networks and transformer models like ViT-B/16. Their findings confirm that state-of-the-art models remain vulnerable, with the ViT-B/16 transformer being the most susceptible to quick and near-certain misclassifications.<br /><br />3. Introducing SPOOF, a new minimalist and efficient black-box attack, the paper demonstrates that it can generate high-confidence fooling images with minimal pixel changes and significantly lower computation compared to previous methods.<br /><br />4. The research explores robustness improvements by retraining models with fooling images as an additional class. This method only provides limited defense, as SPOOF continues to produce effective fooling images, albeit with a slightly increased number of queries.<br /><br />5. The results highlight the persistent vulnerability and fragility of current deep learning classifiers, including advanced transformer architectures, against adversarial images crafted to deceive them consistently. <div>
arXiv:2512.06185v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</title>
<link>https://arxiv.org/abs/2512.06190</link>
<guid>https://arxiv.org/abs/2512.06190</guid>
<content:encoded><![CDATA[
<div> Keywords: food drying, color trajectory, multi-modal prediction, drying process parameters, model accuracy<br /><br />Summary:<br /><br />Food drying is extensively employed to reduce moisture, enhance safety, and prolong the shelf life of food products. Monitoring the color evolution of food samples is crucial as it serves as an important quality indicator during drying. Existing research has mainly focused on analyzing low-dimensional color features under varying drying conditions, which limits the ability to fully characterize the complex and dynamic color changes that occur. Additionally, current modeling techniques struggle to generalize predictions to drying conditions not previously encountered. To overcome these challenges, the authors propose a novel multi-modal color-trajectory prediction approach that integrates high-dimensional temporal color data with drying process parameters. This integration enables the accurate and data-efficient prediction of color trajectories in food drying. The model’s performance was evaluated on cookie and apple drying tasks under unseen drying conditions, achieving root mean squared errors (RMSE) of 2.12 and 1.29, respectively. This represents a reduction in prediction errors by over 90% compared to baseline models. The experimental results confirm the approach’s superior accuracy, robustness, and broad applicability across different drying scenarios, highlighting its potential for improving food drying quality control. <div>
arXiv:2512.06190v1 Announce Type: new 
Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning</title>
<link>https://arxiv.org/abs/2512.06206</link>
<guid>https://arxiv.org/abs/2512.06206</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Glioma Segmentation, MRI, PID Controller, Medical Imaging  

<br /><br />Summary:  
1. The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024 focused on federated learning (FL) for segmenting glioma sub-regions in multi-parametric MRI scans.  
2. The challenge used a multi-institutional dataset from the BraTS glioma benchmark, including 1,251 training cases, 219 validation cases, and 570 hidden test cases, with segmentation labels for enhancing tumor (ET), tumor core (TC), and whole tumor (WT).  
3. Six teams participated and were evaluated under a standardized federated learning framework using a cumulative scoring system that combined segmentation accuracy (Dice Similarity Coefficient and 95th percentile Hausdorff Distance) and communication efficiency (convergence score).  
4. The winning team employed a PID-controller-based weight aggregation method, achieving the highest overall ranking with mean DSCs of 0.733 (ET), 0.761 (TC), and 0.751 (WT), alongside high HD95 values (around 32-34 mm), and the best communication efficiency with a convergence score of 0.764.  
5. Results demonstrate advancements over previous challenge iterations, highlighting the effectiveness of PID controllers in stabilizing and optimizing weight aggregation in federated learning for medical imaging. The challenge code is openly available at https://github.com/FeTS-AI/Challenge. <div>
arXiv:2512.06206v1 Announce Type: new 
Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study</title>
<link>https://arxiv.org/abs/2512.06221</link>
<guid>https://arxiv.org/abs/2512.06221</guid>
<content:encoded><![CDATA[
<div> SVD, WDR, image compression, reproducibility, JPEG2000<br /><br />Summary:<br /><br />This article presents an independent reproducibility study focused on a lossy image compression technique that combines Singular Value Decomposition (SVD) and Wavelet Difference Reduction (WDR). The original publication claimed that this combined SVD+WDR method outperforms both JPEG2000 and standalone WDR in terms of visual quality and compression ratio. To verify these claims, the author re-implemented the technique, carefully addressing missing implementation details such as quantization and threshold initialization, which were ambiguously described in the original paper. The study replicated the original experiments as closely as possible and extended the evaluation to new image datasets. Performance was assessed using standard metrics, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). Contrary to the original claims, findings indicate that SVD+WDR does not generally exceed JPEG2000 or WDR alone in PSNR, and only shows limited improvement in SSIM when compared to JPEG2000. The study underlines how unclear methodological details can significantly hinder reproducibility and affect the reported effectiveness of image compression methods. This highlights the importance of thorough documentation and transparency for accurate performance assessment in image compression research. <div>
arXiv:2512.06221v1 Announce Type: new 
Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking</title>
<link>https://arxiv.org/abs/2512.06230</link>
<guid>https://arxiv.org/abs/2512.06230</guid>
<content:encoded><![CDATA[
<div> multi-target tracking, labeled random finite sets, GLMB filter, GPU acceleration, multi-detections per object<br /><br />Summary:<br /><br />This article addresses challenges in multi-target tracking by focusing on labeled random finite set (RFS) methods, particularly the Generalized Labeled Multi-Bernoulli (GLMB) filter. Labeled RFS methods are valued for maintaining temporal coherence in object labeling and providing closed-form solutions to the multi-target Bayes filter. However, these methods experience high computational complexity due to the maintenance of multiple hypotheses under standard measurement models, even with hypothesis pruning. The authors propose a variant of the GLMB filter that allows for multiple detections per object from the same sensor, addressing scenarios common in distributed networks of machine learning-based virtual sensors. This variant breaks inter-detection dependencies present in standard GLMB updates, enabling significantly enhanced parallel scalability during filter updates. The improved scalability facilitates efficient implementation on GPU hardware. The paper reports preliminary results from a GPU-accelerated GLMB tracker, demonstrating favorable run-time scalability relative to the number of tracked objects and the maximum number of retained hypotheses. Overall, the work contributes a method that reduces computational burdens and improves deployment efficiency for multi-target tracking in sensor networks leveraging advanced computing architectures. <div>
arXiv:2512.06230v1 Announce Type: new 
Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion: Learning Intuitive Physics May Require More than Visual Data</title>
<link>https://arxiv.org/abs/2512.06232</link>
<guid>https://arxiv.org/abs/2512.06232</guid>
<content:encoded><![CDATA[
<div> Keywords: intuitive physics, video joint embedding predictive architecture, SAYCam dataset, deep learning, data distribution<br /><br />Summary:<br />1. Humans develop intuitive physics through rich internal models based on their everyday experiences and understanding of the physical world.<br />2. Current state-of-the-art deep learning models, despite being trained on enormous internet video datasets, have not reached human-level performance on intuitive physics tasks.<br />3. This study explores whether the type and distribution of training data, rather than just the quantity, can enhance learning of intuitive physics.<br />4. The authors pretrained a Video Joint Embedding Predictive Architecture (V-JEPA) on SAYCam, an egocentric video dataset that realistically captures the visual experiences of young children.<br />5. Despite representing only 0.01% of the data volume used by leading models, training on SAYCam did not produce meaningful improvements on the IntPhys2 benchmark.<br />6. The findings suggest that simply using developmentally realistic datasets is not enough for current architectures to acquire effective intuitive physics representations.<br />7. Consequently, increasing or varying visual data volume and distribution alone appears insufficient to build artificial systems with human-like intuitive physics capabilities. <div>
arXiv:2512.06232v1 Announce Type: new 
Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</title>
<link>https://arxiv.org/abs/2512.06251</link>
<guid>https://arxiv.org/abs/2512.06251</guid>
<content:encoded><![CDATA[
<div> Keywords: Partially Supervised Multi-Task Learning, NexusFlow, invertible coupling layers, domain partition, autonomous driving<br /><br />Summary:  
Partially Supervised Multi-Task Learning (PS-MTL) addresses learning from multiple tasks with incomplete annotations. Existing methods predominantly focus on homogeneous, dense prediction tasks, lacking solutions for structurally diverse tasks. NexusFlow is introduced as a novel, lightweight, plug-and-play framework that effectively handles both homogeneous and heterogeneous task settings. Central to NexusFlow are surrogate networks with invertible coupling layers that align latent feature distributions across tasks into a unified representation. The coupling layers are bijective, preserving detailed information and preventing representational collapse while allowing alignment across structurally different tasks without compromising expressive capacity. NexusFlow is first evaluated on a domain-partitioned autonomous driving scenario involving dense map reconstruction and sparse multi-object tracking supervised in different geographic regions, which presents both structural and domain disparities. It achieves state-of-the-art performance on the nuScenes dataset, surpassing strong partially supervised baselines. To validate its general applicability, NexusFlow is also tested on the NYUv2 dataset with three homogeneous dense prediction tasks — segmentation, depth estimation, and surface normal prediction — confirming consistent task improvements. These results demonstrate NexusFlow’s broad utility and effectiveness in advancing PS-MTL across diverse real-world problems. <div>
arXiv:2512.06251v1 Announce Type: new 
Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-driven Fine-grained Retrieval</title>
<link>https://arxiv.org/abs/2512.06255</link>
<guid>https://arxiv.org/abs/2512.06255</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained image retrieval, language-driven supervision, large language models, vision-language models, attribute-level descriptions<br /><br />Summary: Existing fine-grained image retrieval (FGIR) techniques typically use one-hot labels based on category names for supervision, which are semantically sparse and limit the ability to model detailed comparisons across categories, reducing generalization to unseen classes. To overcome this, the paper proposes LaFG, a novel framework that leverages large language models (LLMs) and vision-language models (VLMs) to transform category names into rich attribute-level supervision. LaFG treats category names as semantic anchors and prompts an LLM to generate detailed, attribute-focused descriptions for each class. To address potential omissions in these descriptions, a frozen VLM maps them into a vision-aligned space where attributes are clustered into a comprehensive dataset-wide vocabulary, enriched by attributes mined from related categories. Using this vocabulary, LaFG applies a global prompt template to select relevant attributes and aggregate them into category-specific linguistic prototypes. These prototypes then act as supervision to guide the retrieval model, enabling it to better capture and compare fine-grained details across categories, ultimately improving its ability to generalize to unseen categories in fine-grained image retrieval tasks. <div>
arXiv:2512.06255v1 Announce Type: new 
Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs</title>
<link>https://arxiv.org/abs/2512.06258</link>
<guid>https://arxiv.org/abs/2512.06258</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, reasoning paths, path selection bias, Path-Select Optimization, Group Relative Policy Optimization<br /><br />Summary:  
This paper identifies a critical problem in Large Vision-Language Models (LVLMs) where models often reach correct answers through flawed or unstable reasoning paths, emphasizing that the issue stems from a path selection bias rather than knowledge gaps. Evidence for this comes from significant differences between Pass@K (large K) and Pass@1 performance, indicating misreasoning rather than ignorance is the core challenge. To address this, the authors propose a two-stage post-training framework called Path-Select Optimization (PSO). The first stage uses Group Relative Policy Optimization (GRPO) with template and answer-based rewards to encourage structured, stepwise reasoning. In the second stage, an online preference optimization process is introduced where the model self-evaluates multiple reasoning paths, aligning toward preferred trajectories and storing incorrect paths in a Negative Replay Memory (NRM) to avoid repeating mistakes. This continual refinement mechanism improves reasoning stability and accuracy. Extensive experiments confirm that PSO effectively filters out invalid reasoning trajectories, improves reasoning accuracy by an average of 7.4%, and results in more stable and consistent chains of thought. The authors will release their code publicly to facilitate further research and application. <div>
arXiv:2512.06258v1 Announce Type: new 
Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06269</link>
<guid>https://arxiv.org/abs/2512.06269</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, multi-view triangulation, geometry consistency, photorealistic rendering  

<br /><br />Summary:  
1. The paper addresses the limitations of 3D Gaussian Splatting in real-time novel view synthesis, particularly issues arising from relying solely on photometric loss which leads to "floater" artifacts and unstructured geometry.  
2. To overcome these challenges, the authors propose a novel method that enforces global geometry consistency by integrating constrained multi-view triangulation.  
3. Their approach establishes a consensus on the 3D representation by leveraging multiple estimated views to improve the physical accuracy of reconstructions.  
4. The optimization penalizes deviations of rendered 3D points from a robust consensus point, which is recalculated through self-supervised triangulation over neighboring views.  
5. Experimental results demonstrate state-of-the-art performance, with a notable mean Chamfer Distance of 0.50 mm on the DTU dataset, surpassing other explicit reconstruction methods.  
6. The authors plan to release their code as open-source to promote reproducibility and facilitate further research in the community. <div>
arXiv:2512.06269v1 Announce Type: new 
Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacePhys: State of the Heart Learning</title>
<link>https://arxiv.org/abs/2512.06275</link>
<guid>https://arxiv.org/abs/2512.06275</guid>
<content:encoded><![CDATA[
<div> Keywords: remote photoplethysmography, vital sign measurement, FacePhys, real-time inference, computational efficiency<br /><br />Summary:<br /><br />1. The paper presents FacePhys, a novel memory-efficient algorithm for remote photoplethysmography (rPPG), aimed at measuring vital signs using cameras for comfortable and ubiquitous health monitoring.<br />2. FacePhys addresses the key challenges of model scalability, cross-dataset generalization, and real-time operation through a temporal-spatial state space duality approach.<br />3. It leverages a transferable heart state to capture subtle periodic variations across video frames while maintaining minimal computational overhead, enabling training on extended video sequences and low-latency inference.<br />4. FacePhys achieves a new state-of-the-art performance with a 49% reduction in error compared to existing methods.<br />5. The solution supports real-time inference with a memory footprint of only 3.6 MB and per-frame latency of 9.46 milliseconds, outperforming current models by 83% to 99%, making it suitable for practical deployment.<br />6. A live demonstration of FacePhys is available for public access at https://www.facephys.com/, showcasing its potential for reliable and efficient vital sign monitoring through cameras. <div>
arXiv:2512.06275v1 Announce Type: new 
Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
<div> Referring Expression Comprehension, Multi-modal Large Language Model, RefBench-PRO, Dynamic IoU-based GRPO, Reinforcement Learning<br /><br />Summary:  
Referring Expression Comprehension (REC) involves localizing specific image regions based on textual descriptions. Existing benchmarks mainly focus on perceptual abilities and lack interpretable scoring metrics, limiting insights into the grounding capabilities of Multi-modal Large Language Models (MLLMs) across various cognitive skills. To overcome these limitations, this work introduces RefBench-PRO, a comprehensive REC benchmark that divides referring expressions into two main dimensions: perception and reasoning. These dimensions are further broken down into six progressively challenging tasks: attribute, position, interaction, commonsense, relation, and reject. The authors develop a fully automated data generation pipeline to create diverse referring expressions across these sub-dimensions, ensuring robust evaluation. Additionally, Ref-R1, a reinforcement learning-based method incorporating Dynamic IoU-based GRPO, is proposed to enhance localization accuracy, particularly under complex reasoning scenarios, thereby establishing a stronger baseline for REC. Extensive experiments demonstrate that RefBench-PRO facilitates interpretable and nuanced evaluation of MLLMs on referring expression comprehension tasks, exposing greater challenges in both perceptual and reasoning capabilities. This benchmark and methodology collectively push forward the assessment and improvement of grounding abilities in vision-language models. <div>
arXiv:2512.06276v1 Announce Type: new 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.06281</link>
<guid>https://arxiv.org/abs/2512.06281</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, modality imbalance, latent visual reconstruction, masked image modeling, visual attention  

<br /><br />Summary:  
Multimodal Large Language Models (MLLMs) have achieved strong performance on tasks involving multiple data types, but they often suffer from a modality imbalance issue. This issue arises because visual information is underutilized relative to textual information in the deeper network layers, leading to reduced visual performance and hallucinations. The root cause is the training paradigm centered around next-text-token prediction, which does not provide direct supervisory signals for visual data. To address this, the authors propose Latent Visual Reconstruction (LaVer), a novel training framework that introduces masked image modeling within the joint latent semantic space of LLMs. LaVer provides explicit visual activation, which encourages the model to allocate more attention to visual inputs and maintain more discriminative visual representations throughout its layers. Extensive experiments across multiple benchmarks demonstrate LaVer's effectiveness, especially in scenarios demanding dense visual reasoning. The framework enhances the visual capability of MLLMs and reduces visual hallucination. The authors have also made the code for LaVer publicly available, facilitating further research and application development in this area. <div>
arXiv:2512.06281v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sleep Monitoring System Based on Audio, Video and Depth Information</title>
<link>https://arxiv.org/abs/2512.06282</link>
<guid>https://arxiv.org/abs/2512.06282</guid>
<content:encoded><![CDATA[
<div> motion events, light-on/off events, noise events, infrared depth sensor, event detection algorithm<br /><br />Summary:<br /><br />This paper presents a noninvasive sleep monitoring system designed to quantitatively evaluate sleep disturbances using an event-based method. The study classifies sleep disturbances into three main event types: motion events, light-on/off events, and noise events, which are crucial for understanding the sleep environment. The monitoring device integrates an infrared depth sensor, an RGB camera, and a four-microphone array to capture multimodal data during sleep in low-light home settings. A background model leveraging depth signals is employed to quantify the magnitude of movements, while a separate background model using color images detects lighting changes, addressing the limitation of depth sensors in sensing light variations. An event detection algorithm processes the sensor data to detect the occurrences of the three classified events. Experiments conducted under actual sleep conditions demonstrate the system's reliability and effectiveness in monitoring and classifying sleep disturbances. This integrated approach offers a comprehensive and nonintrusive solution for sleep disturbance evaluation in real-world home environments, showing potential for enhancing sleep quality assessment. <div>
arXiv:2512.06282v1 Announce Type: new 
Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification</title>
<link>https://arxiv.org/abs/2512.06290</link>
<guid>https://arxiv.org/abs/2512.06290</guid>
<content:encoded><![CDATA[
<div> Keywords: Stroke classification, reference points, Inline Sequence Attention, Cross-Ellipse Query, online handwriting  

<br /><br />Summary:  
The paper addresses the challenge of stroke classification in online handwriting, which is complicated by variations in writing styles, ambiguous content, and dynamic writing positions. The key difficulty lies in modeling the semantic relationships between strokes, particularly because stroke interactions tend to be localized, and existing deep learning models struggle to capture these fine-grained relationships. To overcome this, the authors propose representing strokes through selected reference points combined with feature vectors, balancing detail and redundancy. They introduce StrokeNet, an architecture that sequentially encodes these reference points and uses an Inline Sequence Attention (ISA) module to build contextual stroke features. Additionally, a novel Cross-Ellipse Query (CEQ) mechanism clusters reference points to extract spatial features across multiple scales. The framework also incorporates a joint optimization strategy that simultaneously predicts stroke categories via reference point regression and models semantic transitions between adjacent strokes using an Auxiliary Branch. Experimental results across several public online handwriting datasets demonstrate state-of-the-art performance. Notably, on the CASIA-onDo dataset, the accuracy of stroke classification improves significantly from 93.81% to 95.54%, validating the method’s effectiveness and robustness in handling complex stroke interactions. <div>
arXiv:2512.06290v1 Announce Type: new 
Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06306</link>
<guid>https://arxiv.org/abs/2512.06306</guid>
<content:encoded><![CDATA[
<div> Event cameras, human pose estimation, point cloud, temporal modeling, edge enhancement<br /><br />Summary:<br /><br />This paper addresses human pose estimation using event cameras, which provide high temporal resolution and low latency beneficial for scenarios with challenging conditions. Traditional approaches usually convert event streams into dense event frames, which compromises temporal resolution and increases computational load. To overcome this, the authors propose a point cloud-based framework that directly exploits the spatiotemporal characteristics of event streams. The method introduces an Event Temporal Slicing Convolution module designed to capture short-term dependencies across event slices. Alongside this, the Event Slice Sequencing module is employed to enable structured temporal modeling of the event data. To enhance spatial details, especially in sparse event scenarios, an edge enhancement technique is applied within the point cloud-based event representation. Experimental validation on the DHP19 dataset demonstrates consistent performance improvements using the proposed approach across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer. The results confirm that leveraging event-based data in a point cloud format with temporal and edge enhancements can significantly improve human pose estimation models. <div>
arXiv:2512.06306v1 Announce Type: new 
Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06328</link>
<guid>https://arxiv.org/abs/2512.06328</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.06328v1

Keywords: ReCAD, reinforcement learning, parametric CAD, vision-language models, generative models<br /><br />Summary:<br /><br />1. The paper introduces ReCAD, a reinforcement learning (RL) framework designed to generate accurate parametric computer-aided design (CAD) models by leveraging pretrained large models (PLMs) and their generative capabilities from multimodal inputs.<br /><br />2. Unlike previous approaches relying mainly on supervised fine-tuning and limited editability, ReCAD accesses simple functional interfaces such as point coordinates to enable complex CAD operations like pattern replication and mirroring.<br /><br />3. The framework initiates by fine-tuning vision-language models (VLMs) on rewritten CAD scripts translated into parameterized code, which helps generate accurate textual supervision.<br /><br />4. A novel RL strategy incorporates parameterized code guidance, enhancing reasoning ability for complex CAD generation tasks, supported by a hierarchical primitive learning process that progressively builds structured, compositional skills under a unified reward function optimizing both geometric accuracy and semantic fidelity.<br /><br />5. Experimental results show that ReCAD achieves state-of-the-art performance on text-to-CAD and image-to-CAD tasks, significantly reducing mean Chamfer Distance for both in-distribution (73.47 to 29.61) and out-of-distribution (272.06 to 80.23) cases, markedly outperforming existing baselines. <div>
arXiv:2512.06328v1 Announce Type: new 
Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening</title>
<link>https://arxiv.org/abs/2512.06330</link>
<guid>https://arxiv.org/abs/2512.06330</guid>
<content:encoded><![CDATA[
<div> Keywords: Pansharpening, Discrete Wavelet Transform, Cross-modal Interaction, Multi-scale Fusion, Spectral-Spatial Disentanglement<br /><br />Summary:<br />1. The paper introduces S2WMamba, a novel method for pansharpening that fuses high-resolution panchromatic (PAN) images with low-resolution multispectral (LRMS) images to produce high-resolution multispectral (HRMS) outputs.<br />2. A key challenge addressed is the entanglement of spatial details and spectral fidelity when jointly processing PAN and MS data; S2WMamba explicitly disentangles frequency information to overcome this.<br />3. The method employs a 2D Haar Discrete Wavelet Transform (DWT) on PAN images to localize spatial edges and textures, while a channel-wise 1D Haar DWT processes multispectral pixels as 1D signals, separating low- and high-frequency components to minimize spectral distortion.<br />4. S2WMamba features two parallel branches: the Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectral information from the 1D wavelet pyramid.<br />5. Information exchange between branches is facilitated by a Mamba-based cross-modulation mechanism that models long-range dependencies with linear complexity.<br />6. An adaptive multi-scale dynamic gate, combining multiplicative and additive operations, is used to fuse the outputs of both branches effectively.<br />7. Experiments on WV3, GF2, and QB datasets show that S2WMamba matches or surpasses recent strong baselines, achieving up to 0.23 dB PSNR improvement and a high HQNR score of 0.956 on full-resolution WV3.<br />8. Ablation studies confirm the effectiveness of the 2D/1D DWT placement, parallel dual-branch architecture, and fusion gate design.<br />9. The authors provide the code publicly for reproducibility at the specified GitHub repository. <div>
arXiv:2512.06330v1 Announce Type: new 
Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks</title>
<link>https://arxiv.org/abs/2512.06332</link>
<guid>https://arxiv.org/abs/2512.06332</guid>
<content:encoded><![CDATA[
<div> Keywords: cryo-electron microscopy, compositional heterogeneity, transformer, hypernetwork, implicit neural representation<br /><br />Summary: Cryo-electron microscopy (cryo-EM) is a critical method for determining three-dimensional structures of dynamic biomolecular complexes, typically used for imaging a single molecular species. Current approaches mainly address conformational heterogeneity within one or a few structures, lacking the ability to resolve compositional heterogeneity arising from mixtures containing many distinct molecular species. To overcome this limitation, the paper introduces CryoHype, a novel transformer-based hypernetwork designed for cryo-EM reconstruction. CryoHype dynamically adjusts the weights of an implicit neural representation, enabling it to handle multiple molecular species simultaneously. The method achieves state-of-the-art performance on a challenging benchmark dataset consisting of 100 different biomolecular structures. Moreover, CryoHype demonstrates scalability by successfully reconstructing 1,000 distinct structures from unlabeled cryo-EM images in a fixed-pose scenario. This advancement paves the way for high-throughput, simultaneous structural determination of diverse molecular species from mixed cryo-EM data, addressing a key challenge in the field that existing methods could not effectively resolve. <div>
arXiv:2512.06332v1 Announce Type: new 
Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate</title>
<link>https://arxiv.org/abs/2512.06344</link>
<guid>https://arxiv.org/abs/2512.06344</guid>
<content:encoded><![CDATA[
<div> Generative Image Compression, Multimodal Guidance, Semantic Consistency, Task-Aware Semantic Compression, Diffusion Decoder<br /><br />Summary:<br /><br />This paper addresses the challenge of semantic deviations in generative image compression at ultra-low bitrates (below 0.05 bpp), which hampers the deployment of such methods in bandwidth-restricted 6G semantic communication. The authors propose the Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework, which leverages three different guidance modalities to improve semantic consistency: a concise text caption to capture global semantics, a highly compressed image (HCI) for low-level visual details, and Semantic Pseudo-Words (SPWs) that provide fine-grained, task-relevant semantic information. These SPWs are produced by a novel Task-Aware Semantic Compression Module (TASCM), which uses multi-head self-attention to focus on important semantics while filtering unnecessary data. The integration of these guidance signals into the image reconstruction relies on a Multimodal-Guided Diffusion Decoder (MGDD), which utilizes a dual-path cooperative mechanism combining cross-attention and ControlNet residuals to effectively inject the multimodal information into the diffusion process. Experimental results demonstrate that MTGC significantly enhances semantic consistency, perceptual quality, and pixel-level fidelity, achieving a notable 10.59% reduction in DISTS on the DIV2K dataset, validating its effectiveness under ultra-low bitrate conditions. <div>
arXiv:2512.06344v1 Announce Type: new 
Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLUENet: Cluster Attention Makes Neural Networks Have Eyes</title>
<link>https://arxiv.org/abs/2512.06345</link>
<guid>https://arxiv.org/abs/2512.06345</guid>
<content:encoded><![CDATA[
<div> Clustering, Attention, Visual Semantic Understanding, Interpretability, Deep Architecture<br /><br />Summary: Despite the widespread success of convolutional and attention-based models in vision tasks, these approaches face limitations due to their rigid receptive fields and complex structures, which reduce interpretability and hinder the modeling of irregular spatial patterns. To improve transparency and semantic modeling flexibility, clustering paradigms have been explored, but they typically suffer from issues such as limited accuracy, low efficiency, and gradient vanishing during training. Addressing these challenges, the paper introduces CLUster attEntion Network (CLUENet), a transparent deep learning architecture aimed at enhancing visual semantic understanding. CLUENet introduces three main innovations: (i) a Global Soft Aggregation and Hard Assignment mechanism using Temperature-Scaled Cosine Attention combined with gated residual connections for improved local feature modeling; (ii) inter-block Hard and Shared Feature Dispatching to optimize information flow; and (iii) an improved cluster pooling strategy that enhances performance. Experimental results on CIFAR-100 and Mini-ImageNet datasets demonstrate that CLUENet surpasses existing clustering-based methods and conventional visual models by achieving a remarkable balance between classification accuracy, computational efficiency, and model interpretability. This work offers a significant step toward transparent, effective visual models suitable for tasks demanding high levels of model explainability. <div>
arXiv:2512.06345v1 Announce Type: new 
Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search</title>
<link>https://arxiv.org/abs/2512.06353</link>
<guid>https://arxiv.org/abs/2512.06353</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, Quantization, Tree Structured Search, Environmental Noise Guidance, General Monarch Branch<br /><br />Summary: Diffusion Transformers (DiTs) have become a powerful model architecture for image generation, surpassing traditional U-Net designs in scalability and performance but facing deployment challenges due to high computational and memory requirements. This work introduces TreeQ, a novel unified framework specifically designed to address these challenges in DiT quantization. First, TreeQ implements Tree Structured Search (TSS), which exploits the linear structure of DiTs to efficiently explore the solution space in O(n) time while enhancing accuracy through comparison-based pruning. Second, it proposes Environmental Noise Guidance (ENG), a method that harmonizes the optimization objectives of Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) via a single hyperparameter, simplifying the quantization process. Third, to combat information loss in ultra-low-bit precision settings, TreeQ incorporates the General Monarch Branch (GMB), a structured sparse branch that preserves critical details in generated images. Comprehensive experiments show that TreeQ achieves state-of-the-art results on the DiT-XL/2 architecture under both W3A3 and W4A4 PTQ/PEFT configurations. Remarkably, this approach is the first to reach near-lossless 4-bit PTQ performance for DiT models, marking a significant step toward more practical deployment of DiTs. The code and trained models are publicly accessible at the provided GitHub repository. <div>
arXiv:2512.06353v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Latent Space for Generative Single-Image Reflection Removal</title>
<link>https://arxiv.org/abs/2512.06358</link>
<guid>https://arxiv.org/abs/2512.06358</guid>
<content:encoded><![CDATA[
<div> Reflection removal, latent diffusion, VAE, composite image, depth-guided sampling<br /><br />Summary: This paper addresses the challenging problem of single-image reflection removal, which is ill-posed due to the complexity of separating overlapping layers in a reflection-contaminated image. The authors identify a fundamental issue with existing semantic encoder latent spaces, which do not naturally represent composite images as a linear combination of layers, limiting recovery and generalization. To overcome this, the paper introduces a novel framework based on a reflection-equivariant variational autoencoder (VAE) designed to align the latent space with the physics of reflection formation, thereby enabling more meaningful layer disentanglement. Additionally, the method employs a learnable, task-specific text embedding that offers precise guidance while avoiding ambiguity inherent in natural language instructions. The approach also incorporates a depth-guided early-branching sampling technique to leverage generative stochasticity effectively, enhancing the quality and realism of the reconstructed images. Extensive experimental validation demonstrates that this method sets a new state-of-the-art (SOTA) performance across multiple benchmark datasets, showing strong generalization to complex real-world scenarios where reflections are difficult to remove. The combination of physics-aligned latent space, precise guidance, and innovative sampling strategy collectively yields robust and high-quality reflection removal results. <div>
arXiv:2512.06358v1 Announce Type: new 
Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection</title>
<link>https://arxiv.org/abs/2512.06363</link>
<guid>https://arxiv.org/abs/2512.06363</guid>
<content:encoded><![CDATA[
<div> Physical Presentation Attacks, Digital Forgery Attacks, Unified Attack Detection, Spoofing-aware Prompt Learning, CLIP-based Defense<br /><br />Summary:<br /><br />This paper addresses the vulnerability of face recognition systems to both physical presentation attacks (PAs) and digital forgery attacks (DFs), aiming to secure biometric data through a unified detection framework. Existing methods typically use CLIP with regularization to improve generalization across attack types but face challenges due to conflicting optimization objectives in the shared prompt space for physical and digital attacks. To resolve this, the authors propose the Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization by introducing parallel learnable prompt branches specific to physical and digital attacks. This is enhanced by an adaptive Spoofing Context Prompt Generation mechanism, allowing independent control for each attack category. Additionally, the Cues-awareness Augmentation technique leverages the dual-prompt design to create challenging sample mining tasks, which strengthen the model’s robustness against previously unseen attack techniques. Extensive experiments conducted on the large-scale UniAttackDataPlus dataset demonstrate that SPL-UAD significantly improves performance for unified attack detection tasks compared to prior approaches, confirming the effectiveness of decoupling prompt optimization and the benefits of cue-aware augmentation strategies. <div>
arXiv:2512.06363v1 Announce Type: new 
Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos</title>
<link>https://arxiv.org/abs/2512.06368</link>
<guid>https://arxiv.org/abs/2512.06368</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular dynamic video reconstruction, SMPL human body model, geometric priors, human boundary preservation, cross-attention fusion<br /><br />Summary:<br /><br />This article addresses the challenges in monocular dynamic video reconstruction for dynamic human scenes, focusing on issues like geometric inconsistencies and resolution degradation. Existing approaches often fail due to lack of 3D human structural understanding, resulting in distorted limb proportions and unnatural fusion between humans and objects. Moreover, downsampling to reduce memory usage causes human boundaries to drift toward background geometry, reducing accuracy. To overcome these problems, the authors propose a novel method that integrates hybrid geometric priors combining SMPL human body models with monocular depth estimation. This approach leverages structured human priors to maintain surface consistency while capturing fine-grained details in human regions. The proposed Human3R framework features a hierarchical pipeline: it first processes full-resolution images to reconstruct overall scene geometry, then strategically crops and applies cross-attention fusion to enhance human-specific details. The integration of SMPL priors is achieved through a Feature Fusion Module that ensures geometrically plausible reconstructions and preserves fine boundary details of humans. Experiments on TUM Dynamics and GTA-IM datasets demonstrate that Human3R outperforms previous methods in reconstructing dynamic humans accurately and with improved detail. <div>
arXiv:2512.06368v1 Announce Type: new 
Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06373</link>
<guid>https://arxiv.org/abs/2512.06373</guid>
<content:encoded><![CDATA[
arXiv:2512.06373v1 Announce Type: new 
Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</title>
<link>https://arxiv.org/abs/2512.06376</link>
<guid>https://arxiv.org/abs/2512.06376</guid>
<content:encoded><![CDATA[
arXiv:2512.06376v1 Announce Type: new 
Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System</title>
<link>https://arxiv.org/abs/2512.06377</link>
<guid>https://arxiv.org/abs/2512.06377</guid>
<content:encoded><![CDATA[
arXiv:2512.06377v1 Announce Type: new 
Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OCFER-Net: Recognizing Facial Expression in Online Learning System</title>
<link>https://arxiv.org/abs/2512.06379</link>
<guid>https://arxiv.org/abs/2512.06379</guid>
<content:encoded><![CDATA[
arXiv:2512.06379v1 Announce Type: new 
Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement</title>
<link>https://arxiv.org/abs/2512.06400</link>
<guid>https://arxiv.org/abs/2512.06400</guid>
<content:encoded><![CDATA[
arXiv:2512.06400v1 Announce Type: new 
Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[
arXiv:2512.06421v1 Announce Type: new 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Perception CNN for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2512.06422</link>
<guid>https://arxiv.org/abs/2512.06422</guid>
<content:encoded><![CDATA[
arXiv:2512.06422v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DragMesh: Interactive 3D Generation Made Easy</title>
<link>https://arxiv.org/abs/2512.06424</link>
<guid>https://arxiv.org/abs/2512.06424</guid>
<content:encoded><![CDATA[
arXiv:2512.06424v1 Announce Type: new 
Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition</title>
<link>https://arxiv.org/abs/2512.06426</link>
<guid>https://arxiv.org/abs/2512.06426</guid>
<content:encoded><![CDATA[
arXiv:2512.06426v1 Announce Type: new 
Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening</title>
<link>https://arxiv.org/abs/2512.06434</link>
<guid>https://arxiv.org/abs/2512.06434</guid>
<content:encoded><![CDATA[
arXiv:2512.06434v1 Announce Type: new 
Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</title>
<link>https://arxiv.org/abs/2512.06438</link>
<guid>https://arxiv.org/abs/2512.06438</guid>
<content:encoded><![CDATA[
arXiv:2512.06438v1 Announce Type: new 
Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Stable Cross-Domain Depression Recognition under Missing Modalities</title>
<link>https://arxiv.org/abs/2512.06447</link>
<guid>https://arxiv.org/abs/2512.06447</guid>
<content:encoded><![CDATA[
arXiv:2512.06447v1 Announce Type: new 
Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction</title>
<link>https://arxiv.org/abs/2512.06485</link>
<guid>https://arxiv.org/abs/2512.06485</guid>
<content:encoded><![CDATA[
arXiv:2512.06485v1 Announce Type: new 
Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion</title>
<link>https://arxiv.org/abs/2512.06504</link>
<guid>https://arxiv.org/abs/2512.06504</guid>
<content:encoded><![CDATA[
arXiv:2512.06504v1 Announce Type: new 
Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images</title>
<link>https://arxiv.org/abs/2512.06521</link>
<guid>https://arxiv.org/abs/2512.06521</guid>
<content:encoded><![CDATA[
arXiv:2512.06521v1 Announce Type: new 
Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization</title>
<link>https://arxiv.org/abs/2512.06530</link>
<guid>https://arxiv.org/abs/2512.06530</guid>
<content:encoded><![CDATA[
arXiv:2512.06530v1 Announce Type: new 
Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2512.06531</link>
<guid>https://arxiv.org/abs/2512.06531</guid>
<content:encoded><![CDATA[
arXiv:2512.06531v1 Announce Type: new 
Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging spatial awareness and global context in medical image segmentation</title>
<link>https://arxiv.org/abs/2512.06560</link>
<guid>https://arxiv.org/abs/2512.06560</guid>
<content:encoded><![CDATA[
arXiv:2512.06560v1 Announce Type: new 
Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities</title>
<link>https://arxiv.org/abs/2512.06562</link>
<guid>https://arxiv.org/abs/2512.06562</guid>
<content:encoded><![CDATA[
arXiv:2512.06562v1 Announce Type: new 
Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06565</link>
<guid>https://arxiv.org/abs/2512.06565</guid>
<content:encoded><![CDATA[
arXiv:2512.06565v1 Announce Type: new 
Abstract: We present GNC--Pose, a fully learning--free monocular 6D object pose estimation pipeline for textured objects that combines rendering--based initialization, geometry--aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D--3D correspondences obtained through feature matching and rendering--based alignment, our method builds upon the Graduated Non--Convexity (GNC) principle and introduces a geometry--aware, cluster--based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC--Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC--Pose achieves competitive accuracy compared with both learning-based and learning--free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules</title>
<link>https://arxiv.org/abs/2512.06575</link>
<guid>https://arxiv.org/abs/2512.06575</guid>
<content:encoded><![CDATA[
arXiv:2512.06575v1 Announce Type: new 
Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding</title>
<link>https://arxiv.org/abs/2512.06581</link>
<guid>https://arxiv.org/abs/2512.06581</guid>
<content:encoded><![CDATA[
arXiv:2512.06581v1 Announce Type: new 
Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain</title>
<link>https://arxiv.org/abs/2512.06598</link>
<guid>https://arxiv.org/abs/2512.06598</guid>
<content:encoded><![CDATA[
arXiv:2512.06598v1 Announce Type: new 
Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2512.06612</link>
<guid>https://arxiv.org/abs/2512.06612</guid>
<content:encoded><![CDATA[
arXiv:2512.06612v1 Announce Type: new 
Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</title>
<link>https://arxiv.org/abs/2512.06613</link>
<guid>https://arxiv.org/abs/2512.06613</guid>
<content:encoded><![CDATA[
arXiv:2512.06613v1 Announce Type: new 
Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</title>
<link>https://arxiv.org/abs/2512.06642</link>
<guid>https://arxiv.org/abs/2512.06642</guid>
<content:encoded><![CDATA[
arXiv:2512.06642v1 Announce Type: new 
Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextMamba: Scene Text Detector with Mamba</title>
<link>https://arxiv.org/abs/2512.06657</link>
<guid>https://arxiv.org/abs/2512.06657</guid>
<content:encoded><![CDATA[
arXiv:2512.06657v1 Announce Type: new 
Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Image Descriptions from Attention Sequences</title>
<link>https://arxiv.org/abs/2512.06662</link>
<guid>https://arxiv.org/abs/2512.06662</guid>
<content:encoded><![CDATA[
arXiv:2512.06662v1 Announce Type: new 
Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks</title>
<link>https://arxiv.org/abs/2512.06663</link>
<guid>https://arxiv.org/abs/2512.06663</guid>
<content:encoded><![CDATA[
arXiv:2512.06663v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning</title>
<link>https://arxiv.org/abs/2512.06673</link>
<guid>https://arxiv.org/abs/2512.06673</guid>
<content:encoded><![CDATA[
arXiv:2512.06673v1 Announce Type: new 
Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RunawayEvil: Jailbreaking the Image-to-Video Generative Models</title>
<link>https://arxiv.org/abs/2512.06674</link>
<guid>https://arxiv.org/abs/2512.06674</guid>
<content:encoded><![CDATA[
arXiv:2512.06674v1 Announce Type: new 
Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy</title>
<link>https://arxiv.org/abs/2512.06684</link>
<guid>https://arxiv.org/abs/2512.06684</guid>
<content:encoded><![CDATA[
arXiv:2512.06684v1 Announce Type: new 
Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation</title>
<link>https://arxiv.org/abs/2512.06689</link>
<guid>https://arxiv.org/abs/2512.06689</guid>
<content:encoded><![CDATA[
arXiv:2512.06689v1 Announce Type: new 
Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Entropy in Visual Grounding: Analysis and Optimization</title>
<link>https://arxiv.org/abs/2512.06726</link>
<guid>https://arxiv.org/abs/2512.06726</guid>
<content:encoded><![CDATA[
arXiv:2512.06726v1 Announce Type: new 
Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data</title>
<link>https://arxiv.org/abs/2512.06736</link>
<guid>https://arxiv.org/abs/2512.06736</guid>
<content:encoded><![CDATA[
arXiv:2512.06736v1 Announce Type: new 
Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.06738</link>
<guid>https://arxiv.org/abs/2512.06738</guid>
<content:encoded><![CDATA[
arXiv:2512.06738v1 Announce Type: new 
Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.06746</link>
<guid>https://arxiv.org/abs/2512.06746</guid>
<content:encoded><![CDATA[
arXiv:2512.06746v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement</title>
<link>https://arxiv.org/abs/2512.06750</link>
<guid>https://arxiv.org/abs/2512.06750</guid>
<content:encoded><![CDATA[
arXiv:2512.06750v1 Announce Type: new 
Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors</title>
<link>https://arxiv.org/abs/2512.06759</link>
<guid>https://arxiv.org/abs/2512.06759</guid>
<content:encoded><![CDATA[
arXiv:2512.06759v1 Announce Type: new 
Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms</title>
<link>https://arxiv.org/abs/2512.06763</link>
<guid>https://arxiv.org/abs/2512.06763</guid>
<content:encoded><![CDATA[
arXiv:2512.06763v1 Announce Type: new 
Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.06769</link>
<guid>https://arxiv.org/abs/2512.06769</guid>
<content:encoded><![CDATA[
arXiv:2512.06769v1 Announce Type: new 
Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06774</link>
<guid>https://arxiv.org/abs/2512.06774</guid>
<content:encoded><![CDATA[
arXiv:2512.06774v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</title>
<link>https://arxiv.org/abs/2512.06783</link>
<guid>https://arxiv.org/abs/2512.06783</guid>
<content:encoded><![CDATA[
arXiv:2512.06783v1 Announce Type: new 
Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Geometry Encoding Volume for Real-time Stereo Matching</title>
<link>https://arxiv.org/abs/2512.06793</link>
<guid>https://arxiv.org/abs/2512.06793</guid>
<content:encoded><![CDATA[
arXiv:2512.06793v1 Announce Type: new 
Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDOT: Efficient Unified Video Creation via Optimal Transport Distillation</title>
<link>https://arxiv.org/abs/2512.06802</link>
<guid>https://arxiv.org/abs/2512.06802</guid>
<content:encoded><![CDATA[
arXiv:2512.06802v1 Announce Type: new 
Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06810</link>
<guid>https://arxiv.org/abs/2512.06810</guid>
<content:encoded><![CDATA[
arXiv:2512.06810v1 Announce Type: new 
Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06811</link>
<guid>https://arxiv.org/abs/2512.06811</guid>
<content:encoded><![CDATA[
arXiv:2512.06811v1 Announce Type: new 
Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshSplatting: Differentiable Rendering with Opaque Meshes</title>
<link>https://arxiv.org/abs/2512.06818</link>
<guid>https://arxiv.org/abs/2512.06818</guid>
<content:encoded><![CDATA[
arXiv:2512.06818v1 Announce Type: new 
Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseCoop: Cooperative Perception with Kinematic-Grounded Queries</title>
<link>https://arxiv.org/abs/2512.06838</link>
<guid>https://arxiv.org/abs/2512.06838</guid>
<content:encoded><![CDATA[
arXiv:2512.06838v1 Announce Type: new 
Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</title>
<link>https://arxiv.org/abs/2512.06840</link>
<guid>https://arxiv.org/abs/2512.06840</guid>
<content:encoded><![CDATA[
arXiv:2512.06840v1 Announce Type: new 
Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.06845</link>
<guid>https://arxiv.org/abs/2512.06845</guid>
<content:encoded><![CDATA[
arXiv:2512.06845v1 Announce Type: new 
Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</title>
<link>https://arxiv.org/abs/2512.06849</link>
<guid>https://arxiv.org/abs/2512.06849</guid>
<content:encoded><![CDATA[
arXiv:2512.06849v1 Announce Type: new 
Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06862</link>
<guid>https://arxiv.org/abs/2512.06862</guid>
<content:encoded><![CDATA[
arXiv:2512.06862v1 Announce Type: new 
Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training</title>
<link>https://arxiv.org/abs/2512.06864</link>
<guid>https://arxiv.org/abs/2512.06864</guid>
<content:encoded><![CDATA[
arXiv:2512.06864v1 Announce Type: new 
Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Retrieval Augmented Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06865</link>
<guid>https://arxiv.org/abs/2512.06865</guid>
<content:encoded><![CDATA[
arXiv:2512.06865v1 Announce Type: new 
Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[
arXiv:2512.06866v1 Announce Type: new 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective</title>
<link>https://arxiv.org/abs/2512.06870</link>
<guid>https://arxiv.org/abs/2512.06870</guid>
<content:encoded><![CDATA[
arXiv:2512.06870v1 Announce Type: new 
Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification</title>
<link>https://arxiv.org/abs/2512.06877</link>
<guid>https://arxiv.org/abs/2512.06877</guid>
<content:encoded><![CDATA[
arXiv:2512.06877v1 Announce Type: new 
Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion</title>
<link>https://arxiv.org/abs/2512.06882</link>
<guid>https://arxiv.org/abs/2512.06882</guid>
<content:encoded><![CDATA[
arXiv:2512.06882v1 Announce Type: new 
Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoPano: Unified Panorama Generation via Joint Modeling</title>
<link>https://arxiv.org/abs/2512.06885</link>
<guid>https://arxiv.org/abs/2512.06885</guid>
<content:encoded><![CDATA[
arXiv:2512.06885v1 Announce Type: new 
Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Learning for Domain Adaptive Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06886</link>
<guid>https://arxiv.org/abs/2512.06886</guid>
<content:encoded><![CDATA[
arXiv:2512.06886v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation</title>
<link>https://arxiv.org/abs/2512.06888</link>
<guid>https://arxiv.org/abs/2512.06888</guid>
<content:encoded><![CDATA[
arXiv:2512.06888v1 Announce Type: new 
Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Zero-Shot Reference-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.06905</link>
<guid>https://arxiv.org/abs/2512.06905</guid>
<content:encoded><![CDATA[
arXiv:2512.06905v1 Announce Type: new 
Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</title>
<link>https://arxiv.org/abs/2512.06921</link>
<guid>https://arxiv.org/abs/2512.06921</guid>
<content:encoded><![CDATA[
arXiv:2512.06921v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology</title>
<link>https://arxiv.org/abs/2512.06949</link>
<guid>https://arxiv.org/abs/2512.06949</guid>
<content:encoded><![CDATA[
arXiv:2512.06949v1 Announce Type: new 
Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Masking based Self-Supervised Learning for Image Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06981</link>
<guid>https://arxiv.org/abs/2512.06981</guid>
<content:encoded><![CDATA[
arXiv:2512.06981v1 Announce Type: new 
Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues</title>
<link>https://arxiv.org/abs/2512.07034</link>
<guid>https://arxiv.org/abs/2512.07034</guid>
<content:encoded><![CDATA[
arXiv:2512.07034v1 Announce Type: new 
Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating and Preserving High-level Fidelity in Super-Resolution</title>
<link>https://arxiv.org/abs/2512.07037</link>
<guid>https://arxiv.org/abs/2512.07037</guid>
<content:encoded><![CDATA[
arXiv:2512.07037v1 Announce Type: new 
Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07051</link>
<guid>https://arxiv.org/abs/2512.07051</guid>
<content:encoded><![CDATA[
arXiv:2512.07051v1 Announce Type: new 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07052</link>
<guid>https://arxiv.org/abs/2512.07052</guid>
<content:encoded><![CDATA[
arXiv:2512.07052v1 Announce Type: new 
Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v1 Announce Type: new 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Homology-Guided Frequency Filtering for Image Compression</title>
<link>https://arxiv.org/abs/2512.07065</link>
<guid>https://arxiv.org/abs/2512.07065</guid>
<content:encoded><![CDATA[
arXiv:2512.07065v1 Announce Type: new 
Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-measure: Contextualizing Metric for Camouflage</title>
<link>https://arxiv.org/abs/2512.07076</link>
<guid>https://arxiv.org/abs/2512.07076</guid>
<content:encoded><![CDATA[
arXiv:2512.07076v1 Announce Type: new 
Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title>
<link>https://arxiv.org/abs/2512.07078</link>
<guid>https://arxiv.org/abs/2512.07078</guid>
<content:encoded><![CDATA[
arXiv:2512.07078v1 Announce Type: new 
Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</title>
<link>https://arxiv.org/abs/2512.07107</link>
<guid>https://arxiv.org/abs/2512.07107</guid>
<content:encoded><![CDATA[
arXiv:2512.07107v1 Announce Type: new 
Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection</title>
<link>https://arxiv.org/abs/2512.07110</link>
<guid>https://arxiv.org/abs/2512.07110</guid>
<content:encoded><![CDATA[
arXiv:2512.07110v1 Announce Type: new 
Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-free Clothing Region of Interest Self-correction for Virtual Try-On</title>
<link>https://arxiv.org/abs/2512.07126</link>
<guid>https://arxiv.org/abs/2512.07126</guid>
<content:encoded><![CDATA[
arXiv:2512.07126v1 Announce Type: new 
Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP</title>
<link>https://arxiv.org/abs/2512.07128</link>
<guid>https://arxiv.org/abs/2512.07128</guid>
<content:encoded><![CDATA[
arXiv:2512.07128v1 Announce Type: new 
Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07135</link>
<guid>https://arxiv.org/abs/2512.07135</guid>
<content:encoded><![CDATA[
arXiv:2512.07135v1 Announce Type: new 
Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2512.07136</link>
<guid>https://arxiv.org/abs/2512.07136</guid>
<content:encoded><![CDATA[
arXiv:2512.07136v1 Announce Type: new 
Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2512.07141</link>
<guid>https://arxiv.org/abs/2512.07141</guid>
<content:encoded><![CDATA[
arXiv:2512.07141v1 Announce Type: new 
Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</title>
<link>https://arxiv.org/abs/2512.07155</link>
<guid>https://arxiv.org/abs/2512.07155</guid>
<content:encoded><![CDATA[
arXiv:2512.07155v1 Announce Type: new 
Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</title>
<link>https://arxiv.org/abs/2512.07165</link>
<guid>https://arxiv.org/abs/2512.07165</guid>
<content:encoded><![CDATA[
arXiv:2512.07165v1 Announce Type: new 
Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing</title>
<link>https://arxiv.org/abs/2512.07166</link>
<guid>https://arxiv.org/abs/2512.07166</guid>
<content:encoded><![CDATA[
arXiv:2512.07166v1 Announce Type: new 
Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</title>
<link>https://arxiv.org/abs/2512.07170</link>
<guid>https://arxiv.org/abs/2512.07170</guid>
<content:encoded><![CDATA[
arXiv:2512.07170v1 Announce Type: new 
Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration</title>
<link>https://arxiv.org/abs/2512.07171</link>
<guid>https://arxiv.org/abs/2512.07171</guid>
<content:encoded><![CDATA[
arXiv:2512.07171v1 Announce Type: new 
Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>START: Spatial and Textual Learning for Chart Understanding</title>
<link>https://arxiv.org/abs/2512.07186</link>
<guid>https://arxiv.org/abs/2512.07186</guid>
<content:encoded><![CDATA[
arXiv:2512.07186v1 Announce Type: new 
Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification</title>
<link>https://arxiv.org/abs/2512.07190</link>
<guid>https://arxiv.org/abs/2512.07190</guid>
<content:encoded><![CDATA[
arXiv:2512.07190v1 Announce Type: new 
Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction</title>
<link>https://arxiv.org/abs/2512.07191</link>
<guid>https://arxiv.org/abs/2512.07191</guid>
<content:encoded><![CDATA[
arXiv:2512.07191v1 Announce Type: new 
Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression</title>
<link>https://arxiv.org/abs/2512.07192</link>
<guid>https://arxiv.org/abs/2512.07192</guid>
<content:encoded><![CDATA[
arXiv:2512.07192v1 Announce Type: new 
Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07197</link>
<guid>https://arxiv.org/abs/2512.07197</guid>
<content:encoded><![CDATA[
arXiv:2512.07197v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Storytelling Images with Rich Chains-of-Reasoning</title>
<link>https://arxiv.org/abs/2512.07198</link>
<guid>https://arxiv.org/abs/2512.07198</guid>
<content:encoded><![CDATA[
arXiv:2512.07198v1 Announce Type: new 
Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Diffusion Models via Code Execution</title>
<link>https://arxiv.org/abs/2512.07201</link>
<guid>https://arxiv.org/abs/2512.07201</guid>
<content:encoded><![CDATA[
arXiv:2512.07201v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning</title>
<link>https://arxiv.org/abs/2512.07203</link>
<guid>https://arxiv.org/abs/2512.07203</guid>
<content:encoded><![CDATA[
arXiv:2512.07203v1 Announce Type: new 
Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT</title>
<link>https://arxiv.org/abs/2512.07206</link>
<guid>https://arxiv.org/abs/2512.07206</guid>
<content:encoded><![CDATA[
arXiv:2512.07206v1 Announce Type: new 
Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds</title>
<link>https://arxiv.org/abs/2512.07211</link>
<guid>https://arxiv.org/abs/2512.07211</guid>
<content:encoded><![CDATA[
arXiv:2512.07211v1 Announce Type: new 
Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.07215</link>
<guid>https://arxiv.org/abs/2512.07215</guid>
<content:encoded><![CDATA[
arXiv:2512.07215v1 Announce Type: new 
Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title>
<link>https://arxiv.org/abs/2512.07228</link>
<guid>https://arxiv.org/abs/2512.07228</guid>
<content:encoded><![CDATA[
arXiv:2512.07228v1 Announce Type: new 
Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2512.07229</link>
<guid>https://arxiv.org/abs/2512.07229</guid>
<content:encoded><![CDATA[
arXiv:2512.07229v1 Announce Type: new 
Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STRinGS: Selective Text Refinement in Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07230</link>
<guid>https://arxiv.org/abs/2512.07230</guid>
<content:encoded><![CDATA[
arXiv:2512.07230v1 Announce Type: new 
Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07234</link>
<guid>https://arxiv.org/abs/2512.07234</guid>
<content:encoded><![CDATA[
arXiv:2512.07234v1 Announce Type: new 
Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Camera Positional Encoding for Controlled Video Generation</title>
<link>https://arxiv.org/abs/2512.07237</link>
<guid>https://arxiv.org/abs/2512.07237</guid>
<content:encoded><![CDATA[
arXiv:2512.07237v1 Announce Type: new 
Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture</title>
<link>https://arxiv.org/abs/2512.07241</link>
<guid>https://arxiv.org/abs/2512.07241</guid>
<content:encoded><![CDATA[
arXiv:2512.07241v1 Announce Type: new 
Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Textual Explanations via Translating Decision-Critical Features</title>
<link>https://arxiv.org/abs/2512.07245</link>
<guid>https://arxiv.org/abs/2512.07245</guid>
<content:encoded><![CDATA[
arXiv:2512.07245v1 Announce Type: new 
Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title>
<link>https://arxiv.org/abs/2512.07247</link>
<guid>https://arxiv.org/abs/2512.07247</guid>
<content:encoded><![CDATA[
arXiv:2512.07247v1 Announce Type: new 
Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement</title>
<link>https://arxiv.org/abs/2512.07251</link>
<guid>https://arxiv.org/abs/2512.07251</guid>
<content:encoded><![CDATA[
arXiv:2512.07251v1 Announce Type: new 
Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</title>
<link>https://arxiv.org/abs/2512.07253</link>
<guid>https://arxiv.org/abs/2512.07253</guid>
<content:encoded><![CDATA[
arXiv:2512.07253v1 Announce Type: new 
Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</title>
<link>https://arxiv.org/abs/2512.07269</link>
<guid>https://arxiv.org/abs/2512.07269</guid>
<content:encoded><![CDATA[
arXiv:2512.07269v1 Announce Type: new 
Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation</title>
<link>https://arxiv.org/abs/2512.07273</link>
<guid>https://arxiv.org/abs/2512.07273</guid>
<content:encoded><![CDATA[
arXiv:2512.07273v1 Announce Type: new 
Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</title>
<link>https://arxiv.org/abs/2512.07275</link>
<guid>https://arxiv.org/abs/2512.07275</guid>
<content:encoded><![CDATA[
arXiv:2512.07275v1 Announce Type: new 
Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery</title>
<link>https://arxiv.org/abs/2512.07276</link>
<guid>https://arxiv.org/abs/2512.07276</guid>
<content:encoded><![CDATA[
arXiv:2512.07276v1 Announce Type: new 
Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts</title>
<link>https://arxiv.org/abs/2512.07302</link>
<guid>https://arxiv.org/abs/2512.07302</guid>
<content:encoded><![CDATA[
arXiv:2512.07302v1 Announce Type: new 
Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset</title>
<link>https://arxiv.org/abs/2512.07305</link>
<guid>https://arxiv.org/abs/2512.07305</guid>
<content:encoded><![CDATA[
arXiv:2512.07305v1 Announce Type: new 
Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.07328</link>
<guid>https://arxiv.org/abs/2512.07328</guid>
<content:encoded><![CDATA[
arXiv:2512.07328v1 Announce Type: new 
Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers</title>
<link>https://arxiv.org/abs/2512.07331</link>
<guid>https://arxiv.org/abs/2512.07331</guid>
<content:encoded><![CDATA[
arXiv:2512.07331v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Referring Expression Segmentation on Aerial Photos</title>
<link>https://arxiv.org/abs/2512.07338</link>
<guid>https://arxiv.org/abs/2512.07338</guid>
<content:encoded><![CDATA[
arXiv:2512.07338v1 Announce Type: new 
Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07345</link>
<guid>https://arxiv.org/abs/2512.07345</guid>
<content:encoded><![CDATA[
arXiv:2512.07345v1 Announce Type: new 
Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition</title>
<link>https://arxiv.org/abs/2512.07348</link>
<guid>https://arxiv.org/abs/2512.07348</guid>
<content:encoded><![CDATA[
arXiv:2512.07348v1 Announce Type: new 
Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&amp;Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&amp;Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection</title>
<link>https://arxiv.org/abs/2512.07351</link>
<guid>https://arxiv.org/abs/2512.07351</guid>
<content:encoded><![CDATA[
arXiv:2512.07351v1 Announce Type: new 
Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.07360</link>
<guid>https://arxiv.org/abs/2512.07360</guid>
<content:encoded><![CDATA[
arXiv:2512.07360v1 Announce Type: new 
Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency</title>
<link>https://arxiv.org/abs/2512.07379</link>
<guid>https://arxiv.org/abs/2512.07379</guid>
<content:encoded><![CDATA[
arXiv:2512.07379v1 Announce Type: new 
Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</title>
<link>https://arxiv.org/abs/2512.07381</link>
<guid>https://arxiv.org/abs/2512.07381</guid>
<content:encoded><![CDATA[
arXiv:2512.07381v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicCBMs: Logic-Enhanced Concept-Based Learning</title>
<link>https://arxiv.org/abs/2512.07383</link>
<guid>https://arxiv.org/abs/2512.07383</guid>
<content:encoded><![CDATA[
arXiv:2512.07383v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline</title>
<link>https://arxiv.org/abs/2512.07385</link>
<guid>https://arxiv.org/abs/2512.07385</guid>
<content:encoded><![CDATA[
arXiv:2512.07385v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring</title>
<link>https://arxiv.org/abs/2512.07391</link>
<guid>https://arxiv.org/abs/2512.07391</guid>
<content:encoded><![CDATA[
arXiv:2512.07391v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing Objects along Hand Interaction Timelines in Egocentric Video</title>
<link>https://arxiv.org/abs/2512.07394</link>
<guid>https://arxiv.org/abs/2512.07394</guid>
<content:encoded><![CDATA[
arXiv:2512.07394v1 Announce Type: new 
Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs</title>
<link>https://arxiv.org/abs/2512.07410</link>
<guid>https://arxiv.org/abs/2512.07410</guid>
<content:encoded><![CDATA[
arXiv:2512.07410v1 Announce Type: new 
Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Exploration of Mobility Interaction Patterns</title>
<link>https://arxiv.org/abs/2512.07415</link>
<guid>https://arxiv.org/abs/2512.07415</guid>
<content:encoded><![CDATA[
arXiv:2512.07415v1 Announce Type: new 
Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When normalization hallucinates: unseen risks in AI-powered whole slide image processing</title>
<link>https://arxiv.org/abs/2512.07426</link>
<guid>https://arxiv.org/abs/2512.07426</guid>
<content:encoded><![CDATA[
arXiv:2512.07426v1 Announce Type: new 
Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Video Editing with Temporal Reasoner</title>
<link>https://arxiv.org/abs/2512.07469</link>
<guid>https://arxiv.org/abs/2512.07469</guid>
<content:encoded><![CDATA[
arXiv:2512.07469v1 Announce Type: new 
Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance</title>
<link>https://arxiv.org/abs/2512.07480</link>
<guid>https://arxiv.org/abs/2512.07480</guid>
<content:encoded><![CDATA[
arXiv:2512.07480v1 Announce Type: new 
Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior</title>
<link>https://arxiv.org/abs/2512.07498</link>
<guid>https://arxiv.org/abs/2512.07498</guid>
<content:encoded><![CDATA[
arXiv:2512.07498v1 Announce Type: new 
Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.07500</link>
<guid>https://arxiv.org/abs/2512.07500</guid>
<content:encoded><![CDATA[
arXiv:2512.07500v1 Announce Type: new 
Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.07503</link>
<guid>https://arxiv.org/abs/2512.07503</guid>
<content:encoded><![CDATA[
arXiv:2512.07503v1 Announce Type: new 
Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points</title>
<link>https://arxiv.org/abs/2512.07504</link>
<guid>https://arxiv.org/abs/2512.07504</guid>
<content:encoded><![CDATA[
arXiv:2512.07504v1 Announce Type: new 
Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshRipple: Structured Autoregressive Generation of Artist-Meshes</title>
<link>https://arxiv.org/abs/2512.07514</link>
<guid>https://arxiv.org/abs/2512.07514</guid>
<content:encoded><![CDATA[
arXiv:2512.07514v1 Announce Type: new 
Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images</title>
<link>https://arxiv.org/abs/2512.07527</link>
<guid>https://arxiv.org/abs/2512.07527</guid>
<content:encoded><![CDATA[
arXiv:2512.07527v1 Announce Type: new 
Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07564</link>
<guid>https://arxiv.org/abs/2512.07564</guid>
<content:encoded><![CDATA[
arXiv:2512.07564v1 Announce Type: new 
Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation</title>
<link>https://arxiv.org/abs/2512.07568</link>
<guid>https://arxiv.org/abs/2512.07568</guid>
<content:encoded><![CDATA[
arXiv:2512.07568v1 Announce Type: new 
Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\"ive fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs</title>
<link>https://arxiv.org/abs/2512.07580</link>
<guid>https://arxiv.org/abs/2512.07580</guid>
<content:encoded><![CDATA[
arXiv:2512.07580v1 Announce Type: new 
Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Image Technical Report</title>
<link>https://arxiv.org/abs/2512.07584</link>
<guid>https://arxiv.org/abs/2512.07584</guid>
<content:encoded><![CDATA[
arXiv:2512.07584v1 Announce Type: new 
Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07590</link>
<guid>https://arxiv.org/abs/2512.07590</guid>
<content:encoded><![CDATA[
arXiv:2512.07590v1 Announce Type: new 
Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery</title>
<link>https://arxiv.org/abs/2512.07596</link>
<guid>https://arxiv.org/abs/2512.07596</guid>
<content:encoded><![CDATA[
arXiv:2512.07596v1 Announce Type: new 
Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Segment Any 3D Thing as Instance Tracking</title>
<link>https://arxiv.org/abs/2512.07599</link>
<guid>https://arxiv.org/abs/2512.07599</guid>
<content:encoded><![CDATA[
arXiv:2512.07599v1 Announce Type: new 
Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposition Sampling for Efficient Region Annotations in Active Learning</title>
<link>https://arxiv.org/abs/2512.07606</link>
<guid>https://arxiv.org/abs/2512.07606</guid>
<content:encoded><![CDATA[
arXiv:2512.07606v1 Announce Type: new 
Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation</title>
<link>https://arxiv.org/abs/2512.07628</link>
<guid>https://arxiv.org/abs/2512.07628</guid>
<content:encoded><![CDATA[
arXiv:2512.07628v1 Announce Type: new 
Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method</title>
<link>https://arxiv.org/abs/2512.07651</link>
<guid>https://arxiv.org/abs/2512.07651</guid>
<content:encoded><![CDATA[
arXiv:2512.07651v1 Announce Type: new 
Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research</title>
<link>https://arxiv.org/abs/2512.07652</link>
<guid>https://arxiv.org/abs/2512.07652</guid>
<content:encoded><![CDATA[
arXiv:2512.07652v1 Announce Type: new 
Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization-Guided Diffusion for Interactive Scene Generation</title>
<link>https://arxiv.org/abs/2512.07661</link>
<guid>https://arxiv.org/abs/2512.07661</guid>
<content:encoded><![CDATA[
arXiv:2512.07661v1 Announce Type: new 
Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset</title>
<link>https://arxiv.org/abs/2512.07668</link>
<guid>https://arxiv.org/abs/2512.07668</guid>
<content:encoded><![CDATA[
arXiv:2512.07668v1 Announce Type: new 
Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations</title>
<link>https://arxiv.org/abs/2512.07674</link>
<guid>https://arxiv.org/abs/2512.07674</guid>
<content:encoded><![CDATA[
arXiv:2512.07674v1 Announce Type: new 
Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only</title>
<link>https://arxiv.org/abs/2512.07698</link>
<guid>https://arxiv.org/abs/2512.07698</guid>
<content:encoded><![CDATA[
arXiv:2512.07698v1 Announce Type: new 
Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment</title>
<link>https://arxiv.org/abs/2512.07702</link>
<guid>https://arxiv.org/abs/2512.07702</guid>
<content:encoded><![CDATA[
arXiv:2512.07702v1 Announce Type: new 
Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PVeRA: Probabilistic Vector-Based Random Matrix Adaptation</title>
<link>https://arxiv.org/abs/2512.07703</link>
<guid>https://arxiv.org/abs/2512.07703</guid>
<content:encoded><![CDATA[
arXiv:2512.07703v1 Announce Type: new 
Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnCageNet: Tracking and Pose Estimation of Caged Animal</title>
<link>https://arxiv.org/abs/2512.07712</link>
<guid>https://arxiv.org/abs/2512.07712</guid>
<content:encoded><![CDATA[
arXiv:2512.07712v1 Announce Type: new 
Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation</title>
<link>https://arxiv.org/abs/2512.07720</link>
<guid>https://arxiv.org/abs/2512.07720</guid>
<content:encoded><![CDATA[
arXiv:2512.07720v1 Announce Type: new 
Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving action classification with brain-inspired deep networks</title>
<link>https://arxiv.org/abs/2512.07729</link>
<guid>https://arxiv.org/abs/2512.07729</guid>
<content:encoded><![CDATA[
arXiv:2512.07729v1 Announce Type: new 
Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title>
<link>https://arxiv.org/abs/2512.07730</link>
<guid>https://arxiv.org/abs/2512.07730</guid>
<content:encoded><![CDATA[
arXiv:2512.07730v1 Announce Type: new 
Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</title>
<link>https://arxiv.org/abs/2512.07733</link>
<guid>https://arxiv.org/abs/2512.07733</guid>
<content:encoded><![CDATA[
arXiv:2512.07733v1 Announce Type: new 
Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLTCOE Evaluation Team at TREC 2025: VQA Track</title>
<link>https://arxiv.org/abs/2512.07738</link>
<guid>https://arxiv.org/abs/2512.07738</guid>
<content:encoded><![CDATA[
arXiv:2512.07738v1 Announce Type: new 
Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.07745</link>
<guid>https://arxiv.org/abs/2512.07745</guid>
<content:encoded><![CDATA[
arXiv:2512.07745v1 Announce Type: new 
Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.07747</link>
<guid>https://arxiv.org/abs/2512.07747</guid>
<content:encoded><![CDATA[
arXiv:2512.07747v1 Announce Type: new 
Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction</title>
<link>https://arxiv.org/abs/2512.07756</link>
<guid>https://arxiv.org/abs/2512.07756</guid>
<content:encoded><![CDATA[
arXiv:2512.07756v1 Announce Type: new 
Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.07760</link>
<guid>https://arxiv.org/abs/2512.07760</guid>
<content:encoded><![CDATA[
arXiv:2512.07760v1 Announce Type: new 
Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</title>
<link>https://arxiv.org/abs/2512.07776</link>
<guid>https://arxiv.org/abs/2512.07776</guid>
<content:encoded><![CDATA[
arXiv:2512.07776v1 Announce Type: new 
Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Matching Variational AutoEncoder</title>
<link>https://arxiv.org/abs/2512.07778</link>
<guid>https://arxiv.org/abs/2512.07778</guid>
<content:encoded><![CDATA[
arXiv:2512.07778v1 Announce Type: new 
Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory</title>
<link>https://arxiv.org/abs/2512.07802</link>
<guid>https://arxiv.org/abs/2512.07802</guid>
<content:encoded><![CDATA[
arXiv:2512.07802v1 Announce Type: new 
Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-view Pyramid Transformer: Look Coarser to See Broader</title>
<link>https://arxiv.org/abs/2512.07806</link>
<guid>https://arxiv.org/abs/2512.07806</guid>
<content:encoded><![CDATA[
arXiv:2512.07806v1 Announce Type: new 
Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes</title>
<link>https://arxiv.org/abs/2512.07807</link>
<guid>https://arxiv.org/abs/2512.07807</guid>
<content:encoded><![CDATA[
arXiv:2512.07807v1 Announce Type: new 
Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</title>
<link>https://arxiv.org/abs/2512.07821</link>
<guid>https://arxiv.org/abs/2512.07821</guid>
<content:encoded><![CDATA[
arXiv:2512.07821v1 Announce Type: new 
Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing</title>
<link>https://arxiv.org/abs/2512.07826</link>
<guid>https://arxiv.org/abs/2512.07826</guid>
<content:encoded><![CDATA[
arXiv:2512.07826v1 Announce Type: new 
Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</title>
<link>https://arxiv.org/abs/2512.07829</link>
<guid>https://arxiv.org/abs/2512.07829</guid>
<content:encoded><![CDATA[
arXiv:2512.07829v1 Announce Type: new 
Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</title>
<link>https://arxiv.org/abs/2512.07831</link>
<guid>https://arxiv.org/abs/2512.07831</guid>
<content:encoded><![CDATA[
arXiv:2512.07831v1 Announce Type: new 
Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Visual Similarity</title>
<link>https://arxiv.org/abs/2512.07833</link>
<guid>https://arxiv.org/abs/2512.07833</guid>
<content:encoded><![CDATA[
arXiv:2512.07833v1 Announce Type: new 
Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voxify3D: Pixel Art Meets Volumetric Rendering</title>
<link>https://arxiv.org/abs/2512.07834</link>
<guid>https://arxiv.org/abs/2512.07834</guid>
<content:encoded><![CDATA[
arXiv:2512.07834v1 Announce Type: new 
Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.05992</link>
<guid>https://arxiv.org/abs/2512.05992</guid>
<content:encoded><![CDATA[
arXiv:2512.05992v1 Announce Type: cross 
Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Temporal Single-photon LiDAR</title>
<link>https://arxiv.org/abs/2512.06008</link>
<guid>https://arxiv.org/abs/2512.06008</guid>
<content:encoded><![CDATA[
arXiv:2512.06008v1 Announce Type: cross 
Abstract: Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</title>
<link>https://arxiv.org/abs/2512.06147</link>
<guid>https://arxiv.org/abs/2512.06147</guid>
<content:encoded><![CDATA[
arXiv:2512.06147v1 Announce Type: cross 
Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&amp;M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</title>
<link>https://arxiv.org/abs/2512.06589</link>
<guid>https://arxiv.org/abs/2512.06589</guid>
<content:encoded><![CDATA[
arXiv:2512.06589v1 Announce Type: cross 
Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Quantization using Gaussian Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.06609</link>
<guid>https://arxiv.org/abs/2512.06609</guid>
<content:encoded><![CDATA[
arXiv:2512.06609v1 Announce Type: cross 
Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</title>
<link>https://arxiv.org/abs/2512.06628</link>
<guid>https://arxiv.org/abs/2512.06628</guid>
<content:encoded><![CDATA[
arXiv:2512.06628v1 Announce Type: cross 
Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2512.06648</link>
<guid>https://arxiv.org/abs/2512.06648</guid>
<content:encoded><![CDATA[
arXiv:2512.06648v1 Announce Type: cross 
Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning</title>
<link>https://arxiv.org/abs/2512.06649</link>
<guid>https://arxiv.org/abs/2512.06649</guid>
<content:encoded><![CDATA[
arXiv:2512.06649v1 Announce Type: cross 
Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2512.06665</link>
<guid>https://arxiv.org/abs/2512.06665</guid>
<content:encoded><![CDATA[
arXiv:2512.06665v1 Announce Type: cross 
Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data</title>
<link>https://arxiv.org/abs/2512.06730</link>
<guid>https://arxiv.org/abs/2512.06730</guid>
<content:encoded><![CDATA[
arXiv:2512.06730v1 Announce Type: cross 
Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v1 Announce Type: cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association</title>
<link>https://arxiv.org/abs/2512.06757</link>
<guid>https://arxiv.org/abs/2512.06757</guid>
<content:encoded><![CDATA[
arXiv:2512.06757v1 Announce Type: cross 
Abstract: This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both "heard" and "unheard" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices</title>
<link>https://arxiv.org/abs/2512.06848</link>
<guid>https://arxiv.org/abs/2512.06848</guid>
<content:encoded><![CDATA[
arXiv:2512.06848v1 Announce Type: cross 
Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Visual SLAM using a General 3D Prior</title>
<link>https://arxiv.org/abs/2512.06868</link>
<guid>https://arxiv.org/abs/2512.06868</guid>
<content:encoded><![CDATA[
arXiv:2512.06868v1 Announce Type: cross 
Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v1 Announce Type: cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</title>
<link>https://arxiv.org/abs/2512.06963</link>
<guid>https://arxiv.org/abs/2512.06963</guid>
<content:encoded><![CDATA[
arXiv:2512.06963v1 Announce Type: cross 
Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</title>
<link>https://arxiv.org/abs/2512.06990</link>
<guid>https://arxiv.org/abs/2512.06990</guid>
<content:encoded><![CDATA[
arXiv:2512.06990v1 Announce Type: cross 
Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</title>
<link>https://arxiv.org/abs/2512.07040</link>
<guid>https://arxiv.org/abs/2512.07040</guid>
<content:encoded><![CDATA[
arXiv:2512.07040v1 Announce Type: cross 
Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.07130</link>
<guid>https://arxiv.org/abs/2512.07130</guid>
<content:encoded><![CDATA[
arXiv:2512.07130v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.07132</link>
<guid>https://arxiv.org/abs/2512.07132</guid>
<content:encoded><![CDATA[
arXiv:2512.07132v1 Announce Type: cross 
Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</title>
<link>https://arxiv.org/abs/2512.07142</link>
<guid>https://arxiv.org/abs/2512.07142</guid>
<content:encoded><![CDATA[
arXiv:2512.07142v1 Announce Type: cross 
Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers</title>
<link>https://arxiv.org/abs/2512.07150</link>
<guid>https://arxiv.org/abs/2512.07150</guid>
<content:encoded><![CDATA[
arXiv:2512.07150v1 Announce Type: cross 
Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</title>
<link>https://arxiv.org/abs/2512.07224</link>
<guid>https://arxiv.org/abs/2512.07224</guid>
<content:encoded><![CDATA[
arXiv:2512.07224v1 Announce Type: cross 
Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affine Subspace Models and Clustering for Patch-Based Image Denoising</title>
<link>https://arxiv.org/abs/2512.07259</link>
<guid>https://arxiv.org/abs/2512.07259</guid>
<content:encoded><![CDATA[
arXiv:2512.07259v1 Announce Type: cross 
Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Unification of Concept Learning with Concept Cones</title>
<link>https://arxiv.org/abs/2512.07355</link>
<guid>https://arxiv.org/abs/2512.07355</guid>
<content:encoded><![CDATA[
arXiv:2512.07355v1 Announce Type: cross 
Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</title>
<link>https://arxiv.org/abs/2512.07390</link>
<guid>https://arxiv.org/abs/2512.07390</guid>
<content:encoded><![CDATA[
arXiv:2512.07390v1 Announce Type: cross 
Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models</title>
<link>https://arxiv.org/abs/2512.07419</link>
<guid>https://arxiv.org/abs/2512.07419</guid>
<content:encoded><![CDATA[
arXiv:2512.07419v1 Announce Type: cross 
Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</title>
<link>https://arxiv.org/abs/2512.07437</link>
<guid>https://arxiv.org/abs/2512.07437</guid>
<content:encoded><![CDATA[
arXiv:2512.07437v1 Announce Type: cross 
Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Geometry Distribution for 3D Animation Generation</title>
<link>https://arxiv.org/abs/2512.07459</link>
<guid>https://arxiv.org/abs/2512.07459</guid>
<content:encoded><![CDATA[
arXiv:2512.07459v1 Announce Type: cross 
Abstract: Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \times$ higher user study score), achieving the best results across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces</title>
<link>https://arxiv.org/abs/2512.07509</link>
<guid>https://arxiv.org/abs/2512.07509</guid>
<content:encoded><![CDATA[
arXiv:2512.07509v1 Announce Type: cross 
Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX: Reasoning with Latent Exploration for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07558</link>
<guid>https://arxiv.org/abs/2512.07558</guid>
<content:encoded><![CDATA[
arXiv:2512.07558v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework</title>
<link>https://arxiv.org/abs/2512.07574</link>
<guid>https://arxiv.org/abs/2512.07574</guid>
<content:encoded><![CDATA[
arXiv:2512.07574v1 Announce Type: cross 
Abstract: Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation</title>
<link>https://arxiv.org/abs/2512.07576</link>
<guid>https://arxiv.org/abs/2512.07576</guid>
<content:encoded><![CDATA[
arXiv:2512.07576v1 Announce Type: cross 
Abstract: Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</title>
<link>https://arxiv.org/abs/2512.07687</link>
<guid>https://arxiv.org/abs/2512.07687</guid>
<content:encoded><![CDATA[
arXiv:2512.07687v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep transfer learning for image classification: a survey</title>
<link>https://arxiv.org/abs/2205.09904</link>
<guid>https://arxiv.org/abs/2205.09904</guid>
<content:encoded><![CDATA[
arXiv:2205.09904v2 Announce Type: replace 
Abstract: Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for image classification is when large deep models can be trained on abundant labelled data. However there are many real world scenarios where the requirement for large amounts of training data to get the best performance cannot be met. In these scenarios transfer learning can help improve performance. To date there have been no surveys that comprehensively review deep transfer learning as it relates to image classification overall. However, several recent general surveys of deep transfer learning and ones that relate to particular specialised target image classification tasks have been published. We believe it is important for the future progress in the field that all current knowledge is collated and the overarching patterns analysed and discussed. In this survey we formally define deep transfer learning and the problem it attempts to solve in relation to image classification. We survey the current state of the field and identify where recent progress has been made. We show where the gaps in current knowledge are and make suggestions for how to progress the field to fill in these knowledge gaps. We present a new taxonomy of the applications of transfer learning for image classification. This taxonomy makes it easier to see overarching patterns of where transfer learning has been effective and, where it has failed to fulfill its potential. This also allows us to suggest where the problems lie and how it could be used more effectively. We show that under this new taxonomy, many of the applications where transfer learning has been shown to be ineffective or even hinder performance are to be expected when taking into account the source and target datasets and the techniques used.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping</title>
<link>https://arxiv.org/abs/2303.11228</link>
<guid>https://arxiv.org/abs/2303.11228</guid>
<content:encoded><![CDATA[
arXiv:2303.11228v3 Announce Type: replace 
Abstract: Object segmentation for robotic grasping under dynamic conditions often faces challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Deep Learning network that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders, one for each signal input and a spatial pyramidal pooling with atrous convolutions. Encoders capture rich contextual information by pooling the concatenated features at different resolutions while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The evaluation results show a 6-10\% segmentation accuracy improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The model code is available at https://github.com/sanket0707/Bimodal-SegNet.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2308.09388</link>
<guid>https://arxiv.org/abs/2308.09388</guid>
<content:encoded><![CDATA[
arXiv:2308.09388v3 Announce Type: replace 
Abstract: Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, "whether diffusion model can boost image restoration". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Roadside Monocular 3D Detection Prompted by 2D Detection</title>
<link>https://arxiv.org/abs/2404.01064</link>
<guid>https://arxiv.org/abs/2404.01064</guid>
<content:encoded><![CDATA[
arXiv:2404.01064v5 Announce Type: replace 
Abstract: Roadside monocular 3D detection requires detecting objects of predefined classes in an RGB frame and predicting their 3D attributes, such as bird's-eye-view (BEV) locations. It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To address this task, we introduce Promptable 3D Detector (Pro3D), a novel detector design that leverages 2D detections as prompts. We build our Pro3D upon two key insights. First, compared to a typical 3D detector, a 2D detector is ``easier'' to train due to fewer loss terms and performs significantly better at localizing objects w.r.t 2D metrics. Second, once 2D detections precisely locate objects in the image, a 3D detector can focus on lifting these detections into 3D BEV, especially when fixed camera pose or scene geometry provide an informative prior. To encode and incorporate 2D detections, we explore three methods: (a) concatenating features from both 2D and 3D detectors, (b) attentively fusing 2D and 3D detector features, and (c) encoding properties of predicted 2D bounding boxes \{$x$, $y$, width, height, label\} and attentively fusing them with the 3D detector feature. Interestingly, the third method significantly outperforms the others, underscoring the effectiveness of 2D detections as prompts that offer precise object targets and allow the 3D detector to focus on lifting them into 3D. Pro3D is adaptable for use with a wide range of 2D and 3D detectors with minimal modifications. Comprehensive experiments demonstrate that our Pro3D significantly enhances existing methods, achieving state-of-the-art results on two contemporary benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSP-GNN: Learning to Track via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2407.04308</link>
<guid>https://arxiv.org/abs/2407.04308</guid>
<content:encoded><![CDATA[
arXiv:2407.04308v3 Announce Type: replace 
Abstract: We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Model</title>
<link>https://arxiv.org/abs/2408.01627</link>
<guid>https://arxiv.org/abs/2408.01627</guid>
<content:encoded><![CDATA[
arXiv:2408.01627v3 Announce Type: replace 
Abstract: In recent years, the talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high-quality video. However, no single model has yet achieved equivalence across all quantitative and qualitative metrics. We introduce Jamba, a hybrid Transformer-Mamba model, to animate a 3D face. Mamba, a pioneering Structured State Space Model (SSM) architecture, was developed to overcome the limitations of conventional Transformer architectures, particularly in handling long sequences. This challenge has constrained traditional models. Jamba combines the advantages of both the Transformer and Mamba approaches, offering a comprehensive solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and lip sync through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Test Time Adaptation with Few-shot Guidance</title>
<link>https://arxiv.org/abs/2409.01341</link>
<guid>https://arxiv.org/abs/2409.01341</guid>
<content:encoded><![CDATA[
arXiv:2409.01341v4 Announce Type: replace 
Abstract: Deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. To address this issue, Test Time Adaptation (TTA) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. Although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a novel and practical setting that utilizes a few-shot support set on top of TTA. Adhering to the principle of few inputs, big gains, FS-TTA reduces blind exploration in unseen target domains. Furthermore, we propose a two-stage framework to tackle FS-TTA, including (i) fine-tuning the pre-trained source model with few-shot support set, along with using feature diversity augmentation module to avoid overfitting, (ii) implementing test time adaptation based on prototype memory bank guidance to produce high quality pseudo-label for model adaptation. Through extensive experiments on three cross-domain classification benchmarks, we demonstrate the superior performance and reliability of our FS-TTA and framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-Customized Image Generation</title>
<link>https://arxiv.org/abs/2410.02483</link>
<guid>https://arxiv.org/abs/2410.02483</guid>
<content:encoded><![CDATA[
arXiv:2410.02483v2 Announce Type: replace 
Abstract: Customized Image Generation, generating customized images with user-specified concepts, has raised significant attention due to its creativity and novelty. With impressive progress achieved in subject customization, some pioneer works further explored the customization of action and interaction beyond entity (i.e., human, animal, and object) appearance. However, these approaches only focus on basic actions and interactions between two entities, and their effects are limited by insufficient ''exactly same'' reference images. To extend customized image generation to more complex scenes for general real-world applications, we propose a new task: event-customized image generation. Given a single reference image, we define the ''event'' as all specific actions, poses, relations, or interactions between different entities in the scene. This task aims at accurately capturing the complex event and generating customized images with various target entities. To solve this task, we proposed a novel training-free event customization method: FreeEvent. Specifically, FreeEvent introduces two extra paths alongside the general diffusion denoising process: 1) Entity switching path: it applies cross-attention guidance and regulation for target entity generation. 2) Event transferring path: it injects the spatial feature and self-attention maps from the reference image to the target image for event generation. To further facilitate this new task, we collected two evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and ablations have demonstrated the effectiveness of FreeEvent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation</title>
<link>https://arxiv.org/abs/2410.06055</link>
<guid>https://arxiv.org/abs/2410.06055</guid>
<content:encoded><![CDATA[
arXiv:2410.06055v2 Announce Type: replace 
Abstract: While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution (HR) image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one. Instead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for HR image generation; however, existing methods often result in poor image quality and long inference time. We introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel training-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency. Extensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications. Codes: https://github.com/kmittle/RepLDM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation</title>
<link>https://arxiv.org/abs/2410.07618</link>
<guid>https://arxiv.org/abs/2410.07618</guid>
<content:encoded><![CDATA[
arXiv:2410.07618v2 Announce Type: replace 
Abstract: Although Chinese calligraphy generation has achieved style transfer, generating calligraphy by specifying the calligrapher, font, and character style remains challenging. To address this, we propose a new Chinese calligraphy generation model 'Moyun' , which replaces the Unet in the Diffusion model with Vision Mamba and introduces the TripleLabel control mechanism to achieve controllable calligraphy generation. The model was tested on our large-scale dataset 'Mobao' of over 1.9 million images, and the results demonstrate that 'Moyun' can effectively control the generation process and produce calligraphy in the specified style. Even for calligraphy the calligrapher has not written, 'Moyun' can generate calligraphy that matches the style of the calligrapher.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Motion: A Generative Approach for Scene Dynamics Compression</title>
<link>https://arxiv.org/abs/2410.09768</link>
<guid>https://arxiv.org/abs/2410.09768</guid>
<content:encoded><![CDATA[
arXiv:2410.09768v3 Announce Type: replace 
Abstract: This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-https://github.com/xyzysz/GNVDC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion</title>
<link>https://arxiv.org/abs/2411.10036</link>
<guid>https://arxiv.org/abs/2411.10036</guid>
<content:encoded><![CDATA[
arXiv:2411.10036v2 Announce Type: replace 
Abstract: Multimodal image fusion (MMIF) integrates information from different modalities to obtain a comprehensive image, aiding downstream tasks. However, existing research focuses on complementary information fusion and training strategies, overlooking the critical role of underlying architectural components like normalization and convolution kernels. We reevaluate the UNet architecture for end-to-end MMIF, identifying that widely used batch normalization limits performance by smoothing crucial sparse features. To address this, we propose a hybrid of instance and group normalization to maintain sample independence and reinforce intrinsic feature correlations. Crucially, this strategy facilitates richer feature maps, enabling large kernel convolution to fully leverage its receptive field, enhancing detail preservation. Furthermore, the proposed multi-path adaptive fusion module dynamically calibrates features from varying scales and receptive fields, ensuring effective information transfer. Our method achieves SOTA objective performance on MSRS, M$^3$FD, TNO, and Harvard datasets, producing visually clearer salient objects and lesion areas. Notably, it improves MSRS segmentation mIoU by 8.1\% over the infrared image. This performance stems from a synergistic design of normalization and convolution kernels, which preserves critical sparse features. The code is available at https://github.com/HeDan-11/LKC-FUNet.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyViM: Frequency Decoupling for Tiny Hybrid Vision Mamba</title>
<link>https://arxiv.org/abs/2411.17473</link>
<guid>https://arxiv.org/abs/2411.17473</guid>
<content:encoded><![CDATA[
arXiv:2411.17473v2 Announce Type: replace 
Abstract: Mamba has shown great potential for computer vision due to its linear complexity in modeling the global context with respect to the input length. However, existing lightweight Mamba-based backbones cannot demonstrate performance that matches Convolution or Transformer-based methods. By observing, we find that simply modifying the scanning path in the image domain is not conducive to fully exploiting the potential of vision Mamba. In this paper, we first perform comprehensive spectral and quantitative analyses, and verify that the Mamba block mainly models low-frequency information under Convolution-Mamba hybrid architecture. Based on the analyses, we introduce a novel Laplace mixer to decouple the features in terms of frequency and input only the low-frequency components into the Mamba block. In addition, considering the redundancy of the features and the different requirements for high-frequency details and low-frequency global information at different stages, we introduce a frequency ramp inception, i.e., gradually reduce the input dimensions of the high-frequency branches, so as to efficiently trade-off the high-frequency and low-frequency components at different layers. By integrating mobile-friendly convolution and efficient Laplace mixer, we build a series of tiny hybrid vision Mamba called TinyViM. The proposed TinyViM achieves impressive performance on several downstream tasks including image classification, semantic segmentation, object detection and instance segmentation. In particular, TinyViM outperforms Convolution, Transformer and Mamba-based models with similar scales, and the throughput is about 2-3 times higher than that of other Mamba-based models. Code is available at https://github.com/xwmaxwma/TinyViM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-ICE: An Inner Interpretable Framework for Image Classification via Bi-directional Interactions between Concept and Input Embeddings</title>
<link>https://arxiv.org/abs/2411.18645</link>
<guid>https://arxiv.org/abs/2411.18645</guid>
<content:encoded><![CDATA[
arXiv:2411.18645v2 Announce Type: replace 
Abstract: Inner interpretability is a promising field aiming to uncover the internal mechanisms of AI systems through scalable, automated methods. While significant research has been conducted on large language models, limited attention has been paid to applying inner interpretability to large-scale image tasks, focusing primarily on architectural and functional levels to visualize learned concepts. In this paper, we first present a conceptual framework that supports inner interpretability and multilevel analysis for large-scale image classification tasks. Specifically, we introduce the Bi-directional Interaction between Concept and Input Embeddings (Bi-ICE) module, which facilitates interpretability across the computational, algorithmic, and implementation levels. This module enhances transparency by generating predictions based on human-understandable concepts, quantifying their contributions, and localizing them within the inputs. Finally, we showcase enhanced transparency in image classification, measuring concept contributions, and pinpointing their locations within the inputs. Our approach highlights algorithmic interpretability by demonstrating the process of concept learning and its convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twisted Convolutional Networks (TCNs): Enhancing Feature Interactions for Non-Spatial Data Classification</title>
<link>https://arxiv.org/abs/2412.00238</link>
<guid>https://arxiv.org/abs/2412.00238</guid>
<content:encoded><![CDATA[
arXiv:2412.00238v2 Announce Type: replace 
Abstract: Twisted Convolutional Networks (TCNs) are proposed as a novel deep learning architecture for classifying one-dimensional data with arbitrary feature order and minimal spatial relationships. Unlike conventional Convolutional Neural Networks (CNNs) that rely on structured feature sequences, TCNs explicitly combine subsets of input features through theoretically grounded multiplicative and pairwise interaction mechanisms to create enriched representations. This feature combination strategy, formalized through polynomial feature expansions, captures high-order feature interactions that traditional convolutional approaches miss. We provide a comprehensive mathematical framework for TCNs, demonstrating how the twisted convolution operation generalizes standard convolutions while maintaining computational tractability. Through extensive experiments on five benchmark datasets from diverse domains (medical diagnostics, political science, synthetic data, chemometrics, and healthcare), we show that TCNs achieve statistically significant improvements over CNNs, Residual Networks (ResNet), Graph Neural Networks (GNNs), DeepSets, and Support Vector Machine (SVM). The performance gains are validated through statistical testing. TCNs also exhibit superior training stability and generalization capabilities, highlighting their robustness for non-spatial data classification tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Object Detectors via Collective Contribution of Pixels</title>
<link>https://arxiv.org/abs/2412.00666</link>
<guid>https://arxiv.org/abs/2412.00666</guid>
<content:encoded><![CDATA[
arXiv:2412.00666v2 Announce Type: replace 
Abstract: Visual explanations for object detectors are crucial for enhancing their reliability. Object detectors identify and localize instances by assessing multiple visual features collectively. When generating explanations, overlooking these collective influences in detections may lead to missing compositional cues or capturing spurious correlations. However, existing methods typically focus solely on individual pixel contributions, neglecting the collective contribution of multiple pixels. To address this limitation, we propose a game-theoretic method based on Shapley values and interactions to explicitly capture both individual and collective pixel contributions. Our method provides explanations for both bounding box localization and class determination, highlighting regions crucial for detection. Extensive experiments demonstrate that the proposed method identifies important regions more accurately than state-of-the-art methods. The code will be publicly available soon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation</title>
<link>https://arxiv.org/abs/2412.01254</link>
<guid>https://arxiv.org/abs/2412.01254</guid>
<content:encoded><![CDATA[
arXiv:2412.01254v3 Announce Type: replace 
Abstract: This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity: (i) fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait; (ii) even coarse-grained expression control can cause facial changes that compromise identity, since they all act on the face. These limitations remain unaddressed by previous generation methods, which primarily rely on coarse control signals or two-stage inference that integrates portrait animation. Here, we introduce EmojiDiff, the first end-to-end solution that enables simultaneous control of extremely detailed expression (RGB-level) and high-fidelity identity in portrait generation. To address the above challenges, EmojiDiff adopts a two-stage scheme involving decoupled training and fine-tuning. For decoupled training, we innovate ID-irrelevant Data Iteration (IDI) to synthesize cross-identity expression pairs by dividing and optimizing the processes of maintaining expression and altering identity, thereby ensuring stable and high-quality data generation. Training the model with this data, we effectively disentangle fine expression features in the expression template from other extraneous information (e.g., identity, skin). Subsequently, we present ID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves rapid reconstruction and joint supervision of identity and expression information, thus aligning identity representations of images with and without expression control. Experimental results demonstrate that our method remarkably outperforms counterparts, achieves precise expression control with highly maintained identity, and generalizes well to various diffusion models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAMCL: Empowering SAM to Continually Learn from Dynamic Domains with Extreme Storage Efficiency</title>
<link>https://arxiv.org/abs/2412.05012</link>
<guid>https://arxiv.org/abs/2412.05012</guid>
<content:encoded><![CDATA[
arXiv:2412.05012v2 Announce Type: replace 
Abstract: Segment Anything Model (SAM) struggles in open-world scenarios with diverse domains. In such settings, naive fine-tuning with a well-designed learning module is inadequate and often causes catastrophic forgetting issue when learning incrementally. To address this issue, we propose a novel continual learning (CL) method for SAM, termed SAMCL. Rather than relying on a fixed learning module, our method decomposes incremental knowledge into separate modules and trains a selector to choose the appropriate one during inference. However, this intuitive design introduces two key challenges: ensuring effective module learning and selection, and managing storage as tasks accumulate. To tackle these, we introduce two components: AugModule and Module Selector. AugModule reduces the storage of the popular LoRA learning module by sharing parameters across layers while maintaining accuracy. It also employs heatmaps-generated from point prompts-to further enhance domain adaptation with minimal additional cost. Module Selector leverages the observation that SAM's embeddings can effectively distinguish domains, enabling high selection accuracy by training on low-consumed embeddings instead of raw images. Experiments show that SAMCL outperforms state-of-the-art methods, achieving only 0.19% forgetting and at least 2.5% gain on unseen domains. Each AugModule requires just 0.233 MB, reducing storage by at least 24.3% over other fine-tuning approaches. The buffer storage for Module Selector is further reduced by up to 256$\times$.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unsupervised Domain Bridging via Image Degradation in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2412.10339</link>
<guid>https://arxiv.org/abs/2412.10339</guid>
<content:encoded><![CDATA[
arXiv:2412.10339v2 Announce Type: replace 
Abstract: Semantic segmentation suffers from significant performance degradation when the trained network is applied to a different domain. To address this issue, unsupervised domain adaptation (UDA) has been extensively studied. Despite the effectiveness of selftraining techniques in UDA, they still overlook the explicit modeling of domain-shared feature extraction. In this paper, we propose DiDA, an unsupervised domain bridging approach for semantic segmentation. DiDA consists of two key modules: (1) Degradation-based Intermediate Domain Construction, which creates continuous intermediate domains through simple image degradation operations to encourage learning domain-invariant features as domain differences gradually diminish; (2) Semantic Shift Compensation, which leverages a diffusion encoder to disentangle and compensate for semantic shift information with degraded timesteps, preserving discriminative representations in the intermediate domains. As a plug-and-play solution, DiDA supports various degradation operations and seamlessly integrates with existing UDA methods. Extensive experiments on multiple domain adaptive semantic segmentation benchmarks demonstrate that DiDA consistently achieves significant performance improvements across all settings. Code is available at https://github.com/Woof6/DiDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Interpretability for Adversarial Robustness: A Hybrid Generative Classification Approach</title>
<link>https://arxiv.org/abs/2412.20025</link>
<guid>https://arxiv.org/abs/2412.20025</guid>
<content:encoded><![CDATA[
arXiv:2412.20025v2 Announce Type: replace 
Abstract: Deep learning-based discriminative classifiers, despite their remarkable success, remain vulnerable to adversarial examples that can mislead model predictions. While adversarial training can enhance robustness, it fails to address the intrinsic vulnerability stemming from the opaque nature of these black-box models. We present a deep ensemble model that combines discriminative features with generative models to achieve both high accuracy and adversarial robustness. Our approach integrates a bottom-level pre-trained discriminative network for feature extraction with a top-level generative classification network that models adversarial input distributions through a deep latent variable model. Using variational Bayes, our model achieves superior robustness against white-box adversarial attacks without adversarial training. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate our model's superior adversarial robustness. Through evaluations using counterfactual metrics and feature interaction-based metrics, we establish correlations between model interpretability and adversarial robustness. Additionally, preliminary results on Tiny-ImageNet validate our approach's scalability to more complex datasets, offering a practical solution for developing robust image classification models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering</title>
<link>https://arxiv.org/abs/2501.01371</link>
<guid>https://arxiv.org/abs/2501.01371</guid>
<content:encoded><![CDATA[
arXiv:2501.01371v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) demonstrate remarkable capabilities in visual understanding and reasoning, such as in Visual Question Answering (VQA), where the model is asked a question related to a visual input. Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. CLIP-UP leverages CLIP-based similarity measures to extract question-image alignment information to detect unanswerability, requiring efficient training of only a few additional layers, while keeping the original VLMs' weights unchanged. Tested across several models, CLIP-UP achieves significant improvements on benchmarks assessing unanswerability in both multiple-choice and open-ended VQA, surpassing other methods, while preserving original performance on other tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expectation-Maximization as the Engine of Scalable Medical Intelligence</title>
<link>https://arxiv.org/abs/2501.03410</link>
<guid>https://arxiv.org/abs/2501.03410</guid>
<content:encoded><![CDATA[
arXiv:2501.03410v2 Announce Type: replace 
Abstract: Large, high-quality, annotated datasets are the foundation of medical AI research, but constructing even a small, moderate-quality, annotated dataset can take years of effort from multidisciplinary teams. Although active learning can prioritize what to annotate, scaling up still requires extensive manual efforts to revise the noisy annotations. We formulate this as a missing-data problem and develop ScaleMAI, a framework that unifies data annotation and model development co-evolution through an Expectation-Maximization (EM) process. In this iterative process, the AI model automatically identifies and corrects the mistakes in annotations (Expectation), while the refined annotated data retrain the model to improve accuracy (Maximization). In addition to the classical EM algorithm, ScaleMAI brings human experts into the loop to review annotations that cannot be adequately addressed by either Expectation or Maximization step (<5%). As a result, ScaleMAI progressively creates an annotated dataset of 47,315 CT scans (4.8x larger than the largest public dataset, PanTS) including 4,163,720 per-voxel annotations for benign/malignant tumors and 88 anatomical structures. ScaleMAI iteratively trains a model that exceeds human expert performance in tumor diagnosis (+7%), and outperforms models developed from smaller, moderate-quality datasets, with statistically significant gains in tumor detection (+10%) and segmentation (+14%) on two prestigious benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</title>
<link>https://arxiv.org/abs/2501.17792</link>
<guid>https://arxiv.org/abs/2501.17792</guid>
<content:encoded><![CDATA[
arXiv:2501.17792v3 Announce Type: replace 
Abstract: We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through the.se experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</title>
<link>https://arxiv.org/abs/2502.00843</link>
<guid>https://arxiv.org/abs/2502.00843</guid>
<content:encoded><![CDATA[
arXiv:2502.00843v2 Announce Type: replace 
Abstract: In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a "teacher" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from 20.11% to 35.16% across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPFFNet: Strip Perception and Feature Fusion Spatial Pyramid Pooling for Fabric Defect Detection</title>
<link>https://arxiv.org/abs/2502.01445</link>
<guid>https://arxiv.org/abs/2502.01445</guid>
<content:encoded><![CDATA[
arXiv:2502.01445v3 Announce Type: replace 
Abstract: Defect detection in fabrics is critical for quality control, yet existing methods often struggle with complex backgrounds and shape-specific defects. In this paper, we propose an improved fabric defect detection model based on YOLOv11. To enhance the detection of strip defects, we introduce a Strip Perception Module (SPM) that improves feature capture through multi-scale convolution. We further enhance the spatial pyramid pooling fast (SPPF) by integrating a squeeze-and-excitation mechanism, resulting in the SE-SPPF module, which better integrates spatial and channel information for more effective defect feature extraction. Additionally, we propose a novel focal enhanced complete intersection over union (FECIoU) metric with adaptive weights, addressing scale differences and class imbalance by adjusting the weights of hard-to-detect instances through focal loss. Experimental results demonstrate that our model achieves a 0.8-8.1% improvement in mean average precision (mAP) on the Tianchi dataset and a 1.6-13.2% improvement on our custom dataset, outperforming other state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation</title>
<link>https://arxiv.org/abs/2502.09274</link>
<guid>https://arxiv.org/abs/2502.09274</guid>
<content:encoded><![CDATA[
arXiv:2502.09274v2 Announce Type: replace 
Abstract: 3D scene understanding is a critical yet challenging task in autonomous driving due to the irregularity and sparsity of LiDAR data, as well as the computational demands of processing large-scale point clouds. Recent methods leverage range-view representations to enhance efficiency, but they often adopt higher azimuth resolutions to mitigate information loss during spherical projection, where only the closest point is retained for each 2D grid. However, processing wide panoramic range-view images remains inefficient and may introduce additional distortions. Our empirical analysis shows that training with multiple range images, obtained from splitting the full point cloud, improves both segmentation accuracy and computational efficiency. However, this approach also poses new challenges of exacerbated class imbalance and increase in projection artifacts. To address these, we introduce FLARES, a novel training paradigm that incorporates two tailored data augmentation techniques and a specialized post-processing method designed for multi-range settings. Extensive experiments demonstrate that FLARES is highly generalizable across different architectures, yielding 2.1%~7.9% mIoU improvements on SemanticKITTI and 1.8%~3.9% mIoU on nuScenes, while delivering over 40% speed-up in inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v4 Announce Type: replace 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D Large Multimodal Models (LMMs), yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D LMMs. These long-standing challenges include the failure to adapt to varying point cloud resolutions during inference and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the pre-trained encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the state-of-the-art model, PointLLM-PiSA-13B, achieving 57.91%, 61.0%, and 55.20% on the classification, captioning, and VQA tasks, respectively. Our results show that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thicker and Quicker: A Jumbo Token for Fast Plain Vision Transformers</title>
<link>https://arxiv.org/abs/2502.15021</link>
<guid>https://arxiv.org/abs/2502.15021</guid>
<content:encoded><![CDATA[
arXiv:2502.15021v3 Announce Type: replace 
Abstract: ViTs are general and accurate, and address many tasks, but ViTs are slow, and are not always practical when efficiency is key. Existing methods for faster ViTs design hybrid non-ViT architectures, losing generality, or shrink their tokens, sacrificing accuracy. While many non-ViT architectures are both fast and accurate, they cannot flexibly process other input shapes, pre-train by SOTA self-supervised learning, reduce computation by dropping tokens, and more like ViTs can. We make ViTs faster by reducing patch token width while increasing global token width by adding a new Jumbo token. Our wider Jumbo token is processed by its own wider FFN to increase model capacity. Yet our Jumbo FFN is efficient: it processes a single token, for speed, and its parameters are shared across all layers, for memory. Crucially, our Jumbo is attention-only and non-hierarchical, like a plain ViT, so it is simple, scalable, flexible, and compatible with ViT methods new and old. Jumbo improves over ViT baselines with Registers from Nano to Large scales while maintaining speed/throughput on ImageNet-1K (0.1-13%). Jumbo also improves MAE pre-training (4.9% linear probing on ImageNet-1K), test-time adaptation (5.2% on ImageNet-C), and time series modeling. Our Jumbo models even achieve better speed-accuracy trade-offs than specialized non-ViT compute-efficient models, while maintaining plain-ViT compatibility for practicality. Code and weights available: https://github.com/antofuller/jumbo
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Industrial Anomalies Synthesis</title>
<link>https://arxiv.org/abs/2502.16412</link>
<guid>https://arxiv.org/abs/2502.16412</guid>
<content:encoded><![CDATA[
arXiv:2502.16412v2 Announce Type: replace 
Abstract: This paper comprehensively reviews anomaly synthesis methodologies. Existing surveys focus on limited techniques, missing an overall field view and understanding method interconnections. In contrast, our study offers a unified review, covering about 40 representative methods across Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based synthesis. We introduce the first industrial anomaly synthesis (IAS) taxonomy. Prior works lack formal classification or use simplistic taxonomies, hampering structured comparisons and trend identification. Our taxonomy provides a fine-grained framework reflecting methodological progress and practical implications, grounding future research. Furthermore, we explore cross-modality synthesis and large-scale VLM. Previous surveys overlooked multimodal data and VLM in anomaly synthesis, limiting insights into their advantages. Our survey analyzes their integration, benefits, challenges, and prospects, offering a roadmap to boost IAS with multimodal learning. More resources are available at https://github.com/M-3LAB/awesome-anomaly-synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2503.06983</link>
<guid>https://arxiv.org/abs/2503.06983</guid>
<content:encoded><![CDATA[
arXiv:2503.06983v2 Announce Type: replace 
Abstract: While cooperative perception can overcome the limitations of single-vehicle systems, the practical implementation of vehicle-to-vehicle and vehicle-to-infrastructure systems is often impeded by significant economic barriers. Aerial-ground cooperation (AGC), which pairs ground vehicles with drones, presents a more economically viable and rapidly deployable alternative. However, this emerging field has been held back by a critical lack of high-quality public datasets and benchmarks. To bridge this gap, we present \textit{Griffin}, a comprehensive AGC 3D perception dataset, featuring over 250 dynamic scenes (37k+ frames). It incorporates varied drone altitudes (20-60m), diverse weather conditions, realistic drone dynamics via CARLA-AirSim co-simulation, and critical occlusion-aware 3D annotations. Accompanying the dataset is a unified benchmarking framework for cooperative detection and tracking, with protocols to evaluate communication efficiency, altitude adaptability, and robustness to communication latency, data loss and localization noise. By experiments through different cooperative paradigms, we demonstrate the effectiveness and limitations of current methods and provide crucial insights for future research. The dataset and codes are available at https://github.com/wang-jh18-SVM/Griffin.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIRAM: Masked Image Autoencoders Across Multiple Scales with Hybrid-Attention Mechanism for Breast Lesion Risk Prediction</title>
<link>https://arxiv.org/abs/2503.07157</link>
<guid>https://arxiv.org/abs/2503.07157</guid>
<content:encoded><![CDATA[
arXiv:2503.07157v3 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has garnered substantial interest within the machine learning and computer vision communities. Two prominent approaches in SSL include contrastive-based learning and self-distillation utilizing cropping augmentation. Lately, masked image modeling (MIM) has emerged as a more potent SSL technique, employing image inpainting as a pretext task. MIM creates a strong inductive bias toward meaningful spatial and semantic understanding. This has opened up new opportunities for SSL to contribute not only to classification tasks but also to more complex applications like object detection and image segmentation. Building upon this progress, our research paper introduces a scalable and practical SSL approach centered around more challenging pretext tasks that facilitate the acquisition of robust features. Specifically, we leverage multi-scale image reconstruction from randomly masked input images as the foundation for feature learning. Our hypothesis posits that reconstructing high-resolution images enables the model to attend to finer spatial details, particularly beneficial for discerning subtle intricacies within medical images. The proposed SSL features help improve classification performance on the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) dataset. In pathology classification, our method demonstrates a 3\% increase in average precision (AP) and a 1\% increase in the area under the receiver operating characteristic curve (AUC) when compared to state-of-the-art (SOTA) algorithms. Moreover, in mass margins classification, our approach achieves a 4\% increase in AP and a 2\% increase in AUC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEMPLE: Incentivizing Temporal Understanding of Video Large Language Models via Progressive Pre-SFT Alignment</title>
<link>https://arxiv.org/abs/2503.16929</link>
<guid>https://arxiv.org/abs/2503.16929</guid>
<content:encoded><![CDATA[
arXiv:2503.16929v4 Announce Type: replace 
Abstract: Video Large Language Models (Video LLMs) have achieved significant success by adopting the paradigm of large-scale pre-training followed by supervised fine-tuning (SFT). However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and over-reliance on the next-token prediction paradigm}, which collectively result in the absence temporal supervision. To address these limitations, we propose TEMPLE (TEMporal Preference LEarning), a systematic framework that enhances temporal reasoning capabilities through Direct Preference Optimization (DPO). To address temporal information scarcity in data, we introduce an automated pipeline for systematically constructing temporality-intensive preference pairs comprising three steps: selecting temporally rich videos, designing video-specific perturbation strategies, and evaluating model responses on clean and perturbed inputs. Complementing this data pipeline, we provide additional supervision signals via preference learning and propose a novel Progressive Pre-SFT Alignment strategy featuring two key innovations: a curriculum learning strategy which progressively increases perturbation difficulty to maximize data efficiency; and applying preference optimization before instruction tuning to incentivize fundamental temporal alignment. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. Our findings highlight TEMPLE as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title>
<link>https://arxiv.org/abs/2503.19912</link>
<guid>https://arxiv.org/abs/2503.19912</guid>
<content:encoded><![CDATA[
arXiv:2503.19912v2 Announce Type: replace 
Abstract: LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Math Blind: Failures in Diagram Understanding Undermine Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2503.20745</link>
<guid>https://arxiv.org/abs/2503.20745</guid>
<content:encoded><![CDATA[
arXiv:2503.20745v2 Announce Type: replace 
Abstract: Diagrams represent a form of visual language that encodes abstract concepts and relationships through structured symbols and their spatial arrangements. Unlike natural images, they are inherently symbolic, and entirely artificial. They thus pose unique challenges for Multimodal Large Language Models (MLLMs) distinct from natural image processing. Recent studies have shown that MLLMs often exhibit flawed reasoning and hallucinations when handling diagram inputs. We investigate here whether these limitations stem from shortcomings in the models' ability to interpret diagrams themselves. To this end, we develop a diagnostic test suite that isolates perception from reasoning. Our systematic evaluation reveals that MLLMs perform poorly on basic perceptual tasks, e.g., shape classification, object counting, relationship identification, and object grounding, with near-zero accuracy on fine-grained grounding. Further analysis shows that weak diagram perception leads to "blind faith in text", where models rely on textual shortcuts rather than visual understanding (that is, they are Math Blind). We hypothesize that enabling models to capture the inherent structural properties of diagrams, represented as graphs of primitives and their interrelationships, is essential for improving diagram understanding. Experiments with 7B and 32B MLLMs validate this assumption, with models trained on such representations achieving a +79% gain on the grounding task. Crucially, these gains transfer to reasoning, achieving 3-4% cross-suite improvements on three public benchmarks even without additional chain-of-thought reasoning data. Our findings demonstrate that low-level perception supports faithful high-level reasoning in mathematical MLLMs. We provide both methodological frameworks and empirical evidence to guide future research in this direction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TranSplat: Instant Cross-Scene Object Relighting in Gaussian Splatting via Spherical Harmonic Transfer</title>
<link>https://arxiv.org/abs/2503.22676</link>
<guid>https://arxiv.org/abs/2503.22676</guid>
<content:encoded><![CDATA[
arXiv:2503.22676v3 Announce Type: replace 
Abstract: We present TranSplat, a method for fast and accurate object relighting for the 3D Gaussian Splatting (GS) framework when transferring a 3D object from a source GS scene to a target GS scene. TranSplat is based on a theoretical radiance transfer identity for cross-scene relighting of objects with radially symmetric BRDFs that involves only taking simple products of spherical harmonic appearance coefficients of the object, source, and target environment maps without any explicit computation of scene quantities (e.g., the BRDFs themselves). TranSplat is the first method to demonstrate how this theoretical identity may be used to perform relighting within the GS framework, and furthermore, by automatically inferring unknown source and target environment maps directly from the source and target scene GS representations. We evaluated TranSplat on several synthetic and real-world scenes and objects, demonstrating comparable 3D object relighting performance to recent conventional inverse rendering-based GS methods with a fraction of their runtime. While TranSplat is theoretically best-suited for radially symmetric BRDFs, results demonstrate that TranSplat still offers perceptually realistic renderings on real scenes and opens a valuable, lightweight path forward to relighting with the GS framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries</title>
<link>https://arxiv.org/abs/2503.23606</link>
<guid>https://arxiv.org/abs/2503.23606</guid>
<content:encoded><![CDATA[
arXiv:2503.23606v2 Announce Type: replace 
Abstract: Extracting depth information from photon-limited, defocused images is challenging because depth from defocus (DfD) relies on accurate estimation of defocus blur, which is fundamentally sensitive to image noise. We present a novel approach to robustly measure object depths from photon-limited images along the defocused boundaries. It is based on a new image patch representation, Blurry-Edges, that explicitly stores and visualizes a rich set of low-level patch information, including boundaries, color, and smoothness. We develop a deep neural network architecture that predicts the Blurry-Edges representation from a pair of differently defocused images, from which depth can be calculated using a closed-form DfD relation we derive. The experimental results on synthetic and real data show that our method achieves the highest depth estimation accuracy on photon-limited images compared to a broad range of state-of-the-art DfD methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</title>
<link>https://arxiv.org/abs/2504.00438</link>
<guid>https://arxiv.org/abs/2504.00438</guid>
<content:encoded><![CDATA[
arXiv:2504.00438v2 Announce Type: replace 
Abstract: The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Forensic Cues for JPEG AI Images</title>
<link>https://arxiv.org/abs/2504.03191</link>
<guid>https://arxiv.org/abs/2504.03191</guid>
<content:encoded><![CDATA[
arXiv:2504.03191v2 Announce Type: replace 
Abstract: The JPEG standard was vastly successful. Currently, the first AI-based compression method ``JPEG AI'' will be standardized. JPEG AI brings remarkable benefits. JPEG AI images exhibit impressive image quality at bitrates that are an order of magnitude lower than images compressed with traditional JPEG. However, forensic analysis of JPEG AI has to be completely re-thought: forensic tools for traditional JPEG do not transfer to JPEG AI, and artifacts from JPEG AI are easily confused with artifacts from artificially generated images (``DeepFakes''). This creates a need for novel forensic approaches to detection and distinction of JPEG AI images. In this work, we make a first step towards a forensic JPEG AI toolset. We propose three cues for forensic algorithms for JPEG AI. These algorithms address three forensic questions: first, we show that the JPEG AI preprocessing introduces correlations in the color channels that do not occur in uncompressed images. Second, we show that repeated compression of JPEG AI images leads to diminishing distortion differences. This can be used to detect recompression, in a spirit similar to some classic JPEG forensics methods. Third, we show that the quantization of JPEG AI images in the latent space can be used to distinguish real images with JPEG AI compression from synthetically generated images. The proposed methods are interpretable for a forensic analyst, and we hope that they inspire further research in the forensics of AI-compressed images.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2504.09261</link>
<guid>https://arxiv.org/abs/2504.09261</guid>
<content:encoded><![CDATA[
arXiv:2504.09261v2 Announce Type: replace 
Abstract: Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\mathcal{O}(n^4)$ to $\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\times$ memory reduction and a $1.57\times$ speedup on Infinity-8B.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2504.09973</link>
<guid>https://arxiv.org/abs/2504.09973</guid>
<content:encoded><![CDATA[
arXiv:2504.09973v2 Announce Type: replace 
Abstract: All-in-One Image Restoration (AiOIR), which addresses diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but discard critical visual information needed for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a framework that aims to improve prompt-task alignment through two complementary components: a Sparse Prompt Module (SPM) that efficiently captures degradation-aware representations while reducing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL directly optimizes the interaction between prompts and the restoration model. Extensive experiments across five benchmarks show that CPL consistently boosts the performance of strong AiOIR baselines across diverse scenarios. Our approach achieves state-of-the-art average performance on these benchmarks, providing a general and robust solution for AiOIR. The code is available at https://github.com/Aitical/CPLIR
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.17432</link>
<guid>https://arxiv.org/abs/2504.17432</guid>
<content:encoded><![CDATA[
arXiv:2504.17432v4 Announce Type: replace 
Abstract: The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</title>
<link>https://arxiv.org/abs/2505.01267</link>
<guid>https://arxiv.org/abs/2505.01267</guid>
<content:encoded><![CDATA[
arXiv:2505.01267v4 Announce Type: replace 
Abstract: The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</title>
<link>https://arxiv.org/abs/2505.06905</link>
<guid>https://arxiv.org/abs/2505.06905</guid>
<content:encoded><![CDATA[
arXiv:2505.06905v3 Announce Type: replace 
Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) optical imagery remains challenging due to limited structural cues and the high cost and geographic constraints of conventional elevation data such as airborne LiDAR and multi-view stereo. Although recent MHE and monocular depth estimation (MDE) models show strong performance, their robustness under varied illumination and scene conditions is still limited. We introduce a fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements from ICESat-2 with deep learning predictions to enhance accuracy and stability. The workflow relies entirely on publicly available models and data and requires only a single georeferenced optical image to produce corrected height maps, enabling low-cost and globally scalable deployment. We also establish the first benchmark for this task, evaluating two random forest based approaches, four parameter efficient fine tuning methods, and full fine tuning. Experiments across six diverse regions at 0.5 m resolution (297 km2), covering the urban cores of Tokyo, Paris, and Sao Paulo as well as suburban and forested areas, show substantial gains. The best method reduces the MHE model's mean absolute error (MAE) by 30.9 percent and improves its F1HE score by 44.2 percent. For the MDE model, MAE improves by 24.1 percent and the F1HE score by 25.1 percent. These results validate the effectiveness of our correction pipeline and demonstrate how sparse global LiDAR can systematically strengthen both MHE and MDE models, enabling scalable and widely accessible 3D height mapping.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</title>
<link>https://arxiv.org/abs/2505.08013</link>
<guid>https://arxiv.org/abs/2505.08013</guid>
<content:encoded><![CDATA[
arXiv:2505.08013v5 Announce Type: replace 
Abstract: As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision</title>
<link>https://arxiv.org/abs/2505.18051</link>
<guid>https://arxiv.org/abs/2505.18051</guid>
<content:encoded><![CDATA[
arXiv:2505.18051v2 Announce Type: replace 
Abstract: Vision transformers are ever larger, more accurate, and more expensive to compute. The expense is even more extreme at high resolution as the number of tokens grows quadratically with the image size. We turn to adaptive computation to cope with this cost by learning to predict where to compute. Our LookWhere method divides the computation between a low-resolution selector and a high-resolution extractor without ever processing the full high-resolution input. We jointly pretrain the selector and extractor without task supervision by distillation from a self-supervised teacher, in effect, learning where and what to compute simultaneously. Unlike prior token reduction methods, which pay to save by pruning already-computed tokens, and prior token selection methods, which require complex and expensive per-task optimization, LookWhere economically and accurately selects and extracts transferrable representations of images. We show that LookWhere excels at sparse recognition on high-resolution inputs (Traffic Signs), maintaining accuracy while reducing FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks that are global (ImageNet classification) or local (ADE20K segmentation), improving accuracy while reducing time by 1.36x. See https://github.com/antofuller/lookwhere for the code and weights.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Initialization for Vision Transformers</title>
<link>https://arxiv.org/abs/2505.19985</link>
<guid>https://arxiv.org/abs/2505.19985</guid>
<content:encoded><![CDATA[
arXiv:2505.19985v2 Announce Type: replace 
Abstract: Convolutional Neural Networks (CNNs) inherently encode strong inductive biases, enabling effective generalization on small-scale datasets. In this paper, we propose integrating this inductive bias into ViTs, not through an architectural intervention but solely through initialization. The motivation here is to have a ViT that can enjoy strong CNN-like performance when data assets are small, but can still scale to ViT-like performance as the data expands. Our approach is motivated by our empirical results that random impulse filters can achieve commensurate performance to learned filters within a CNN. We improve upon current ViT initialization strategies, which typically rely on empirical heuristics such as using attention weights from pretrained models or focusing on the distribution of attention weights without enforcing structures. Empirical results demonstrate that our method significantly outperforms standard ViT initialization across numerous small and medium-scale benchmarks, including Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, and Pets, while maintaining comparative performance on large-scale datasets such as ImageNet-1K. Moreover, our initialization strategy can be easily integrated into various transformer-based architectures such as Swin Transformer and MLP-Mixer with consistent improvements in performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DRS: MLLMs Need 3D-Aware Representation Supervision for Scene Understanding</title>
<link>https://arxiv.org/abs/2506.01946</link>
<guid>https://arxiv.org/abs/2506.01946</guid>
<content:encoded><![CDATA[
arXiv:2506.01946v2 Announce Type: replace 
Abstract: Recent advances in scene understanding have leveraged multimodal large language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D pretraining. However, the lack of explicit 3D data during MLLM pretraining limits 3D representation capability. In this paper, we investigate the 3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a strong positive correlation between the quality of 3D-aware representation and downstream task performance. Motivated by this, we propose 3DRS, a framework that enhances MLLM 3D representation learning by introducing supervision from pretrained 3D foundation models. Our approach aligns MLLM visual features with rich 3D knowledge distilled from 3D models, effectively improving scene understanding. Extensive experiments across multiple benchmarks and MLLMs -- including visual grounding, captioning, and question answering -- demonstrate consistent performance gains. Project page: https://visual-ai.github.io/3drs
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normalize Filters! Classical Wisdom for Deep Vision</title>
<link>https://arxiv.org/abs/2506.04401</link>
<guid>https://arxiv.org/abs/2506.04401</guid>
<content:encoded><![CDATA[
arXiv:2506.04401v3 Announce Type: replace 
Abstract: Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title>
<link>https://arxiv.org/abs/2506.06389</link>
<guid>https://arxiv.org/abs/2506.06389</guid>
<content:encoded><![CDATA[
arXiv:2506.06389v2 Announce Type: replace 
Abstract: Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlayerOne: Egocentric World Simulator</title>
<link>https://arxiv.org/abs/2506.09995</link>
<guid>https://arxiv.org/abs/2506.09995</guid>
<content:encoded><![CDATA[
arXiv:2506.09995v2 Announce Type: replace 
Abstract: We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration</title>
<link>https://arxiv.org/abs/2506.10573</link>
<guid>https://arxiv.org/abs/2506.10573</guid>
<content:encoded><![CDATA[
arXiv:2506.10573v2 Announce Type: replace 
Abstract: Learning medical visual representations from image-report pairs through joint learning has garnered increasing research attention due to its potential to alleviate the data scarcity problem in the medical domain. The primary challenges stem from the lengthy reports that feature complex discourse relations and semantic pathologies. Previous works have predominantly focused on instance-wise or token-wise cross-modal alignment, often neglecting the importance of pathological-level consistency. This paper presents a novel framework PLACE that promotes the Pathological-Level Alignment and enriches the fine-grained details via Correlation Exploration without additional human annotations. Specifically, we propose a novel pathological-level cross-modal alignment (PCMA) approach to maximize the consistency of pathology observations from both images and reports. To facilitate this, a Visual Pathology Observation Extractor is introduced to extract visual pathological observation representations from localized tokens. The PCMA module operates independently of any external disease annotations, enhancing the generalizability and robustness of our methods. Furthermore, we design a proxy task that enforces the model to identify correlations among image patches, thereby enriching the fine-grained details crucial for various downstream tasks. Experimental results demonstrate that our proposed framework achieves new state-of-the-art performance on multiple downstream tasks, including classification, image-to-text retrieval, semantic segmentation, object detection and report generation. Code is available at https://github.com/Markin-Wang/PLACE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability</title>
<link>https://arxiv.org/abs/2506.13558</link>
<guid>https://arxiv.org/abs/2506.13558</guid>
<content:encoded><![CDATA[
arXiv:2506.13558v3 Announce Type: replace 
Abstract: Diffusion models are advancing autonomous driving by enabling realistic data synthesis, predictive end-to-end planning, and closed-loop simulation, with a primary focus on temporally consistent generation. However, large-scale 3D scene generation requiring spatial coherence remains underexplored. In this paper, we present X-Scene, a novel framework for large-scale driving scene generation that achieves geometric intricacy, appearance fidelity, and flexible controllability. Specifically, X-Scene supports multi-granular control, including low-level layout conditioning driven by user input or text for detailed scene composition, and high-level semantic guidance informed by user intent and LLM-enriched prompts for efficient customization. To enhance geometric and visual fidelity, we introduce a unified pipeline that sequentially generates 3D semantic occupancy and corresponding multi-view images and videos, ensuring alignment and temporal consistency across modalities. We further extend local regions into large-scale scenes via consistency-aware outpainting, which extrapolates occupancy and images from previously generated areas to maintain spatial and visual coherence. The resulting scenes are lifted into high-quality 3DGS representations, supporting diverse applications such as simulation and scene exploration. Extensive experiments demonstrate that X-Scene substantially advances controllability and fidelity in large-scale scene generation, empowering data generation and simulation for autonomous driving.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
arXiv:2506.19852v2 Announce Type: replace 
Abstract: Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $\mathcal{O}(n \log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $\mathcal{O}(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\times$ longer while reducing training costs by up to 4.4$\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\times$ compared to dense attention inference. Code is released at \href{https://github.com/mit-han-lab/radial-attention}{https://github.com/mit-han-lab/radial-attention}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Bilingual Multimodal Misinformation Detection and Localization</title>
<link>https://arxiv.org/abs/2506.22930</link>
<guid>https://arxiv.org/abs/2506.22930</guid>
<content:encoded><![CDATA[
arXiv:2506.22930v2 Announce Type: replace 
Abstract: The increasing realism of multimodal content has made misinformation more subtle and harder to detect, especially in news media where images are frequently paired with bilingual (e.g., Chinese-English) subtitles. Such content often includes localized image edits and cross-lingual inconsistencies that jointly distort meaning while remaining superficially plausible. We introduce BiMi, a bilingual multimodal framework that jointly performs region-level localization, cross-modal and cross-lingual consistency detection, and natural language explanation for misinformation analysis. To support generalization, BiMi integrates an online retrieval module that supplements model reasoning with up-to-date external context. We further release BiMiBench, a large-scale and comprehensive benchmark constructed by systematically editing real news images and subtitles, comprising 104,000 samples with realistic manipulations across visual and linguistic modalities. To enhance interpretability, we apply Group Relative Policy Optimization (GRPO) to improve explanation quality, marking the first use of GRPO in this domain. Extensive experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, advancing state-of-the-art performance in realistic, multilingual misinformation detection. Code, models, and datasets will be released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions</title>
<link>https://arxiv.org/abs/2507.14555</link>
<guid>https://arxiv.org/abs/2507.14555</guid>
<content:encoded><![CDATA[
arXiv:2507.14555v2 Announce Type: replace 
Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires reasoning about the spatial and semantic relationships between them. Current 3D scene-language models often struggle with this relational understanding, particularly when visual embeddings alone do not adequately convey the roles and interactions of objects. In this paper, we introduce Descrip3D, a novel and powerful framework that explicitly encodes the relationships between objects using natural language. Unlike previous methods that rely only on 2D and 3D embeddings, Descrip3D enhances each object with a textual description that captures both its intrinsic attributes and contextual relationships. These relational cues are incorporated into the model through a dual-level integration: embedding fusion and prompt-level injection. This allows for unified reasoning across various tasks such as grounding, captioning, and question answering, all without the need for task-specific heads or additional supervision. When evaluated on five benchmark datasets, including ScanRefer, Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms strong baseline models, demonstrating the effectiveness of language-guided relational representation for understanding complex indoor scenes. Our code and data are publicly available at https://github.com/jintangxue/Descrip3D.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images</title>
<link>https://arxiv.org/abs/2507.14670</link>
<guid>https://arxiv.org/abs/2507.14670</guid>
<content:encoded><![CDATA[
arXiv:2507.14670v3 Announce Type: replace 
Abstract: Accurately predicting gene expression from histopathology images offers a scalable and non-invasive approach to molecular profiling, with significant implications for precision medicine and computational pathology. However, existing methods often underutilize the cross-modal representation alignment between histopathology images and gene expression profiles across multiple representational levels, thereby limiting their prediction performance. To address this, we propose Gene-DML, a unified framework that structures latent space through Dual-pathway Multi-Level discrimination to enhance correspondence between morphological and transcriptional modalities. The multi-scale instance-level discrimination pathway aligns hierarchical histopathology representations extracted at local, neighbor, and global levels with gene expression profiles, capturing scale-aware morphological-transcriptional relationships. In parallel, the cross-level instance-group discrimination pathway enforces structural consistency between individual (image/gene) instances and modality-crossed (gene/image, respectively) groups, strengthening the alignment across modalities. By jointly modeling fine-grained and structural-level discrimination, Gene-DML is able to learn robust cross-modal representations, enhancing both predictive accuracy and generalization across diverse biological contexts. Extensive experiments on public spatial transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art performance in gene expression prediction. The code and processed datasets are available at https://github.com/YXSong000/Gene-DML.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[
arXiv:2507.18405v2 Announce Type: replace 
Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2507.18594</link>
<guid>https://arxiv.org/abs/2507.18594</guid>
<content:encoded><![CDATA[
arXiv:2507.18594v3 Announce Type: replace 
Abstract: Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions</title>
<link>https://arxiv.org/abs/2507.18657</link>
<guid>https://arxiv.org/abs/2507.18657</guid>
<content:encoded><![CDATA[
arXiv:2507.18657v3 Announce Type: replace 
Abstract: In recent years, advanced deep learning architectures have shown strong performance in medical imaging tasks. However, the traditional centralized learning paradigm poses serious privacy risks as all data is collected and trained on a single server. To mitigate this challenge, decentralized approaches such as federated learning and swarm learning have emerged, allowing model training on local nodes while sharing only model weights. While these methods enhance privacy, they struggle with heterogeneous and imbalanced data and suffer from inefficiencies due to frequent communication and the aggregation of weights. More critically, the dynamic and complex nature of clinical environments demands scalable AI systems capable of continuously learning from diverse modalities and multilabels. Yet, both centralized and decentralized models are prone to catastrophic forgetting during system expansion, often requiring full model retraining to incorporate new data. To address these limitations, we propose VGS-ATD, a novel distributed learning framework. To validate VGS-ATD, we evaluate it in experiments spanning 30 datasets and 80 independent labels across distributed nodes, VGS-ATD achieved an overall accuracy of 92.7%, outperforming centralized learning (84.9%) and swarm learning (72.99%), while federated learning failed under these conditions due to high requirements on computational resources. VGS-ATD also demonstrated strong scalability, with only a 1% drop in accuracy on existing nodes after expansion, compared to a 20% drop in centralized learning, highlighting its resilience to catastrophic forgetting. Additionally, it reduced computational costs by up to 50% relative to both centralized and swarm learning, confirming its superior efficiency and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures</title>
<link>https://arxiv.org/abs/2507.19961</link>
<guid>https://arxiv.org/abs/2507.19961</guid>
<content:encoded><![CDATA[
arXiv:2507.19961v2 Announce Type: replace 
Abstract: The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases. However, many disease patterns are derived from outdated datasets and traditional stepwise algorithms with limited accuracy. This study presents a method for direct cardiovascular disease (CVD) diagnosis from ECG images, eliminating the need for digitization. The proposed approach utilizes a two-step curriculum learning framework, beginning with the pre-training of a classification model on segmentation masks, followed by fine-tuning on grayscale, inverted ECG images. Robustness is further enhanced through an ensemble of three models with averaged outputs, achieving an AUC of 0.9534 and an F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming individual models. By effectively handling real-world artifacts and simplifying the diagnostic process, this method offers a reliable solution for automated CVD diagnosis, particularly in resource-limited settings where printed or scanned ECG images are commonly used. Such an automated procedure enables rapid and accurate diagnosis, which is critical for timely intervention in CVD cases that often demand urgent care.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2508.03209</link>
<guid>https://arxiv.org/abs/2508.03209</guid>
<content:encoded><![CDATA[
arXiv:2508.03209v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable ability to infer users' locations from public shared images, posing a substantial risk to geoprivacy. Although adversarial perturbations offer a potential defense, current methods are ill-suited for this scenario: they often perform poorly on high-resolution images and low perturbation budgets, and may introduce irrelevant semantic content. To address these limitations, we propose GeoShield, a novel adversarial framework designed for robust geoprivacy protection in real-world scenarios. GeoShield comprises three key modules: a feature disentanglement module that separates geographical and non-geographical information, an exposure element identification module that pinpoints geo-revealing regions within an image, and a scale-adaptive enhancement module that jointly optimizes perturbations at both global and local levels to ensure effectiveness across resolutions. Extensive experiments on challenging benchmarks show that GeoShield consistently surpasses prior methods in black-box settings, achieving strong privacy protection with minimal impact on visual or semantic quality. To our knowledge, this work is the first to explore adversarial perturbations for defending against geolocation inference by advanced VLMs, providing a practical and effective solution to escalating privacy concerns.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCoAR: Deep Concept Injection into Unified Autoregressive Models for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.07341</link>
<guid>https://arxiv.org/abs/2508.07341</guid>
<content:encoded><![CDATA[
arXiv:2508.07341v2 Announce Type: replace 
Abstract: The unified autoregressive (AR) model excels at multimodal understanding and generation. However, its full potential in the domain of customized image generation has yet to be fully realized. Existing customization approaches for unified AR models face a fundamental dilemma: adaptation-based methods suffer from overfitting and scalability bottlenecks, while concept-injection paradigms are constrained by a shallow injection strategy that leads to poor visual fidelity and impaired re-contextualization. To address this, we propose DCoAR, a novel deep concept injection framework that maintains a completely frozen pre-trained model. DCoAR deeply integrates new concepts through a Layer-wise Multimodal Context Learning (LMCL) strategy, which is stabilized by a multi-faceted regularization scheme: a Dual Prior Preservation (DPP) loss to mitigate semantic drift and a Context-Aware Self-Regularization (CASR) loss to enhance re-contextualization. The framework also enables training-free subject customization in user-provided styles. Experiments demonstrate that DCoAR significantly outperforms previous injection-based methods and achieves performance competitive with adaptation-based approaches while requiring substantially fewer trainable parameters. Code: https://github.com/KZF-kzf/CoAR
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVCtrl: Efficient Control Adapter for Visual Generation</title>
<link>https://arxiv.org/abs/2508.10963</link>
<guid>https://arxiv.org/abs/2508.10963</guid>
<content:encoded><![CDATA[
arXiv:2508.10963v2 Announce Type: replace 
Abstract: Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLAIR: Frequency- and Locality-Aware Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2508.13544</link>
<guid>https://arxiv.org/abs/2508.13544</guid>
<content:encoded><![CDATA[
arXiv:2508.13544v4 Announce Type: replace 
Abstract: Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity and spatial localization, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is Band-Localized Activation (BLA), a novel activation designed for joint frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). Through structured frequency control and spatially localized responses, BLA effectively mitigates spectral bias and enhances training stability. The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform to compute energy scores and explicitly guide frequency information to the network, enabling precise frequency selection and adaptive band control. Our method consistently outperforms existing INRs in 2D image representation, as well as 3D shape reconstruction and novel view synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance</title>
<link>https://arxiv.org/abs/2508.16644</link>
<guid>https://arxiv.org/abs/2508.16644</guid>
<content:encoded><![CDATA[
arXiv:2508.16644v2 Announce Type: replace 
Abstract: Diffusion models have shown remarkable progress in photorealistic image synthesis, yet they remain unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings. We present CountLoop, a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. The approach alternates between image generation and multimodal agent evaluation, where a language-guided planner and critic assess object counts, spatial arrangements, and attribute consistency. This feedback is then used to refine layouts and guide subsequent generations. To further improve separation between objects, especially in occluded scenes, we introduce instance-driven attention masking and compositional generation techniques. Experiments on COCO Count, T2I CompBench, and two new high-instance benchmarks show that CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning</title>
<link>https://arxiv.org/abs/2508.21451</link>
<guid>https://arxiv.org/abs/2508.21451</guid>
<content:encoded><![CDATA[
arXiv:2508.21451v3 Announce Type: replace 
Abstract: Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching</title>
<link>https://arxiv.org/abs/2509.05952</link>
<guid>https://arxiv.org/abs/2509.05952</guid>
<content:encoded><![CDATA[
arXiv:2509.05952v4 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2509.12069</link>
<guid>https://arxiv.org/abs/2509.12069</guid>
<content:encoded><![CDATA[
arXiv:2509.12069v3 Announce Type: replace 
Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing first place in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.84, HD95 of 38.17 with the held-out test data, with an average inference time of 40.58s. In Task 2, U-Mamba2 achieved the mean Dice of 0.87 and HD95 of 2.15 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds</title>
<link>https://arxiv.org/abs/2509.15675</link>
<guid>https://arxiv.org/abs/2509.15675</guid>
<content:encoded><![CDATA[
arXiv:2509.15675v2 Announce Type: replace 
Abstract: Point cloud data represents a crucial category of information for mathematical modeling, and surface reconstruction from such data is an important task across various disciplines. However, during the scanning process, the collected point cloud data may fail to cover the entire surface due to factors such as high light-absorption rate and occlusions, resulting in incomplete datasets. Inferring surface structures in data-missing regions and successfully reconstructing the surface poses a challenge. In this paper, we present a Principal Component Analysis (PCA) based model for surface reconstruction from incomplete point cloud data. Initially, we employ PCA to estimate the normal information of the underlying surface from the available point cloud data. This estimated normal information serves as a regularizer in our model, guiding the reconstruction of the surface, particularly in areas with missing data. Additionally, we introduce an operator-splitting method to effectively solve the proposed model. Through systematic experimentation, we demonstrate that our model successfully infers surface structures in data-missing regions and well reconstructs the underlying surfaces, outperforming existing methodologies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View</title>
<link>https://arxiv.org/abs/2509.23008</link>
<guid>https://arxiv.org/abs/2509.23008</guid>
<content:encoded><![CDATA[
arXiv:2509.23008v2 Announce Type: replace 
Abstract: Diffusion models have achieved impressive results in world modeling tasks, including novel view generation from sparse inputs. However, most existing diffusion-based NVS methods generate target views jointly via an iterative denoising process, which makes it less straightforward to impose a strictly causal structure along a camera trajectory. In contrast, autoregressive (AR) models operate in a causal fashion, generating each token based on all previously generated tokens. In this work, we introduce ARSS, a novel framework that leverages a GPT-style decoder-only AR model to generate novel views from a single image, conditioned on a predefined camera trajectory. We employ an off-the-shelf video tokenizer to map continuous image sequences into discrete tokens and propose a camera encoder that converts camera trajectories into 3D positional guidance. Then to enhance generation quality while preserving the autoregressive structure, we propose an autoregressive transformer module that randomly permutes the spatial order of tokens while maintaining their temporal order. Qualitative and quantitative experiments on public datasets demonstrate that our method achieves overall performance comparable to state-of-the-art view synthesis approaches based on diffusion models. Project page: https://wbteng9526.github.io/arss/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D</title>
<link>https://arxiv.org/abs/2509.24528</link>
<guid>https://arxiv.org/abs/2509.24528</guid>
<content:encoded><![CDATA[
arXiv:2509.24528v3 Announce Type: replace 
Abstract: 3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaChest: Generalized few-shot learning of pathologies from chest X-rays</title>
<link>https://arxiv.org/abs/2509.25590</link>
<guid>https://arxiv.org/abs/2509.25590</guid>
<content:encoded><![CDATA[
arXiv:2509.25590v2 Announce Type: replace 
Abstract: The limited availability of annotated data presents a major challenge for applying deep learning methods to medical image analysis. Few-shot learning methods aim to recognize new classes from only a small number of labeled examples. These methods are typically studied under the standard few-shot learning setting, where all classes in a task are new. However, medical applications such as pathology classification from chest X-rays often require learning new classes while simultaneously leveraging knowledge of previously known ones, a scenario more closely aligned with generalized few-shot classification. Despite its practical relevance, few-shot learning has been scarcely studied in this context. In this work, we present MetaChest, a large-scale dataset of 479,215 chest X-rays collected from four public databases. MetaChest includes a meta-set partition specifically designed for standard few-shot classification, as well as an algorithm for generating multi-label episodes. We conduct extensive experiments evaluating both a standard transfer learning approach and an extension of ProtoNet across a wide range of few-shot multi-label classification tasks. Our results demonstrate that increasing the number of classes per episode and the number of training examples per class improves classification performance. Notably, the transfer learning approach consistently outperforms the ProtoNet extension, despite not being tailored for few-shot learning. We also show that higher-resolution images improve accuracy at the cost of additional computation, while efficient model architectures achieve comparable performance to larger models with significantly reduced resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title>
<link>https://arxiv.org/abs/2509.25774</link>
<guid>https://arxiv.org/abs/2509.25774</guid>
<content:encoded><![CDATA[
arXiv:2509.25774v2 Announce Type: replace 
Abstract: While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos</title>
<link>https://arxiv.org/abs/2509.26360</link>
<guid>https://arxiv.org/abs/2509.26360</guid>
<content:encoded><![CDATA[
arXiv:2509.26360v3 Announce Type: replace 
Abstract: Identifying key temporal intervals within long videos, known as temporal grounding (TG), is important to video understanding and reasoning tasks. In this paper, we introduce a new form of the temporal grounding problem, \textbf{Task-oriented Temporal Grounding} (\textbf{ToTG}), which is driven by the requirements of downstream tasks rather than explicit time-interval descriptions. For example, a ToTG input may be "explain why the man in the video is sent to the hospital," whereas traditional TG would take an explicit temporal description such as "the moments when the man is tripped by a stone and falls to the ground." This new ToTG formulation presents significant challenges for existing TG methods, as it requires jointly performing deep task comprehension and fine-grained temporal localization within long videos. To address these challenges, we conduct a systematic set of studies. First, we construct \textbf{a new benchmark ToTG-Bench}, which comprehensively evaluates ToTG performance across diverse settings. Second, we introduce \textbf{a new temporal-ground method TimeScope}, which performs coarse-to-fine localization through a progressive reasoning process. Leveraging extensive supervised fine-tuning with carefully curated chain-of-thought (CoT) data from a variety of scenarios, TimeScope generalizes effectively across tasks and domains. Our evaluation demonstrates \textbf{TimeScope's empirical advantages} over existing baselines from three perspectives: (1) substantial improvements in grounding precision, (2) significant benefits to downstream tasks, and (3) strong generalizability across different scenarios. All models, datasets, and source code will be fully open-sourced to support future research in this area.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extreme Amodal Face Detection</title>
<link>https://arxiv.org/abs/2510.06791</link>
<guid>https://arxiv.org/abs/2510.06791</guid>
<content:encoded><![CDATA[
arXiv:2510.06791v2 Announce Type: replace 
Abstract: Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches. Code, data, and models are available at https://charliesong1999.github.io/exaft_web/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSplat: Learning Recurrent Gaussian Splats</title>
<link>https://arxiv.org/abs/2510.08575</link>
<guid>https://arxiv.org/abs/2510.08575</guid>
<content:encoded><![CDATA[
arXiv:2510.08575v2 Announce Type: replace 
Abstract: While feed-forward Gaussian splatting models offer computational efficiency and can generalize to sparse input settings, their performance is fundamentally constrained by relying on a single forward pass for inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization across datasets, view counts and image resolutions. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16, 32), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV, RealEstate10K and ACID) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</title>
<link>https://arxiv.org/abs/2510.13515</link>
<guid>https://arxiv.org/abs/2510.13515</guid>
<content:encoded><![CDATA[
arXiv:2510.13515v3 Announce Type: replace 
Abstract: Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</title>
<link>https://arxiv.org/abs/2510.19060</link>
<guid>https://arxiv.org/abs/2510.19060</guid>
<content:encoded><![CDATA[
arXiv:2510.19060v2 Announce Type: replace 
Abstract: While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FootFormer: Estimating Stability from Visual Input</title>
<link>https://arxiv.org/abs/2510.19170</link>
<guid>https://arxiv.org/abs/2510.19170</guid>
<content:encoded><![CDATA[
arXiv:2510.19170v2 Announce Type: replace 
Abstract: We propose FootFormer, a cross-modality approach for jointly predicting human motion dynamics directly from visual input. On multiple datasets, FootFormer achieves statistically significantly better or equivalent estimates of foot pressure distributions, foot contact maps, and center of mass (CoM), as compared with existing methods that generate one or two of those measures. Furthermore, FootFormer achieves SOTA performance in estimating stability-predictive components (CoP, CoM, BoS) used in classic kinesiology metrics. Code and data are available at https://github.com/keatonkraiger/Vision-to-Stability.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRELLISWorld: Training-Free World Generation from Object Generators</title>
<link>https://arxiv.org/abs/2510.23880</link>
<guid>https://arxiv.org/abs/2510.23880</guid>
<content:encoded><![CDATA[
arXiv:2510.23880v2 Announce Type: replace 
Abstract: Text-driven 3D scene generation holds promise for a wide range of applications, from virtual prototyping to AR/VR and simulation. However, existing methods are often constrained to single-object generation, require domain-specific training, or lack support for full 360-degree viewability. In this work, we present a training-free approach to 3D scene synthesis by repurposing general-purpose text-to-3D object diffusion models as modular tile generators. We reformulate scene generation as a multi-tile denoising problem, where overlapping 3D regions are independently generated and seamlessly blended via weighted averaging. This enables scalable synthesis of large, coherent scenes while preserving local semantic control. Our method eliminates the need for scene-level datasets or retraining, relies on minimal heuristics, and inherits the generalization capabilities of object-level priors. We demonstrate that our approach supports diverse scene layouts, efficient generation, and flexible editing, establishing a simple yet powerful foundation for general-purpose, language-driven 3D scene construction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
<link>https://arxiv.org/abs/2511.01266</link>
<guid>https://arxiv.org/abs/2511.01266</guid>
<content:encoded><![CDATA[
arXiv:2511.01266v2 Announce Type: replace 
Abstract: Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastGS: Training 3D Gaussian Splatting in 100 Seconds</title>
<link>https://arxiv.org/abs/2511.04283</link>
<guid>https://arxiv.org/abs/2511.04283</guid>
<content:encoded><![CDATA[
arXiv:2511.04283v3 Announce Type: replace 
Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition</title>
<link>https://arxiv.org/abs/2511.07040</link>
<guid>https://arxiv.org/abs/2511.07040</guid>
<content:encoded><![CDATA[
arXiv:2511.07040v2 Announce Type: replace 
Abstract: Deep neural networks have recently achieved notable progress in 3D point cloud recognition, yet their vulnerability to adversarial perturbations poses critical security challenges in practical deployments. Conventional defense mechanisms struggle to address the evolving landscape of multifaceted attack patterns. Through systematic analysis of existing defenses, we identify that their unsatisfactory performance primarily originates from an entangled feature space, where adversarial attacks can be performed easily. To this end, we present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC) mechanism to orchestrate discriminative feature learning. In particular, NC depicts where last-layer features and classifier weights jointly evolve into a simplex equiangular tight frame (ETF) arrangement, establishing maximally separable class prototypes. However, leveraging this advantage in 3D recognition confronts two substantial challenges: (1) prevalent class imbalance in point cloud datasets, and (2) complex geometric similarities between object categories. To tackle these obstacles, our solution combines an ETF-aligned classification module with an adaptive training framework consisting of representation-balanced learning (RBL) and dynamic feature direction loss (FDL). 3D-ANC seamlessly empowers existing models to develop disentangled feature spaces despite the complexity in 3D data distribution. Comprehensive evaluations state that 3D-ANC significantly improves the robustness of models with various structures on two datasets. For instance, DGCNN's classification accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain that surpasses leading baselines by 34.0%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title>
<link>https://arxiv.org/abs/2511.11030</link>
<guid>https://arxiv.org/abs/2511.11030</guid>
<content:encoded><![CDATA[
arXiv:2511.11030v4 Announce Type: replace 
Abstract: Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patient's health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.70 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal was unlikely contributed by demographic features by our machine learning study combining age, race, and sex labels to predict health insurance types; it also remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STONE: Pioneering the One-to-N Universal Backdoor Threat in 3D Point Cloud</title>
<link>https://arxiv.org/abs/2511.11210</link>
<guid>https://arxiv.org/abs/2511.11210</guid>
<content:encoded><![CDATA[
arXiv:2511.11210v3 Announce Type: replace 
Abstract: Backdoor attacks pose a critical threat to deep learning, especially in safety-sensitive 3D domains such as autonomous driving and robotics. While potent, existing attacks on 3D point clouds are predominantly limited to one-to-one paradigms. The more flexible and universal one-to-N multi-target backdoor threat remains largely unexplored, lacking both theoretical and practical foundations. To bridge this gap, we propose STONE (Spherical Trigger One-to-N universal backdoor Enabling), the first method to instantiate this threat via a configurable spherical trigger design. Its parameterized spatial properties establish a dynamic key space, enabling a single trigger to map to multiple target labels. Theoretically, we ground STONE in a Neural Tangent Kernel (NTK) analysis, providing the first formal basis for one-to-N mappings in 3D models. Empirically, extensive evaluations demonstrate high attack success rates (up to 100\%) without compromising clean-data accuracy. This work establishes a foundational benchmark for multi-target backdoor threats under dirty-label and black-box settings in 3D vision -- a crucial step toward securing future intelligent systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data</title>
<link>https://arxiv.org/abs/2511.13036</link>
<guid>https://arxiv.org/abs/2511.13036</guid>
<content:encoded><![CDATA[
arXiv:2511.13036v2 Announce Type: replace 
Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language</title>
<link>https://arxiv.org/abs/2511.13127</link>
<guid>https://arxiv.org/abs/2511.13127</guid>
<content:encoded><![CDATA[
arXiv:2511.13127v2 Announce Type: replace 
Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models. Our demos and codes can be found at https://github.com/NY1024/VEIL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Matching Distillation Meets Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13649</link>
<guid>https://arxiv.org/abs/2511.13649</guid>
<content:encoded><![CDATA[
arXiv:2511.13649v3 Announce Type: replace 
Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v2 Announce Type: replace 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment</title>
<link>https://arxiv.org/abs/2511.17397</link>
<guid>https://arxiv.org/abs/2511.17397</guid>
<content:encoded><![CDATA[
arXiv:2511.17397v2 Announce Type: replace 
Abstract: Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models</title>
<link>https://arxiv.org/abs/2511.18271</link>
<guid>https://arxiv.org/abs/2511.18271</guid>
<content:encoded><![CDATA[
arXiv:2511.18271v2 Announce Type: replace 
Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems. The code is available at https://github.com/D4-Lab/PicWorld}{https://github.com/D4-Lab/PicWorld.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</title>
<link>https://arxiv.org/abs/2511.20415</link>
<guid>https://arxiv.org/abs/2511.20415</guid>
<content:encoded><![CDATA[
arXiv:2511.20415v2 Announce Type: replace 
Abstract: Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our project page: https://longhz140516.github.io/MajutsuCity/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[
arXiv:2511.20821v2 Announce Type: replace 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22019</link>
<guid>https://arxiv.org/abs/2511.22019</guid>
<content:encoded><![CDATA[
arXiv:2511.22019v2 Announce Type: replace 
Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
<link>https://arxiv.org/abs/2511.22699</link>
<guid>https://arxiv.org/abs/2511.22699</guid>
<content:encoded><![CDATA[
arXiv:2511.22699v3 Announce Type: replace 
Abstract: The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.23292</link>
<guid>https://arxiv.org/abs/2511.23292</guid>
<content:encoded><![CDATA[
arXiv:2511.23292v2 Announce Type: replace 
Abstract: Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</title>
<link>https://arxiv.org/abs/2511.23478</link>
<guid>https://arxiv.org/abs/2511.23478</guid>
<content:encoded><![CDATA[
arXiv:2511.23478v2 Announce Type: replace 
Abstract: Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Code: https://github.com/mbzuai-oryx/Video-R2
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Perspective for Loss-Oriented Imbalanced Learning via Localization</title>
<link>https://arxiv.org/abs/2310.04752</link>
<guid>https://arxiv.org/abs/2310.04752</guid>
<content:encoded><![CDATA[
arXiv:2310.04752v2 Announce Type: replace-cross 
Abstract: Due to the inherent imbalance in real-world datasets, na\"ive Empirical Risk Minimization (ERM) tends to bias the learning process towards the majority classes, hindering generalization to minority classes. To rebalance the learning process, one straightforward yet effective approach is to modify the loss function via class-dependent terms, such as re-weighting and logit-adjustment. However, existing analysis of these loss-oriented methods remains coarse-grained and fragmented, failing to explain some empirical results. After reviewing prior work, we find that the properties used through their analysis are typically global, i.e., defined over the whole dataset. Hence, these properties fail to effectively capture how class-dependent terms influence the learning process. To bridge this gap, we turn to explore the localized versions of such properties i.e., defined within each class. Specifically, we employ localized calibration to provide consistency validation across a broader range of losses and localized Lipschitz continuity to provide a fine-grained generalization bound. In this way, we reach a unified perspective for improving and adjusting loss-oriented methods. Finally, a principled learning algorithm is developed based on these insights. Empirical results on both traditional ResNets and foundation models validate our theoretical analyses and demonstrate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Bioplausible Neuron for SNN for Event Vision</title>
<link>https://arxiv.org/abs/2311.11853</link>
<guid>https://arxiv.org/abs/2311.11853</guid>
<content:encoded><![CDATA[
arXiv:2311.11853v3 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) offer a biologically inspired approach to computer vision that can lead to more efficient processing of visual data with reduced energy consumption. However, maintaining homeostasis within these networks is challenging, as it requires continuous adjustment of neural responses to preserve equilibrium and optimal processing efficiency amidst diverse and often unpredictable input signals. In response to these challenges, we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing mechanism to auto-adjust the variations in the input signal. Comprehensive evaluation across various datasets demonstrates ABN's enhanced performance in image classification and segmentation, maintenance of neural equilibrium, and energy efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic Images: Leveraging Contextual Attention and Residual Learning</title>
<link>https://arxiv.org/abs/2401.13147</link>
<guid>https://arxiv.org/abs/2401.13147</guid>
<content:encoded><![CDATA[
arXiv:2401.13147v3 Announce Type: replace-cross 
Abstract: This study presents a deep convolutional autoencoder network for filtering reverberation clutter from transthoracic echocardiographic (TTE) image sequences. Given the spatiotemporal nature of this type of clutter, the filtering network employs 3D convolutional layers to suppress it throughout the cardiac cycle. The design of the network incorporates two key features that contribute to the effectiveness of the filter: 1) an attention mechanism for focusing on cluttered regions and leveraging contextual information, and 2) residual learning for preserving fine image structures. To train the network, a diverse set of artifact patterns was simulated and superimposed onto ultra-realistic synthetic TTE sequences from six ultrasound vendors, generating input for the filtering network. The artifact-free sequences served as ground-truth. Performance of the filtering network was evaluated using unseen synthetic and in vivo artifactual sequences. Results from the in vivo dataset confirmed the network's strong generalization capabilities, despite being trained solely on synthetic data and simulated artifacts. The suitability of the filtered sequences for downstream processing was assessed by computing segmental strain curves. A significant reduction in the discrepancy between strain profiles computed from cluttered and clutter-free segments was observed after filtering the cluttered sequences with the proposed network. The trained network processes a TTE sequence in a fraction of a second, enabling real-time clutter filtering and potentially improving the precision of clinically relevant indices derived from TTE sequences. The source code of the proposed method and example video files of the filtering results are available at: https://github.com/MahdiTabassian/Deep-ClutterFiltering/tree/main.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tyche: Stochastic In-Context Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2401.13650</link>
<guid>https://arxiv.org/abs/2401.13650</guid>
<content:encoded><![CDATA[
arXiv:2401.13650v2 Announce Type: replace-cross 
Abstract: Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-supervised Learning-based Reconstruction of High-resolution 4D Light Fields</title>
<link>https://arxiv.org/abs/2402.19020</link>
<guid>https://arxiv.org/abs/2402.19020</guid>
<content:encoded><![CDATA[
arXiv:2402.19020v2 Announce Type: replace-cross 
Abstract: Hand-held light field (LF) cameras often exhibit low spatial resolution due to the inherent trade-off between spatial and angular dimensions. Existing supervised learning-based LF spatial super-resolution (SR) methods, which rely on pre-defined image degradation models, struggle to overcome the domain gap between the training phase -- where LFs with natural resolution are used as ground truth -- and the inference phase, which aims to reconstruct higher-resolution LFs, especially when applied to real-world data.To address this challenge, this paper introduces a novel self-supervised learning-based method for LF spatial SR, which can produce higher spatial resolution LF images than originally captured ones without pre-defined image degradation models. The self-supervised method incorporates a hybrid LF imaging prototype, a real-world hybrid LF dataset, and a self-supervised LF spatial SR framework. The prototype makes reference image pairs between low-resolution central-view sub-aperture images and high-resolution (HR) images. The self-supervised framework consists of a well-designed LF spatial SR network with hybrid input, a central-view synthesis network with an HR-aware loss that enables side-view sub-aperture images to learn high-frequency information from the only HR central view reference image, and a backward degradation network with an epipolar-plane image gradient loss to preserve LF parallax structures. Extensive experiments on both simulated and real-world datasets demonstrate the significant superiority of our approach over state-of-the-art ones in reconstructing higher spatial resolution LF images without pre-defined degradation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation</title>
<link>https://arxiv.org/abs/2403.17770</link>
<guid>https://arxiv.org/abs/2403.17770</guid>
<content:encoded><![CDATA[
arXiv:2403.17770v2 Announce Type: replace-cross 
Abstract: Despite the significant success achieved by deep learning methods in medical image segmentation, researchers still struggle in the computer-aided diagnosis of abdominal lymph nodes due to the complex abdominal environment, small and indistinguishable lesions, and limited annotated data. To address these problems, we present a pipeline that integrates the conditional diffusion model for lymph node generation and the nnU-Net model for lymph node segmentation to improve the segmentation performance of abdominal lymph nodes through synthesizing a diversity of realistic abdominal lymph node data. We propose LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical structure masks as model conditions. These conditions work in two conditioning mechanisms: global structure conditioning and local detail conditioning, to distinguish between lymph nodes and their surroundings and better capture lymph node characteristics. The obtained paired abdominal lymph node images and masks are used for the downstream segmentation task. Experimental results on the abdominal lymph node datasets demonstrate that LN-DDPM outperforms other generative methods in the abdominal lymph node image synthesis and better assists the downstream abdominal lymph node segmentation task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration</title>
<link>https://arxiv.org/abs/2502.20333</link>
<guid>https://arxiv.org/abs/2502.20333</guid>
<content:encoded><![CDATA[
arXiv:2502.20333v2 Announce Type: replace-cross 
Abstract: Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echo-E$^3$Net: Efficient Endocardial Spatio-Temporal Network for Ejection Fraction Estimation</title>
<link>https://arxiv.org/abs/2503.17543</link>
<guid>https://arxiv.org/abs/2503.17543</guid>
<content:encoded><![CDATA[
arXiv:2503.17543v2 Announce Type: replace-cross 
Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and is routinely used to diagnose heart failure and guide treatment decisions. Although deep learning has advanced automated LVEF estimation, many existing approaches are computationally demanding and underutilize the joint structure of spatial and temporal information in echocardiography videos, limiting their suitability for real-time clinical deployment. We propose Echo-E$^3$Net, an efficient endocardial spatio-temporal network specifically designed for LVEF estimation from echocardiography videos. Echo-E$^3$Net comprises two complementary modules: (1) a dual-phase Endocardial Border Detector (E$^2$CBD), which uses phase-specific cross-attention to predict ED/ES endocardial landmarks (EBs) and learn phase-aware landmark embeddings (LEs), and (2) an Endocardial Feature Aggregator (E$^2$FA), which fuses these embeddings with global statistical descriptors (mean, maximum, variance) of deep feature maps to refine EF regression. A multi-component loss function, inspired by Simpson's biplane method, jointly supervises EF, volumes, and landmark geometry, thereby aligning optimization with the clinical definition of LVEF and promoting robust spatio-temporal representation learning. Evaluated on the EchoNet-Dynamic dataset, Echo-E$^3$Net achieves an RMSE of 5.20 and an $R^2$ score of 0.82, while using only 1.54M parameters and 8.05 GFLOPs. The model operates without external pre-training, heavy data augmentation, or test-time ensembling, making it highly suitable for real-time point-of-care ultrasound (POCUS) applications. Code is available at https://github.com/UltrAi-lab/Echo-E3Net.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAVERIX: Multimodal Audio-Visual Evaluation and Recognition IndeX</title>
<link>https://arxiv.org/abs/2503.21699</link>
<guid>https://arxiv.org/abs/2503.21699</guid>
<content:encoded><![CDATA[
arXiv:2503.21699v2 Announce Type: replace-cross 
Abstract: We introduce MAVERIX (Multimodal audiovisual Evaluation and Recognition IndeX), a unified benchmark to probe the video understanding in multimodal LLMs, encompassing video, audio, text inputs with human performance baselines. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework to thoroughly assess their cross-modality comprehension performance. MAVERIX curates 2,556 questions from 700 videos, in the form of both multiple-choice and open-ended formats, explicitly designed to evaluate multimodal models through questions that necessitate tight integration of video and audio information, spanning a broad spectrum of agentic scenarios. MAVERIX uniquely provides models with audiovisual questions, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration in such granularity. Experiments with state-of-the-art models, including Qwen 2.5 Omni and Gemini 2.5 Flash-Lite, show performance around 64% accuracy, while human experts reach near-ceiling performance of 92.8%, exposing a substantial gap to human-level comprehension. With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos</title>
<link>https://arxiv.org/abs/2504.06084</link>
<guid>https://arxiv.org/abs/2504.06084</guid>
<content:encoded><![CDATA[
arXiv:2504.06084v2 Announce Type: replace-cross 
Abstract: Large-scale egocentric video datasets capture diverse human activities across a wide range of scenarios, offering rich and detailed insights into how humans interact with objects, especially those that require fine-grained dexterous control. Such complex, dexterous skills with precise controls are crucial for many robotic manipulation tasks, yet are often insufficiently addressed by traditional data-driven approaches to robotic manipulation. To address this gap, we leverage manipulation priors learned from large-scale egocentric video datasets to improve policy learning for dexterous robotic manipulation tasks. We present MAPLE, a novel method for dexterous robotic manipulation that learns features to predict object contact points and detailed hand poses at the moment of contact from egocentric images. We then use the learned features to train policies for downstream manipulation tasks. Experimental results demonstrate the effectiveness of MAPLE across 4 existing simulation benchmarks, as well as a newly designed set of 4 challenging simulation tasks requiring fine-grained object control and complex dexterous skills. The benefits of MAPLE are further highlighted in real-world experiments using a 17 DoF dexterous robotic hand, whereas the simultaneous evaluation across both simulation and real-world experiments has remained underexplored in prior work. We additionally showcase the efficacy of our model on an egocentric contact point prediction task, validating its usefulness beyond dexterous manipulation policy learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust brain age estimation from structural MRI with contrastive learning</title>
<link>https://arxiv.org/abs/2507.01794</link>
<guid>https://arxiv.org/abs/2507.01794</guid>
<content:encoded><![CDATA[
arXiv:2507.01794v2 Announce Type: replace-cross 
Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for characterizing normative and pathological aging. In this work, we explore contrastive learning as a scalable and robust alternative to L1-supervised approaches for brain age estimation. We introduce a novel contrastive loss function, $\mathcal{L}^{exp}$, and evaluate it across multiple public neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four key findings. First, scaling pre-training on diverse, multi-site data consistently improves generalization performance, cutting external mean absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to site-related confounds, maintaining low scanner-predictability as training size increases. Third, contrastive models reliably capture accelerated aging in patients with cognitive impairment and Alzheimer's disease, as shown through brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike L1-supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation between brain age accuracy and downstream diagnostic performance, supporting its potential as a foundation model for neuroimaging. These results position contrastive learning as a promising direction for building generalizable and clinically meaningful brain representations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title>
<link>https://arxiv.org/abs/2507.16302</link>
<guid>https://arxiv.org/abs/2507.16302</guid>
<content:encoded><![CDATA[
arXiv:2507.16302v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are found to be fragile to downstream fine-tuning, as we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety, while effectively preserving benign generation capability. Our code and pretrained models are publicly available at https://github.com/AntigoneRandy/ResAlign.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling</title>
<link>https://arxiv.org/abs/2507.16329</link>
<guid>https://arxiv.org/abs/2507.16329</guid>
<content:encoded><![CDATA[
arXiv:2507.16329v2 Announce Type: replace-cross 
Abstract: Despite the integration of safety alignment and external filters, text-to-image (T2I) generative systems are still susceptible to producing harmful content, such as sexual or violent imagery. This raises serious concerns about unintended exposure and potential misuse. Red teaming, which aims to proactively identify diverse prompts that can elicit unsafe outputs from the T2I system, is increasingly recognized as an essential method for assessing and improving safety before real-world deployment. However, existing automated red teaming approaches often treat prompt discovery as an isolated, prompt-level optimization task, which limits their scalability, diversity, and overall effectiveness. To bridge this gap, in this paper, we propose DREAM, a scalable red teaming framework to automatically uncover diverse problematic prompts from a given T2I system. Unlike prior work that optimizes prompts individually, DREAM directly models the probabilistic distribution of the target system's problematic prompts, which enables explicit optimization over both effectiveness and diversity, and allows efficient large-scale sampling after training. To achieve this without direct access to representative training samples, we draw inspiration from energy-based models and reformulate the objective into a simple and tractable form. We further introduce GC-SPSA, an efficient optimization algorithm that provides stable gradient estimates through the long and potentially non-differentiable T2I pipeline. During inference, we also propose a diversity-aware sampling strategy to enhance prompt variety. The effectiveness of DREAM is validated through extensive experiments, demonstrating state-of-the-art performance across a wide range of T2I models and safety filters in terms of both prompt success rate and diversity. Our code is available at https://github.com/AntigoneRandy/DREAM
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists</title>
<link>https://arxiv.org/abs/2508.13157</link>
<guid>https://arxiv.org/abs/2508.13157</guid>
<content:encoded><![CDATA[
arXiv:2508.13157v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Converting circuit diagrams to netlists help LLMs to enrich the knowledge of analog IC. Nevertheless, previously proposed conversion frameworks face challenges in further application because of limited support of image styles and circuit elements. Up to now, it still remains a challenging task to effectively convert complex circuit diagrams into netlists. To this end, this paper constructs and opensources a new dataset with rich styles of circuit diagrams as well as balanced distribution of simple and complex analog ICs. And a hybrid framework, named Image2Net, is proposed for practical conversion from circuit diagrams to netlists. The netlist edit distance (NED) is also introduced to precisely assess the difference between the converted netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77% successful rate, which is 34.62%-45.19% higher than previous works. Specifically, the proposed work shows 0.116 averaged NED, which is 62.1%-69.6% lower than state-of-the-arts. Our datasets and benchmark are available at https://github.com/LAD021/ci2n_datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
<link>https://arxiv.org/abs/2509.01839</link>
<guid>https://arxiv.org/abs/2509.01839</guid>
<content:encoded><![CDATA[
arXiv:2509.01839v5 Announce Type: replace-cross 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saliency Guided Longitudinal Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2509.25374</link>
<guid>https://arxiv.org/abs/2509.25374</guid>
<content:encoded><![CDATA[
arXiv:2509.25374v2 Announce Type: replace-cross 
Abstract: Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder-decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language-vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving</title>
<link>https://arxiv.org/abs/2510.06784</link>
<guid>https://arxiv.org/abs/2510.06784</guid>
<content:encoded><![CDATA[
arXiv:2510.06784v2 Announce Type: replace-cross 
Abstract: In this report, we compare the performance of our UltraGroth-based zero-knowledge machine learning framework Bionetta to other tools of similar purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a significant boost in the proving time for custom-crafted neural networks: they can be proven even on mobile devices, enabling numerous client-side proving applications. While our scheme increases the cost of one-time preprocessing steps, such as circuit compilation and generating trusted setup, our approach is, to the best of our knowledge, the only one that is deployable on the native EVM smart contracts without overwhelming proof size and verification overheads.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis</title>
<link>https://arxiv.org/abs/2510.09365</link>
<guid>https://arxiv.org/abs/2510.09365</guid>
<content:encoded><![CDATA[
arXiv:2510.09365v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) inpainting supports numerous clinical and research applications. We introduce the first generative model that conditions on voxel-level, continuous tumor concentrations to synthesize high-fidelity brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this architecture to the complementary task of healthy tissue restoration by setting the tumor concentrations to zero. Our latent diffusion model conditioned on both tissue segmentations and the tumor concentrations generates 3D spatially coherent and anatomically consistent images for both tumor synthesis and healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5, and for tumor inpainting, we achieve 17.4. Our code is available at: https://github.com/valentin-biller/ldm.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2510.10181</link>
<guid>https://arxiv.org/abs/2510.10181</guid>
<content:encoded><![CDATA[
arXiv:2510.10181v2 Announce Type: replace-cross 
Abstract: Embodied agents face a fundamental limitation: once deployed in real-world environments to perform specific tasks, they are unable to acquire additional knowledge to enhance task performance. In this paper, we propose a general post-deployment learning framework Dejavu, which employs an Experience Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA) policy with retrieved execution memories. EFN identifies contextually prior action experiences and conditions action prediction on this retrieved guidance. We adopt reinforcement learning with semantic similarity rewards to train EFN, ensuring that the predicted actions align with past behaviors under current observations. During deployment, EFN continually enriches its memory with new trajectories, enabling the agent to exhibit "learning from experience". Experiments across diverse embodied tasks show that EFN improves adaptability, robustness, and success rates over frozen baselines. We provide code and demo in our supplementary material.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualHash: A Stochastic Primal-Dual Algorithm with Theoretical Guarantee for Deep Hashing</title>
<link>https://arxiv.org/abs/2510.18218</link>
<guid>https://arxiv.org/abs/2510.18218</guid>
<content:encoded><![CDATA[
arXiv:2510.18218v2 Announce Type: replace-cross 
Abstract: Deep hashing converts high-dimensional feature vectors into compact binary codes, enabling efficient large-scale retrieval. A fundamental challenge in deep hashing stems from the discrete nature of quantization in generating the codes. W-type regularizations, such as $||z|-1|$, have been proven effective as they encourage variables toward binary values. However, existing methods often directly optimize these regularizations without convergence guarantees. While proximal gradient methods offer a promising solution, the coupling between W-type regularizers and neural network outputs results in composite forms that generally lack closed-form proximal solutions. In this paper, we present a stochastic primal-dual hashing algorithm, referred to as DualHash, that provides rigorous complexity bounds. Using Fenchel duality, we partially transform the nonconvex W-type regularization optimization into the dual space, which results in a proximal operator that admits closed-form solutions. We derive two algorithm instances: a momentum-accelerated version with $\mathcal{O}(\varepsilon^{-4})$ complexity and an improved $\mathcal{O}(\varepsilon^{-3})$ version using variance reduction. Experiments on three image retrieval databases demonstrate the superior performance of DualHash.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game</title>
<link>https://arxiv.org/abs/2511.17925</link>
<guid>https://arxiv.org/abs/2511.17925</guid>
<content:encoded><![CDATA[
arXiv:2511.17925v2 Announce Type: replace-cross 
Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion</title>
<link>https://arxiv.org/abs/2511.22505</link>
<guid>https://arxiv.org/abs/2511.22505</guid>
<content:encoded><![CDATA[
arXiv:2511.22505v2 Announce Type: replace-cross 
Abstract: Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance</title>
<link>https://arxiv.org/abs/2512.05131</link>
<guid>https://arxiv.org/abs/2512.05131</guid>
<content:encoded><![CDATA[
<div> Active 3D reconstruction, view selection, uncertainty modeling, vision-language guidance, sparse-view reconstruction

<br /><br />Summary:  
1. The paper presents AREA3D, an active 3D reconstruction agent designed to autonomously select viewpoints to efficiently capture accurate and complete 3D scene geometry.  
2. Unlike existing methods that depend on hand-crafted geometric heuristics, AREA3D leverages feed-forward 3D reconstruction models combined with vision-language guidance to improve viewpoint selection.  
3. The framework decouples view-uncertainty modeling from the feed-forward reconstructor, allowing precise uncertainty estimation without the need for costly online optimization procedures.  
4. Integrating a vision-language model provides high-level semantic cues, encouraging the agent to choose informative and diverse viewpoints that go beyond purely geometric considerations.  
5. Extensive experiments conducted on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, especially when limited to sparse-view input scenarios.  
6. The proposed approach reduces redundant observations and enhances reconstruction quality effectively, particularly in settings where viewpoint data is limited.  
7. The authors plan to make their code publicly available, facilitating further research and development in active 3D reconstruction. <div>
arXiv:2512.05131v1 Announce Type: new 
Abstract: Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training</title>
<link>https://arxiv.org/abs/2512.05132</link>
<guid>https://arxiv.org/abs/2512.05132</guid>
<content:encoded><![CDATA[
<div> Zero-Shot Super-Resolution, Spatiotemporal Forecasting, Scale Anchoring, Frequency Representation Learning, Nyquist Frequency  

<br /><br />Summary:  
This paper addresses the challenge of zero-shot super-resolution spatiotemporal forecasting, where models trained on low-resolution data are expected to perform inference on higher-resolution data. Existing methods evaluate success by maintaining similar error levels across resolutions, but the authors argue that true generalization should reduce errors as resolution increases, reflecting physical laws more accurately. They identify a fundamental problem termed "Scale Anchoring," caused by the Nyquist frequency limit of low-resolution data that restricts the representation of higher-frequency signals during inference at higher resolutions. This causes error rates to be anchored at the low-resolution level and misinterpreted as successful generalization. To overcome this, the authors propose Frequency Representation Learning (FRL), an architecture-agnostic framework that aligns frequency representations with resolution and employs spectral consistency training. FRL leads to more stable frequency responses in high-frequency bands on grids with higher Nyquist frequencies. Empirically, FRL-enhanced models demonstrate decreasing error with increasing resolution, significantly outperforming baseline methods within the studied task and resolution ranges. This improvement comes at only a modest computational cost, making FRL a promising solution for enhanced multi-resolution spatiotemporal forecasting. <div>
arXiv:2512.05132v1 Announce Type: new 
Abstract: Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05134</link>
<guid>https://arxiv.org/abs/2512.05134</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, acceleration, feature invariance, caching, deterministic sampling  

<br /><br />Summary:  
1. Diffusion models generate high-quality synthetic data but suffer from slow inference due to iterative sampling processes.  
2. The paper identifies feature invariance present in deterministic sampling that can be exploited to accelerate inference.  
3. The authors propose InvarDiff, a training-free acceleration technique leveraging relative temporal invariance across both timestep and layer scales.  
4. InvarDiff analyzes outputs from a few deterministic runs to create a binary cache plan matrix, indicating which modules at specific timesteps or layers can reuse cached results instead of recomputing.  
5. A re-sampling correction is introduced to prevent quality degradation (drift) when consecutive caches are utilized.  
6. The caching strategy operates at two levels: cross-timestep caching, where entire steps can reuse previous computations, and layer-wise caching within steps.  
7. The approach is applied to diffusion models like DiT and FLUX, demonstrating a 2 to 3 times speed-up in end-to-end inference without requiring retraining.  
8. Experimental results show minimal impact on standard quality metrics and almost no visual quality loss compared to full computation.  
9. InvarDiff thus effectively reduces redundant computations while maintaining synthesis fidelity, offering a practical method for faster diffusion model inference. <div>
arXiv:2512.05134v1 Announce Type: new 
Abstract: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes</title>
<link>https://arxiv.org/abs/2512.05136</link>
<guid>https://arxiv.org/abs/2512.05136</guid>
<content:encoded><![CDATA[
<div> Keywords: Coronary artery disease, AI-ECG, stenosis prediction, coronary CT angiography, interpretability

<br /><br />Summary:  
Coronary artery disease (CAD) is a significant global health challenge, requiring accurate identification of culprit vessels and stenosis severity for personalized treatment. While coronary CT angiography (CCTA) is the preferred non-invasive diagnostic tool, its reliance on advanced equipment, radiation exposure, and patient cooperation limits widespread application. This study developed an interpretable artificial intelligence model using electrocardiography (AI-ECG) to predict severe or complete stenosis in the four major coronary arteries (RCA, LM, LAD, LCX) based on CCTA findings. The model achieved strong predictive performance, with internal validation AUCs of 0.794 (RCA), 0.818 (LM), 0.744 (LAD), and 0.755 (LCX), and external validation AUCs of 0.749, 0.971, 0.667, and 0.727 respectively. The AI-ECG model showed robustness even in patients with clinically normal ECGs, indicating its ability to detect subtler ischemic changes. Stability across different demographic groups and ECG acquisition times was confirmed through subgroup analyses. Risk stratification using vessel-specific incidence thresholds provided clear separations in calibration and event outcome curves. Interpretability analyses identified distinct ECG waveform differences between high- and low-risk groups, pinpointing key electrophysiological regions influencing model decisions and advancing understanding of ECG correlates of coronary stenosis. <div>
arXiv:2512.05136v1 Announce Type: new 
Abstract: Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images</title>
<link>https://arxiv.org/abs/2512.05137</link>
<guid>https://arxiv.org/abs/2512.05137</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, ChromouVQA, figure-ground segregation, chromatic camouflage, multimodal understanding<br /><br />Summary:  
1. This paper addresses a persistent challenge in Vision-Language Models (VLMs), specifically their difficulty in recognizing targets embedded within cluttered, camouflaged backgrounds that require figure-ground segregation.  
2. The authors introduce ChromouVQA, a novel large-scale, multi-task benchmark designed using Ishihara-style chromatic camouflage images to evaluate and improve multimodal vision-question-answering.  
3. ChromouVQA extends traditional Ishihara dot plates by incorporating multiple fill geometries and varying parameters such as chromatic separation, density, size, occlusion, and rotation while recording comprehensive metadata for reproducibility.  
4. The benchmark encompasses nine different vision-question-answering tasks including recognition, counting, comparison, and spatial reasoning, providing a diverse and challenging evaluation platform.  
5. Evaluations of both human subjects and state-of-the-art VLMs revealed significant performance gaps, particularly under conditions with subtle chromatic contrast or disruptive geometric fills, highlighting current model limitations.  
6. To improve shape recovery and figure-ground segregation, the authors propose a model-agnostic contrastive learning approach that aligns object silhouettes with their camouflaged renderings, enhancing global shape recognition.  
7. ChromouVQA serves as a compact, controlled, and reproducible benchmark that facilitates consistent evaluation, benchmarking, and future research extensions, with code and dataset publicly available on GitHub. <div>
arXiv:2512.05137v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models</title>
<link>https://arxiv.org/abs/2512.05139</link>
<guid>https://arxiv.org/abs/2512.05139</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, generative downscaling, satellite imagery, diffusion models, spatiotemporal representation<br /><br />Summary:<br /><br />1. The article introduces a transfer-learning generative framework aimed at reconstructing high-resolution satellite images from coarse resolution inputs by combining a lightweight pretrained U-Net encoder with a diffusion-based generative model.<br /><br />2. The U-Net encoder is initially pretrained on extensive coarse resolution data to capture spatiotemporal features, then frozen and integrated into a larger downscaling diffusion model to serve as physically meaningful latent features.<br /><br />3. The approach uses NASA's MERRA-2 reanalysis data at 50 km resolution as the low-resolution source and the GEOS-5 Nature Run (G5NR) dataset at 7 km resolution as the high-resolution target.<br /><br />4. The study focuses on a large Asian region, divided into two subregions and four seasons to manage computational complexity.<br /><br />5. Domain similarity between the datasets was validated through Wasserstein distances, confirming minimal distributional shift and justifying frozen parameter transfer.<br /><br />6. The proposed model delivered strong performance across seasons and regions (R² between 0.65 and 0.94), outperforming deterministic U-Nets, variational autoencoders, and existing transfer learning baselines.<br /><br />7. Out-of-sample evaluations using semivariograms, autocorrelation functions, and lag-based error metrics demonstrated the spatial and temporal consistency of the downscaled images, supporting stable autoregressive forecasting beyond the original high-resolution record.<br /><br />8. This method offers a robust, physically coherent solution for downscaling long time-series satellite images with limited training data, with important applications in environmental exposure assessment and long-term environmental monitoring. <div>
arXiv:2512.05139v1 Announce Type: new 
Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation</title>
<link>https://arxiv.org/abs/2512.05140</link>
<guid>https://arxiv.org/abs/2512.05140</guid>
<content:encoded><![CDATA[
<div> Keywords: Earth observation, unsupervised domain adaptation, flow matching, remote sensing, image translation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of distribution shifts in Earth observation data, which arise due to heterogeneous sources including different sensors, regions, acquisition times, and atmospheric conditions, limiting the generalization of pretrained remote sensing models. <br />2. To overcome these challenges, the authors propose FlowEO, a novel framework that applies generative models and flow matching techniques to achieve image-space unsupervised domain adaptation (UDA) for Earth observation images.<br />3. FlowEO learns a semantically preserving mapping that transports images from the source domain distribution to the target domain, enabling effective alignment despite large domain shifts.<br />4. The framework is evaluated on four diverse datasets, covering complex domain adaptation scenarios such as synthetic aperture radar (SAR) to optical image translation, as well as temporal and semantic shifts caused by natural disasters.<br />5. Experimental results demonstrate that FlowEO outperforms existing image translation methods in domain adaptation tasks related to classification and semantic segmentation, achieving better or comparable perceptual image quality and showcasing the potential of flow-matching-based UDA in remote sensing applications. <div>
arXiv:2512.05140v1 Announce Type: new 
Abstract: The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Improving VLM Judges Without Human Annotations</title>
<link>https://arxiv.org/abs/2512.05145</link>
<guid>https://arxiv.org/abs/2512.05145</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, self-training, judge model, multimodal instruction-response, VL-RewardBench<br /><br />Summary:  
This paper addresses the challenge of developing effective judges for Vision-Language Models (VLMs), which are essential for evaluating and improving these models. Traditional approaches rely heavily on large-scale human preference annotations, which are costly and quickly outdated due to rapid model advancements. To overcome this, the authors propose a novel framework to self-train a VLM judge model without using any human preference data, instead leveraging only self-synthesized multimodal data. The method is iterative and includes three main stages: generating diverse multimodal instruction-response pairs at various quality levels, producing reasoning traces and judgments to filter out inconsistent data based on expected quality, and training the judge model on correct answers paired with their reasoning traces. Evaluation on Multimodal RewardBench and VL-RewardBench benchmarks demonstrates significant improvement in overall accuracy, raising a Llama-3.2-11B multimodal judge model's performance from 0.38 to 0.51. This approach often outperforms larger models like Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general accuracy, hallucination detection, and reasoning ability. The success of this human-annotation-free method indicates promising potential for future self-evolving judge models in tandem with advancing VLM capabilities. <div>
arXiv:2512.05145v1 Announce Type: new 
Abstract: Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</title>
<link>https://arxiv.org/abs/2512.05150</link>
<guid>https://arxiv.org/abs/2512.05150</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal generative models, inference efficiency, distillation, 1-step generative models, TwinFlow<br /><br />Summary:<br /><br />This paper addresses the challenge of slow inference in large multi-modal generative models that typically rely on multi-step frameworks like diffusion and flow matching, requiring 40-100 Number of Function Evaluations (NFEs). Existing few-step acceleration methods face limitations, including iterative distillation requirements, performance degradation under very few steps, and complexity from adversarial training. The authors propose TwinFlow, a novel framework designed to train 1-step generative models without relying on fixed pretrained teacher models or adversarial networks, thus simplifying training and reducing computational overhead. TwinFlow excels specifically in text-to-image generation, achieving a high GenEval score of 0.83 at 1-NFE, outperforming strong existing methods such as SANA-Sprint (GAN-based) and RCGM (consistency-based). Demonstrating scalability, TwinFlow was fully trained on Qwen-Image-20B and efficiently adapted into a few-step generator. With only 1-NFE, the method matches the quality of original 100-NFE models on benchmarks including GenEval and DPG-Bench, thereby reducing computational cost by a factor of 100 with minimal image quality loss. The approach promises a significant step forward in building large-scale, efficient multi-modal generative models, with code and details available at the project page. <div>
arXiv:2512.05150v1 Announce Type: new 
Abstract: Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models</title>
<link>https://arxiv.org/abs/2512.05152</link>
<guid>https://arxiv.org/abs/2512.05152</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, fine-grained image generation, tiered embedder, super-resolution, ProAttention

<br /><br />Summary: This paper addresses challenges in large-scale fine-grained image generation using diffusion models, which are known for their controllability and diverse image outputs but struggle with semantic entanglement and insufficient image detail. The authors introduce a novel concept called the tiered embedder that incorporates semantic information from both superclasses and child classes, enhancing the model's ability to manage and utilize semantic relationships effectively. To improve the quality and detail of generated fine-grained images, the study integrates super-resolution techniques at the perceptual information generation stage, employing both enhancement and degradation models to highlight finer image features. Additionally, a new ProAttention mechanism is proposed to optimize the diffusion model's performance, offering an efficient attention method suitable for the generation process. The approach was rigorously evaluated on public benchmark datasets and demonstrated superior results compared to existing state-of-the-art fine-tuning methods. This work contributes to advancing fine-grained image synthesis by combining hierarchical semantic embedding, attention improvements, and super-resolution enhancements within diffusion models. <div>
arXiv:2512.05152v1 Announce Type: new 
Abstract: Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05172</link>
<guid>https://arxiv.org/abs/2512.05172</guid>
<content:encoded><![CDATA[
<div> Keywords: Enhanced Semantic Motion Representations, Vision-Language Models, Reinforcement Learning, Dual-path backbone, Text-image alignment<br /><br />Summary:<br /><br />This paper addresses limitations in current Large Language Model (LLM)-based reinforcement learning (RL) methods, particularly the restricted representational capacity of backbone networks used for control policy guidance. To overcome this, the authors propose Semore, a novel framework that leverages Vision-Language Models (VLM) to provide enriched semantic and motion representations from RGB flow inputs through a dual-path backbone architecture. Semore incorporates common-sense knowledge from VLMs to extract critical information directly from the visual observations while employing pre-trained CLIP for effective text-image alignment, which helps ground-truth representations to be embedded in the backbone network. To optimize the fusion of semantic and motion features for decision-making, the framework uses a separate supervision strategy enabling concurrent guidance for both representation types while allowing natural interaction between them. Extensive experimental results demonstrate that Semore significantly improves adaptive and efficient performance in visual RL tasks compared to state-of-the-art approaches. The framework's design enables better interpretability and robustness by integrating semantic understanding with dynamic motion cues. The authors have also released all code to facilitate reproducibility and further research. <div>
arXiv:2512.05172v1 Announce Type: new 
Abstract: The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05198</link>
<guid>https://arxiv.org/abs/2512.05198</guid>
<content:encoded><![CDATA[
<div> Keywords: latent inpainting, Pixel-Equivalent Latent Compositing, DecFormer, diffusion models, mask interpolation  

<br /><br />Summary:  
This paper addresses the limitations of latent inpainting in diffusion models, which commonly rely on linear interpolation of VAE latents under downsampled masks, leading to artifacts and color inconsistencies at mask boundaries. The authors propose a new principle called Pixel-Equivalent Latent Compositing (PELC), which ensures that compositing latents produces the same result as pixel-space alpha compositing, enabling full-resolution mask control and smooth soft-edge blending despite the spatial compression of VAEs. To achieve this, they introduce DecFormer, a lightweight 7.7M-parameter transformer model that predicts per-channel blend weights and an off-manifold residual correction to perform mask-consistent latent fusion. DecFormer is designed to be plug-compatible with existing diffusion pipelines without needing backbone finetuning, adding minimal computational and parameter overhead. Experiments on the FLUX.1 diffusion model family demonstrate that DecFormer significantly improves global color consistency, mask sharpness, and soft-mask support, reducing edge errors by up to 53% compared to linear interpolation. When used as an inpainting prior, a lightweight LoRA trained with DecFormer achieves fidelity comparable to fully finetuned inpainting models. Beyond inpainting, PELC is presented as a versatile framework for pixel-equivalent latent editing, with demonstrated success on complex color-correction tasks. <div>
arXiv:2512.05198v1 Announce Type: new 
Abstract: Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering</title>
<link>https://arxiv.org/abs/2512.05209</link>
<guid>https://arxiv.org/abs/2512.05209</guid>
<content:encoded><![CDATA[
<div> Keywords: Image Quality Assessment, Aesthetic Evaluation, Rendering Styles, Crowdsourcing, DEAR Dataset  

<br /><br />Summary:  
1. Traditional Image Quality Assessment (IQA) primarily targets technical degradations such as noise, blur, and compression artifacts, using objective full-reference and no-reference metrics.  
2. Evaluating rendering aesthetics—important in photographic editing, content creation, and AI-generated imagery—remains underexplored due to the subjective and style-based nature of quality assessment.  
3. This work introduces DEAR (Dataset for Evaluating the Aesthetics of Rendering), a novel benchmark dataset built upon the MIT-Adobe FiveK dataset to model human aesthetic judgments of image rendering styles.  
4. DEAR is created through large-scale crowdsourcing, collecting pairwise human preference scores with 25 distinct evaluators per image pair, totaling 13,648 participants, capturing nuanced and context-sensitive aesthetic preferences.  
5. The dataset enables new tasks focused on Evaluation of Aesthetics of Rendering (EAR), going beyond distortion-based IQA, and supports use cases such as style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling.  
6. The data collection pipeline and human voting patterns are analyzed in detail.  
7. DEAR is reported to be the first dataset addressing image aesthetics of rendering assessment grounded on subjective human preferences, with a subset of 100 marked-up images published on HuggingFace, facilitating further research. <div>
arXiv:2512.05209v1 Announce Type: new 
Abstract: Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction</title>
<link>https://arxiv.org/abs/2512.05240</link>
<guid>https://arxiv.org/abs/2512.05240</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, RGB video reconstruction, diffusion model, autoregressive model, hybrid capture paradigm  

<br /><br />Summary:  
The paper addresses the challenge of continuous video monitoring in energy-constrained applications such as surveillance and wearable systems, where conventional RGB cameras consume significant power due to fixed-rate capture. To reduce energy consumption, the authors propose a hybrid capture paradigm that combines sparse RGB keyframes with continuous asynchronous event camera streams, enabling offline reconstruction of full RGB video sequences. They define a new task called Image and Event to Video (IE2Video), which involves generating RGB video from a single initial RGB keyframe and subsequent event data. Two architectural strategies are explored: an autoregressive model named HyperE2VID adapted for RGB video generation, and a diffusion-based model (LTX) that integrates event representations into a pretrained text-to-video diffusion framework using learned encoders and low-rank adaptation techniques. Experimental results reveal that the diffusion-based method outperforms the autoregressive baseline by 33% in perceptual quality, measured by the LPIPS metric (0.283 vs. 0.422). The approach is validated on three event camera datasets (BS-ERGB, HS-ERGB far, and HS-ERGB close) across various sequence lengths (32 to 128 frames) and shows strong cross-dataset generalization and robustness to unseen capture configurations, indicating its practical applicability for energy-efficient video reconstruction from event and RGB data. <div>
arXiv:2512.05240v1 Announce Type: new 
Abstract: Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization</title>
<link>https://arxiv.org/abs/2512.05259</link>
<guid>https://arxiv.org/abs/2512.05259</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D shape estimation, children and infants, SMPL-A body model, transformer-based model, privacy-preserving 3D reconstruction  

<br /><br />Summary:  
This paper addresses the challenge of 3D shape and pose estimation, which typically underperforms on children and infants despite strong results on adults. The authors introduce AionHMR, a framework that incorporates the SMPL-A body model to enable simultaneous and accurate 3D modeling across adults, children, and infants. They propose an optimization-based extension to state-of-the-art models, generating pseudo-ground-truth annotations for existing child and infant image datasets. Using these annotations, they trained a transformer-based deep learning model capable of real-time, age-inclusive 3D human reconstruction. Extensive experiments reveal that their approach significantly improves pose and shape estimation accuracy for younger populations without degrading adult estimation performance. Additionally, the reconstructed 3D meshes act as privacy-preserving alternatives to raw images, capturing key action, pose, and geometry data while allowing anonymized dataset sharing. To demonstrate practical use, the authors introduce the 3D-BabyRobot dataset, featuring 3D reconstructions of children interacting with robots that retain action fidelity. Overall, this work bridges a critical gap in inclusive 3D human modeling by enabling accurate, privacy-aware reconstructions across all ages and fostering potential applications that require age-diverse human data. <div>
arXiv:2512.05259v1 Announce Type: new 
Abstract: While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARD: Correlation Aware Restoration with Diffusion</title>
<link>https://arxiv.org/abs/2512.05268</link>
<guid>https://arxiv.org/abs/2512.05268</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, correlated noise, image restoration, denoising, CARD<br /><br />Summary:  
This paper addresses the limitation of traditional denoising diffusion models that assume i.i.d. Gaussian noise, which is often not true for real-world sensor noise due to spatial correlations caused by sensor readout mechanisms. The authors propose a novel method called Correlation Aware Restoration with Diffusion (CARD), which is a training-free extension of DDRM, designed to explicitly handle correlated Gaussian noise. CARD operates by first whitening the noisy image, transforming the correlated noise into an i.i.d. form suitable for diffusion modeling. Subsequently, the standard diffusion restoration steps are replaced with noise-whitened updates that maintain DDRM’s efficient closed-form sampling while effectively addressing correlated noise. To support evaluation of such methods, the authors introduce CIN-D, a dataset containing real rolling-shutter sensor noise captured under various illumination conditions, filling an important gap in realistic correlated noise benchmarks. Empirical results demonstrate that CARD consistently outperforms existing techniques on synthetic correlated noise benchmarks as well as the real CIN-D dataset. The improvements are shown across multiple image restoration tasks including denoising, deblurring, and super-resolution, highlighting the effectiveness of explicitly modeling noise correlations in diffusion-based restoration frameworks. <div>
arXiv:2512.05268v1 Announce Type: new 
Abstract: Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Compositional 4D Scenes without Ever Seeing One</title>
<link>https://arxiv.org/abs/2512.05272</link>
<guid>https://arxiv.org/abs/2512.05272</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D reconstruction, compositional modeling, spatio-temporal attention, monocular video, dynamic objects  

<br /><br />Summary:  
1. This paper addresses the challenge of capturing 4D structures, compositions, and spatio-temporal configurations of scenes consisting of multiple static and dynamic objects in-the-wild.  
2. Existing methods typically focus on either single objects or rely on parametric shape models limited to specific object categories, resulting in inconsistent scene reconstructions and limited generalizability.  
3. The authors propose COM4D, a novel method that jointly and consistently predicts the 4D structure and dynamics of multiple objects using only static multi-object images or videos of single dynamic objects as supervision.  
4. COM4D’s training disentangles spatial and temporal attention learning from different data sources—object compositions for spatial modeling and single object videos for temporal dynamics—thus avoiding the need for explicit 4D compositional training data.  
5. During inference, an attention mixing mechanism combines the separately learned spatial and temporal attentions, enabling reconstruction of complete, persistent 4D scenes with multiple interacting objects from monocular video input.  
6. The approach alternates between spatial and temporal reasoning to capture both structure and motion effectively.  
7. COM4D achieves state-of-the-art performance in separate tasks involving 4D object reconstruction and compositional 3D reconstruction, despite being purely data-driven and not relying on category-specific models. <div>
arXiv:2512.05272v1 Announce Type: new 
Abstract: Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.05277</link>
<guid>https://arxiv.org/abs/2512.05277</guid>
<content:encoded><![CDATA[
<div> Temporal understanding, autonomous driving, vision-language models, temporal reasoning, ego-centric footage<br /><br />Summary:<br /><br />1. Temporal understanding remains a critical challenge in autonomous driving (AD), particularly for state-of-the-art vision-language models (VLMs).<br />2. Existing temporal reasoning datasets and benchmarks focus on diverse video categories such as sports, cooking, and movies, but none concentrate exclusively on ego-centric AD video content.<br />3. To address this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is introduced, consisting of nearly 6,000 question-answer pairs across seven human-designed tasks aimed at assessing VLMs' temporal reasoning capabilities specific to AD.<br />4. An extensive evaluation of nine generalist and specialist SoTA AD models on TAD revealed subpar performance, mainly due to limited fine-grained motion understanding.<br />5. Two novel, training-free methods named Scene-CoT (leveraging Chain-of-Thought reasoning) and TCogMap (an ego-centric temporal cognitive map) are proposed to enhance motion understanding and overall model accuracy.<br />6. Integration of these approaches with existing VLMs led to accuracy improvements of up to 17.72% on the TAD benchmark.<br />7. The introduction of TAD, comprehensive benchmarking, and effective model enhancements are intended to drive further research on temporal understanding in autonomous driving.<br />8. The TAD dataset and evaluation code are publicly accessible via Hugging Face and GitHub repositories. <div>
arXiv:2512.05277v1 Announce Type: new 
Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling</title>
<link>https://arxiv.org/abs/2512.05343</link>
<guid>https://arxiv.org/abs/2512.05343</guid>
<content:encoded><![CDATA[
<div> 3D generation, spatial control, generative models, geometric fidelity, interactive editing<br /><br />Summary:<br /><br />The paper introduces SpaceControl, a novel method aimed at providing explicit spatial control over 3D asset generation without the need for additional training. Unlike existing approaches that depend heavily on text or image prompts—often ambiguous or difficult to edit—SpaceControl can utilize various geometric inputs ranging from simple primitives to complex meshes. It integrates seamlessly with existing pre-trained generative models, functioning entirely at test time, which eliminates the overhead of retraining. A key feature of SpaceControl is a controllable parameter that allows users to balance geometric accuracy against visual realism, addressing the common trade-off found in generative 3D methods. Extensive evaluations, including quantitative metrics and user studies, indicate that SpaceControl surpasses both training-based and optimization-based baselines in maintaining geometric faithfulness without compromising image quality. The authors also contribute an interactive user interface that facilitates real-time editing of superquadrics, enabling straightforward conversion to textured 3D assets. This tool supports practical use in creative workflows and speeds up asset creation. Overall, SpaceControl represents a significant step toward more intuitive, precise, and flexible 3D asset generation that can be incorporated into a variety of content creation pipelines. <div>
arXiv:2512.05343v1 Announce Type: new 
Abstract: Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training</title>
<link>https://arxiv.org/abs/2512.05354</link>
<guid>https://arxiv.org/abs/2512.05354</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, interactive editing, Test-Time Training, photorealistic 3D, feedforward model<br /><br />Summary:  
The paper addresses the challenge of interactive refinement and editing of 3D Gaussian Splatting assets, which has been difficult due to limitations in existing diffusion and optimization-based methods. These previous approaches tend to be slow, can alter the original asset's identity, and lack precision for fine-grained control. To overcome these issues, the authors propose a new method called \ourmethod, a state-aware feedforward model designed for continuous editing of 3D Gaussian assets using user-provided 2D views. This method predicts attribute updates directly on a compact and feature-rich Gaussian representation, enabling efficient modifications. An important aspect of the approach is Test-Time Training, which facilitates a state-aware iterative editing workflow, allowing the model to adapt dynamically during editing sessions. The versatility of \ourmethod is demonstrated by its ability to perform multiple tasks with a single architecture, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring. Importantly, these operations are executed at interactive speeds, significantly improving user experience. Overall, this work paves the way for more fluid, intuitive, and precise 3D content creation and authoring, addressing a critical gap in the practical usability of 3D Gaussian Splatting technology. <div>
arXiv:2512.05354v1 Announce Type: new 
Abstract: The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Orthogonal Low-Rank Adaptation for RGB-T Tracking</title>
<link>https://arxiv.org/abs/2512.05359</link>
<guid>https://arxiv.org/abs/2512.05359</guid>
<content:encoded><![CDATA[
<div> Keywords: Parameter-efficient fine-tuning, RGB-T tracking, low-rank adaptation, group orthogonal constraint, singular value decomposition<br /><br />Summary:<br /><br />The paper addresses redundancy in low-rank adaptation methods for RGB-T tracking, which limits the model's ability to learn diverse and robust features. It proposes the Group Orthogonal Low-Rank Adaptation (GOLA) framework to enhance parameter efficiency and expressiveness by better leveraging the rank space. The approach involves decomposing the rank space using singular value decomposition (SVD) to identify and freeze crucial ranks, preserving pretrained knowledge while grouping redundant ranks. An inter-group orthogonal constraint is introduced to enforce orthogonality between these groups, encouraging complementary feature learning that addresses various tracking challenges. This structured parameter learning reduces information overlap and enables the model to acquire more diverse knowledge. Experimental evaluation on four benchmark RGB-T tracking datasets shows that GOLA significantly reduces parameter redundancy and improves feature representation capability. The method achieves superior performance compared to state-of-the-art approaches, validating both its theoretical motivation and practical effectiveness in adapting pretrained models to RGB-T tracking tasks while maintaining parameter efficiency. <div>
arXiv:2512.05359v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning has emerged as a promising paradigm in RGB-T tracking, enabling downstream task adaptation by freezing pretrained parameters and fine-tuning only a small set of parameters. This set forms a rank space made up of multiple individual ranks, whose expressiveness directly shapes the model's adaptability. However, quantitative analysis reveals low-rank adaptation exhibits significant redundancy in the rank space, with many ranks contributing almost no practical information. This hinders the model's ability to learn more diverse knowledge to address the various challenges in RGB-T tracking. To address this issue, we propose the Group Orthogonal Low-Rank Adaptation (GOLA) framework for RGB-T tracking, which effectively leverages the rank space through structured parameter learning. Specifically, we adopt a rank decomposition partitioning strategy utilizing singular value decomposition to quantify rank importance, freeze crucial ranks to preserve the pretrained priors, and cluster the redundant ranks into groups to prepare for subsequent orthogonal constraints. We further design an inter-group orthogonal constraint strategy. This constraint enforces orthogonality between rank groups, compelling them to learn complementary features that target diverse challenges, thereby alleviating information redundancy. Experimental results demonstrate that GOLA effectively reduces parameter redundancy and enhances feature representation capabilities, significantly outperforming state-of-the-art methods across four benchmark datasets and validating its effectiveness in RGB-T tracking tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoolNet: Deep Learning for 2D to 3D Video Process Validation</title>
<link>https://arxiv.org/abs/2512.05362</link>
<guid>https://arxiv.org/abs/2512.05362</guid>
<content:encoded><![CDATA[
<div> Structure-from-Motion, deep learning, PoolNet, camera pose, data validation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of extracting Structure-from-Motion (SfM) information from both sequential and non-sequential image datasets, highlighting the process as being time-consuming and computationally demanding. 2. It identifies a key problem that much of the publicly available image data is unsuitable for SfM due to limited variation in camera poses, presence of obscured scene elements, and noisy inputs, which negatively affect reconstruction quality. 3. To overcome these issues, the authors propose PoolNet, a deep learning framework designed to validate image data at both the frame and scene levels, focusing specifically on in-the-wild data scenarios. 4. PoolNet is demonstrated to effectively distinguish between scenes that are suitable for SfM processing and those that are not, thereby acting as a pre-processing filter. 5. An important advantage shown by the model is its ability to reduce the processing time considerably compared to current state-of-the-art SfM algorithms, making the data preparation workflow more efficient and reliable. <div>
arXiv:2512.05362v1 Announce Type: new 
Abstract: Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration</title>
<link>https://arxiv.org/abs/2512.05385</link>
<guid>https://arxiv.org/abs/2512.05385</guid>
<content:encoded><![CDATA[
<div> Video Large Language Models, attention-based pruning, positional debiasing, segment-aware causal masking, token deduplication<br /><br />Summary:<br /><br />This paper addresses the computational challenges faced by Video Large Language Models (VLLMs), particularly during the pre-filling stage where a large quantity of visual tokens is processed. Existing attention-based pruning techniques are commonly used to speed up inference but often lead to significant performance drops, especially when applied to early decoder layers at high compression rates. The authors identify that the limitations of these methods in shallow layers stem from positional encoding bias and insufficient interaction among token information. To overcome these issues, the paper proposes ShaRP, an enhanced attention-based pruning framework that incorporates segment-aware causal masking to better preserve temporal relationships, positional debiasing to reduce bias from positional encoding, and token deduplication to eliminate redundant tokens. ShaRP enables effective pruning even in early decoder layers without the need for retraining and maintains stable performance under high compression rates. Extensive experiments conducted on multiple video understanding benchmarks demonstrate that ShaRP achieves competitive performance while significantly accelerating VLLM inference. This work introduces a new paradigm for efficient video language model processing by balancing token reduction and model accuracy effectively. <div>
arXiv:2512.05385v1 Announce Type: new 
Abstract: Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.05391</link>
<guid>https://arxiv.org/abs/2512.05391</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole Slide Image, Multimodal Large Language Models, Redundancy Reduction, Sparse Token Merger, Cross-Attention Routing Adapter<br /><br />Summary:  
This work addresses the challenge of understanding Whole Slide Images (WSIs), which are extremely large and contain sparse diagnostically relevant regions. Traditional slide-level multimodal large language models (MLLMs) use computationally expensive slide-level encoders that process thousands of patch features in a brute-force manner, leading to high computation and memory costs. The authors observe that many tile-level features in WSIs exhibit strong global and local redundancy, and only a small subset of these tiles are truly relevant for the diagnostic task. To tackle this, they propose LoC-Path, an efficient MLLM framework designed to reduce redundancy and computational load. LoC-Path introduces two key modules: the Sparse Token Merger (STM) combined with an MAE-pretrained resampler to compress and remove redundancy in tile tokens, and the Cross-Attention Routing Adapter (CARA) along with a Token Importance Scorer (TIS) to integrate the compressed visual data into the language model efficiently. Extensive experiments show that LoC-Path achieves performance on par with current state-of-the-art whole-slide MLLMs but with significantly reduced computation and memory requirements, making it a promising approach for practical WSI-language modeling tasks. <div>
arXiv:2512.05391v1 Announce Type: new 
Abstract: Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability</title>
<link>https://arxiv.org/abs/2512.05394</link>
<guid>https://arxiv.org/abs/2512.05394</guid>
<content:encoded><![CDATA[
<div> Keywords: latent diffusion models, video VAE, spectral properties, regularizers, text-to-video generation<br /><br />Summary:<br /><br />This paper addresses the impact of latent space structure in video Variational Autoencoders (VAEs) on the efficiency of latent diffusion model training. The authors point out that while current video VAEs focus mainly on reconstruction fidelity, they neglect the importance of latent space properties for diffusion backbones. Through a statistical analysis, two crucial spectral characteristics of video VAE latents are identified: a spatio-temporal frequency spectrum that is biased toward low frequencies, and a channel-wise eigenspectrum dominated by only a few modes. To incorporate these desirable spectral traits into the latent representations, the paper introduces two novel, lightweight, and backbone-agnostic regularization techniques named Local Correlation Regularization and Latent Masked Reconstruction. These are designed to shape the latent space more effectively without heavy modifications to existing architectures. Experimental results demonstrate that the proposed Spectral-Structured VAE (SSVAE) provides a threefold acceleration in convergence speed during text-to-video generation, while also delivering a 10% improvement in video reward metrics compared to strong open-source VAE baselines. The contribution highlights the importance of latent space design for diffusion-based video generation and offers practical tools to enhance training efficiency and outcome quality. The authors have made their implementation publicly available for community use at the linked repository. <div>
arXiv:2512.05394v1 Announce Type: new 
Abstract: Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\times$ speedup in text-to-video generation convergence and a 10\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos</title>
<link>https://arxiv.org/abs/2512.05398</link>
<guid>https://arxiv.org/abs/2512.05398</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic Prior, Vision-Language Models, motion segmentation, camera pose optimization, 3D scene understanding<br /><br />Summary:<br /><br />This paper addresses the challenge of accurately estimating camera poses, 3D scene geometry, and object motion in videos containing dynamic objects, a problem that traditional structure-from-motion techniques struggle to solve. The authors identify that previous learning-based methods depend heavily on large-scale motion segmentation datasets, which limits their ability to generalize and results in inaccurate segmentation of dynamic objects. To overcome this limitation, they propose a novel approach called the Dynamic Prior (\ourmodel), which leverages Vision-Language Models (VLMs) for their semantic reasoning capabilities and Segment Anything Model 2 (SAM2) for precise spatial segmentation. \ourmodel operates without requiring task-specific training, enabling it to robustly detect dynamic objects in diverse video settings. This method can be integrated into existing pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation, enhancing their performance. Extensive experimental evaluation on both synthetic and real-world video datasets demonstrates that \ourmodel achieves state-of-the-art results in motion segmentation. Moreover, incorporating \ourmodel substantially improves the accuracy and robustness of structural 3D understanding tasks, paving the way for more reliable dynamic scene analysis in-the-wild. <div>
arXiv:2512.05398v1 Announce Type: new 
Abstract: Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects. Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background. However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding. In this work, we introduce the Dynamic Prior (\ourmodel) to robustly identify dynamic objects without task-specific training, leveraging the powerful reasoning capabilities of Vision-Language Models (VLMs) and the fine-grained spatial segmentation capacity of SAM2. \ourmodel can be seamlessly integrated into state-of-the-art pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation. Extensive experiments on both synthetic and real-world videos demonstrate that \ourmodel not only achieves state-of-the-art performance on motion segmentation, but also significantly improves accuracy and robustness for structural 3D understanding.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images</title>
<link>https://arxiv.org/abs/2512.05410</link>
<guid>https://arxiv.org/abs/2512.05410</guid>
<content:encoded><![CDATA[
<div> Keywords: Genetic Algorithm, Stereo Matching, UAV, Parameter Optimization, Forestry Imaging  

<br /><br />Summary:  
This paper addresses the challenge of parameter tuning in traditional stereo matching algorithms, specifically Semi-Global Block Matching (SGBM) combined with Weighted Least Squares (WLS) filtering, used for UAV-based distance measurement to tree branches. The authors propose a novel Genetic Algorithm (GA)-based parameter optimization framework that automates the search for optimal SGBM and WLS parameters, removing the need for manual tuning. Their contributions include developing this framework, establishing a thorough evaluation methodology using multiple image quality metrics, and delivering a practical solution tailored to the constraints of UAV systems in forestry environments. Experimental results reveal that the GA-optimized parameters significantly enhance disparity map quality: Mean Squared Error is reduced by 42.86%, Peak Signal-to-Noise Ratio improves by 8.47%, and Structural Similarity increases by 28.52% compared to baseline setups. Additionally, the method demonstrates strong generalization across diverse imaging conditions, which is critical for robust real-world applications in forestry. The approach retains the computational efficiency advantage of traditional methods, generating disparity maps in approximately 0.5 seconds per frame, making it suitable for resource-limited UAV platforms. Overall, the study presents an effective and efficient solution for enhancing stereo vision accuracy in UAV forestry monitoring via intelligent parameter optimization. <div>
arXiv:2512.05410v1 Announce Type: new 
Abstract: Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame. However, these algorithms require meticulous parameter tuning. We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency. Our contributions include: (1) a novel GA-based parameter optimization framework that eliminates manual tuning; (2) a comprehensive evaluation methodology using multiple image quality metrics; and (3) a practical solution for resource-constrained UAV systems. Experimental results demonstrate that our GA-optimized approach reduces Mean Squared Error by 42.86% while increasing Peak Signal-to-Noise Ratio and Structural Similarity by 8.47% and 28.52%, respectively, compared with baseline configurations. Furthermore, our approach demonstrates superior generalization performance across varied imaging conditions, which is critcal for real-world forestry applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications</title>
<link>https://arxiv.org/abs/2512.05412</link>
<guid>https://arxiv.org/abs/2512.05412</guid>
<content:encoded><![CDATA[
<div> Keywords: Radiata pine, autonomous pruning, YOLO, stereo vision, branch detection  

<br /><br />Summary:  
This paper addresses the safety issues inherent in manual pruning of radiata pine trees, which involves working at dangerous heights and difficult terrain. To mitigate these risks, the authors propose an autonomous pruning system that leverages computer vision techniques. The framework integrates the YOLO object detection model with Semi-Global Block Matching (SGBM) stereo vision, enabling precise branch detection and accurate depth estimation. A key advantage of this approach is the reliance solely on stereo camera input, which removes the need for costly LiDAR sensors traditionally used in such applications. Experimental results demonstrate that YOLO outperforms Mask R-CNN in branch segmentation, achieving an impressive 82.0% mAPmask50-95 metric. The system can localize branches reliably within a 2-meter operational range and operates efficiently with processing times under one second per frame. These findings validate the system's potential as a cost-effective solution to improve worker safety and operational efficiency in commercial forestry by automating pruning tasks using drones equipped with stereo vision and advanced object detection. <div>
arXiv:2512.05412v1 Announce Type: new 
Abstract: Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moving object detection from multi-depth images with an attention-enhanced CNN</title>
<link>https://arxiv.org/abs/2512.05415</link>
<guid>https://arxiv.org/abs/2512.05415</guid>
<content:encoded><![CDATA[
<div> Keywords: moving object detection, convolutional neural network, attention module, multi-input architecture, solar system survey  

<br /><br />Summary:  
This paper addresses the challenge of distinguishing true moving objects in the solar system from noise in wide-field survey data, which traditionally depends heavily on labor-intensive human verification. The authors propose a novel multi-input convolutional neural network (CNN) integrated with a convolutional block attention module (CBAM) to improve detection efficiency and accuracy. The model processes multiple stacked images simultaneously through its multi-input architecture, allowing it to learn more effectively from temporal or spatial image data. The CBAM enhances the model's focus on important features across spatial and channel dimensions, bolstering its ability to identify genuine moving objects amidst noise. Evaluated on a dataset of roughly 2,000 observational images, the method achieved nearly 99% accuracy with an Area Under the Curve (AUC) above 0.99, demonstrating excellent classification performance. Importantly, by adjusting the detection threshold, the new system is able to reduce human verification workload by over 99%, significantly decreasing manual labor and associated costs. This advancement paves the way for more automated, robust, and scalable detection of moving objects in solar system surveys. <div>
arXiv:2512.05415v1 Announce Type: new 
Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Evaluation of Deep Learning for Tree Branch Segmentation in Autonomous Forestry Systems</title>
<link>https://arxiv.org/abs/2512.05418</link>
<guid>https://arxiv.org/abs/2512.05418</guid>
<content:encoded><![CDATA[
<div> Tree Branch Segmentation, UAV Forestry, Deep Learning, Multi-Resolution Analysis, Performance Benchmarking<br /><br />Summary:<br /><br />1. This paper addresses the challenge of rapid and precise tree branch segmentation for UAV-based autonomous forestry operations, crucial for safe navigation and automated pruning.  
2. The study evaluates various deep learning methods on three input resolutions—256x256, 512x512, and 1024x1024 pixels—using the Urban Street Tree Dataset.  
3. Standard evaluation metrics such as Intersection over Union (IoU) and Dice coefficient are employed, alongside specialized metrics like Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR), to assess model performance comprehensively.  
4. Among 22 tested configurations, U-Net with a MiT-B4 backbone performs best at 256x256 resolution, MiT-B4 leads at 512x512 in multiple metrics including IoU and Boundary-F1, while at 1024x1024 resolution, U-Net with MiT-B3 achieves top IoU/Dice scores and precision, with U-Net++ excelling in boundary quality.  
5. PSPNet is identified as the most computationally efficient model, requiring significantly fewer GFLOPs but sacrificing some accuracy compared to top performers.  
6. These results establish new multi-resolution benchmarks that balance accuracy and computational efficiency for deployment in embedded forestry systems.  
7. The implementation code is publicly available at the provided GitHub repository, facilitating further research and practical applications in autonomous forestry. <div>
arXiv:2512.05418v1 Announce Type: new 
Abstract: UAV-based autonomous forestry operations require rapid and precise tree branch segmentation for safe navigation and automated pruning across varying pixel resolutions and operational conditions. We evaluate different deep learning methods at three resolutions (256x256, 512x512, 1024x1024) using the Urban Street Tree Dataset, employing standard metrics (IoU, Dice) and specialized measures including Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR). Among 22 configurations tested, U-Net with MiT-B4 backbone achieves strong performance at 256x256. At 512x512, MiT-B4 leads in IoU, Dice, TS-IoU, and Boundary-F1. At 1024x1024, U-Net+MiT-B3 shows the best validation performance for IoU/Dice and precision, while U-Net++ excels in boundary quality. PSPNet provides the most efficient option (2.36/9.43/37.74 GFLOPs) with 25.7/19.6/11.8 percentage point IoU reductions compared to top performers at respective resolutions. These results establish multi-resolution benchmarks for accuracy-efficiency trade-offs in embedded forestry systems. Implementation is available at https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction</title>
<link>https://arxiv.org/abs/2512.05422</link>
<guid>https://arxiv.org/abs/2512.05422</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified multimodal models, vision-language models, diffusion models, Layer Integration Module, Reinforcement Learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of improving visual generation by combining Vision-Language Models (VLMs) with diffusion models in a unified multimodal framework. Existing methods face difficulties balancing sufficient interaction across modalities and maintaining a flexible implementation due to the large differences in representations. <br /><br />2. The authors propose ParaUni, a novel approach that extracts features from multiple layers of various VLMs in parallel, leveraging the hierarchical nature of VLM layers that capture both low-level details and high-level semantics for comprehensive information interaction. <br /><br />3. ParaUni includes a Layer Integration Module (LIM) that fuses visual features from all VLM layers efficiently, producing a combined representation used as conditioning input to the diffusion model to enhance image generation quality. <br /><br />4. The paper introduces a Layer-wise Dynamic Adjustment Mechanism (LDAM) that leverages Reinforcement Learning to adjust the multi-layer features with respect to different rewards, aligning the hierarchical properties of the layers for better performance during training. <br /><br />5. Extensive experiments demonstrate that ParaUni significantly improves generation quality by effectively utilizing complementary features from multiple VLM layers and shows strong capability for multiple reward improvements during the RL stage. The code has been made publicly available. <div>
arXiv:2512.05422v1 Announce Type: new 
Abstract: Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \textbf{ParaUni}. It extracts features from variants VLM's layers in a \textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at https://github.com/JosephTiTan/ParaUni.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</title>
<link>https://arxiv.org/abs/2512.05446</link>
<guid>https://arxiv.org/abs/2512.05446</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, dynamic scene representation, deformation scheme, rate-distortion compression, implicit neural representation  

<br /><br />Summary:  
This paper presents TED-4DGS, a novel method for dynamic 3D scene representation using 4D Gaussian Splatting (4DGS). The authors address limitations in prior work, which either over-specify short-lived Gaussian primitives in space-time 4DGS or use canonical 3DGS with insufficient temporal control for deformation. TED-4DGS introduces a temporally activated and embedding-based deformation scheme that combines the advantages of both approaches. It is based on a sparse anchor 3DGS representation where each anchor has learnable temporal-activation parameters to control its appearance and disappearance over time. Additionally, a lightweight per-anchor temporal embedding queries a shared deformation bank to generate anchor-specific deformations, enabling explicit temporal control. For compression, TED-4DGS integrates an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions and employs a channel-wise autoregressive model to capture intra-anchor correlations. These elements contribute to a rate-distortion-optimized compression framework for dynamic 3DGS representations. Experimental results demonstrate that TED-4DGS achieves state-of-the-art rate-distortion performance on real-world datasets. To the authors’ knowledge, this work is among the first to focus on a rate-distortion-optimized compression strategy specifically for dynamic 3D Gaussian Splatting. <div>
arXiv:2512.05446v1 Announce Type: new 
Abstract: Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>University Building Recognition Dataset in Thailand for the mission-oriented IoT sensor system</title>
<link>https://arxiv.org/abs/2512.05468</link>
<guid>https://arxiv.org/abs/2512.05468</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Federated Learning, Vision Transformer, Edge Devices, Building Recognition<br /><br />Summary:  
This paper addresses the emerging trend of performing machine learning training directly on edge devices, enabled by advances in semiconductor technology. It focuses on Wireless Ad Hoc Federated Learning (WAFL), an approach that leverages device-to-device communication among edge devices for collaborative model training without relying on centralized data. The study specifically examines the integration of WAFL with Vision Transformer models (WAFL-ViT) in the context of image recognition tasks, previously tested on the UTokyo Building Recognition Dataset (UTBR). Recognizing the mission-specific nature of WAFL-ViT sensor systems, the authors introduce a new dataset, the Chulalongkorn University Building Recognition Dataset (CUBR), tailored as a case study for Chulalongkorn University in Thailand. Their experiments demonstrate that training models under WAFL scenarios yields superior accuracy compared to self-training on individual devices, highlighting the benefits of collaborative learning frameworks in edge environments. Additionally, they provide open access to the CUBR dataset to promote further research, available at their GitHub repository. This work contributes to advancing federated learning strategies and deployment of vision models on resource-constrained edge devices for mission-oriented applications. <div>
arXiv:2512.05468v1 Announce Type: new 
Abstract: Many industrial sectors have been using of machine learning at inference mode on edge devices. Future directions show that training on edge devices is promising due to improvements in semiconductor performance. Wireless Ad Hoc Federated Learning (WAFL) has been proposed as a promising approach for collaborative learning with device-to-device communication among edges. In particular, WAFL with Vision Transformer (WAFL-ViT) has been tested on image recognition tasks with the UTokyo Building Recognition Dataset (UTBR). Since WAFL-ViT is a mission-oriented sensor system, it is essential to construct specific datasets by each mission. In our work, we have developed the Chulalongkorn University Building Recognition Dataset (CUBR), which is specialized for Chulalongkorn University as a case study in Thailand. Additionally, our results also demonstrate that training on WAFL scenarios achieves better accuracy than self-training scenarios. Dataset is available in https://github.com/jo2lxq/wafl/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoStyle: Emotion-Driven Image Stylization</title>
<link>https://arxiv.org/abs/2512.05478</link>
<guid>https://arxiv.org/abs/2512.05478</guid>
<content:encoded><![CDATA[
<div> Affective Image Stylization, EmoStyle, Emotion-Content Reasoner, Style Quantizer, Emotion-aware stylization  

<br /><br />Summary:  
This paper introduces Affective Image Stylization (AIS), a new task aimed at applying artistic styles to images that evoke specific emotions while preserving the original content. To support this, the authors create EmoStyleSet, a unique dataset of content-emotion-stylized image triplets derived from ArtEmis, addressing the lack of training data in this domain. The EmoStyle framework features an Emotion-Content Reasoner, which adaptively integrates emotional cues with image content to generate coherent style guidance. Recognizing the discrete characteristics of artistic styles, the authors design a Style Quantizer that transforms continuous style representations into emotion-related codebook entries. Extensive evaluations, both qualitative and quantitative including user studies, show that EmoStyle improves the emotional expressiveness of stylized images without compromising content fidelity. Furthermore, the emotion-aware style dictionary learned during training is versatile and can be leveraged for other generative tasks. Overall, this work lays foundational ground for emotion-driven image stylization, enhancing the expressive and creative abilities of AI-generated art by incorporating emotional dimensions into style transfer. <div>
arXiv:2512.05478v1 Announce Type: new 
Abstract: Art has long been a profound medium for expressing emotions. While existing image stylization methods effectively transform visual appearance, they often overlook the emotional impact carried by styles. To bridge this gap, we introduce Affective Image Stylization (AIS), a task that applies artistic styles to evoke specific emotions while preserving content. We present EmoStyle, a framework designed to address key challenges in AIS, including the lack of training data and the emotion-style mapping. First, we construct EmoStyleSet, a content-emotion-stylized image triplet dataset derived from ArtEmis to support AIS. We then propose an Emotion-Content Reasoner that adaptively integrates emotional cues with content to learn coherent style queries. Given the discrete nature of artistic styles, we further develop a Style Quantizer that converts continuous style features into emotion-related codebook entries. Extensive qualitative and quantitative evaluations, including user studies, demonstrate that EmoStyle enhances emotional expressiveness while maintaining content consistency. Moreover, the learned emotion-aware style dictionary is adaptable to other generative tasks, highlighting its potential for broader applications. Our work establishes a foundation for emotion-driven image stylization, expanding the creative potential of AI-generated art.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion</title>
<link>https://arxiv.org/abs/2512.05481</link>
<guid>https://arxiv.org/abs/2512.05481</guid>
<content:encoded><![CDATA[
<div> keywords: Multi-Contrast MR Reconstruction, k-space undersampling, frequency-spatial fusion, adaptive prompt learning, cross-modal fusion  

<br /><br />Summary:  
This paper addresses the challenge of generalization across different k-space undersampling patterns in Multi-Contrast MR Reconstruction (MCMR), where current methods require retraining for each pattern, limiting their practicality. The authors propose UniFS, a Unified Frequency-Spatial Fusion model capable of handling multiple undersampling patterns without retraining. UniFS incorporates three key modules: a Cross-Modal Frequency Fusion module to leverage complementary frequency information from multiple contrasts, an Adaptive Mask-Based Prompt Learning module to dynamically adapt to variations in undersampling masks, and a Dual-Branch Complementary Refinement module to extract domain-invariant features effectively. Unlike existing methods that focus mainly on spatial information or extract only shallow frequency features, UniFS introduces an adaptive prompt-guided frequency fusion approach designed for robust k-space learning and enhanced generalizability. The model was evaluated on the BraTS and HCP datasets with various undersampling patterns and acceleration factors, including patterns not seen during training. Results show that UniFS achieves state-of-the-art performance across multiple scenarios, demonstrating strong generalization capability. The authors also provide their implementation publicly for reproducibility and further research at the given GitHub repository. <div>
arXiv:2512.05481v1 Announce Type: new 
Abstract: Recently, Multi-Contrast MR Reconstruction (MCMR) has emerged as a hot research topic that leverages high-quality auxiliary modalities to reconstruct undersampled target modalities of interest. However, existing methods often struggle to generalize across different k-space undersampling patterns, requiring the training of a separate model for each specific pattern, which limits their practical applicability. To address this challenge, we propose UniFS, a Unified Frequency-Spatial Fusion model designed to handle multiple k-space undersampling patterns for MCMR tasks without any need for retraining. UniFS integrates three key modules: a Cross-Modal Frequency Fusion module, an Adaptive Mask-Based Prompt Learning module, and a Dual-Branch Complementary Refinement module. These modules work together to extract domain-invariant features from diverse k-space undersampling patterns while dynamically adapt to their own variations. Another limitation of existing MCMR methods is their tendency to focus solely on spatial information while neglect frequency characteristics, or extract only shallow frequency features, thus failing to fully leverage complementary cross-modal frequency information. To relieve this issue, UniFS introduces an adaptive prompt-guided frequency fusion module for k-space learning, significantly enhancing the model's generalization performance. We evaluate our model on the BraTS and HCP datasets with various k-space undersampling patterns and acceleration factors, including previously unseen patterns, to comprehensively assess UniFS's generalizability. Experimental results across multiple scenarios demonstrate that UniFS achieves state-of-the-art performance. Our code is available at https://github.com/LIKP0/UniFS.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-based Explainable Data Mining with VLM for 3D Detection</title>
<link>https://arxiv.org/abs/2512.05482</link>
<guid>https://arxiv.org/abs/2512.05482</guid>
<content:encoded><![CDATA[
<div> Rare-object detection, autonomous driving, Vision-Language Models, outlier detection, 3D object detection<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting rare objects in autonomous driving systems using point cloud data. It introduces a novel cross-modal framework that leverages 2D Vision-Language Models (VLMs) to mine rare objects from driving scenes, thus improving 3D object detection. The approach integrates techniques including object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into an explainable pipeline. Specifically, the method combines Isolation Forest and t-SNE-based outlier detection with concept-based filtering to effectively identify semantically meaningful rare objects such as construction vehicles, motorcycles, and barriers. This targeted mining reduces the annotation effort by focusing only on valuable training samples. Experimental results on the nuScenes dataset demonstrate that this concept-guided strategy enhances 3D detection performance while using only a fraction of the training data. The improvements are particularly significant for challenging categories like trailers and bicycles compared to using the same amount of randomly selected data. This work has important implications for the efficient curation of datasets in safety-critical systems like autonomous driving, ensuring better identification of rare but critical objects. <div>
arXiv:2512.05482v1 Announce Type: new 
Abstract: Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field</title>
<link>https://arxiv.org/abs/2512.05492</link>
<guid>https://arxiv.org/abs/2512.05492</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater video enhancement, temporal consistency, wavelet-based temporal consistency field, underwater flow correction, video tracking  

<br /><br />Summary:  
The paper addresses the challenge of enhancing underwater videos, which are difficult to obtain and suffer from temporal inconsistency when processed frame-by-frame using single-image enhancement techniques. To tackle this, the authors investigate the temporal manifold of natural videos and identify a temporal consistency prior from a local temporal frequency perspective in dynamic scenes. Leveraging this prior and working under a no paired-data condition, they propose an implicit representation method called WaterWave, which operates in a wavelet-based temporal consistency field. WaterWave progressively filters out inconsistent components while preserving motion details and scene integrity, resulting in smooth, natural-flowing underwater videos. The authors also introduce an underwater flow correction module to accurately represent temporal frequency bands by rectifying flow estimations to better reflect underwater transmission characteristics. Extensive experiments demonstrate that WaterWave considerably improves the quality of videos enhanced by existing single-image underwater enhancement models. Additionally, the method shows strong potential for downstream underwater tracking tasks such as UOSTrack and MAT, achieving significant precision improvements of 19.7% and 9.7%, respectively, compared to the original video inputs. <div>
arXiv:2512.05492v1 Announce Type: new 
Abstract: Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding with Structured Awareness: Integrating Directional, Frequency-Spatial, and Structural Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.05494</link>
<guid>https://arxiv.org/abs/2512.05494</guid>
<content:encoded><![CDATA[
<div> Adaptive Cross-Fusion Attention, Triple Feature Fusion Attention, Structural-aware Multi-scale Masking, Medical Image Segmentation, Frequency-Spatial Representation  

<br /><br />Summary:  
This paper introduces a novel decoder framework tailored for medical image segmentation to overcome the limitations of traditional Transformer decoders in capturing fine edge details, local textures, and spatial continuity. The framework consists of three main modules: first, the Adaptive Cross-Fusion Attention (ACFA) module, which integrates channel enhancement with spatial attention and incorporates learnable guidance along planar, horizontal, and vertical directions, improving focus on key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module combines features from Spatial, Fourier, and Wavelet domains, enabling joint frequency-spatial representation that enhances global dependency and structural modeling, while preserving local edge and texture information, particularly useful in scenarios with complex or blurred boundaries. Third, the Structural-aware Multi-scale Masking Module (SMMM) refines skip connections between encoder and decoder by utilizing multi-scale context and structural saliency filtering, reducing feature redundancy and boosting semantic interaction quality. These components work synergistically to address decoder shortcomings and significantly improve segmentation performance in high-precision tasks such as tumor segmentation and organ boundary extraction. Experimental results confirm that this framework delivers both high accuracy and strong generalization ability, making it an efficient, practical solution for medical image segmentation challenges. <div>
arXiv:2512.05494v1 Announce Type: new 
Abstract: To address the limitations of Transformer decoders in capturing edge details, recognizing local textures and modeling spatial continuity, this paper proposes a novel decoder framework specifically designed for medical image segmentation, comprising three core modules. First, the Adaptive Cross-Fusion Attention (ACFA) module integrates channel feature enhancement with spatial attention mechanisms and introduces learnable guidance in three directions (planar, horizontal, and vertical) to enhance responsiveness to key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module fuses features from Spatial, Fourier and Wavelet domains, achieving joint frequency-spatial representation that strengthens global dependency and structural modeling while preserving local information such as edges and textures, making it particularly effective in complex and blurred boundary scenarios. Finally, the Structural-aware Multi-scale Masking Module (SMMM) optimizes the skip connections between encoder and decoder by leveraging multi-scale context and structural saliency filtering, effectively reducing feature redundancy and improving semantic interaction quality. Working synergistically, these modules not only address the shortcomings of traditional decoders but also significantly enhance performance in high-precision tasks such as tumor segmentation and organ boundary extraction, improving both segmentation accuracy and model generalization. Experimental results demonstrate that this framework provides an efficient and practical solution for medical image segmentation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm</title>
<link>https://arxiv.org/abs/2512.05511</link>
<guid>https://arxiv.org/abs/2512.05511</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Foundation Models, Infrared Small Target Detection, Semantic Alignment, Self-Distillation, Evaluation Metric<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting single-frame infrared small targets (SIRST), leveraging large-scale Visual Foundation Models (VFMs) which have not been previously explored in this domain. The authors propose a Foundation-Driven Efficient Paradigm (FDEP) that integrates frozen representations from VFMs into existing encoder-decoder architectures, improving detection accuracy without increasing inference time. A key component, the Semantic Alignment Modulation Fusion (SAMF) module, dynamically aligns and deeply fuses global semantic priors from VFMs with task-specific features for better performance. To mitigate inference overhead, the Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy is introduced; this involves parameter sharing and synchronized backpropagation between a main and lightweight branch to enable implicit semantic transfer. Additionally, the authors identify fragmentation in current evaluation methods and propose a unified Holistic SIRST Evaluation (HSE) metric, which performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness for a more stable and comprehensive comparison. Extensive experiments demonstrate that integrating FDEP into existing SIRST networks achieves state-of-the-art results across multiple public datasets. The authors also provide their codebase publicly for reproducibility and further research. <div>
arXiv:2512.05511v1 Announce Type: new 
Abstract: While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning</title>
<link>https://arxiv.org/abs/2512.05513</link>
<guid>https://arxiv.org/abs/2512.05513</guid>
<content:encoded><![CDATA[
<div> Keywords: Video-Language Models, spatio-temporal reasoning, Know-Show benchmark, GRAM plug-in, fine-grained grounding  

<br /><br />Summary:  
This paper introduces Know-Show, a novel benchmark designed to evaluate spatio-temporal grounded reasoning in video-language models (Video-LMs). Know-Show assesses a model’s ability to reason about actions and their semantics while grounding inferences in both visual and temporal evidence. The benchmark integrates reasoning and localization in a unified framework consisting of five scenarios across spatial (person, object, person-object, hand-object) and temporal dimensions. It is built using datasets Charades, Action Genome, and Ego4D, and includes 2.5K human-authored questions to thoroughly test models. Experimental results highlight significant performance gaps between current state-of-the-art Video-LMs (such as Qwen, VideoLLaVA, GPT-4o, and Gemini) and human reasoning capacities, particularly in handling fine-grained hand-object interactions. To address these issues, the authors propose GRAM, a training-free plug-in that enhances Video-LMs by applying attention-based video token selection and explicit timestamp encoding to achieve fine-grained grounding without retraining the models. Through extensive experiments, Know-Show reveals that existing models struggle to concurrently "show what they know" by connecting reasoning with clear evidence in video data. The benchmark thus sets a unified standard for evaluating grounded reasoning, aiming to guide development towards more interpretable and reliable multimodal reasoning systems. The project’s codebase will be publicly released to foster further research. <div>
arXiv:2512.05513v1 Announce Type: new 
Abstract: Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to "show what they know" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.05515</link>
<guid>https://arxiv.org/abs/2512.05515</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, Alignment, Fusion, Contrastive Learning, Hierarchical Bottleneck Fusion  

<br /><br />Summary:  
Multimodal sentiment analysis (MSA) aims to combine different data modalities such as text, image, and audio to better understand sentiment. A major challenge in MSA is effectively aligning and fusing multimodal features. Alignment involves synchronizing temporal sequences and semantic content across different modalities, while fusion integrates these aligned features into a unified representation. Existing approaches typically handle alignment or fusion separately, which limits their overall effectiveness and efficiency. To address this, the paper proposes DashFusion, a novel framework consisting of two main components. First, the dual-stream alignment module performs temporal alignment using cross-modal attention to match frame-level information and semantic alignment through contrastive learning to maintain feature-space consistency. Second, supervised contrastive learning exploits label information to further enhance modality features. For fusion, DashFusion introduces hierarchical bottleneck fusion, which progressively compresses and integrates multimodal features using bottleneck tokens, balancing accuracy and computational load. The framework is evaluated on three popular datasets—CMU-MOSI, CMU-MOSEI, and CH-SIMS—demonstrating state-of-the-art results across multiple metrics. Ablation studies validate the effectiveness of the proposed alignment and fusion strategies. The authors have open-sourced their code for reproducibility and further research at the provided GitHub repository. <div>
arXiv:2512.05515v1 Announce Type: new 
Abstract: Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation</title>
<link>https://arxiv.org/abs/2512.05524</link>
<guid>https://arxiv.org/abs/2512.05524</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatio-temporal scene graph generation, vision-language models, dual-source query initialization, multi-modal feature bank, Action Genome dataset<br /><br />Summary:<br /><br />1. Spatio-temporal scene graph generation (ST-SGG) aims to represent objects and their dynamic relationships across video frames to facilitate tasks like video captioning and visual question answering.  
2. Current DETR-style single-stage ST-SGG models face challenges, particularly due to semantically uninformed and instance-agnostic learnable queries used for attention mechanisms.  
3. These models also depend solely on unimodal visual features for predicate classification, limiting their reasoning capabilities.  
4. The proposed VOST-SGG framework integrates vision-language models (VLMs), leveraging their common sense reasoning to improve ST-SGG by introducing a dual-source query initialization that separates semantic content ("what to attend to") from spatial location ("where to attend").  
5. VOST-SGG incorporates a multi-modal feature bank that combines visual, textual, and spatial information from VLMs for enhanced predicate classification.  
6. Extensive experiments on the Action Genome dataset confirm the approach achieves state-of-the-art performance, demonstrating the effectiveness of integrating VLM-aided semantic priors and multi-modal features into the ST-SGG pipeline.  
7. The authors plan to release the code to promote further research and application development at https://github.com/LUNAProject22/VOST. <div>
arXiv:2512.05524v1 Announce Type: new 
Abstract: Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors</title>
<link>https://arxiv.org/abs/2512.05529</link>
<guid>https://arxiv.org/abs/2512.05529</guid>
<content:encoded><![CDATA[
<div> Depth estimation, surgical scene segmentation, monocular depth, vision foundation models, template matching<br /><br />Summary:<br /><br />1. The article addresses the challenge of pixel-wise segmentation in laparoscopic surgical scenes, which is crucial for computer-assisted surgery but hampered by the high cost of creating dense annotations.<br />2. The authors propose a novel framework named Depth-guided Surgical Scene Segmentation (DepSeg), which operates without the need for training by leveraging monocular depth information as a geometric prior in combination with pretrained vision foundation models.<br />3. DepSeg works by first estimating a relative depth map using a pretrained monocular depth estimation network, which helps generate depth-guided point prompts.<br />4. These prompts are input into SAM2 (Segment Anything Model version 2) to produce class-agnostic segmentation masks.<br />5. Each mask is then described using pooled pretrained visual features and classified by matching against a template bank created from annotated frames.<br />6. Experimental results on the CholecSeg8k dataset demonstrate that DepSeg significantly outperforms a baseline SAM2 auto segmentation method, achieving 35.9% mean Intersection over Union (mIoU) compared to 14.7%.<br />7. Moreover, DepSeg maintains competitive segmentation performance even when using only 10–20% of the available object templates, illustrating its annotation efficiency.<br />8. The study concludes that depth-guided prompting together with template-based classification offers a promising and scalable approach for surgical scene segmentation without heavy annotation requirements. <div>
arXiv:2512.05529v1 Announce Type: new 
Abstract: Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ideal Observer for Segmentation of Dead Leaves Images</title>
<link>https://arxiv.org/abs/2512.05539</link>
<guid>https://arxiv.org/abs/2512.05539</guid>
<content:encoded><![CDATA[
<div> Keywords: dead leaves model, Bayesian ideal observer, image segmentation, occlusion, generative model

<br /><br />Summary:  
1. The human visual environment consists of numerous surfaces arranged spatially, where visibility depends on occlusions created by overlapping objects.  
2. Dead leaves models simulate these occlusions by layering randomly sampled objects ("leaves") from distributions of position, shape, color, and texture to generate images.  
3. Image generation in dead leaves models continues until the image is fully covered or a preset number of leaves is sampled.  
4. The paper develops a theoretical Bayesian ideal observer framework to infer the partitioning of a set of pixels based on independent distributions within the dead leaves model.  
5. Detailed step-by-step computations for the posterior probability are provided, alongside an analysis of the practical feasibility of implementing this ideal observer approach.  
6. The model and its associated ideal observer serve as an upper bound on segmentation performance for a limited set of pixels, offering a benchmark for comparing human vision and computer vision algorithms in segmentation tasks. <div>
arXiv:2512.05539v1 Announce Type: new 
Abstract: The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.05546</link>
<guid>https://arxiv.org/abs/2512.05546</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Text Inertia, Cognitive Demand Sensor, Attention Reorientation, Game-Theoretic Interpretability<br /><br />Summary:  
This paper addresses the problem of text inertia in large Vision-Language Models (VLMs), where model attention drifts away from visual inputs towards linguistic priors, causing object hallucinations. Existing methods either intervene only at the output logits, which cannot correct internal reasoning drifts, or rely on heuristic controls lacking theoretical foundation. The authors propose Conscious Gaze (CG-VLM), a novel framework that operates during inference without retraining the model. CG-VLM leverages game-theoretic interpretability via a Cognitive Demand Sensor that uses Harsanyi interactions to estimate real-time vision-text synergy and detect when the model needs stronger visual grounding. When necessary, a Focused Consensus Induction module selectively redirects mid-layer attention back to visual tokens, preventing collapse into text prior biases. Experimental results on benchmarks including POPE and CHAIR demonstrate that CG-VLM improves performance across multiple VLM architectures such as InstructBLIP, LLaVA, Qwen-VL, and mPLUG. Crucially, this approach preserves general model capabilities while enabling precise, context-aware attention intervention at the token level. The study shows that integrating interpretable, token-level sensing mechanisms can effectively mitigate hallucinations and enhance visual grounding in large multimodal models without compromising foundational knowledge. <div>
arXiv:2512.05546v1 Announce Type: new 
Abstract: Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency</title>
<link>https://arxiv.org/abs/2512.05557</link>
<guid>https://arxiv.org/abs/2512.05557</guid>
<content:encoded><![CDATA[
<div> Keywords: sequential identity consistency, transient attribute control, stylized narrative dataset, Human-in-the-Loop pipeline, decoupled control scheme<br /><br />Summary:<br />1. The paper addresses the challenge of maintaining sequential identity consistency in visual storytelling, focusing on precise control over transient attributes such as pose, expression, and scene composition. <br />2. Current datasets fall short in providing high-fidelity data that disentangles stable character identities from varying transient attributes, limiting controllable and reliable visual narrative synthesis.<br />3. To overcome these limitations, the authors introduce "2K-Characters-10K-Stories," a large-scale multi-modal dataset featuring 2,000 uniquely stylized characters appearing across 10,000 illustration stories, explicitly pairing stable identities with decoupled control signals.<br />4. A novel Human-in-the-Loop (HiL) pipeline is proposed, combining expert-verified character templates with large language model (LLM)-guided narrative planning to generate structured and highly aligned data.<br />5. The dataset employs a decoupled control scheme separating persistent identity features from transient attributes and integrates a Quality-Gated loop that combines multimodal language model (MMLM) evaluation, Auto-Prompt Tuning, and Local Image Editing to ensure pixel-level consistency.<br />6. Experimental results show that models fine-tuned on this dataset achieve comparable performance to closed-source systems in generating coherent and visually consistent storytelling sequences. <div>
arXiv:2512.05557v1 Announce Type: new 
Abstract: Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \textbf{2{,}000} uniquely stylized characters appearing across \textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProPhy: Progressive Physical Alignment for Dynamic World Simulation</title>
<link>https://arxiv.org/abs/2512.05564</link>
<guid>https://arxiv.org/abs/2512.05564</guid>
<content:encoded><![CDATA[
<div> Physics-aware video generation, Mixture-of-Physics-Experts, anisotropic generation, physical alignment, vision-language models<br /><br />Summary:<br /><br />This paper addresses the challenge of generating videos that are physically consistent, especially in cases involving large-scale or complex physical dynamics where current models fall short due to isotropic responses to physical prompts and lack of localized physical cue alignment. The authors propose ProPhy, a Progressive Physical Alignment Framework designed to incorporate explicit physics-aware conditioning and enable anisotropic video generation. ProPhy introduces a novel two-stage Mixture-of-Physics-Experts (MoPE) mechanism, consisting of Semantic Experts that infer high-level physical principles from textual inputs and Refinement Experts that capture fine-grained, token-level physical dynamics. This dual-expert approach facilitates learning of detailed, physics-aligned video representations that adhere more closely to physical laws. Additionally, the framework includes a physical alignment strategy to transfer physical reasoning abilities from vision-language models (VLMs) into the Refinement Experts, enhancing the model’s capacity to accurately represent dynamic physical phenomena. Extensive experimental evaluation on established physics-aware video generation benchmarks demonstrates that ProPhy outperforms current state-of-the-art methods by producing videos that are more realistic, dynamic, and physically coherent. The results indicate significant progress towards creating advanced world simulators based on video generation. <div>
arXiv:2512.05564v1 Announce Type: new 
Abstract: Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging</title>
<link>https://arxiv.org/abs/2512.05571</link>
<guid>https://arxiv.org/abs/2512.05571</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, diffusion models, voxel descriptors, spatial correspondence, lung CT

<br /><br />Summary: Accurate spatial correspondence between medical images is critical for applications such as longitudinal analysis, lesion tracking, and image-guided interventions. Traditional medical image registration methods rely on local intensity-based similarity measures, which often fail to capture global semantic structures and struggle in low-contrast or anatomically variable regions. This study introduces MedDIFT, a novel 3D correspondence framework that is training-free and leverages multi-scale features extracted from a pretrained latent medical diffusion model as voxel-wise descriptors. MedDIFT integrates diffusion activations to form rich descriptors which are matched via cosine similarity, and optionally refined using a local-search prior. Evaluation on a publicly available lung CT dataset demonstrates that MedDIFT achieves correspondence accuracy on par with the state-of-the-art learning-based UniGradICON model and outperforms traditional B-spline-based registration methods, all without requiring any task-specific model training. Ablation studies reveal that fusing multi-level diffusion features and applying a modest diffusion noise level positively impact performance, confirming the method's robustness and effectiveness. This work highlights the potential of leveraging pretrained diffusion model features for improved medical image registration without the need for retraining or supervision. <div>
arXiv:2512.05571v1 Announce Type: new 
Abstract: Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning High-Fidelity Cloth Animation via Skinning-Free Image Transfer</title>
<link>https://arxiv.org/abs/2512.05593</link>
<guid>https://arxiv.org/abs/2512.05593</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D garment deformation, skinning-free, vertex position and normal, high-frequency wrinkles, image-based texture encoding<br /><br />Summary:  
The paper introduces a novel skinning-free method for generating 3D garment deformations from given body poses, crucial for applications such as virtual try-on and extended reality. Traditional methods rely on linear blend skinning to capture low-frequency garment shapes and regress high-frequency wrinkles, but they suffer from misalignment due to lack of explicit skinning supervision. To overcome this, the authors independently estimate vertex positions for low-frequency shapes and vertex normals for high-frequency wrinkle details, enabling effective decoupling and direct supervision via garment geometry. To enhance animation quality, both vertex attributes are encoded as rendered texture images, facilitating 3D deformation through 2D image transfer. This approach leverages powerful pretrained image models to recover fine-grained wrinkle details while supporting multiple garment topologies without manual UV mapping. Additionally, a multimodal fusion strategy integrates constraints from both frequency modalities for robust garment deformation recovery from transferred images. Experimental results demonstrate that the method significantly improves animation realism across various garment types and outperforms state-of-the-art approaches by recovering finer wrinkles and more accurate garment shapes. <div>
arXiv:2512.05593v1 Announce Type: new 
Abstract: We present a novel method for generating 3D garment deformations from given body poses, which is key to a wide range of applications, including virtual try-on and extended reality. To simplify the cloth dynamics, existing methods mostly rely on linear blend skinning to obtain low-frequency posed garment shape and only regress high-frequency wrinkles. However, due to the lack of explicit skinning supervision, such skinning-based approach often produces misaligned shapes when posing the garment, consequently corrupts the high-frequency signals and fails to recover high-fidelity wrinkles. To tackle this issue, we propose a skinning-free approach by independently estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. To further improve the visual quality of animation, we propose to encode both vertex attributes as rendered texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image models to recover fine-grained visual details in wrinkles, while maintaining superior scalability for garments of diverse topologies without relying on manual UV partition. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and robustly recover deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves animation quality on various garment types and recovers finer wrinkles than state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2512.05597</link>
<guid>https://arxiv.org/abs/2512.05597</guid>
<content:encoded><![CDATA[
<div> Keywords: Fast SceneScript, multi-token prediction, 3D scene layout estimation, self-speculative decoding, confidence-guided decoding  

<br /><br />Summary:  
This paper introduces Fast SceneScript, a structured language model designed for efficient and accurate 3D scene layout estimation. Traditional perception-generalist language models rely on autoregressive next-token prediction, which is inherently slow due to sequential token generation. To address this, Fast SceneScript employs multi-token prediction (MTP), enabling the model to generate multiple tokens per decoding step and thereby significantly speeding up inference. However, generating multiple tokens simultaneously can introduce unreliable predictions, potentially affecting accuracy. To mitigate this, the authors adapt self-speculative decoding (SSD) tailored for structured language models and propose confidence-guided decoding (CGD), which uses an improved scoring mechanism to assess and filter token reliability effectively. Additionally, the method includes a parameter-efficient design to minimize the overhead introduced by MTP, adding only about 7.5% more parameters. Extensive experiments conducted on the ASE and Structured3D benchmarks validate the model's ability to produce up to nine tokens per decoder step with no loss in accuracy, demonstrating a good balance between speed and performance. This approach presents a valuable advancement in accelerating 3D scene layout tasks while maintaining state-of-the-art accuracy. <div>
arXiv:2512.05597v1 Announce Type: new 
Abstract: Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\sim7.5\%$ additional parameters.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections</title>
<link>https://arxiv.org/abs/2512.05610</link>
<guid>https://arxiv.org/abs/2512.05610</guid>
<content:encoded><![CDATA[
arXiv:2512.05610v1 Announce Type: new 
Abstract: Laser scanning has proven to be an invaluable tool in assessing the decomposition of forest environments. Mobile laser scanning (MLS) has shown to be highly promising for extremely accurate, tree level inventory. In this study, we present NormalView, a sensor-agnostic projection-based deep learning method for classifying tree species from point cloud data. NormalView embeds local geometric information into two-dimensional projections, in the form of normal vector estimates, and uses the projections as inputs to an image classification network, YOLOv11. In addition, we inspected the effect of multispectral radiometric intensity information on classification performance. We trained and tested our model on high-density MLS data (7 species, ~5000 pts/m^2), as well as high-density airborne laser scanning (ALS) data (9 species, >1000 pts/m^2). On the MLS data, NormalView achieves an overall accuracy (macro-average accuracy) of 95.5 % (94.8 %), and 91.8 % (79.1 %) on the ALS data. We found that having intensity information from multiple scanners provides benefits in tree species classification, and the best model on the multispectral ALS dataset was a model using intensity information from all three channels of the multispectral ALS. This study demonstrates that projection-based methods, when enhanced with geometric information and coupled with state-of-the-art image classification backbones, can achieve exceptional results. Crucially, these methods are sensor-agnostic, relying only on geometric information. Additionally, we publically release the MLS dataset used in the study.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model</title>
<link>https://arxiv.org/abs/2512.05613</link>
<guid>https://arxiv.org/abs/2512.05613</guid>
<content:encoded><![CDATA[
arXiv:2512.05613v1 Announce Type: new 
Abstract: Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experts-Guided Unbalanced Optimal Transport for ISP Learning from Unpaired and/or Paired Data</title>
<link>https://arxiv.org/abs/2512.05635</link>
<guid>https://arxiv.org/abs/2512.05635</guid>
<content:encoded><![CDATA[
arXiv:2512.05635v1 Announce Type: new 
Abstract: Learned Image Signal Processing (ISP) pipelines offer powerful end-to-end performance but are critically dependent on large-scale paired raw-to-sRGB datasets. This reliance on costly-to-acquire paired data remains a significant bottleneck. To address this challenge, we introduce a novel, unsupervised training framework based on Optimal Transport capable of training arbitrary ISP architectures in both unpaired and paired modes. We are the first to successfully apply Unbalanced Optimal Transport (UOT) for this complex, cross-domain translation task. Our UOT-based framework provides robustness to outliers in the target sRGB data, allowing it to discount atypical samples that would be prohibitively costly to map. A key component of our framework is a novel ``committee of expert discriminators,'' a hybrid adversarial regularizer. This committee guides the optimal transport mapping by providing specialized, targeted gradients to correct specific ISP failure modes, including color fidelity, structural artifacts, and frequency-domain realism. To demonstrate the superiority of our approach, we retrained existing state-of-the-art ISP architectures using our paired and unpaired setups. Our experiments show that while our framework, when trained in paired mode, exceeds the performance of the original paired methods across all metrics, our unpaired mode concurrently achieves quantitative and qualitative performance that rivals, and in some cases surpasses, the original paired-trained counterparts. The code and pre-trained models are available at: https://github.com/gosha20777/EGUOT-ISP.git.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective</title>
<link>https://arxiv.org/abs/2512.05651</link>
<guid>https://arxiv.org/abs/2512.05651</guid>
<content:encoded><![CDATA[
arXiv:2512.05651v1 Announce Type: new 
Abstract: The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title>
<link>https://arxiv.org/abs/2512.05663</link>
<guid>https://arxiv.org/abs/2512.05663</guid>
<content:encoded><![CDATA[
arXiv:2512.05663v1 Announce Type: new 
Abstract: Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features</title>
<link>https://arxiv.org/abs/2512.05669</link>
<guid>https://arxiv.org/abs/2512.05669</guid>
<content:encoded><![CDATA[
arXiv:2512.05669v1 Announce Type: new 
Abstract: Facial expression recognition is a crucial component in enhancing human-computer interaction and developing emotion-aware systems. Real-time detection and interpretation of facial expressions have become increasingly important for various applications, from user experience personalization to intelligent surveillance systems. This study presents a novel approach to real-time sequential facial expression recognition using deep learning and geometric features. The proposed method utilizes MediaPipe FaceMesh for rapid and accurate facial landmark detection. Geometric features, including Euclidean distances and angles, are extracted from these landmarks. Temporal dynamics are incorporated by analyzing feature differences between consecutive frames, enabling the detection of onset, apex, and offset phases of expressions. For classification, a ConvLSTM1D network followed by multilayer perceptron blocks is employed. The method's performance was evaluated on multiple publicly available datasets, including CK+, Oulu-CASIA (VIS and NIR), and MMI. Accuracies of 93%, 79%, 77%, and 68% were achieved respectively. Experiments with composite datasets were also conducted to assess the model's generalization capabilities. The approach demonstrated real-time applicability, processing approximately 165 frames per second on consumer-grade hardware. This research contributes to the field of facial expression analysis by providing a fast, accurate, and adaptable solution. The findings highlight the potential for further advancements in emotion-aware technologies and personalized user experiences, paving the way for more sophisticated human-computer interaction systems. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/facial-expression-analysis.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem</title>
<link>https://arxiv.org/abs/2512.05672</link>
<guid>https://arxiv.org/abs/2512.05672</guid>
<content:encoded><![CDATA[
arXiv:2512.05672v1 Announce Type: new 
Abstract: Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Unmixing with 3D Convolutional Sparse Coding and Projected Simplex Volume Maximization</title>
<link>https://arxiv.org/abs/2512.05674</link>
<guid>https://arxiv.org/abs/2512.05674</guid>
<content:encoded><![CDATA[
arXiv:2512.05674v1 Announce Type: new 
Abstract: Hyperspectral unmixing (HSU) aims to separate each pixel into its constituent endmembers and estimate their corresponding abundance fractions. This work presents an algorithm-unrolling-based network for the HSU task, named the 3D Convolutional Sparse Coding Network (3D-CSCNet), built upon a 3D CSC model. Unlike existing unrolling-based networks, our 3D-CSCNet is designed within the powerful autoencoder (AE) framework. Specifically, to solve the 3D CSC problem, we propose a 3D CSC block (3D-CSCB) derived through deep algorithm unrolling. Given a hyperspectral image (HSI), 3D-CSCNet employs the 3D-CSCB to estimate the abundance matrix. The use of 3D CSC enables joint learning of spectral and spatial relationships in the 3D HSI data cube. The estimated abundance matrix is then passed to the AE decoder to reconstruct the HSI, and the decoder weights are extracted as the endmember matrix. Additionally, we propose a projected simplex volume maximization (PSVM) algorithm for endmember estimation, and the resulting endmembers are used to initialize the decoder weights of 3D-CSCNet. Extensive experiments on three real datasets and one simulated dataset with three different signal-to-noise ratio (SNR) levels demonstrate that our 3D-CSCNet outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction</title>
<link>https://arxiv.org/abs/2512.05683</link>
<guid>https://arxiv.org/abs/2512.05683</guid>
<content:encoded><![CDATA[
arXiv:2512.05683v1 Announce Type: new 
Abstract: Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at https://github.com/janetkok/ZRNet.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</title>
<link>https://arxiv.org/abs/2512.05698</link>
<guid>https://arxiv.org/abs/2512.05698</guid>
<content:encoded><![CDATA[
arXiv:2512.05698v1 Announce Type: new 
Abstract: Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning</title>
<link>https://arxiv.org/abs/2512.05710</link>
<guid>https://arxiv.org/abs/2512.05710</guid>
<content:encoded><![CDATA[
arXiv:2512.05710v1 Announce Type: new 
Abstract: Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision</title>
<link>https://arxiv.org/abs/2512.05740</link>
<guid>https://arxiv.org/abs/2512.05740</guid>
<content:encoded><![CDATA[
arXiv:2512.05740v1 Announce Type: new 
Abstract: Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05746</link>
<guid>https://arxiv.org/abs/2512.05746</guid>
<content:encoded><![CDATA[
arXiv:2512.05746v1 Announce Type: new 
Abstract: Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>USV: Unified Sparsification for Accelerating Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05754</link>
<guid>https://arxiv.org/abs/2512.05754</guid>
<content:encoded><![CDATA[
arXiv:2512.05754v1 Announce Type: new 
Abstract: The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Efficient Point Cloud Segmentation with Active Learning</title>
<link>https://arxiv.org/abs/2512.05759</link>
<guid>https://arxiv.org/abs/2512.05759</guid>
<content:encoded><![CDATA[
arXiv:2512.05759v1 Announce Type: new 
Abstract: Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2512.05762</link>
<guid>https://arxiv.org/abs/2512.05762</guid>
<content:encoded><![CDATA[
arXiv:2512.05762v1 Announce Type: new 
Abstract: We present FNOpt, a self-supervised cloth simulation framework that formulates time integration as an optimization problem and trains a resolution-agnostic neural optimizer parameterized by a Fourier neural operator (FNO). Prior neural simulators often rely on extensive ground truth data or sacrifice fine-scale detail, and generalize poorly across resolutions and motion patterns. In contrast, FNOpt learns to simulate physically plausible cloth dynamics and achieves stable and accurate rollouts across diverse mesh resolutions and motion patterns without retraining. Trained only on a coarse grid with physics-based losses, FNOpt generalizes to finer resolutions, capturing fine-scale wrinkles and preserving rollout stability. Extensive evaluations on a benchmark cloth simulation dataset demonstrate that FNOpt outperforms prior learning-based approaches in out-of-distribution settings in both accuracy and robustness. These results position FNO-based meta-optimization as a compelling alternative to previous neural simulators for cloth, thus reducing the need for curated data and improving cross-resolution reliability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.05774</link>
<guid>https://arxiv.org/abs/2512.05774</guid>
<content:encoded><![CDATA[
arXiv:2512.05774v1 Announce Type: new 
Abstract: Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth</title>
<link>https://arxiv.org/abs/2512.05783</link>
<guid>https://arxiv.org/abs/2512.05783</guid>
<content:encoded><![CDATA[
arXiv:2512.05783v1 Announce Type: new 
Abstract: When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bring Your Dreams to Life: Continual Text-to-Video Customization</title>
<link>https://arxiv.org/abs/2512.05802</link>
<guid>https://arxiv.org/abs/2512.05802</guid>
<content:encoded><![CDATA[
arXiv:2512.05802v1 Announce Type: new 
Abstract: Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at https://github.com/JiahuaDong/CCVD.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</title>
<link>https://arxiv.org/abs/2512.05809</link>
<guid>https://arxiv.org/abs/2512.05809</guid>
<content:encoded><![CDATA[
arXiv:2512.05809v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2512.05814</link>
<guid>https://arxiv.org/abs/2512.05814</guid>
<content:encoded><![CDATA[
arXiv:2512.05814v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning</title>
<link>https://arxiv.org/abs/2512.05830</link>
<guid>https://arxiv.org/abs/2512.05830</guid>
<content:encoded><![CDATA[
arXiv:2512.05830v1 Announce Type: new 
Abstract: This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack</title>
<link>https://arxiv.org/abs/2512.05853</link>
<guid>https://arxiv.org/abs/2512.05853</guid>
<content:encoded><![CDATA[
arXiv:2512.05853v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edit-aware RAW Reconstruction</title>
<link>https://arxiv.org/abs/2512.05859</link>
<guid>https://arxiv.org/abs/2512.05859</guid>
<content:encoded><![CDATA[
arXiv:2512.05859v1 Announce Type: new 
Abstract: Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator</title>
<link>https://arxiv.org/abs/2512.05866</link>
<guid>https://arxiv.org/abs/2512.05866</guid>
<content:encoded><![CDATA[
arXiv:2512.05866v1 Announce Type: new 
Abstract: Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</title>
<link>https://arxiv.org/abs/2512.05905</link>
<guid>https://arxiv.org/abs/2512.05905</guid>
<content:encoded><![CDATA[
arXiv:2512.05905v1 Announce Type: new 
Abstract: Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \textbf{SCAIL} (\textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction</title>
<link>https://arxiv.org/abs/2512.05920</link>
<guid>https://arxiv.org/abs/2512.05920</guid>
<content:encoded><![CDATA[
arXiv:2512.05920v1 Announce Type: new 
Abstract: Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</title>
<link>https://arxiv.org/abs/2512.05922</link>
<guid>https://arxiv.org/abs/2512.05922</guid>
<content:encoded><![CDATA[
arXiv:2512.05922v1 Announce Type: new 
Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</title>
<link>https://arxiv.org/abs/2512.05927</link>
<guid>https://arxiv.org/abs/2512.05927</guid>
<content:encoded><![CDATA[
arXiv:2512.05927v1 Announce Type: new 
Abstract: Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition</title>
<link>https://arxiv.org/abs/2512.05928</link>
<guid>https://arxiv.org/abs/2512.05928</guid>
<content:encoded><![CDATA[
arXiv:2512.05928v1 Announce Type: new 
Abstract: Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition</title>
<link>https://arxiv.org/abs/2512.05936</link>
<guid>https://arxiv.org/abs/2512.05936</guid>
<content:encoded><![CDATA[
arXiv:2512.05936v1 Announce Type: new 
Abstract: In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</title>
<link>https://arxiv.org/abs/2512.05937</link>
<guid>https://arxiv.org/abs/2512.05937</guid>
<content:encoded><![CDATA[
arXiv:2512.05937v1 Announce Type: new 
Abstract: Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].
  Download: synset.de/datasets/synset-signset-ger/background-effect
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</title>
<link>https://arxiv.org/abs/2512.05941</link>
<guid>https://arxiv.org/abs/2512.05941</guid>
<content:encoded><![CDATA[
arXiv:2512.05941v1 Announce Type: new 
Abstract: Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2512.05960</link>
<guid>https://arxiv.org/abs/2512.05960</guid>
<content:encoded><![CDATA[
arXiv:2512.05960v1 Announce Type: new 
Abstract: Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EditThinker: Unlocking Iterative Reasoning for Any Image Editor</title>
<link>https://arxiv.org/abs/2512.05965</link>
<guid>https://arxiv.org/abs/2512.05965</guid>
<content:encoded><![CDATA[
arXiv:2512.05965v1 Announce Type: new 
Abstract: Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncVoice: Towards Video Dubbing with Vision-Augmented Pretrained TTS Model</title>
<link>https://arxiv.org/abs/2512.05126</link>
<guid>https://arxiv.org/abs/2512.05126</guid>
<content:encoded><![CDATA[
arXiv:2512.05126v1 Announce Type: cross 
Abstract: Video dubbing aims to generate high-fidelity speech that is precisely temporally aligned with the visual content. Existing methods still suffer from limitations in speech naturalness and audio-visual synchronization, and are limited to monolingual settings. To address these challenges, we propose SyncVoice, a vision-augmented video dubbing framework built upon a pretrained text-to-speech (TTS) model. By fine-tuning the TTS model on audio-visual data, we achieve strong audiovisual consistency. We propose a Dual Speaker Encoder to effectively mitigate inter-language interference in cross-lingual speech synthesis and explore the application of video dubbing in video translation scenarios. Experimental results show that SyncVoice achieves high-fidelity speech generation with strong synchronization performance, demonstrating its potential in video dubbing tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety</title>
<link>https://arxiv.org/abs/2512.05299</link>
<guid>https://arxiv.org/abs/2512.05299</guid>
<content:encoded><![CDATA[
arXiv:2512.05299v1 Announce Type: cross 
Abstract: Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support. This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets. By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view. The system also enables multi-headset coordination through shared world anchoring. Evaluated in real-world pedestrian interactions with e-scooters and vehicles (180 trials), ARCAS nearly doubled pedestrians' time-to-collision and increased counterparts' reaction margins by up to 4x compared to unaided-eye conditions. Results validate the feasibility and effectiveness of LiDAR-driven AR guidance and highlight the potential of wearable AR as a promising next-generation safety tool for urban mobility.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EXR: An Interactive Immersive EHR Visualization in Extended Reality</title>
<link>https://arxiv.org/abs/2512.05438</link>
<guid>https://arxiv.org/abs/2512.05438</guid>
<content:encoded><![CDATA[
arXiv:2512.05438v1 Announce Type: cross 
Abstract: This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interleaved Latent Visual Reasoning with Selective Perceptual Modeling</title>
<link>https://arxiv.org/abs/2512.05665</link>
<guid>https://arxiv.org/abs/2512.05665</guid>
<content:encoded><![CDATA[
arXiv:2512.05665v1 Announce Type: cross 
Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation</title>
<link>https://arxiv.org/abs/2512.05812</link>
<guid>https://arxiv.org/abs/2512.05812</guid>
<content:encoded><![CDATA[
arXiv:2512.05812v1 Announce Type: cross 
Abstract: Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma</title>
<link>https://arxiv.org/abs/2512.05824</link>
<guid>https://arxiv.org/abs/2512.05824</guid>
<content:encoded><![CDATA[
arXiv:2512.05824v1 Announce Type: cross 
Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically-Based Simulation of Automotive LiDAR</title>
<link>https://arxiv.org/abs/2512.05932</link>
<guid>https://arxiv.org/abs/2512.05932</guid>
<content:encoded><![CDATA[
arXiv:2512.05932v1 Announce Type: cross 
Abstract: We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.
  Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.
  Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01{\deg} resolution, which marks the best available resolution for measuring the beam pattern.
  The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.05955</link>
<guid>https://arxiv.org/abs/2512.05955</guid>
<content:encoded><![CDATA[
arXiv:2512.05955v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</title>
<link>https://arxiv.org/abs/2512.05959</link>
<guid>https://arxiv.org/abs/2512.05959</guid>
<content:encoded><![CDATA[
arXiv:2512.05959v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</title>
<link>https://arxiv.org/abs/2405.05258</link>
<guid>https://arxiv.org/abs/2405.05258</guid>
<content:encoded><![CDATA[
arXiv:2405.05258v3 Announce Type: replace 
Abstract: Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Direction-Aware Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2406.02037</link>
<guid>https://arxiv.org/abs/2406.02037</guid>
<content:encoded><![CDATA[
arXiv:2406.02037v3 Announce Type: replace 
Abstract: Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on edge and shape features, but ignore the richer structural differences and detailed information embedded in high-frequency components from different directions, thereby failing to fully exploit the value of high-frequency directional features in target perception. To address this limitation, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, to fully mine the high-frequency directional features, on the one hand, a high-frequency direction injection (HFDI) module without trainable parameters is constructed to inject the high-frequency directional information of the original image into the network. On the other hand, a multi-scale direction-aware (MSDA) module is constructed, which promotes the full extraction of local relations at different scales and the full perception of key features in different directions. In addition, considering the characteristics of infrared small targets, we construct a feature aggregation (FA) structure to address target disappearance in high-level feature maps, and a feature calibration fusion (FCF) module to alleviate feature bias during cross-layer feature fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on multiple public datasets. The code can be available at https://github.com/YuChuang1205/MSDA-Net
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMotion-LLM: Instruction-Conditioned Trajectory Generation</title>
<link>https://arxiv.org/abs/2406.06211</link>
<guid>https://arxiv.org/abs/2406.06211</guid>
<content:encoded><![CDATA[
arXiv:2406.06211v3 Announce Type: replace 
Abstract: We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scene-aware Models Adaptation Scheme for Cross-scene Online Inference on Mobile Devices</title>
<link>https://arxiv.org/abs/2407.03331</link>
<guid>https://arxiv.org/abs/2407.03331</guid>
<content:encoded><![CDATA[
arXiv:2407.03331v2 Announce Type: replace 
Abstract: Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmanned aerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2407.16344</link>
<guid>https://arxiv.org/abs/2407.16344</guid>
<content:encoded><![CDATA[
arXiv:2407.16344v5 Announce Type: replace 
Abstract: High frame-rate (HFR) videos of action recognition improve fine-grained expression while reducing the spatio-temporal relation and motion information density. Thus, large amounts of video samples are continuously required for traditional data-driven training. However, samples are not always sufficient in real-world scenarios, promoting few-shot action recognition (FSAR) research. We observe that most recent FSAR works build spatio-temporal relation of video samples via temporal alignment after spatial feature extraction, cutting apart spatial and temporal features within samples. They also capture motion information via narrow perspectives between adjacent frames without considering density, leading to insufficient motion information capturing. Therefore, we propose a novel plug-and-play architecture for FSAR called Spatio-tempOral frAme tuPle enhancer (SOAP) in this paper. The model we designed with such architecture refers to SOAP-Net. Temporal connections between different feature channels and spatio-temporal relation of features are considered instead of simple feature extraction. Comprehensive motion information is also captured, using frame tuples with multiple frames containing more motion information than adjacent frames. Combining frame tuples of diverse frame counts further provides a broader perspective. SOAP-Net achieves new state-of-the-art performance across well-known benchmarks such as SthSthV2, Kinetics, UCF101, and HMDB51. Extensive empirical evaluations underscore the competitiveness, pluggability, generalization, and robustness of SOAP. The code is released at https://github.com/wenbohuang1002/SOAP.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLANesT-3D: A new annotated dataset for segmentation of 3D plant point clouds</title>
<link>https://arxiv.org/abs/2407.21150</link>
<guid>https://arxiv.org/abs/2407.21150</guid>
<content:encoded><![CDATA[
arXiv:2407.21150v2 Announce Type: replace 
Abstract: Creation of new annotated public datasets is crucial in helping advances in 3D computer vision and machine learning meet their full potential for automatic interpretation of 3D plant models. Despite the proliferation of deep neural network architectures for segmentation and phenotyping of 3D plant models in the last decade, the amount of data, and diversity in terms of species and data acquisition modalities are far from sufficient for evaluation of such tools for their generalization ability. To contribute to closing this gap, we introduce PLANesT-3D; a new annotated dataset of 3D color point clouds of plants. PLANesT-3D is composed of 34 point cloud models representing 34 real plants from three different plant species: \textit{Capsicum annuum}, \textit{Rosa kordana}, and \textit{Ribes rubrum}. Both semantic labels in terms of "leaf" and "stem", and organ instance labels were manually annotated for the full point clouds. PLANesT-3D introduces diversity to existing datasets by adding point clouds of two new species and providing 3D data acquired with the low-cost SfM/MVS technique as opposed to laser scanning or expensive setups. Point clouds reconstructed with SfM/MVS modality exhibit challenges such as missing data, variable density, and illumination variations. As an additional contribution, SP-LSCnet, a novel semantic segmentation method that is a combination of unsupervised superpoint extraction and a 3D point-based deep learning approach is introduced and evaluated on the new dataset. The advantages of SP-LSCnet over other deep learning methods are its modular structure and increased interpretability. Two existing deep neural network architectures, PointNet++ and RoseSegNet, were also tested on the point clouds of PLANesT-3D for semantic segmentation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</title>
<link>https://arxiv.org/abs/2408.01826</link>
<guid>https://arxiv.org/abs/2408.01826</guid>
<content:encoded><![CDATA[
arXiv:2408.01826v5 Announce Type: replace 
Abstract: Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2408.09449</link>
<guid>https://arxiv.org/abs/2408.09449</guid>
<content:encoded><![CDATA[
arXiv:2408.09449v3 Announce Type: replace 
Abstract: In whole slide images (WSIs) analysis, attention-based multi-instance learning (MIL) models are susceptible to spurious correlations and degrade under domain shift. These methods may assign high attention weights to non-tumor regions, such as staining biases or artifacts, leading to unreliable tumor region localization. In this paper, we revisit max-pooling-based MIL methods from a causal perspective. Under mild assumptions, our theoretical results demonstrate that max-pooling encourages the model to focus on causal factors while ignoring bias-related factors. Furthermore, we discover that existing max-pooling-based methods may overfit the training set through rote memorization of instance features and fail to learn meaningful patterns. To address these issues, we propose FocusMIL, which couples max-pooling with an instance-level variational information bottleneck (VIB) to learn compact, predictive latent representations, and employs a multi-bag mini-batch scheme to stabilize optimization. We conduct comprehensive experiments on three real-world datasets and one semi-synthetic dataset. The results show that, by capturing causal factors, FocusMIL exhibits significant advantages in out-of-distribution scenarios and instance-level tumor region localization tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-Guided Semantic Pseudo-LiDAR Point Generation for 3D Object Detection</title>
<link>https://arxiv.org/abs/2409.14985</link>
<guid>https://arxiv.org/abs/2409.14985</guid>
<content:encoded><![CDATA[
arXiv:2409.14985v3 Announce Type: replace 
Abstract: In autonomous driving scenarios, accurate perception is becoming an even more critical task for safe navigation. While LiDAR provides precise spatial data, its inherent sparsity makes it difficult to detect small or distant objects. Existing methods try to address this by generating additional points within a Region of Interest (RoI), but relying on LiDAR alone often leads to false positives and a failure to recover meaningful structures. To address these limitations, we propose Image-Guided Semantic Pseudo-LiDAR Point Generation model, called ImagePG, a novel framework that leverages rich RGB image features to generate dense and semantically meaningful 3D points. Our framework includes an Image-Guided RoI Points Generation (IG-RPG) module, which creates pseudo-points guided by image features, and an Image-Aware Occupancy Prediction Network (I-OPN), which provides spatial priors to guide point placement. A multi-stage refinement (MR) module further enhances point quality and detection robustness. To the best of our knowledge, ImagePG is the first method to directly leverage image features for point generation. Extensive experiments on the KITTI and Waymo datasets demonstrate that ImagePG significantly improves the detection of small and distant objects like pedestrians and cyclists, reducing false positives by nearly 50%. On the KITTI benchmark, our framework improves mAP by +1.38%p (car), +7.91%p (pedestrian), and +5.21%p (cyclist) on the test set over the baseline, achieving state-of-the-art cyclist performance on the KITTI leaderboard. The code is available at: https://github.com/MS-LIMA/ImagePG
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point-PNG: Conditional Pseudo-Negatives Generation for Point Cloud Pre-Training</title>
<link>https://arxiv.org/abs/2409.15832</link>
<guid>https://arxiv.org/abs/2409.15832</guid>
<content:encoded><![CDATA[
arXiv:2409.15832v3 Announce Type: replace 
Abstract: We propose Point-PNG, a novel self-supervised learning framework that generates conditional pseudo-negatives in the latent space to learn point cloud representations that are both discriminative and transformation-sensitive. Conventional self-supervised learning methods focus on achieving invariance, discarding transformation-specific information. Recent approaches incorporate transformation sensitivity by explicitly modeling relationships between original and transformed inputs. However, they often suffer from an invariant-collapse phenomenon, where the predictor degenerates into identity mappings, resulting in latent representations with limited variation across transformations. To address this, we propose Point-PNG that explicitly penalizes invariant collapse through pseudo-negatives generation, enabling the network to capture richer transformation cues while preserving discriminative representations. To this end, we introduce a parametric network, COnditional Pseudo-Negatives Embedding (COPE), which learns localized displacements induced by transformations within the latent space. A key challenge arises when jointly training COPE with the MAE, as it tends to converge to trivial identity mappings. To overcome this, we design a loss function based on pseudo-negatives conditioned on the transformation, which penalizes such trivial invariant solutions and enforces meaningful representation learning. We validate Point-PNG on shape classification and relative pose estimation tasks, showing competitive performance on ModelNet40 and ScanObjectNN under challenging evaluation protocols, and achieving superior accuracy in relative pose estimation compared to supervised baselines.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Eulerian Scene Flow Fields</title>
<link>https://arxiv.org/abs/2410.02031</link>
<guid>https://arxiv.org/abs/2410.02031</guid>
<content:encoded><![CDATA[
arXiv:2410.02031v3 Announce Type: replace 
Abstract: We reframe scene flow as the task of estimating a continuous space-time ODE that describes motion for an entire observation sequence, represented with a neural prior. Our method, EulerFlow, optimizes this neural prior estimate against several multi-observation reconstruction objectives, enabling high quality scene flow estimation via pure self-supervision on real-world data. EulerFlow works out-of-the-box without tuning across multiple domains, including large-scale autonomous driving scenes and dynamic tabletop settings. Remarkably, EulerFlow produces high quality flow estimates on small, fast moving objects like birds and tennis balls, and exhibits emergent 3D point tracking behavior by solving its estimated ODE over long-time horizons. On the Argoverse 2 2024 Scene Flow Challenge, EulerFlow outperforms all prior art, surpassing the next-best unsupervised method by more than 2.5x, and even exceeding the next-best supervised method by over 10%.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedDiff-FM: A Diffusion-based Foundation Model for Versatile Medical Image Applications</title>
<link>https://arxiv.org/abs/2410.15432</link>
<guid>https://arxiv.org/abs/2410.15432</guid>
<content:encoded><![CDATA[
arXiv:2410.15432v3 Announce Type: replace 
Abstract: Diffusion models have achieved significant success in both natural image and medical image domains, encompassing a wide range of applications. Previous investigations in medical images have often been constrained to specific anatomical regions, particular applications, and limited datasets, resulting in isolated diffusion models. This paper introduces a diffusion-based foundation model to address a diverse range of medical image tasks, namely MedDiff-FM. MedDiff-FM leverages 3D CT images from multiple publicly available datasets, covering anatomical regions from head to abdomen, to pre-train a diffusion foundation model, and explores the capabilities of the diffusion foundation model across a variety of application scenarios. The diffusion foundation model handles multi-level integrated image processing both at the image-level and patch-level, utilizes position embedding to establish multi-level spatial relationships, and leverages region classes and anatomical structures to capture certain anatomical regions. MedDiff-FM manages several downstream tasks seamlessly, including image denoising, anomaly detection, and image synthesis. MedDiff-FM is also capable of performing super-resolution, lesion generation, and lesion inpainting by rapidly fine-tuning the diffusion foundation model using ControlNet with task-specific conditions. The experimental results demonstrate the effectiveness of MedDiff-FM in addressing diverse downstream medical image tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dressing the Imagination: A Dataset for AI-Powered Translation of Text into Fashion Outfits and A Novel KAN Adapter for Enhanced Feature Adaptation</title>
<link>https://arxiv.org/abs/2411.13901</link>
<guid>https://arxiv.org/abs/2411.13901</guid>
<content:encoded><![CDATA[
arXiv:2411.13901v4 Announce Type: replace 
Abstract: Specialized datasets that capture the fashion industry's rich language and styling elements can boost progress in AI-driven fashion design. We present FLORA, (Fashion Language Outfit Representation for Apparel Generation), the first comprehensive dataset containing 4,330 curated pairs of fashion outfits and corresponding textual descriptions. Each description utilizes industry-specific terminology and jargon commonly used by professional fashion designers, providing precise and detailed insights into the outfits. Hence, the dataset captures the delicate features and subtle stylistic elements necessary to create high-fidelity fashion designs.
  We demonstrate that fine-tuning generative models on the FLORA dataset significantly enhances their capability to generate accurate and stylistically rich images from textual descriptions of fashion sketches. FLORA will catalyze the creation of advanced AI models capable of comprehending and producing subtle, stylistically rich fashion designs. It will also help fashion designers and end-users to bring their ideas to life.
  As a second orthogonal contribution, we introduce NeRA (Nonlinear low-rank Expressive Representation Adapter), a novel adapter architecture based on Kolmogorov-Arnold Networks (KAN). Unlike traditional PEFT techniques such as LoRA, LoKR, DoRA, and LoHA that use MLP adapters, NeRA uses learnable spline-based nonlinear transformations, enabling superior modeling of complex semantic relationships, achieving strong fidelity, faster convergence and semantic alignment. Extensive experiments on our proposed FLORA and LAION-5B datasets validate the superiority of NeRA over existing adapters.
  We will open-source both the FLORA dataset and our implementation code.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</title>
<link>https://arxiv.org/abs/2412.07755</link>
<guid>https://arxiv.org/abs/2412.07755</guid>
<content:encoded><![CDATA[
arXiv:2412.07755v3 Announce Type: replace 
Abstract: Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships, and not dynamic awareness of motion and space, i.e., reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset utilizing 3D simulators, comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos -- even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM</title>
<link>https://arxiv.org/abs/2503.04504</link>
<guid>https://arxiv.org/abs/2503.04504</guid>
<content:encoded><![CDATA[
arXiv:2503.04504v4 Announce Type: replace 
Abstract: Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive results on VAD benchmarks, achieving state-of-the-art performance on UBnormal and UCF-Crime and surpassing other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2503.08485</link>
<guid>https://arxiv.org/abs/2503.08485</guid>
<content:encoded><![CDATA[
arXiv:2503.08485v3 Announce Type: replace 
Abstract: Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense occupancy decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and once trained, such models struggle to adapt to varying voxel resolutions or novel object categories without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-Occ. Our method incrementally constructs, optimizes and voxelizes time-aware 3D Gaussians from raw sensor streams by integrating vision foundation models (VFMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at arbitrary user-specified resolutions, while the generalization ability of VFMs enables accurate perception and open-vocabulary recognition, without any network training or fine-tuning. Specifically, TT-Occ operates in a lift-track-voxelize symphony: We first lift the geometry and semantics of surrounding-view extracted from VFMs to instantiate Gaussians at 3D space; Next, we track dynamic Gaussians while accumulating static ones to complete the scene and enforce temporal consistency; Finally, we voxelize the optimized Gaussians to generate occupancy prediction. Optionally, inherent noise in VFM predictions and tracking is mitigated by periodically smoothing neighboring Gaussians during optimization. To validate the generality and effectiveness of our framework, we offer two variants: one LiDAR-based and one vision-centric, and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying voxel resolutions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Self-Supervised Video Alignment and Action Segmentation</title>
<link>https://arxiv.org/abs/2503.16832</link>
<guid>https://arxiv.org/abs/2503.16832</guid>
<content:encoded><![CDATA[
arXiv:2503.16832v3 Announce Type: replace 
Abstract: We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused Gromov-Wasserstein optimal transport formulation with a structural prior, which trains efficiently on GPUs and needs only a few iterations for solving the optimal transport problem. Our single-task method achieves the state-of-the-art performance on multiple video alignment benchmarks and outperforms VAVA, which relies on a traditional Kantorovich optimal transport formulation with an optimality prior. Furthermore, we extend our approach by proposing a unified optimal transport framework for joint self-supervised video alignment and action segmentation, which requires training and storing a single model and saves both time and memory consumption as compared to two different single-task models. Extensive evaluations on several video alignment and action segmentation datasets demonstrate that our multi-task method achieves comparable video alignment yet superior action segmentation results over previous methods in video alignment and action segmentation respectively. Finally, to the best of our knowledge, this is the first work to unify video alignment and action segmentation into a single model. Our code is available on our research website: https://retrocausal.ai/research/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Ordinal Bias in Action Recognition for Instructional Videos</title>
<link>https://arxiv.org/abs/2504.06580</link>
<guid>https://arxiv.org/abs/2504.06580</guid>
<content:encoded><![CDATA[
arXiv:2504.06580v2 Announce Type: replace 
Abstract: Action recognition models have achieved promising results in understanding instructional videos. However, they often rely on dominant, dataset-specific action sequences rather than true video comprehension, a problem that we define as ordinal bias. To address this issue, we propose two effective video manipulation methods: Action Masking, which masks frames of frequently co-occurring actions, and Sequence Shuffling, which randomizes the order of action segments. Through comprehensive experiments, we demonstrate that current models exhibit significant performance drops when confronted with nonstandard action sequences, underscoring their vulnerability to ordinal bias. Our findings emphasize the importance of rethinking evaluation strategies and developing models capable of generalizing beyond fixed action patterns in diverse instructional videos.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartQA-X: Generating Explanations for Visual Chart Reasoning</title>
<link>https://arxiv.org/abs/2504.13275</link>
<guid>https://arxiv.org/abs/2504.13275</guid>
<content:encoded><![CDATA[
arXiv:2504.13275v4 Announce Type: replace 
Abstract: The ability to explain complex information from chart images is vital for effective data-driven decision-making. In this work, we address the challenge of generating detailed explanations alongside answering questions about charts. We present ChartQA-X, a comprehensive dataset comprising 30,799 chart samples across four chart types, each paired with contextually relevant questions, answers, and explanations. Explanations are generated and selected based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our human evaluation with 245 participants shows that model-generated explanations in ChartQA-X surpass human-written explanations in accuracy and logic and are comparable in terms of clarity and overall quality. Moreover, models fine-tuned on ChartQA-X show substantial improvements across various metrics, including absolute gains of up to 24.57 points in explanation quality, 18.96 percentage points in question-answering accuracy, and 14.75 percentage points on unseen benchmarks for the same task. By integrating explanatory narratives with answers, our approach enables agents to convey complex visual information more effectively, improving comprehension and greater trust in the generated responses.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3DHMR: Monocular 3D Hand Mesh Recovery</title>
<link>https://arxiv.org/abs/2505.20058</link>
<guid>https://arxiv.org/abs/2505.20058</guid>
<content:encoded><![CDATA[
arXiv:2505.20058v2 Announce Type: replace 
Abstract: Monocular 3D hand mesh recovery is challenging due to high degrees of freedom of hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are either inefficient or less straightforward for predicting the position of 3D mesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh Recovery (M3DHMR) to directly estimate the positions of hand mesh vertices. M3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral decoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region of Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the weights based on the vertex positions and extract the vertex features in both spatial and channel dimensions. On the other hand, ROI Layer utilizes the physical information and refines mesh vertices in each predefined hand region separately. Extensive experiments on popular dataset FreiHAND demonstrate that M3DHMR significantly outperforms state-of-the-art real-time methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Question Answering via only 2D Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.22143</link>
<guid>https://arxiv.org/abs/2505.22143</guid>
<content:encoded><![CDATA[
arXiv:2505.22143v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning</title>
<link>https://arxiv.org/abs/2506.02738</link>
<guid>https://arxiv.org/abs/2506.02738</guid>
<content:encoded><![CDATA[
arXiv:2506.02738v3 Announce Type: replace 
Abstract: In biomedical vision-language modeling, datasets are typically mined from scientific literature, pairing compound figures with captions that are short, context-dependent, and oftern partially informative. Prior work on subfigure extraction has been limited in both dataset size and generalizability. In addition, no existing effort has incorporated rich medical context in image-text pairs. We revisit data curation as a foundational component of effective biomedical representation learning. Our data curation process integrates transformer-based subfigure detection, subcaption extraction, and contextual text enrichment derived from inline references. Our subfigure extraction model, trained on a corpus of 500,000 compound figures, achieves state-of-the-art performance on real and synthetic benchmarks. Using this process, we curate and release Open-PMC-18M, a large-scale high-fidelity biomedical dataset comprising 18 million image-text pairs, spanning radiology, microscopy, and visible light photography. We train vision-language models on our dataset and perform extensive evaluation on 6 retrieval and 19 zero-shot classification tasks across three major modalities. The models trained on our dataset set a new state-of-the-art results in medical representation learning. We release our dataset, models, and code to support reproducible benchmarks and further study into biomedical vision-language modeling and representation learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Validation for Robust Few-Shot Recognition</title>
<link>https://arxiv.org/abs/2506.04713</link>
<guid>https://arxiv.org/abs/2506.04713</guid>
<content:encoded><![CDATA[
arXiv:2506.04713v2 Announce Type: replace 
Abstract: Few-Shot Recognition (FSR) tackles classification tasks by training with minimal task-specific labeled data. Prevailing methods adapt or finetune a pretrained Vision-Language Model (VLM) and augment the scarce training data by retrieving task-relevant but noisy samples from open data sources. The finetuned VLM generalizes decently well to the task-specific in-distribution (ID) test data but struggles with out-of-distribution (OOD) test data. This motivates our study of robust FSR with VLM finetuning. The core challenge of FSR is data scarcity, extending beyond limited training data to a complete lack of validation data. We identify a key paradox as a potential solution: repurposing the retrieved open data for validation. As such retrieved data are inherently OOD compared with the task-specific ID training data, finetuned VLMs yield degraded performance on the retrieved data. This causes the validation logic to favor the pretrained model without any finetuning, hindering improvements w.r.t generalization. To resolve this dilemma, we introduce a novel validation strategy that harmonizes performance gain and degradation on the few-shot ID data and the retrieved data, respectively. Our validation enables parameter selection for partial finetuning and checkpoint selection, mitigating overfitting and improving test-data generalization. We unify this strategy with robust learning into a cohesive framework: Validation-Enabled Stage-wise Tuning (VEST). Extensive experiments on the established ImageNet OOD benchmarks show that VEST significantly outperforms existing VLM adaptation methods, achieving state-of-the-art FSR performance on both ID and OOD data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion</title>
<link>https://arxiv.org/abs/2506.15747</link>
<guid>https://arxiv.org/abs/2506.15747</guid>
<content:encoded><![CDATA[
arXiv:2506.15747v2 Announce Type: replace 
Abstract: The single-view image guided point cloud completion (SVIPC) task aims to reconstruct a complete point cloud from a partial input with the help of a single-view image. While previous works have demonstrated the effectiveness of this multimodal approach, the fundamental necessity of image guidance remains largely unexamined. To explore this, we propose a strong baseline approach for SVIPC based on an attention-based multi-branch encoder-decoder network that only takes partial point clouds as input, view-free. Our hierarchical self-fusion mechanism, driven by cross-attention and self-attention layers, effectively integrates information across multiple streams, enriching feature representations and strengthening the networks ability to capture geometric structures. Extensive experiments and ablation studies on the ShapeNet-ViPC dataset demonstrate that our view-free framework performs superiorly to state-of-the-art SVIPC methods. We hope our findings provide new insights into the development of multimodal learning in SVIPC. Our demo code will be available at https://github.com/Zhang-VISLab.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Martian World Model: Controllable Video Synthesis with Physically Accurate 3D Reconstructions</title>
<link>https://arxiv.org/abs/2507.07978</link>
<guid>https://arxiv.org/abs/2507.07978</guid>
<content:encoded><![CDATA[
arXiv:2507.07978v2 Announce Type: replace 
Abstract: Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains</title>
<link>https://arxiv.org/abs/2507.13326</link>
<guid>https://arxiv.org/abs/2507.13326</guid>
<content:encoded><![CDATA[
arXiv:2507.13326v2 Announce Type: replace 
Abstract: Hand-object interaction detection remains an open challenge in real-time applications, where intuitive user experiences depend on fast and accurate detection of interactions with surrounding objects. We propose an efficient approach for detecting hand-objects interactions from streaming egocentric vision that operates in real time. Our approach consists of an action recognition module and an object detection module for identifying active objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object. We implement our models in a cascaded architecture where the action recognition and object detection modules operate sequentially. When the action recognition predicts a contact state, it activates the object detection module, which in turn performs inference on the relevant frame to detect and classify the active object.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression</title>
<link>https://arxiv.org/abs/2507.14997</link>
<guid>https://arxiv.org/abs/2507.14997</guid>
<content:encoded><![CDATA[
arXiv:2507.14997v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based regression tasks, but current approaches face key limitations. Recent methods fine-tune MLLMs using preset output vocabularies and generic task-level prompts (e.g., "How would you rate this image?"), assuming this mimics human rating behavior. \textbf{Our analysis reveals that these approaches provide no benefit over image-only training}. Models using preset vocabularies and generic prompts perform equivalently to image-only models, failing to leverage semantic understanding from textual input. We propose \textbf{Regression via Transformer-Based Classification} (RvTC), which replaces vocabulary-constrained classification with a flexible bin-based approach. Unlike approaches that address discretization errors through complex distributional modeling, RvTC eliminates manual vocabulary crafting through straightforward bin increase, achieving state-of-the-art performance on four image assessment datasets using only images. \textbf{More importantly, we demonstrate that data-specific prompts dramatically improve performance}. Unlike generic task descriptions, prompts containing semantic information about specific images enable MLLMs to leverage cross-modal understanding. On the AVA dataset, adding challenge titles to prompts substantially improves our already state-of-the-art image-only baseline. We demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that MLLMs benefit from semantic prompt information, surpassing mere statistical biases. We validate RvTC across two different MLLM architectures, demonstrating consistent improvements and method generalizability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perspective-Invariant 3D Object Detection</title>
<link>https://arxiv.org/abs/2507.17665</link>
<guid>https://arxiv.org/abs/2507.17665</guid>
<content:encoded><![CDATA[
arXiv:2507.17665v2 Announce Type: replace 
Abstract: With the rise of robotics, LiDAR-based 3D object detection has garnered significant attention in both academia and industry. However, existing datasets and methods predominantly focus on vehicle-mounted platforms, leaving other autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET, the first benchmark featuring LiDAR data and 3D bounding box annotations collected from multiple platforms: vehicle, quadruped, and drone, thereby facilitating research in 3D object detection for non-vehicle platforms as well as cross-platform 3D detection. Based on Pi3DET, we propose a novel cross-platform adaptation framework that transfers knowledge from the well-studied vehicle platform to other platforms. This framework achieves perspective-invariant 3D detection through robust alignment at both geometric and feature levels. Additionally, we establish a benchmark to evaluate the resilience and robustness of current 3D detectors in cross-platform scenarios, providing valuable insights for developing adaptive 3D perception systems. Extensive experiments validate the effectiveness of our approach on challenging cross-platform tasks, demonstrating substantial gains over existing adaptation methods. We hope this work paves the way for generalizable and unified 3D perception systems across diverse and complex environments. Our Pi3DET dataset, cross-platform benchmark suite, and annotation toolkit have been made publicly available.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</title>
<link>https://arxiv.org/abs/2509.01183</link>
<guid>https://arxiv.org/abs/2509.01183</guid>
<content:encoded><![CDATA[
arXiv:2509.01183v2 Announce Type: replace 
Abstract: High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2509.16054</link>
<guid>https://arxiv.org/abs/2509.16054</guid>
<content:encoded><![CDATA[
arXiv:2509.16054v2 Announce Type: replace 
Abstract: Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level  token and multiple cluster-specific  tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the  token and  tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the  token's ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM's hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-CECE: Visual Counterfactual Explanations via Conceptual Edits</title>
<link>https://arxiv.org/abs/2509.16567</link>
<guid>https://arxiv.org/abs/2509.16567</guid>
<content:encoded><![CDATA[
arXiv:2509.16567v2 Announce Type: replace 
Abstract: Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning</title>
<link>https://arxiv.org/abs/2509.19552</link>
<guid>https://arxiv.org/abs/2509.19552</guid>
<content:encoded><![CDATA[
arXiv:2509.19552v3 Announce Type: replace 
Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Grounding IDs: How External Cues Shape Multimodal Binding</title>
<link>https://arxiv.org/abs/2509.24072</link>
<guid>https://arxiv.org/abs/2509.24072</guid>
<content:encoded><![CDATA[
arXiv:2509.24072v3 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as consistent within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism that explains how external cues enhance multimodal binding and offer both interpretability and practical improvements.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZQBA: Zero Query Black-box Adversarial Attack</title>
<link>https://arxiv.org/abs/2510.00769</link>
<guid>https://arxiv.org/abs/2510.00769</guid>
<content:encoded><![CDATA[
arXiv:2510.00769v2 Announce Type: replace 
Abstract: Current black-box adversarial attacks either require multiple queries or diffusion models to produce adversarial samples that can impair the target model performance. However, these methods require training a surrogate loss or diffusion models to produce adversarial samples, which limits their applicability in real-world settings. Thus, we propose a Zero Query Black-box Adversarial (ZQBA) attack that exploits the representations of Deep Neural Networks (DNNs) to fool other networks. Instead of requiring thousands of queries to produce deceiving adversarial samples, we use the feature maps obtained from a DNN and add them to clean images to impair the classification of a target model. The results suggest that ZQBA can transfer the adversarial samples to different models and across various datasets, namely CIFAR and Tiny ImageNet. The experiments also show that ZQBA is more effective than state-of-the-art black-box attacks with a single query, while maintaining the imperceptibility of perturbations, evaluated both quantitatively (SSIM) and qualitatively, emphasizing the vulnerabilities of employing DNNs in real-world contexts. All the source code is available at https://github.com/Joana-Cabral/ZQBA.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging</title>
<link>https://arxiv.org/abs/2510.01498</link>
<guid>https://arxiv.org/abs/2510.01498</guid>
<content:encoded><![CDATA[
arXiv:2510.01498v2 Announce Type: replace 
Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming</title>
<link>https://arxiv.org/abs/2510.01660</link>
<guid>https://arxiv.org/abs/2510.01660</guid>
<content:encoded><![CDATA[
arXiv:2510.01660v4 Announce Type: replace 
Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for every new source-and-target pair, resulting in the number of training parameters and storage memory growing linearly with each new pair, and also preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases, we propose making use of domain-specific textural bias for domain adaptation via visual reprogramming, namely VirDA. Instead of fine-tuning the full backbone, VirDA prepends a domain-specific visual reprogramming layer to the backbone. This layer produces visual prompts that act as an added textural bias to the input image, adapting its "style" to a target domain. To optimize these visual reprogramming layers, we use multiple objective functions that optimize the intra- and inter-domain distribution differences when domain-adapting visual prompts are applied. This process does not require modifying the backbone parameters, allowing the same backbone to be reused across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M trainable parameters. VirDA surpasses PDA, the state-of-the-art parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8% of their trainable parameters. Relative to the strongest current methods (PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only 2.2% and 1.1% accuracy, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoControl: Temporal Attention Guidance for Text-to-Video Models</title>
<link>https://arxiv.org/abs/2510.02226</link>
<guid>https://arxiv.org/abs/2510.02226</guid>
<content:encoded><![CDATA[
arXiv:2510.02226v2 Announce Type: replace 
Abstract: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal pattern with a control signal (correlation), adjusting its strength where visibility is required (magnitude), and preserving semantic consistency (entropy). TempoControl provides precise temporal control while maintaining high video quality and diversity. We demonstrate its effectiveness across various applications, including temporal reordering of single and multiple objects, action timing, and audio-aligned video generation. Please see our project page for more details: https://shira-schiber.github.io/TempoControl/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeleEgo: Benchmarking Egocentric AI Assistants in the Wild</title>
<link>https://arxiv.org/abs/2510.23981</link>
<guid>https://arxiv.org/abs/2510.23981</guid>
<content:encoded><![CDATA[
arXiv:2510.23981v3 Announce Type: replace 
Abstract: Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work \& study, lifestyle \& routines, social activities, and outings \& culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.TeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting. We propose Real-Time Accuracy (RTA) to jointly capture correctness and responsiveness under tight decision windows, and Memory Persistence Time (MPT) as a forward-looking metric for long-term retention in continuous streams. In this work, we report RTA results for current models and release TeleEgo, together with an MPT evaluation framework, as a realistic and extensible benchmark for future egocentric assistants with stronger streaming memory, enabling systematic study of both real-time behavior and long-horizon memory.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Saliency-Guided Contrastive Embeddings</title>
<link>https://arxiv.org/abs/2511.12744</link>
<guid>https://arxiv.org/abs/2511.12744</guid>
<content:encoded><![CDATA[
arXiv:2511.12744v2 Announce Type: replace 
Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views</title>
<link>https://arxiv.org/abs/2511.12878</link>
<guid>https://arxiv.org/abs/2511.12878</guid>
<content:encoded><![CDATA[
arXiv:2511.12878v3 Announce Type: replace 
Abstract: Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2511.13026</link>
<guid>https://arxiv.org/abs/2511.13026</guid>
<content:encoded><![CDATA[
arXiv:2511.13026v2 Announce Type: replace 
Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity</title>
<link>https://arxiv.org/abs/2511.18200</link>
<guid>https://arxiv.org/abs/2511.18200</guid>
<content:encoded><![CDATA[
arXiv:2511.18200v2 Announce Type: replace 
Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.19907</link>
<guid>https://arxiv.org/abs/2511.19907</guid>
<content:encoded><![CDATA[
arXiv:2511.19907v2 Announce Type: replace 
Abstract: This paper employs a multimodal approach for continuous sign recognition by first using ML for detecting the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then by recognizing the segmented signs. For improved robustness we use 3D skeletal features extracted from sign language videos to take into account the convergence of sign properties and their dynamics that tend to cluster at sign boundaries. Another focus of this paper is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for detection of 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing-as such signs often differ a bit in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title>
<link>https://arxiv.org/abs/2511.22294</link>
<guid>https://arxiv.org/abs/2511.22294</guid>
<content:encoded><![CDATA[
arXiv:2511.22294v3 Announce Type: replace 
Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Only Universal Adversarial Attacks in Distributed Learning</title>
<link>https://arxiv.org/abs/2411.10500</link>
<guid>https://arxiv.org/abs/2411.10500</guid>
<content:encoded><![CDATA[
arXiv:2411.10500v2 Announce Type: replace-cross 
Abstract: Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems, but may also introduce new vulnerabilities to evasion attacks, often in the form of adversarial perturbations. In this work, we present a new threat model that explores the feasibility of generating universal adversarial perturbations (UAPs) when the attacker has access only to the edge portion of the model, consisting of its initial network layers. Unlike traditional attacks that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud component by manipulating key feature representations at the edge. Following the proposed threat model, we introduce both edge-only untargeted and targeted formulations of UAPs designed to control intermediate features before the split point. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part, and we compare the proposed method with classical white-box and black-box techniques, highlighting its effectiveness. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effects with edge-only knowledge, revealing intriguing behaviors across multiple networks. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-centric Token Compression in Large Language Model</title>
<link>https://arxiv.org/abs/2502.00791</link>
<guid>https://arxiv.org/abs/2502.00791</guid>
<content:encoded><![CDATA[
arXiv:2502.00791v4 Announce Type: replace-cross 
Abstract: Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation</title>
<link>https://arxiv.org/abs/2503.16848</link>
<guid>https://arxiv.org/abs/2503.16848</guid>
<content:encoded><![CDATA[
arXiv:2503.16848v3 Announce Type: replace-cross 
Abstract: Despite advances in indoor 3D scene layout generation, synthesizing scenes with dense object arrangements remains challenging. Existing methods focus on large furniture while neglecting smaller objects, resulting in unrealistically empty scenes. Those that place small objects typically do not honor arrangement specifications, resulting in largely random placement not following the text description. We present Hierarchical Scene Motifs (HSM): a hierarchical framework for indoor scene generation with dense object arrangements across spatial scales. Indoor scenes are inherently hierarchical, with surfaces supporting objects at different scales, from large furniture on floors to smaller objects on tables and shelves. HSM embraces this hierarchy and exploits recurring cross-scale spatial patterns to generate complex and realistic scenes in a unified manner. Our experiments show that HSM outperforms existing methods by generating scenes that better conform to user input across room types and spatial configurations. Project website is available at https://3dlg-hcvc.github.io/hsm .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight</title>
<link>https://arxiv.org/abs/2504.20454</link>
<guid>https://arxiv.org/abs/2504.20454</guid>
<content:encoded><![CDATA[
arXiv:2504.20454v2 Announce Type: replace-cross 
Abstract: This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.07413</link>
<guid>https://arxiv.org/abs/2506.07413</guid>
<content:encoded><![CDATA[
arXiv:2506.07413v3 Announce Type: replace-cross 
Abstract: Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. Our code is available at https://github.com/ziwenwang28/VarContrast.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.13348</link>
<guid>https://arxiv.org/abs/2506.13348</guid>
<content:encoded><![CDATA[
arXiv:2506.13348v2 Announce Type: replace-cross 
Abstract: Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas. Code will be available at https://github.com/maeyounes/TextureSplat
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
arXiv:2506.16402v3 Announce Type: replace-cross 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under https://github.com/AI45Lab/IS-Bench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation</title>
<link>https://arxiv.org/abs/2507.18262</link>
<guid>https://arxiv.org/abs/2507.18262</guid>
<content:encoded><![CDATA[
arXiv:2507.18262v3 Announce Type: replace-cross 
Abstract: Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos are available at https://github.com/scy-v/ReSem3D and https://resem3d.github.io.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments</title>
<link>https://arxiv.org/abs/2510.23928</link>
<guid>https://arxiv.org/abs/2510.23928</guid>
<content:encoded><![CDATA[
arXiv:2510.23928v2 Announce Type: replace-cross 
Abstract: In this paper, we propose an adaptive keyframe selection method for improved 3D scene reconstruction in dynamic environments. The proposed method integrates two complementary modules: an error-based selection module utilizing photometric and structural similarity (SSIM) errors, and a momentum-based update module that dynamically adjusts keyframe selection thresholds according to scene motion dynamics. By dynamically curating the most informative frames, our approach addresses a key data bottleneck in real-time perception. This allows for the creation of high-quality 3D world representations from a compressed data stream, a critical step towards scalable robot learning and deployment in complex, dynamic environments. Experimental results demonstrate significant improvements over traditional static keyframe selection strategies, such as fixed temporal intervals or uniform frame skipping. These findings highlight a meaningful advancement toward adaptive perception systems that can dynamically respond to complex and evolving visual scenes. We evaluate our proposed adaptive keyframe selection module on two recent state-of-the-art 3D reconstruction networks, Spann3r and CUT3R, and observe consistent improvements in reconstruction quality across both frameworks. Furthermore, an extensive ablation study confirms the effectiveness of each individual component in our method, underlining their contribution to the overall performance gains.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title>
<link>https://arxiv.org/abs/2511.04555</link>
<guid>https://arxiv.org/abs/2511.04555</guid>
<content:encoded><![CDATA[
arXiv:2511.04555v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</title>
<link>https://arxiv.org/abs/2511.07820</link>
<guid>https://arxiv.org/abs/2511.07820</guid>
<content:encoded><![CDATA[
arXiv:2511.07820v2 Announce Type: replace-cross 
Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited set of behaviors, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leveraging dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video</title>
<link>https://arxiv.org/abs/2511.18322</link>
<guid>https://arxiv.org/abs/2511.18322</guid>
<content:encoded><![CDATA[
arXiv:2511.18322v2 Announce Type: replace-cross 
Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fractional Variational Approach to Spectral Filtering Using the Fourier Transform</title>
<link>https://arxiv.org/abs/2511.20675</link>
<guid>https://arxiv.org/abs/2511.20675</guid>
<content:encoded><![CDATA[
arXiv:2511.20675v2 Announce Type: replace-cross 
Abstract: The interference of fluorescence signals and noise remains a significant challenge in Raman spectrum analysis, often obscuring subtle spectral features that are critical for accurate analysis. Inspired by variational methods similar to those used in image denoising, our approach minimizes a functional involving fractional derivatives to balance noise suppression with the preservation of essential chemical features of the signal, such as peak position, intensity, and area. The original problem is reformulated in the frequency domain through the Fourier transform, making the implementation simple and fast. In this work, we discuss the theoretical framework, practical implementation, and the advantages and limitations of this method in the context of {simulated} Raman data, as well as in image processing. The main contribution of this article is the combination of a variational approach in the frequency domain, the use of fractional derivatives, and the optimization of the {regularization parameter and} derivative order through the concept of Shannon entropy. This work explores how the fractional order, combined with the regularization parameter, affects noise removal and preserves the essential features of the spectrum {and image}. Finally, the study shows that the combination of the proposed strategies produces an efficient, robust, and easily implementable filter.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SO-Bench: A Structural Output Evaluation of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.21750</link>
<guid>https://arxiv.org/abs/2511.21750</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, schema-grounded extraction, visual inputs, structured reasoning, SO-Bench  

<br /><br />Summary:  
1. Multimodal large language models (MLLMs) are increasingly used in real-world scenarios requiring outputs that are not only accurate but also comply with predefined data schemas.  
2. Despite advances in structured output generation for textual data, there is no existing benchmark to evaluate schema-grounded information extraction and reasoning specifically over visual inputs.  
3. The authors introduce SO-Bench, a comprehensive benchmark covering four visual domains: UI screens, natural images, documents, and charts. SO-Bench is built from over 6,500 diverse JSON schemas and 1,800 curated image-schema pairs validated by humans.  
4. Benchmarking experiments on both open-source and proprietary models indicate significant gaps in the ability of MLLMs to produce accurate and schema-compliant outputs, exposing the need for improved multimodal structured reasoning capabilities.  
5. Beyond evaluation, the authors conduct training experiments that substantially enhance a model’s ability to generate structured outputs in visual contexts. They plan to release SO-Bench to benefit the research community and accelerate progress in this area. <div>
arXiv:2511.21750v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</title>
<link>https://arxiv.org/abs/2511.22345</link>
<guid>https://arxiv.org/abs/2511.22345</guid>
<content:encoded><![CDATA[
<div> Keywords: Normalizing Flows, generative models, invertibility, feature alignment, test-time optimization<br /><br />Summary:<br /><br />This paper addresses limitations in the generative quality of Normalizing Flows (NFs), which are generative models featuring an invertible architecture. The forward pass of NFs transforms data into latent space for density estimation, while the reverse pass generates new data samples, linking representation learning and data generation. Traditional log-likelihood optimization leads to suboptimal semantic representations, reducing generative performance. To overcome this, the authors propose a novel alignment strategy that uniquely exploits NF invertibility by aligning intermediate features from the generative (reverse) pass with embeddings from powerful vision foundation models, rather than regularizing the forward pass. This method achieves better results than naive alignment techniques. Additionally, the paper introduces a novel, training-free, test-time optimization algorithm for classification, offering an intrinsic measure of the semantic knowledge embedded in NFs. Experimental results show that the approach accelerates NF training over 3.3 times and simultaneously enhances generative quality and classification accuracy. Notably, the approach establishes new state-of-the-art performance for NFs on ImageNet datasets at resolutions 64×64 and 256×256. The authors provide public access to their implementation via a GitHub repository. <div>
arXiv:2511.22345v2 Announce Type: replace 
Abstract: Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\times$64 and 256$\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization</title>
<link>https://arxiv.org/abs/2511.23002</link>
<guid>https://arxiv.org/abs/2511.23002</guid>
<content:encoded><![CDATA[
<div> Agent-based editing, instruction hallucination, reward hacking, multimodal chain-of-thought, policy optimization<br /><br />Summary:<br /><br />This paper introduces JarvisEvo, an advanced image editing agent designed to overcome two major challenges in interactive editing models: instruction hallucination and reward hacking. First, JarvisEvo addresses instruction hallucination by employing an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism, which improves the agent’s ability to follow instructions accurately and enhances the overall editing quality. Second, to tackle reward hacking, the authors propose a synergistic editor-evaluator policy optimization (SEPO) framework. This framework allows the agent to self-improve iteratively without relying on external reward functions, effectively preventing exploitation of reward model flaws. Third, JarvisEvo supports both global and local fine-grained image editing by seamlessly integrating with Adobe Lightroom, providing versatile control over the editing process. The system emulates an expert human designer, iteratively editing, selecting suitable tools, evaluating results, and reflecting on decisions to refine the output continuously. Evaluations on the ArtEdit-Bench dataset show that JarvisEvo significantly outperforms the prior state-of-the-art model Nano-Banana, with an average improvement of 18.95% on preservative editing metrics and a remarkable 44.96% gain in pixel-level content fidelity. The project demonstrates a promising direction for creating more reliable and effective autonomous image editing agents. <div>
arXiv:2511.23002v2 Announce Type: replace 
Abstract: Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity. Project page: https://jarvisevo.vercel.app/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.23075</link>
<guid>https://arxiv.org/abs/2511.23075</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, spatial reasoning, camera-guided fusion, dual-encoder architecture, 3D understanding<br /><br />Summary:<br /><br />Large vision-language models (VLMs) demonstrate strong multimodal understanding but face challenges in 3D spatial reasoning tasks such as distance estimation, size comparison, and maintaining cross-view consistency. Existing approaches either rely on additional 3D data or incorporate geometry encoders with RGB-only models via shallow feature fusion, limiting effectiveness. SpaceMind is proposed as a new multimodal large language model designed to perform spatial reasoning using only RGB inputs. It employs a dual-encoder architecture combining VGGT for spatial understanding and InternViT for 2D visual encoding. A key innovation is treating camera representation as an active guiding modality, rather than passive metadata, through a novel Camera-Guided Modality Fusion module. This module introduces camera-conditioned bias to spatial tokens, assigns query-independent weights based on geometric importance, and gates the fused representation using the camera embedding before processing by the language model. Empirical evaluation shows SpaceMind achieves state-of-the-art results on VSI-Bench, SQA3D, and SPBench benchmarks, outperforming both open-source and proprietary systems by wide margins. These findings highlight camera-guided modality fusion as an effective inductive bias, enabling VLMs to acquire genuinely spatially grounded intelligence. The authors plan to release code and model checkpoints to facilitate future research. <div>
arXiv:2511.23075v2 Announce Type: replace 
Abstract: Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection</title>
<link>https://arxiv.org/abs/2512.04175</link>
<guid>https://arxiv.org/abs/2512.04175</guid>
<content:encoded><![CDATA[
<div> Deepfake detection, video manipulation, facial landmarks, motion inconsistencies, autoencoder<br /><br />Summary:<br /><br />This paper addresses the challenge of generalizing deepfake detection to unseen manipulations, specifically focusing on videos rather than static images. Existing methods primarily detect temporal artifacts as frame-to-frame instabilities but neglect the natural motion dependencies that exist between different facial regions. To overcome this limitation, the authors propose a novel synthetic video generation method that introduces subtle kinematic inconsistencies by disrupting natural correlations in facial movements. They achieve this by training an autoencoder to decompose facial landmark configurations into motion bases, which they then manipulate selectively. These manipulated motion bases enable the creation of training videos with biomechanical flaws via face morphing techniques. Networks trained on this synthetic data gain the ability to detect sophisticated biomechanical inconsistencies that are hard to spot with conventional methods. Experimental results demonstrate that the proposed approach achieves state-of-the-art generalization performance on multiple popular deepfake detection benchmarks. This work advances the robustness of video deepfake detectors by exploiting nuanced biomechanical motion cues, offering a promising direction for more reliable detection of unseen facial video manipulations. <div>
arXiv:2512.04175v1 Announce Type: new 
Abstract: Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology</title>
<link>https://arxiv.org/abs/2512.04187</link>
<guid>https://arxiv.org/abs/2512.04187</guid>
<content:encoded><![CDATA[
<div> Keywords: OnSight Pathology, AI histological analysis, digital pathology, real-time inference, multi-modal chat assistant<br /><br />Summary:  
The article introduces OnSight Pathology, a platform-agnostic computer vision software designed to provide real-time artificial intelligence (AI) inferences during digital slide image review, addressing challenges in the deployment of proprietary digital pathology solutions. The software operates locally on consumer-grade personal computers via a single executable file, avoiding complex software integrations and enabling cost-effective, secure use in research and clinical workflows. The utility of OnSight Pathology is demonstrated using over 2,500 publicly available whole slide images across various viewers and clinical cases, showcasing its robustness in key histopathological tasks such as brain tumor classification, mitosis detection, and immunohistochemical stain quantification. A novel built-in multi-modal chat assistant enhances quality control by delivering verifiable, flexible image descriptions without rigid class labels. Additionally, the software supports live microscope camera feeds, including those from smartphones, highlighting its potential application in analog, inter-operative, and telepathology settings. Overall, OnSight Pathology facilitates broad adoption of AI tools in histopathology by overcoming barriers to real-world deployment and streamlining AI integration into diverse pathology workflows. <div>
arXiv:2512.04187v1 Announce Type: new 
Abstract: The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers</title>
<link>https://arxiv.org/abs/2512.04213</link>
<guid>https://arxiv.org/abs/2512.04213</guid>
<content:encoded><![CDATA[
<div> multi-camera tracking, transformer, attention mechanism, 3D point representation, temporal consistency<br /><br />Summary:<br /><br />1. The paper introduces LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture designed for multi-camera point tracking that integrates appearance-based matching with geometric constraints. <br />2. Unlike traditional pipelines that separate detection, association, and tracking—leading to error propagation and temporal inconsistency—LAPA jointly reasons across views and time using attention mechanisms to improve robustness in challenging scenarios. <br />3. A key innovation is the cross-view attention mechanism enhanced by geometric priors, which establishes soft correspondences without relying on classical triangulation, instead constructing 3D point representations through attention-weighted aggregation. This approach inherently handles uncertainty and partial observations. <br />4. Temporal consistency is ensured by a transformer decoder that models long-range dependencies, enabling identity preservation through extended occlusions. <br />5. LAPA’s performance is validated on challenging datasets, including newly created multi-camera versions of TAPVid-3D panoptic and PointOdyssey, where it achieves substantial improvements—37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC—especially excelling in complex motion and occlusion scenarios. The codebase is publicly available for further research and application. <div>
arXiv:2512.04213v1 Announce Type: new 
Abstract: This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning</title>
<link>https://arxiv.org/abs/2512.04219</link>
<guid>https://arxiv.org/abs/2512.04219</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical event segmentation, predictive learning, streaming video, temporal abstraction, unsupervised learning  

<br /><br />Summary:  
1. The paper introduces PARSE, a novel unsupervised framework designed to segment video into a hierarchy of events at multiple temporal scales, reflecting the natural human perception of nested actions within coarser routines.  
2. PARSE employs a hierarchy of recurrent predictors where lower layers focus on short-term dynamics and higher layers capture longer-term context through attention-based feedback, enabling temporal granularity in processing.  
3. Event boundaries are identified as transient peaks in the prediction error of the model, which naturally lead to the emergence of nested event structures that respect containment relations similar to human event perception.  
4. The framework is evaluated on three established benchmarks—Breakfast Actions, 50 Salads, and Assembly 101—where it achieves state-of-the-art performance among streaming methods and competes closely with offline methods in temporal alignment and structural consistency metrics (H-GEBD, TED, hF1).  
5. These results underline the effectiveness of predictive learning under uncertainty for scalable, hierarchical temporal abstraction and compositional event understanding, pushing forward the field of computer vision toward human-like event segmentation and anticipation. <div>
arXiv:2512.04219v1 Announce Type: new 
Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis</title>
<link>https://arxiv.org/abs/2512.04221</link>
<guid>https://arxiv.org/abs/2512.04221</guid>
<content:encoded><![CDATA[
<div> Newtonian motion, text-to-video generation, physics simulation, motion coherence, evaluation benchmark<br /><br />Summary:<br /><br />This paper addresses the challenge of generating text-to-video (T2V) content that is not only photorealistic but also physically accurate and intent-aligned with Newtonian motion principles. The authors propose MoReGen, a novel framework that combines large language models (LLMs), physics simulators, and rendering engines to produce reproducible videos grounded in physical laws from code-based text prompts. To evaluate physical validity, they introduce a new metric called object-trajectory correspondence, which directly measures how well generated videos adhere to expected object motions. Furthermore, they present MoReSet, a comprehensive benchmark dataset containing 1,275 human-annotated videos across nine categories of Newtonian phenomena, including detailed scene descriptions, spatiotemporal relations, and ground-truth trajectories. Through extensive experiments using MoReSet, the study evaluates existing T2V models, revealing that state-of-the-art approaches often fail to maintain physical correctness. In contrast, MoReGen demonstrates superior performance in generating physically coherent videos. This work highlights the importance of integrating physics knowledge into T2V systems and provides valuable tools and data to foster progress toward physically faithful video synthesis. <div>
arXiv:2512.04221v1 Announce Type: new 
Abstract: While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonX: MLLM-Guided Intrinsic Image Decomposition</title>
<link>https://arxiv.org/abs/2512.04222</link>
<guid>https://arxiv.org/abs/2512.04222</guid>
<content:encoded><![CDATA[
<div> Intrinsic image decomposition, multimodal large language model, comparative supervision, GRPO rewards, intrinsic predictors<br /><br />Summary:  
Intrinsic image decomposition aims to separate an image into its fundamental physical components such as albedo, depth, surface normals, and illumination. Existing diffusion- and transformer-based models trained on synthetic paired data struggle to generalize well to diverse, real-world images. The paper introduces ReasonX, a novel framework that leverages a multimodal large language model (MLLM) to act as a perceptual judge by providing relative intrinsic comparisons between image components. These comparisons are then used as GRPO (Gradient-based Reinforcement Policy Optimization) rewards to fine-tune intrinsic decomposition models using unlabeled, in-the-wild images. Unlike traditional reinforcement learning approaches for generative models, ReasonX aligns the conditional intrinsic predictors by encouraging agreement between the MLLM’s relational assessments and analytically derived relationships from the model’s outputs. This approach is model-agnostic and can be applied across different intrinsic decomposition architectures and modalities. Experiments demonstrate that ReasonX achieves substantial improvements, including a 9-25% reduction in Weighted Human Disagreement Rate (WHDR) on IIW albedo prediction and up to 46% gains in depth accuracy on the ETH3D dataset. These results highlight the effectiveness of MLLM-guided comparative supervision in bridging the gap between low-level image decomposition and high-level vision reasoning tasks. <div>
arXiv:2512.04222v1 Announce Type: new 
Abstract: Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04238</link>
<guid>https://arxiv.org/abs/2512.04238</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, rare anatomical variants, AdversarialAnatomyBench, anatomical bias, medical AI

<br /><br />Summary:  
This paper introduces AdversarialAnatomyBench, the first benchmark designed to evaluate vision-language models (VLMs) on naturally occurring rare anatomical variants across various imaging modalities and body regions, addressing an important gap in current model evaluation. The study defines these rare variants as "natural adversarial anatomy," highlighting how they deviate from common anatomical patterns learned by models. When testing 22 state-of-the-art VLMs on basic medical perception tasks, average accuracy significantly dropped from 74% on typical anatomy to 29% on atypical presentations, with top models like GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick experiencing performance declines between 41% and 51%. Analysis revealed that model errors strongly aligned with inherent anatomical biases. Attempts to mitigate these shortcomings through model scaling, bias-aware prompting, and test-time reasoning interventions failed to substantially improve generalization to rare anatomical variants. These results unearth a critical and previously unquantified limitation in current vision-language medical AI systems: poor adaptability to rare anatomical presentations. By providing AdversarialAnatomyBench, the authors establish a systematic platform for measuring and ultimately mitigating anatomical bias in multimodal medical AI, fostering the development of more robust clinical diagnostic tools. <div>
arXiv:2512.04238v1 Announce Type: new 
Abstract: Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models</title>
<link>https://arxiv.org/abs/2512.04248</link>
<guid>https://arxiv.org/abs/2512.04248</guid>
<content:encoded><![CDATA[
<div> MVRoom, novel view synthesis, multi-view diffusion, 3D indoor scenes, epipolar attention<br /><br />Summary:<br /><br />This article introduces MVRoom, a novel pipeline designed for controllable novel view synthesis (NVS) of 3D indoor scenes by leveraging multi-view diffusion conditioned on a coarse 3D layout. The approach utilizes a two-stage design: the first stage creates novel representations that effectively connect the 3D layout with consistent image-based conditioning signals to enable reliable multi-view generation. The second stage applies an image-conditioned multi-view generation process, which integrates a layout-aware epipolar attention mechanism to boost multi-view consistency throughout the diffusion process. Furthermore, the authors propose an iterative framework that generates 3D scenes with varying complexity and object counts by recursively applying multi-view generation, thereby enabling text-to-scene generation. Experimental evaluations demonstrate that MVRoom achieves high-fidelity and controllable 3D scene synthesis, outperforming state-of-the-art baseline methods in both quantitative metrics and qualitative results. Ablation studies confirm the critical roles and effectiveness of the key components in the generation pipeline, validating the design choices and mechanisms introduced for improved multi-view consistency and scene controllability. This work advances the state-of-the-art in controllable 3D indoor scene synthesis using diffusion-based multi-view methods. <div>
arXiv:2512.04248v1 Announce Type: new 
Abstract: We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLight: A Unified Representation for Lighting</title>
<link>https://arxiv.org/abs/2512.04267</link>
<guid>https://arxiv.org/abs/2512.04267</guid>
<content:encoded><![CDATA[
<div> lighting, multimodal representation, environment maps, spherical harmonics, image synthesis<br /><br />Summary:<br /><br />1. The article addresses the challenge of representing and understanding lighting in images, which is known to significantly affect visual appearance but remains difficult to model effectively.<br />2. Existing lighting representations like environment maps, irradiance, spherical harmonics, and textual descriptions are largely incompatible with each other, limiting their combined utility and cross-modal applications.<br />3. To overcome this, the authors propose UniLight, a unified latent space that represents lighting across multiple modalities within a shared embedding.<br />4. UniLight employs modality-specific encoders for text, images, irradiance, and environment maps trained with contrastive learning to align their embeddings, complemented by an auxiliary spherical harmonics prediction task that enhances directional lighting understanding.<br />5. The system is trained and evaluated on large-scale, multi-modal data for three key tasks: lighting-based retrieval, environment-map generation, and controlling lighting in diffusion-based image synthesis.<br /><br />Results demonstrate that UniLight successfully captures consistent, transferable lighting features, enabling flexible and effective manipulation of lighting information across different modalities, thus advancing lighting representation for various computer vision and graphics applications. <div>
arXiv:2512.04267v1 Announce Type: new 
Abstract: Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer</title>
<link>https://arxiv.org/abs/2512.04282</link>
<guid>https://arxiv.org/abs/2512.04282</guid>
<content:encoded><![CDATA[
<div> Keywords: video motion transfer, normalizing flows, stochastic sampling, GRU, multimodal forecasting  

<br /><br />Summary:  
The paper addresses the challenge of generating diverse and accurate future predictions for real-time video motion transfer, which is critical in applications like immersive gaming and vision-based anomaly detection. It presents a novel inference-time refinement technique that enhances the diversity of sequential forecasts by combining Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. Although GRU-NF integrates normalizing flows within a temporal forecasting model to capture multimodal distributions, its deterministic transformations can limit the model's expressive capacity. To overcome this, the authors incorporate Markov Chain Monte Carlo (MCMC) steps inspired by Stochastic Normalizing Flows (SNF) during inference, enabling exploration of a richer output space without retraining. The method, named Gated Recurrent Unit-Stochastic Normalizing Flows (GRU-SNF), is validated on a keypoint-based video motion transfer pipeline where temporal coherence and perceptual diversity are essential. Experimental results demonstrate that GRU-SNF produces more diverse outputs than GRU-NF while maintaining accuracy, even over longer prediction horizons. By injecting stochasticity during inference, the approach better captures multimodal behaviors in future trajectories, showcasing the potential benefits of integrating stochastic dynamics into flow-based sequence models for generative time series forecasting. <div>
arXiv:2512.04282v1 Announce Type: new 
Abstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint</title>
<link>https://arxiv.org/abs/2512.04283</link>
<guid>https://arxiv.org/abs/2512.04283</guid>
<content:encoded><![CDATA[
<div> Flow matching, plug-and-play, stochastic differential equation, image restoration, acceleration  

<br /><br />Summary:  
This paper addresses the gap between the empirical success and theoretical understanding of plug-and-play flow matching (PnP-Flow) models in image restoration. First, the authors derive a continuous limit of PnP-Flow, formulating it as a stochastic differential equation (SDE) surrogate model. This SDE model provides a rigorous framework to analyze PnP-Flow and offers two key insights for enhancements. One, it allows quantification of the image restoration error, which guides improvements in step scheduling and the regularization of the Lipschitz constant in the neural network-parameterized vector field, ultimately reducing error. Two, the SDE model inspires an acceleration strategy by extrapolating off-the-shelf PnP-Flow models, leading to a rescaled version of the SDE for faster convergence. The authors validate their theoretical findings on benchmark image restoration tasks including denoising, deblurring, super-resolution, and inpainting. Experimental results demonstrate that the SDE-informed improvements outperform baseline PnP-Flow and other state-of-the-art methods, achieving superior performance across multiple evaluation metrics. This work thus bridges theoretical and practical aspects, offering both a deeper understanding and practical advancements for PnP-Flow in image restoration. <div>
arXiv:2512.04283v1 Announce Type: new 
Abstract: Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Single-Image Super-Resolution in the JPEG Compressed Domain</title>
<link>https://arxiv.org/abs/2512.04284</link>
<guid>https://arxiv.org/abs/2512.04284</guid>
<content:encoded><![CDATA[
<div> JPEG features, deep learning, super-resolution, data loading, discrete cosine transform

<br /><br />Summary:  
Deep learning models have increased in complexity alongside the growth in input data size, causing data loading to remain a significant bottleneck impacting training and inference speeds despite advances in specialized hardware. This work addresses this challenge by proposing a method to train models directly on encoded JPEG features, which avoids the full JPEG decoding process and thus reduces computational overhead and improves data loading efficiency. Unlike previous studies that primarily focused on recognition tasks, this research explores the viability of the approach for single-image super-resolution (SISR), a restoration task. The authors develop a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain rather than in the pixel domain. Experimentally, this pipeline achieves a 2.6x speedup in data loading time and a 2.5x speedup in overall training time. Despite these efficiency gains, the model maintains visual quality that is comparable to traditional SISR methods, demonstrating that training directly on compressed features is an effective approach for accelerating training without sacrificing output quality. <div>
arXiv:2512.04284v1 Announce Type: new 
Abstract: Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications</title>
<link>https://arxiv.org/abs/2512.04303</link>
<guid>https://arxiv.org/abs/2512.04303</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular depth estimation, road surface geometry, planar parallax, self-supervised learning, metric depth recovery<br /><br />Summary:<br /><br />1. The paper introduces Gamma-from-Mono (GfM), a novel lightweight method for monocular geometry estimation designed to improve the accuracy of 3D perception of vehicle surroundings, particularly focusing on fine-scale road surface details such as bumps and slopes. <br />2. GfM addresses the projective ambiguity inherent in single-camera reconstruction by decoupling global and local structure, predicting a dominant road surface plane along with residual local variations expressed as gamma—a dimensionless ratio representing vertical deviation relative to depth from the camera. <br />3. This gamma parameter is grounded in planar parallax geometry and, combined with the known camera height above the ground, allows deterministic recovery of metric depth via a closed-form solution, bypassing the need for full extrinsic calibration.<br />4. The approach naturally prioritizes near-road details critical for vehicle control, uses a physically interpretable representation well-suited for self-supervised learning, and does not require large annotated datasets.<br />5. Experimental evaluation on the KITTI and Road Surface Reconstruction Dataset (RSRD) demonstrates that GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation, maintains competitive global depth performance, and exhibits robust adaptability across diverse camera setups with a lightweight 8.88 million parameter model. <div>
arXiv:2512.04303v1 Announce Type: new 
Abstract: Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How (Mis)calibrated is Your Federated CLIP and What To Do About It?</title>
<link>https://arxiv.org/abs/2512.04305</link>
<guid>https://arxiv.org/abs/2512.04305</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP calibration, federated learning, Textual Prompt Tuning, LoRA, FL2oRA<br /><br />Summary:<br />1. The paper investigates the calibration of vision-language models, specifically CLIP, under federated learning (FL) conditions, a topic previously unexplored despite CLIP's widespread study.<br />2. It identifies that Textual Prompt Tuning methods, when applied in FL, tend to degrade calibration performance, highlighting challenges specific to the distributed setup.<br />3. The study evaluates existing in-training calibration methods combined with various global aggregation strategies and finds these provide only limited improvements in calibration.<br />4. A key insight is that the choice of model components selected for fine-tuning significantly impacts calibration, beyond the aggregation or calibration techniques themselves.<br />5. To address this, the authors propose FL²oRA, a novel approach based on LoRA (Low-Rank Adaptation), which enhances calibration naturally within FL without requiring explicit calibration steps.<br />6. Extensive experiments across multiple benchmarks demonstrate FL²oRA consistently yields better-calibrated models, simplifying reliable deployment of CLIP models in distributed learning environments.<br />7. The authors also provide a detailed analysis of factors contributing to FL²oRA's effectiveness and release the codebase publicly for further research and application. <div>
arXiv:2512.04305v1 Announce Type: new 
Abstract: While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction</title>
<link>https://arxiv.org/abs/2512.04309</link>
<guid>https://arxiv.org/abs/2512.04309</guid>
<content:encoded><![CDATA[
<div> Keywords: image captioning, text-only training, modality gap reduction, CLIP, retrieval-augmentation<br /><br />Summary:<br /><br />This paper addresses the challenge of generating image captions without relying on human-annotated image-text pairs, focusing on reducing dependency on curated datasets. The authors propose TOMCap, a novel method that leverages a pre-trained language model decoder prompted by information extracted from CLIP representations, which undergo a specific process to minimize the modality gap between visual and textual domains. A key innovation of TOMCap is the combined use of retrieved caption examples and latent vector representations, which together guide the caption generation process more effectively. Extensive experiments demonstrate that TOMCap outperforms existing training-free and text-only image captioning methods, showcasing its potential as a strong alternative to fully supervised approaches. Furthermore, the paper includes an in-depth analysis of how different configurations of retrieval-augmentation and modality gap reduction influence the overall performance, providing insights into optimal design choices. The method highlights the feasibility of training image captioning models through text-only data by effectively bridging the visual-textual modality gap, opening new avenues for caption generation under limited supervision. <div>
arXiv:2512.04309v1 Announce Type: new 
Abstract: Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time Cricket Sorting By Sex</title>
<link>https://arxiv.org/abs/2512.04311</link>
<guid>https://arxiv.org/abs/2512.04311</guid>
<content:encoded><![CDATA[
<div> Keywords: Acheta domesticus, automated sex sorting, computer vision, YOLOv8, sustainable protein<br /><br />Summary:<br /><br />1. The rising global demand for sustainable protein sources has highlighted edible insects, with Acheta domesticus (house cricket) recognized as a prime candidate for industrial farming due to its nutritional value and production potential.<br /><br />2. Existing cricket farming typically involves mixed-sex populations without sex-based sorting, missing out on advantages such as optimized breeding ratios, selective breeding improvements, and tailored nutritional outcomes.<br /><br />3. This study introduces a low-cost, real-time automated system specifically designed for sex-based sorting of Acheta domesticus, integrating computer vision techniques with physical actuation.<br /><br />4. The system employs a Raspberry Pi 5 paired with the official Raspberry AI Camera alongside a custom-trained YOLOv8 nano object detection model to identify and classify crickets by sex.<br /><br />5. Achieving a high mean Average Precision (mAP@0.5) of 0.977 in testing and an 86.8% sorting accuracy in real-world trials, the results demonstrate the capability of lightweight deep learning models to operate effectively on resource-limited devices.<br /><br />6. This technology offers a practical and scalable solution to enhance efficiency, sustainability, and precision in cricket farming operations, potentially transforming practices in edible insect production. <div>
arXiv:2512.04311v1 Announce Type: new 
Abstract: The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding</title>
<link>https://arxiv.org/abs/2512.04313</link>
<guid>https://arxiv.org/abs/2512.04313</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG decoding, facial expression synthesis, CNN-Transformer, 3D Gaussian Splatting, emotion recognition<br /><br />Summary:  
1. This study introduces Mind-to-Face, a novel framework that decodes non-invasive EEG signals directly into high-fidelity, dynamic facial expressions.  
2. The authors constructed a dual-modality recording setup that synchronizes EEG data with multi-view facial videos under emotion-eliciting stimuli, enabling precise supervision for training the neural-to-visual mapping.  
3. Their model employs a CNN-Transformer encoder architecture to convert EEG inputs into dense 3D position maps with over 65,000 vertices, capturing fine facial geometry and subtle emotional dynamics.  
4. The generated 3D facial data is rendered using a modified 3D Gaussian Splatting pipeline, producing photorealistic and view-consistent avatar outputs.  
5. Extensive evaluations demonstrate that EEG signals alone can reliably and subject-specifically predict dynamic facial expressions, including nuanced emotional states, revealing richer affective and geometric information in neural signals than previously recognized.  
6. Mind-to-Face represents a new paradigm in neural-driven avatars, unlocking personalized, emotion-aware telepresence and enhanced cognitive interaction for immersive virtual environments. <div>
arXiv:2512.04313v1 Announce Type: new 
Abstract: Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision</title>
<link>https://arxiv.org/abs/2512.04314</link>
<guid>https://arxiv.org/abs/2512.04314</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, DisentangleFormer, hyperspectral imaging, spatial-channel decoupling, multi-scale FFN<br /><br />Summary:<br /><br />1. Vision Transformers (ViTs) typically use self-attention that jointly processes spatial and channel dimensions, leading to entangled representations that hinder the separate modeling of structural and semantic information. This challenge is particularly critical in hyperspectral imaging where each channel carries distinct biophysical or biochemical information.<br /><br />2. The paper introduces DisentangleFormer, a novel architecture designed to achieve robust multi-channel vision representation by explicitly decoupling spatial and channel information, inspired by information-theoretic principles favoring decorrelated representations.<br /><br />3. DisentangleFormer features three key components: (a) Parallel Disentanglement, which independently processes spatial-token and channel-token streams to ensure decorrelated features across spatial and spectral domains; (b) Squeezed Token Enhancer, an adaptive calibration module that dynamically fuses spatial and channel streams to optimize information flow; (c) Multi-Scale Feed-Forward Network (FFN), which supplements global attention with multi-scale local context to capture intricate structural and semantic dependencies.<br /><br />4. Extensive experiments on multiple hyperspectral benchmarks—including Indian Pine, Pavia University, Houston datasets, the large-scale BigEarthNet remote sensing dataset, and an infrared pathology dataset—show that DisentangleFormer consistently outperforms state-of-the-art models.<br /><br />5. Additionally, DisentangleFormer maintains competitive accuracy on the natural image classification benchmark ImageNet while reducing computational cost by 17.8% in FLOPs, demonstrating efficiency alongside superior performance. The code will be publicly released upon paper acceptance. <div>
arXiv:2512.04314v1 Announce Type: new 
Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.04315</link>
<guid>https://arxiv.org/abs/2512.04315</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D Gaussian Splatting, multi-video synchronization, dynamic 3D scenes, Fused Gromov-Wasserstein, temporal alignment  

<br /><br />Summary:  
This paper addresses the challenge of modeling dynamic 3D scenes from multiple unsynchronized videos by introducing a novel 4D Gaussian Splatting (4DGS) method named SyncTrack4D. The approach starts by computing dense 4D feature tracks for each video and establishes cross-video correspondences using a Fused Gromov-Wasserstein optimal transport method. It then performs a global frame-level temporal alignment to maximize the overlap of matched dynamic scene motions across videos. To refine synchronization, SyncTrack4D applies sub-frame synchronization leveraging a motion-spline scaffold representation within the multi-video 4D Gaussian splatting framework. The output is a fully synchronized 4DGS model with explicit 3D trajectories and precise temporal offsets for every input video. The method is evaluated on datasets such as Panoptic Studio and SyncNeRF Blender, demonstrating state-of-the-art sub-frame synchronization accuracy with average temporal errors below 0.26 frames. Additionally, high-fidelity 4D reconstructions were achieved, reaching a peak signal-to-noise ratio (PSNR) of 26.3 on the Panoptic Studio dataset. Notably, this approach does not require predefined scene objects or prior models, marking the first general 4D Gaussian Splatting framework for handling unsynchronized multi-video dynamic scene reconstruction in real-world scenarios. <div>
arXiv:2512.04315v1 Announce Type: new 
Abstract: Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2512.04323</link>
<guid>https://arxiv.org/abs/2512.04323</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Image Correlation, non-uniform B-spline, displacement fields, Bayes-DIC Net, Bayesian neural network<br /><br />Summary:<br /><br />This paper presents a novel method to generate high-quality Digital Image Correlation (DIC) datasets utilizing non-uniform B-spline surfaces. The approach randomly generates control point coordinates to construct diverse displacement fields that mimic realistic scenarios, which are then used to create speckle pattern datasets. This enables the creation of large-scale datasets that better represent real-world displacement conditions, thereby improving the training and generalization ability of deep learning-based DIC algorithms. The paper also introduces a new network architecture named Bayes-DIC Net, which captures information at multiple levels during down-sampling and aggregates multi-level features using a single skip connection in the up-sampling phase. Bayes-DIC Net employs lightweight convolutional blocks to expand the receptive field and capture rich contextual information while keeping computational costs low. Incorporating dropout modules activated during inference transforms Bayes-DIC Net into a Bayesian neural network, allowing it to produce both predictions and uncertainty estimates on unlabeled real datasets. This capability significantly enhances the network’s reliability and practicality for real-world displacement field predictions. Overall, the paper contributes innovative dataset generation techniques and algorithmic improvements in DIC analysis. <div>
arXiv:2512.04323v1 Announce Type: new 
Abstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks</title>
<link>https://arxiv.org/abs/2512.04329</link>
<guid>https://arxiv.org/abs/2512.04329</guid>
<content:encoded><![CDATA[
<div> Keywords: neural modules, PyTorch codebases, retrieval-augmented generation, code reuse, neural architectures<br /><br />Summary: NN-RAG is a novel retrieval-augmented generation system designed to enhance research efficiency by enabling the discovery, extraction, and validation of reusable neural network components from large, heterogeneous PyTorch codebases. Unlike traditional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, preserves imports, and utilizes validator-gated promotion to ensure that every retrieved neural block is scope-closed, compilable, and runnable. When applied to 19 major repositories, NN-RAG extracted 1,289 candidate blocks and validated 941 of them (73.0%), with over 80% being structurally unique. Multi-level de-duplication techniques (exact, lexical, structural) revealed that NN-RAG contributes approximately 72% of novel network architectures in the LEMUR dataset, underscoring its capacity to significantly expand the diversity of executable neural architectures. Beyond quantity, NN-RAG uniquely supports cross-repository migration of architectural patterns, allowing automatic identification and regeneration of dependency-complete reusable modules in new contexts. The framework’s neutral design permits optional integration with language models for synthesis or dataset registration while avoiding redistribution of third-party code. Overall, NN-RAG offers the first open-source solution that transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery at scale. <div>
arXiv:2512.04329v1 Announce Type: new 
Abstract: Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Set Face Forgery Detection via Dual-Level Evidence Collection</title>
<link>https://arxiv.org/abs/2512.04331</link>
<guid>https://arxiv.org/abs/2512.04331</guid>
<content:encoded><![CDATA[
<div> Face forgery detection, open set recognition, uncertainty estimation, dual-level evidential learning, frequency-spatial fusion<br /><br />Summary:<br /><br />1. The paper addresses the problem of face forgery detection, focusing on the limitations of current methods which mainly perform binary Real-vs-Fake classification or identify only known types of forgeries.<br /><br />2. It introduces the Open Set Face Forgery Detection (OSFFD) problem, which requires detection models to recognize novel, previously unseen fake categories, reflecting real-world challenges as new forgery methods constantly emerge.<br /><br />3. The authors propose a novel Dual-Level Evidential face forgery Detection (DLED) approach that leverages uncertainty estimation to better handle these new fake types.<br /><br />4. DLED collects and integrates category-specific evidence from both spatial and frequency domains in the visual data, enhancing the model’s ability to estimate prediction uncertainty effectively.<br /><br />5. Extensive experiments demonstrate that DLED outperforms multiple baseline models by an average of 20% in detecting novel fake categories and maintains competitive performance in the traditional Real-versus-Fake detection task, establishing state-of-the-art results in this field. <div>
arXiv:2512.04331v1 Announce Type: new 
Abstract: The proliferation of face forgeries has increasingly undermined confidence in the authenticity of online content. Given the rapid development of face forgery generation algorithms, new fake categories are likely to keep appearing, posing a major challenge to existing face forgery detection methods. Despite recent advances in face forgery detection, existing methods are typically limited to binary Real-vs-Fake classification or the identification of known fake categories, and are incapable of detecting the emergence of novel types of forgeries. In this work, we study the Open Set Face Forgery Detection (OSFFD) problem, which demands that the detection model recognize novel fake categories. We reformulate the OSFFD problem and address it through uncertainty estimation, enhancing its applicability to real-world scenarios. Specifically, we propose the Dual-Level Evidential face forgery Detection (DLED) approach, which collects and fuses category-specific evidence on the spatial and frequency levels to estimate prediction uncertainty. Extensive evaluations conducted across diverse experimental settings demonstrate that the proposed DLED method achieves state-of-the-art performance, outperforming various baseline models by an average of 20% in detecting forgeries from novel fake categories. Moreover, on the traditional Real-versus-Fake face forgery detection task, our DLED method concurrently exhibits competitive performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</title>
<link>https://arxiv.org/abs/2512.04356</link>
<guid>https://arxiv.org/abs/2512.04356</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, hallucination, video captioning, contrastive alignment, spurious correlations<br /><br />Summary:<br /><br />1. The paper addresses the problem of hallucination in video captioning by multimodal large language models (MLLMs), which causes factual inaccuracies in generated video descriptions involving objects and actions.<br />2. Unlike previous works focused on static images, this research targets the combined challenge of mitigating both visual object and temporal action hallucinations in dynamic video content.<br />3. To resolve this, the authors propose the Self-Augmented Contrastive Alignment (SANTA) framework, designed to improve faithfulness by reducing spurious correlations and emphasizing accurate visual facts.<br />4. SANTA introduces a hallucinative self-augmentation approach that generates contrasted negative captions by identifying potential hallucinations in the model’s output, thereby guiding the system to distinguish between factual and spurious content.<br />5. Additionally, the framework includes a tracklet-phrase contrastive alignment mechanism that aligns tracked regional objects and relation-guided temporal actions with their corresponding visual and temporal phrases to enforce semantic consistency.<br />6. Extensive experiments on benchmarks focused on hallucination detection demonstrate that SANTA significantly outperforms existing methods in reducing both object and action hallucinations, improving the overall descriptive accuracy of MLLMs for video captioning.<br /><br />This study highlights an effective strategy to enhance the reliability of multimodal models in generating faithful video descriptions by jointly addressing spatial and temporal visual hallucinations. <div>
arXiv:2512.04356v1 Announce Type: new 
Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching</title>
<link>https://arxiv.org/abs/2512.04358</link>
<guid>https://arxiv.org/abs/2512.04358</guid>
<content:encoded><![CDATA[
<div> Keywords: stereo matching, multi-frequency, adaptive fusion, disparity estimation, real-time performance<br /><br />Summary:<br /><br />The paper addresses limitations in existing stereo matching networks, which either rely on costly 3D convolution-based cost-volume aggregation or iterative deformation methods that fail to capture non-local contextual information. These constraints hinder deployment on resource-limited mobile devices, especially for real-time applications. To overcome this, the authors propose the Multi-frequency Adaptive Fusion Network (MAFNet), designed to generate high-quality disparity maps efficiently with only 2D convolutions. A key innovation is the adaptive frequency-domain filtering attention module that separates the full cost volume into high- and low-frequency components, enabling frequency-aware feature aggregation tailored to these distinct bands. Furthermore, the method incorporates a Linformer-based low-rank attention mechanism to effectively fuse the frequency-separated features in an adaptive manner, enhancing disparity estimation robustness. Experimental results on benchmarks like Scene Flow and KITTI 2015 demonstrate that MAFNet outperforms current real-time stereo matching methods, offering an improved trade-off between accuracy and computational efficiency. This approach shows promise for stereo vision applications requiring real-time processing on devices with limited resources. <div>
arXiv:2512.04358v1 Announce Type: new 
Abstract: Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring</title>
<link>https://arxiv.org/abs/2512.04390</link>
<guid>https://arxiv.org/abs/2512.04390</guid>
<content:encoded><![CDATA[
<div> Keywords: video restoration, dynamic exposure, super-resolution, deblurring, FMA-Net++

<br /><br />Summary:  
1. The paper addresses the challenge of real-world video restoration affected by complex degradations stemming from motion combined with dynamically varying exposure, a problem often overlooked in prior research and common in auto-exposure or low-light video capture.  
2. The authors propose FMA-Net++, a novel framework for joint video super-resolution and deblurring that explicitly models the coupled effects of motion and changing exposure conditions to better restore video quality.  
3. FMA-Net++ features a sequence-level architecture using Hierarchical Refinement with Bidirectional Propagation blocks, which facilitates parallel processing and long-range temporal modeling to capture temporal dependencies effectively.  
4. Within each block, an Exposure Time-aware Modulation layer adjusts features based on each frame’s exposure, feeding into an exposure-aware Flow-Guided Dynamic Filtering module that infers degradation kernels sensitive to both motion and exposure variations.  
5. The method decouples degradation learning from restoration by first predicting exposure- and motion-aware priors to guide the restoration process, enhancing both accuracy and computational efficiency.  
6. To evaluate performance under realistic capture conditions, the authors introduce two new benchmarks: REDS-ME (multi-exposure) and REDS-RE (random exposure).  
7. Trained exclusively on synthetic data, FMA-Net++ achieves state-of-the-art results in restoration quality and temporal consistency on these benchmarks and the GoPro dataset, outperforming recent methods in accuracy and inference speed.  
8. Moreover, the approach generalizes effectively to challenging real-world videos, demonstrating robustness and practical applicability. <div>
arXiv:2512.04390v1 Announce Type: new 
Abstract: Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04395</link>
<guid>https://arxiv.org/abs/2512.04395</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Fourier Analysis, Representation Disentanglement, Cross-Attention, Few-Shot Learning<br /><br />Summary:<br />1. The paper addresses the limitation of current large-scale pre-trained Vision-Language Models (VLMs) that learn holistic image representations, where domain-invariant structures and domain-specific styles are entangled, which can hinder generalization.<br />2. It introduces a novel framework called Fourier-Attentive Representation Learning (FARL) that explicitly disentangles visual features into structural and stylistic components using Fourier analysis.<br />3. The core innovation involves a dual cross-attention mechanism where learnable tokens separately query the phase spectrum for structural features and the amplitude spectrum for stylistic features, producing enriched and disentangled representations.<br />4. These disentangled tokens are injected deeply into the VLM encoders via an asymmetric injection strategy, which enhances the robustness of vision-language alignment and model adaptation.<br />5. Extensive experiments conducted across 15 diverse datasets validate the effectiveness of FARL, showing improved few-shot learning capabilities due to the better disentanglement of structural and stylistic visual cues. <div>
arXiv:2512.04395v1 Announce Type: new 
Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection</title>
<link>https://arxiv.org/abs/2512.04397</link>
<guid>https://arxiv.org/abs/2512.04397</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, medical image classification, deep convolutional neural networks, chest X-ray, InceptionV3  

<br /><br />Summary:  
The paper addresses the challenge of training large deep learning models from scratch for medical image classification tasks, specifically using chest X-rays. It evaluates the effectiveness of transfer learning (TL) by reusing six pre-trained convolutional neural network models: AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3. The study finds that InceptionV3 consistently achieves the highest performance across standard evaluation metrics, while the ResNet models demonstrate improved accuracy with increasing depth. AlexNet and VGG16 perform adequately but lag behind the deeper architectures in terms of accuracy. Additionally, the authors examine the robustness of these models through uncertainty analysis and compare their computational efficiency via runtime measurements. The results indicate that TL provides clear benefits, particularly when the available dataset is limited in size. However, the degree of improvement depends on various factors including the model architecture, the size of the dataset, and how similar the source and target domains are. The research also highlights that leveraging a well-trained feature extractor allows the use of a simple lightweight feedforward network for efficient predictions. Overall, the findings offer valuable insights into selecting suitable TL models for medical image classification based on specific requirements such as accuracy, robustness, and computational resources. <div>
arXiv:2512.04397v1 Announce Type: new 
Abstract: Medical image classification plays an increasingly vital role in identifying various diseases by classifying medical images, such as X-rays, MRIs and CT scans, into different categories based on their features. In recent years, deep learning techniques have attracted significant attention in medical image classification. However, it is usually infeasible to train an entire large deep learning model from scratch. To address this issue, one of the solutions is the transfer learning (TL) technique, where a pre-trained model is reused for a new task. In this paper, we present a comprehensive analysis of TL techniques for medical image classification using deep convolutional neural networks. We evaluate six pre-trained models (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) on a custom chest X-ray dataset for disease detection. The experimental results demonstrate that InceptionV3 consistently outperforms other models across all the standard metrics. The ResNet family shows progressively better performance with increasing depth, whereas VGG16 and AlexNet perform reasonably well but with lower accuracy. In addition, we also conduct uncertainty analysis and runtime comparison to assess the robustness and computational efficiency of these models. Our findings reveal that TL is beneficial in most cases, especially with limited data, but the extent of improvement depends on several factors such as model architecture, dataset size, and domain similarity between source and target tasks. Moreover, we demonstrate that with a well-trained feature extractor, only a lightweight feedforward model is enough to provide efficient prediction. As such, this study contributes to the understanding of TL in medical image classification, and provides insights for selecting appropriate models based on specific requirements.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title>
<link>https://arxiv.org/abs/2512.04413</link>
<guid>https://arxiv.org/abs/2512.04413</guid>
<content:encoded><![CDATA[
<div> Knowledge distillation, remote sensing object detection, spectral decomposition, wavelet transform, density-independent scale weight<br /><br />Summary:<br /><br />This paper addresses the challenges of knowledge distillation in remote sensing object detection, particularly issues arising from mixed features and subtle variations leading to knowledge confusion. The authors propose a novel, architecture-agnostic distillation method called Dual-Stream Spectral Decoupling Distillation (DS2D2) that integrates both explicit and implicit distillation strategies using spectral decomposition. First, the method applies a first-order wavelet transform to decompose spectral features, preserving critical spatial characteristics of remote sensing images, which is important for accurate object detection. Building on this spatial preservation, a Density-Independent Scale Weight (DISW) is introduced to specifically target the difficulties associated with dense and small objects common in remote sensing imagery. Second, implicit knowledge is extracted from subtle discrepancies between student and teacher model features, which significantly impact prediction accuracy when processed through detection heads. These discrepancies are captured via full-frequency and high-frequency amplifiers, translating feature differences into prediction deviations. The effectiveness of DS2D2 is validated through extensive experiments on benchmark datasets DIOR and DOTA, demonstrating improvements of 4.2% and 3.8% in AP50 for RetinaNet and Faster R-CNN respectively, outperforming existing distillation approaches. The authors also provide the source code for reproducibility and further research. <div>
arXiv:2512.04413v1 Announce Type: new 
Abstract: Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes</title>
<link>https://arxiv.org/abs/2512.04421</link>
<guid>https://arxiv.org/abs/2512.04421</guid>
<content:encoded><![CDATA[
<div> Gaussian Particles, Ray Tracing, Triangle Rendering, Novel-View Synthesis, Real-Time Performance  

<br /><br />Summary:  
This work addresses limitations in current 3D Gaussian particle ray tracing methods which depend on proxy geometry, such as intermediate mesh construction and expensive intersection computations. The authors propose a novel differentiable ray tracing pipeline that directly uses triangles as rendering primitives, eliminating the need for proxy geometry. By treating triangles as the fundamental unit for both ray tracing and rasterization, the method unifies rendering primitives used in novel-view synthesis. Experimental results demonstrate that this triangle-based approach achieves significantly better rendering quality compared to traditional Gaussian particle ray tracing techniques while still maintaining real-time rendering speeds. Additionally, the proposed pipeline supports direct rendering of triangles optimized via the rasterization-based technique known as Triangle Splatting. This enables seamless integration and improved flexibility for rendering realistic effects like depth of field and refraction in novel-view synthesis applications. The approach therefore overcomes inefficiencies and quality issues present in previous methods by combining the strengths of rasterization and ray tracing in a single, efficient framework. <div>
arXiv:2512.04421v1 Announce Type: new 
Abstract: Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</title>
<link>https://arxiv.org/abs/2512.04425</link>
<guid>https://arxiv.org/abs/2512.04425</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson’s disease, gait analysis, multimodal fusion, explainability, RGB-D data<br /><br />Summary:  
Accurate and interpretable gait analysis is essential for the early detection of Parkinson’s disease (PD). Most existing methods are limited by reliance on single-modality inputs, low robustness, and lack of clinical transparency. This paper proposes an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic and challenging conditions such as low lighting and occlusion. The system uses dual YOLOv11-based encoders to perform modality-specific feature extraction. A novel Multi-Scale Local-Global Extraction (MLGE) module along with a Cross-Spatial Neck Fusion mechanism enhances spatial-temporal representation, capturing detailed limb movements (e.g., reduced arm swing) and overall gait dynamics (e.g., short strides, turning difficulty). For clinical interpretability, a frozen Large Language Model (LLM) translates the fused visual embeddings and structured metadata into meaningful textual explanations. Experimental results demonstrate that this RGB-D fusion framework outperforms single-input baselines in accuracy and robustness, while providing clear visual-linguistic reasoning. By combining multimodal feature learning with language-based explainability, the study bridges the gap between visual recognition and clinical understanding, presenting a novel vision-language paradigm for reliable, interpretable Parkinson’s disease gait analysis. The code for the system is publicly available at the provided GitHub link. <div>
arXiv:2512.04425v1 Announce Type: new 
Abstract: Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Paced and Self-Corrective Masked Prediction for Movie Trailer Generation</title>
<link>https://arxiv.org/abs/2512.04426</link>
<guid>https://arxiv.org/abs/2512.04426</guid>
<content:encoded><![CDATA[
<div> Movie trailer generation, Transformer encoder, self-paced learning, masked prediction, self-correction  

<br /><br />Summary:  
This paper addresses the task of automatic movie trailer generation, which involves selecting and reorganizing movie shots to create engaging trailers. It critiques the dominant "selection-then-ranking" approach that first selects key shots and then ranks them, noting this method suffers from error propagation that limits trailer quality. To overcome these limitations, the authors propose a novel approach called SSMP, a self-paced and self-corrective masked prediction method. SSMP employs a Transformer encoder that models bi-directional context by taking entire sequences of movie shots as input prompts and generates trailer shot sequences. Training involves reconstructing trailer shot sequences from randomly masked versions, with the mask ratio controlled in a self-paced manner to adapt task difficulty to the model's performance. During generation, SSMP progressively fills in shot positions with high confidence predictions and re-masks remaining positions for further refinement, implementing a self-correction mechanism inspired by human editing processes. Extensive quantitative evaluations and user studies verify that SSMP significantly outperforms existing automatic trailer generation techniques. A demonstration of the method is made available via a publicly accessible GitHub repository. <div>
arXiv:2512.04426v1 Announce Type: new 
Abstract: As a challenging video editing task, movie trailer generation involves selecting and reorganizing movie shots to create engaging trailers. Currently, most existing automatic trailer generation methods employ a "selection-then-ranking" paradigm (i.e., first selecting key shots and then ranking them), which suffers from inevitable error propagation and limits the quality of the generated trailers. Beyond this paradigm, we propose a new self-paced and self-corrective masked prediction method called SSMP, which achieves state-of-the-art results in automatic trailer generation via bi-directional contextual modeling and progressive self-correction. In particular, SSMP trains a Transformer encoder that takes the movie shot sequences as prompts and generates corresponding trailer shot sequences accordingly. The model is trained via masked prediction, reconstructing each trailer shot sequence from its randomly masked counterpart. The mask ratio is self-paced, allowing the task difficulty to adapt to the model and thereby improving model performance. When generating a movie trailer, the model fills the shot positions with high confidence at each step and re-masks the remaining positions for the next prediction, forming a progressive self-correction mechanism that is analogous to how human editors work. Both quantitative results and user studies demonstrate the superiority of SSMP in comparison to existing automatic movie trailer generation methods. Demo is available at: https://github.com/Dixin-Lab/SSMP.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.04441</link>
<guid>https://arxiv.org/abs/2512.04441</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, trajectory planning, context simulation, vision-language model, multi-objective evaluation<br /><br />Summary:<br /><br />This paper presents MindDrive, an innovative framework for End-to-End autonomous driving that harmonizes high-quality trajectory generation with comprehensive decision reasoning. The approach is structured into three key stages: context simulation, candidate trajectory generation, and multi-objective trade-off evaluation. The Future-aware Trajectory Generator (FaTG), leveraging a World Action Model, enables ego-conditioned "what-if" simulations to forecast potential future scenarios and produce foresighted trajectory candidates. Complementing this, the VLM-oriented Evaluator (VLoE) applies a large vision-language model's reasoning capabilities to evaluate these candidates on multiple objectives, including safety, comfort, and efficiency, facilitating decision-making aligned with human reasoning. Extensive testing on the NAVSIM-v1 and NAVSIM-v2 benchmarks confirms that MindDrive outperforms current methods by achieving superior performance in multi-dimensional driving metrics. The framework notably enhances safety, regulatory compliance, and generalization in autonomous driving tasks. Ultimately, this work marks a significant advancement toward interpretable and cognitively guided autonomous driving systems that are both robust and aligned with human-like decision processes. <div>
arXiv:2512.04441v1 Announce Type: new 
Abstract: End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of "context simulation - candidate generation - multi-objective trade-off". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned "what-if" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios</title>
<link>https://arxiv.org/abs/2512.04451</link>
<guid>https://arxiv.org/abs/2512.04451</guid>
<content:encoded><![CDATA[
<div> Embodied Intelligence, Streaming Video, Question Answering, Video-LLMs, Benchmark

<br /><br />Summary:  
This paper introduces StreamEQA, a novel benchmark designed for streaming video question answering specifically in embodied scenarios, addressing the need for continuous perception and reasoning over real-world streaming visual inputs. The benchmark evaluates multimedia large language models (MLLMs) along two key dimensions: Embodied and Streaming. The Embodied dimension categorizes questions into three levels—perception (fine-grained visual recognition), interaction (reasoning about agent-object interactions), and planning (high-level goal-directed reasoning). The Streaming dimension divides questions into backward, real-time, and forward reasoning, each requiring different temporal contexts to answer. StreamEQA is built on 156 independent long videos, encompassing 42 distinct tasks and approximately 21,000 question-answer pairs annotated with precise timestamps. The dataset was created using a hybrid pipeline that combines automated question generation with human refinement to ensure quality. Evaluations conducted on 13 state-of-the-art video-based large language models show that, despite their strong performance on conventional benchmarks, these models still face significant challenges in understanding streaming videos within embodied scenarios. The authors anticipate that StreamEQA will drive future research efforts toward improving streaming video comprehension in embodied intelligence applications. <div>
arXiv:2512.04451v1 Announce Type: new 
Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis</title>
<link>https://arxiv.org/abs/2512.04456</link>
<guid>https://arxiv.org/abs/2512.04456</guid>
<content:encoded><![CDATA[
<div> Keywords: image denoising, diffusion model, noise synthesis, guidance, data augmentation<br /><br />Summary:<br /><br />This paper addresses the challenge of acquiring real-world noisy image data for training denoising models by proposing a novel noise synthesis method called GuidNoise. Unlike previous generative approaches requiring extensive camera metadata and large noisy-clean image datasets, GuidNoise operates using only a single noisy/clean image pair as guidance, which is easier to obtain. The method leverages a diffusion model enhanced with a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss designed to improve the backward diffusion process, enabling the generation of realistic and diverse noise patterns. GuidNoise functions without the need for additional metadata during both training and inference, providing flexible noise synthesis across varying noise environments. A key advantage of GuidNoise is its ability to efficiently generate synthetic noisy-clean image pairs on demand at inference time. These synthetic pairs serve as a powerful data augmentation tool that significantly boosts denoising performance, particularly when using lightweight models or limited training data in practical applications. The paper also provides implementation code, facilitating reproducibility and adoption of the method in related image denoising tasks. <div>
arXiv:2512.04456v1 Announce Type: new 
Abstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning</title>
<link>https://arxiv.org/abs/2512.04459</link>
<guid>https://arxiv.org/abs/2512.04459</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, vision-language models, discrete diffusion, end-to-end driving, behavior-trajectory consistency<br /><br />Summary:<br /><br />The paper addresses the challenge of out-of-distribution (OOD) scenarios in autonomous driving by focusing on improving end-to-end (E2E) driving systems through the integration of vision-language models (VLMs). It critiques existing autoregressive (AR) VLMs for their limitations caused by causal attention and sequential token generation, which hinder consistency and controllability between high-level reasoning and low-level planning. The authors propose dVLM-AD, a novel diffusion-based vision-language model employing discrete diffusion and bidirectional attention mechanisms to enhance controllability and reliability via iterative denoising. This unified model integrates perception, structured reasoning, and low-level planning into an E2E driving framework. Evaluations on nuScenes and WOD-E2E datasets show that dVLM-AD achieves more consistent reasoning-action pairs compared to AR-based baselines. Despite using a modest backbone, it yields planning performance on par with existing VLM/VLA driving systems. Quantitatively, dVLM-AD improves behavior-trajectory consistency by 9% and success rates (RFS) by 6% in long-tail driving scenarios on the WOD-E2E benchmark. The results highlight discrete diffusion VLMs as a promising and controllable approach for scalable and reliable autonomous driving solutions. <div>
arXiv:2512.04459v1 Announce Type: new 
Abstract: The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniTS: Unified Time Series Generative Model for Remote Sensing</title>
<link>https://arxiv.org/abs/2512.04461</link>
<guid>https://arxiv.org/abs/2512.04461</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified Time Series Generative Model, spatiotemporal modeling, satellite remote sensing, cloud removal, time series forecasting<br /><br />Summary: This paper addresses the challenge of modeling complex Earth environment dynamics from satellite remote sensing data, focusing on tasks such as reconstructing continuous cloud-free image time series, detecting land cover changes, and forecasting surface evolution. Existing approaches typically rely on specialized models for each task without a unified framework. To overcome this, the authors propose UniTS, a Unified Time Series Generative Model that applies a single framework to multiple time series tasks including reconstruction, cloud removal, semantic change detection, and forecasting. UniTS is built on a flow matching generative paradigm that deterministically evolves data from noise to target guided by task-specific conditions, enabling unified spatiotemporal feature modeling across tasks. Its architecture features a diffusion transformer with spatiotemporal blocks enhanced by an Adaptive Condition Injector (ACor) to better incorporate multimodal inputs and a Spatiotemporal-aware Modulator (STM) to capture complex dependencies. To support evaluation, the authors present two new multimodal datasets, TS-S12 and TS-S12CR, targeting cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS outperforms current methods, especially under difficult conditions such as heavy cloud contamination, missing modalities, and phenological changes, showing strong generation and cognition capabilities for both low- and high-level time series tasks. <div>
arXiv:2512.04461v1 Announce Type: new 
Abstract: One of the primary objectives of satellite remote sensing is to capture the complex dynamics of the Earth environment, which encompasses tasks such as reconstructing continuous cloud-free time series images, detecting land cover changes, and forecasting future surface evolution. However, existing methods typically require specialized models tailored to different tasks, lacking unified modeling of spatiotemporal features across multiple time series tasks. In this paper, we propose a Unified Time Series Generative Model (UniTS), a general framework applicable to various time series tasks, including time series reconstruction, time series cloud removal, time series semantic change detection, and time series forecasting. Based on the flow matching generative paradigm, UniTS constructs a deterministic evolution path from noise to targets under the guidance of task-specific conditions, achieving unified modeling of spatiotemporal representations for multiple tasks. The UniTS architecture consists of a diffusion transformer with spatio-temporal blocks, where we design an Adaptive Condition Injector (ACor) to enhance the model's conditional perception of multimodal inputs, enabling high-quality controllable generation. Additionally, we design a Spatiotemporal-aware Modulator (STM) to improve the ability of spatio-temporal blocks to capture complex spatiotemporal dependencies. Furthermore, we construct two high-quality multimodal time series datasets, TS-S12 and TS-S12CR, filling the gap of benchmark datasets for time series cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS exhibits exceptional generative and cognitive capabilities in both low-level and high-level time series tasks. It significantly outperforms existing methods, particularly when facing challenges such as severe cloud contamination, modality absence, and forecasting phenological variations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeRA: Decoupled Representation Alignment for Video Tokenization</title>
<link>https://arxiv.org/abs/2512.04483</link>
<guid>https://arxiv.org/abs/2512.04483</guid>
<content:encoded><![CDATA[
<div> Keywords: DeRA, video tokenizer, spatial-temporal representation, Symmetric Alignment-Conflict Projection, autoregressive video generation<br /><br />Summary: This paper introduces DeRA, a novel one-dimensional (1D) video tokenizer designed to improve both training efficiency and performance by decoupling spatial and temporal representation learning. DeRA maintains a compact 1D latent space while factorizing video encoding into two distinct streams: appearance and motion. These streams are aligned with pretrained vision foundation models to effectively capture spatial semantics and temporal dynamics separately. To overcome the gradient conflicts caused by the heterogeneous supervision of these two streams, the authors propose a Symmetric Alignment-Conflict Projection (SACP) module. The SACP module proactively reformulates gradients by suppressing components along conflicting directions, thus stabilizing and improving training. Extensive evaluations demonstrate that DeRA significantly outperforms LARP, the previous state-of-the-art video tokenizer, by 25% on the UCF-101 dataset based on relative Frechet Video Distance (rFVD). Furthermore, when applied to autoregressive video generation tasks, DeRA achieves new state-of-the-art results both in class-conditional video generation on UCF-101 and frame prediction on Kinetics-600 (K600). Overall, DeRA presents a significant advancement in efficient and effective video tokenization through its novel factorization approach and conflict-aware gradient adjustment mechanism. <div>
arXiv:2512.04483v1 Announce Type: new 
Abstract: This paper presents DeRA, a novel 1D video tokenizer that decouples the spatial-temporal representation learning in video tokenization to achieve better training efficiency and performance. Specifically, DeRA maintains a compact 1D latent space while factorizing video encoding into appearance and motion streams, which are aligned with pretrained vision foundation models to capture the spatial semantics and temporal dynamics in videos separately. To address the gradient conflicts introduced by the heterogeneous supervision, we further propose the Symmetric Alignment-Conflict Projection (SACP) module that proactively reformulates gradients by suppressing the components along conflicting directions. Extensive experiments demonstrate that DeRA outperforms LARP, the previous state-of-the-art video tokenizer by 25% on UCF-101 in terms of rFVD. Moreover, using DeRA for autoregressive video generation, we also achieve new state-of-the-art results on both UCF-101 class-conditional generation and K600 frame prediction.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Birds Look The Same: Identity-Preserving Generation For Birds</title>
<link>https://arxiv.org/abs/2512.04485</link>
<guid>https://arxiv.org/abs/2512.04485</guid>
<content:encoded><![CDATA[
<div> birds, identity-preserving generation, NABirds Look-Alikes, zero-shot models, fine-grained recognition<br /><br />Summary:<br /><br />Since the development of controllable image generation, models allowing zero-shot and identity-preserving manipulation, like Insert Anything and OmniControl, have enhanced user customization without requiring fine-tuning. However, these models struggle with non-rigid and fine-grained categories, which lack high-quality accessible data, especially multi-view images or videos, limiting evaluation and improvement. Birds represent a challenging but important domain due to their diversity, fine-grained identification cues, and varied poses. To address this gap, the authors introduce the NABirds Look-Alikes (NABLA) dataset, featuring 4,759 expert-curated image pairs alongside 1,073 pairs from multi-image iNaturalist observations and a small set of videos, forming a benchmark for identity-preserving bird image generation. Evaluations show that existing state-of-the-art baselines perform poorly in maintaining bird identity on this dataset. The study further demonstrates that training methods grouping images by species, age, and sex—which act as proxies for identity—result in significant performance improvements for both species seen during training and novel unseen species. This work highlights the importance of specialized datasets and training strategies to improve fine-grained, identity-aware image generation for complex, non-rigid categories like birds. <div>
arXiv:2512.04485v1 Announce Type: new 
Abstract: Since the advent of controllable image generation, increasingly rich modes of control have enabled greater customization and accessibility for everyday users. Zero-shot, identity-preserving models such as Insert Anything and OminiControl now support applications like virtual try-on without requiring additional fine-tuning. While these models may be fitting for humans and rigid everyday objects, they still have limitations for non-rigid or fine-grained categories. These domains often lack accessible, high-quality data -- especially videos or multi-view observations of the same subject -- making them difficult both to evaluate and to improve upon. Yet, such domains are essential for moving beyond content creation toward applications that demand accuracy and fine detail. Birds are an excellent domain for this task: they exhibit high diversity, require fine-grained cues for identification, and come in a wide variety of poses. We introduce the NABirds Look-Alikes (NABLA) dataset, consisting of 4,759 expert-curated image pairs. Together with 1,073 pairs collected from multi-image observations on iNaturalist and a small set of videos, this forms a benchmark for evaluating identity-preserving generation of birds. We show that state-of-the-art baselines fail to maintain identity on this dataset, and we demonstrate that training on images grouped by species, age, and sex -- used as a proxy for identity -- substantially improves performance on both seen and unseen species.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Long-term Motion Generation with Extended Joint Targets</title>
<link>https://arxiv.org/abs/2512.04487</link>
<guid>https://arxiv.org/abs/2512.04487</guid>
<content:encoded><![CDATA[
<div> COMET, character motion, Transformer, real-time, style transfer<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating stable and controllable character motion in real-time, a critical issue in computer animation. 2. It introduces COMET, an autoregressive framework built on an efficient Transformer-based conditional Variational Autoencoder (VAE), which enables versatile and precise control of character joints for various animation tasks such as goal-reaching and in-betweening. 3. Unlike existing methods that struggle with fine-grained control or experience motion degradation over long sequences, COMET maintains high-quality motion synthesis during extended durations through a novel reference-guided feedback mechanism, which prevents error accumulation. 4. This feedback mechanism also functions as a plug-and-play stylization module, allowing real-time style transfer to be integrated seamlessly into the motion generation process. 5. Extensive evaluations demonstrate COMET’s superiority over current state-of-the-art methods, producing robust, high-quality motion at real-time speeds, making it suitable for demanding interactive applications in computer animation and character control. <div>
arXiv:2512.04487v1 Announce Type: new 
Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shift-Window Meets Dual Attention: A Multi-Model Architecture for Specular Highlight Removal</title>
<link>https://arxiv.org/abs/2512.04496</link>
<guid>https://arxiv.org/abs/2512.04496</guid>
<content:encoded><![CDATA[
<div> Keywords: specular highlight removal, multi-model architecture, convolutional neural networks, attention mechanism, Omni-Directional Attention Integration Block<br /><br />Summary:<br /><br />1. The presence of specular highlights in practical environments adversely affects visual quality and task performance, motivating the need for effective removal methods.  
2. Existing methods either focus on local details using convolutional neural networks or on global context using transformer models, but single-type approaches face a challenge balancing fine-grained local features and long-range dependencies across varying highlight scales.  
3. The proposed Multi-Model Specular Highlight Removal (MM-SHR) architecture integrates convolutional operations in shallow layers for local detail extraction, while employing attention mechanisms in deeper layers to capture global features efficiently and accurately.  
4. To handle long-range dependencies without excessive computational overhead, MM-SHR introduces two key modules: the Omni-Directional Attention Integration Block (OAIBlock) and the Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network (HDDAConv), which utilize omni-directional pixel-shifting and window-dividing operations on raw features.  
5. Extensive evaluations across three benchmarks and six surface material types show that MM-SHR surpasses state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The authors will release their implementation publicly on GitHub. <div>
arXiv:2512.04496v1 Announce Type: new 
Abstract: Inevitable specular highlights in practical environments severely impair the visual performance, thus degrading the task effectiveness and efficiency. Although there exist considerable methods that focus on local information from convolutional neural network models or global information from transformer models, the single-type model falls into a modeling dilemma between local fine-grained details and global long-range dependencies, thus deteriorating for specular highlights with different scales. Therefore, to accommodate specular highlights of all scales, we propose a multi-model architecture for specular highlight removal (MM-SHR) that effectively captures fine-grained features in highlight regions and models long-range dependencies between highlight and highlight-free areas. Specifically, we employ convolution operations to extract local details in the shallow layers of MM-SHR, and utilize the attention mechanism to capture global features in the deep layers, ensuring both operation efficiency and removal accuracy. To model long-range dependencies without compromising computational complexity, we utilize a coarse-to-fine manner and propose Omni-Directional Attention Integration Block(OAIBlock) and Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network(HDDAConv) , which leverage omni-directiona pixel-shifting and window-dividing operations at the raw features to achieve specular highlight removal. Extensive experimental results on three benchmark tasks and six types of surface materials demonstrate that MM-SHR outperforms state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The implementation will be made publicly available at https://github.com/Htcicv/MM-SHR.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model</title>
<link>https://arxiv.org/abs/2512.04499</link>
<guid>https://arxiv.org/abs/2512.04499</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, motion synthesis, motion representations, loss functions, training efficiency  

<br /><br />Summary:  
This paper explores fundamental aspects of diffusion models in human motion synthesis, focusing on task-oriented applications such as action-to-motion, text-to-motion, and audio-to-motion. The study uses a proxy motion diffusion model (MDM) and introduces the v loss objective, defined as a weighted sum of motion data and noise, to better understand latent data distributions. First, the authors evaluate six common motion representations from existing literature, comparing their performance using quality and diversity metrics across diverse datasets, revealing significant differences in effectiveness. Second, the paper investigates various training configurations, analyzing their impact on training time to offer insights into accelerating the training process of motion diffusion models. Third, extensive evaluation on a large motion dataset provides robust empirical evidence supporting the earlier findings. The results highlight how choices related to motion representation and training setup critically influence model performance and efficiency. Ultimately, the paper provides a comprehensive controlled study that advances the foundational understanding of conditional motion diffusion models, potentially guiding future research and development in the field for enhanced generative motion synthesis systems. <div>
arXiv:2512.04499v1 Announce Type: new 
Abstract: Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.04504</link>
<guid>https://arxiv.org/abs/2512.04504</guid>
<content:encoded><![CDATA[
<div> Keywords: image diffusion transformers, frequency analysis, positional embeddings, adaptive attention, ultra-high-resolution generation<br /><br />Summary: UltraImage addresses key challenges in image diffusion transformers related to large-scale image generation, particularly content repetition and quality degradation. By performing frequency-wise analysis of positional embeddings, the authors identify that content repetition is caused by the periodicity of a dominant frequency that corresponds to the training image resolution. To mitigate this, they propose a recursive dominant frequency correction method that confines the frequency within a single period during extrapolation. Furthermore, the research reveals that degraded image quality results from diluted attention in the transformer network. To counteract this, UltraImage employs entropy-guided adaptive attention concentration which adjusts the attention focus dynamically—sharpening local attention for fine details while maintaining lower attention on global structures to preserve overall consistency. Experimental results on Qwen-Image and Flux datasets (around 4K resolution) demonstrate that UltraImage reduces repetition and enhances visual fidelity compared to existing methods. Impressively, UltraImage can extrapolate beyond training resolution (1328p) to generate images up to 6K by 6K without relying on low-resolution guidance, underscoring its strong extrapolation capability. The project and additional resources can be accessed via their webpage. <div>
arXiv:2512.04504v1 Announce Type: new 
Abstract: Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at \href{https://thu-ml.github.io/ultraimage.github.io/}{https://thu-ml.github.io/ultraimage.github.io/}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance</title>
<link>https://arxiv.org/abs/2512.04511</link>
<guid>https://arxiv.org/abs/2512.04511</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared imaging, foundation model, masked autoencoder, dual-domain guidance, large-scale dataset

<br /><br />Summary: Infrared imaging is essential for effective vision in low-light and adverse weather conditions but existing foundation models like Masked Autoencoder (MAE), trained primarily on visible spectrum data, do not perform optimally for infrared images. To address this issue, the authors developed an infrared-specific foundation model called InfMAE, pretrained on large infrared datasets. However, InfMAE has limitations such as missing informative tokens, weak modeling of global token relationships, and lack of noise handling for non-uniform background noise found in infrared imagery. To overcome these challenges, the paper proposes DuGI-MAE, a Dual-domain Guided Infrared MAE model. DuGI-MAE employs a deterministic masking strategy based on token entropy that retains only high-entropy tokens to improve informativeness in reconstruction. In addition, the model incorporates a Dual-Domain Guidance (DDG) module to simultaneously capture global token associations and adaptively filter out non-uniform background noise. To support extensive pretraining, the authors introduce Inf-590K, a comprehensive infrared image dataset with diverse scenes, target types, and spatial resolutions. The model pretrained on Inf-590K demonstrates strong generalization across various downstream tasks including infrared object detection, semantic segmentation, and small target detection. Experimental results show that DuGI-MAE outperforms both supervised and self-supervised baselines, confirming its effectiveness. The code is provided as supplementary material. <div>
arXiv:2512.04511v1 Announce Type: new 
Abstract: Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoLCD: Egocentric Video Generation with Long Context Diffusion</title>
<link>https://arxiv.org/abs/2512.04515</link>
<guid>https://arxiv.org/abs/2512.04515</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric video generation, long-term memory, content drift, memory management, temporal consistency<br /><br />Summary:<br /><br />1. Generating long and coherent egocentric videos is challenging due to the complexity of hand-object interactions and procedural tasks that require reliable long-term memory retention.<br />2. Existing autoregressive video generation models tend to suffer from content drift, where the identity of objects and overall scene semantics degrade as the video progresses.<br />3. The proposed framework, EgoLCD, addresses these limitations by framing long video synthesis as a problem of efficient and stable memory management, combining both long-term and short-term memory components.<br />4. EgoLCD integrates a Long-Term Sparse Key-Value (KV) Cache to maintain stable global context and an attention-based short-term memory module extended via Low-Rank Adaptation (LoRA) for local content adaptation.<br />5. A novel Memory Regulation Loss is introduced to enforce consistent memory usage throughout the sequence, while Structured Narrative Prompting provides explicit temporal guidance to improve coherence.<br />6. Experimental results on the EgoVid-5M benchmark show that EgoLCD outperforms existing methods in perceptual quality and temporal consistency, effectively reducing generative forgetting.<br />7. The work represents an important step toward scalable world models for embodied AI, supported by publicly available code and a project website for further research and development. <div>
arXiv:2512.04515v1 Announce Type: new 
Abstract: Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory</title>
<link>https://arxiv.org/abs/2512.04519</link>
<guid>https://arxiv.org/abs/2512.04519</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive diffusion, long-video generation, state-space model, temporal consistency, interactive control<br /><br />Summary:<br /><br />This paper addresses the challenge of generating coherent long videos through autoregressive (AR) diffusion, which produces frames sequentially but faces issues like accumulated errors, motion drift, and repetitive content over minute-scale durations. To overcome these problems, the authors propose VideoSSM, a novel Long Video Model that combines autoregressive diffusion with a hybrid state-space memory system. The state-space model (SSM) acts as a global memory that evolves with the scene dynamics throughout the entire video sequence, while a localized context window captures finer motion details and local nuances. This hybrid memory design helps maintain global consistency in the video content, preventing frozen or repetitive patterns typically seen in long-video generation. Additionally, VideoSSM supports prompt-adaptive interaction, allowing dynamic user control during generation, and efficiently scales linearly with the length of the video sequence. Experimental results on both short- and long-range video benchmarks show that VideoSSM achieves state-of-the-art temporal consistency and motion stability for autoregressive video generation, especially over minute-long horizons. Overall, the work establishes a scalable framework that integrates memory-aware mechanisms for producing diverse, interactive, and coherent long-duration video content. <div>
arXiv:2512.04519v1 Announce Type: new 
Abstract: Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.04520</link>
<guid>https://arxiv.org/abs/2512.04520</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot segmentation, test-time adaptation, medical image segmentation, foundation models, boundary-aware attention<br /><br />Summary: This paper addresses the limitations of conventional tuning methods in medical image segmentation, which struggle due to scarce annotated data and high computational costs. It highlights the promise of zero-shot segmentation using foundation models like SAM (Segment Anything Model), while noting SAM’s reduced effectiveness on medical datasets caused by domain shifts. To tackle these issues, the authors propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that enhances SAM's zero-shot segmentation performance without requiring any source-domain training data. The framework incorporates two key innovations: (1) encoder-level Gaussian prompt injection, which embeds Gaussian-based prompts into the image encoder for improved initial representation learning, and (2) cross-layer boundary-aware attention alignment, which leverages hierarchical interactions within the ViT backbone to align deep semantic features with shallow boundary cues. Experimental results across four public medical datasets (ISIC, Kvasir, BUSI, and REFUGE) demonstrate an average DICE score improvement of 12.4% over SAM’s zero-shot baseline. The proposed method consistently outperforms state-of-the-art models in medical image segmentation, significantly boosting SAM’s generalization capability without additional training on source domain data. The code is publicly available for further research and implementation. <div>
arXiv:2512.04520v1 Announce Type: new 
Abstract: Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WiFi-based Cross-Domain Gesture Recognition Using Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.04521</link>
<guid>https://arxiv.org/abs/2512.04521</guid>
<content:encoded><![CDATA[
<div> Keywords: WiFi sensing, gesture recognition, cross-domain, Doppler spectrum, attention mechanism<br /><br />Summary:<br /><br />This paper addresses the challenge of cross-domain gesture recognition using WiFi signals, which are advantageous due to their widespread availability, low cost, and robustness against environmental factors. The authors propose a novel approach that extracts Doppler spectra from channel state information (CSI) collected by multiple receivers. These spectra are concatenated along the time axis to form fused multi-angle images, enriching the input features for gesture recognition. A new neural network architecture is introduced, inspired by the convolutional block attention module (CBAM), which combines a multi-semantic spatial attention mechanism with a self-attention-based channel mechanism. This design enables the network to generate attention maps that highlight important spatiotemporal gesture features, crucial for capturing domain-independent characteristics. The well-known ResNet18 model is integrated as the backbone to extract deeper-level features effectively. The authors validate their network on the public Widar3 dataset, demonstrating that their approach achieves near-perfect accuracy in the trained domain (99.72%) and maintains high accuracy when tested across different domains (97.61%). This performance significantly surpasses existing state-of-the-art methods, indicating that their multi-angle Doppler fusion and attention-based network effectively enhance cross-domain recognition capabilities in WiFi-based gesture sensing. <div>
arXiv:2512.04521v1 Announce Type: new 
Abstract: While fulfilling communication tasks, wireless signals can also be used to sense the environment. Among various types of sensing media, WiFi signals offer advantages such as widespread availability, low hardware cost, and strong robustness to environmental conditions like light, temperature, and humidity. By analyzing Wi-Fi signals in the environment, it is possible to capture dynamic changes of the human body and accomplish sensing applications such as gesture recognition. Although many existing gesture sensing solutions perform well in-domain but lack cross-domain capabilities (i.e., recognition performance in untrained environments). To address this, we extract Doppler spectra from the channel state information (CSI) received by all receivers and concatenate each Doppler spectrum along the same time axis to generate fused images with multi-angle information as input features. Furthermore, inspired by the convolutional block attention module (CBAM), we propose a gesture recognition network that integrates a multi-semantic spatial attention mechanism with a self-attention-based channel mechanism. This network constructs attention maps to quantify the spatiotemporal features of gestures in images, enabling the extraction of key domain-independent features. Additionally, ResNet18 is employed as the backbone network to further capture deep-level features. To validate the network performance, we evaluate the proposed network on the public Widar3 dataset, and the results show that it not only maintains high in-domain accuracy of 99.72%, but also achieves high performance in cross-domain recognition of 97.61%, significantly outperforming existing best solutions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.04522</link>
<guid>https://arxiv.org/abs/2512.04522</guid>
<content:encoded><![CDATA[
<div> Visible-Infrared Person Re-Identification, cross-modal matching, modality-specific attributes, semantic distillation, identity-aware features  

<br /><br />Summary:  
This paper addresses the challenges of Visible-Infrared Person Re-Identification (VI-ReID), a cross-modal matching task complicated by significant differences between visual and infrared data modalities. Unlike existing approaches that primarily learn modality-invariant features by focusing only on shared discriminative semantics, this work emphasizes the importance of modality-specific identity-aware information for improved feature discrimination. The authors introduce the Identity Clue Refinement and Enhancement (ICRE) network, which incorporates several novel components. First, the Multi-Perception Feature Refinement (MPFR) module aggregates shallow features from shared network branches to better capture overlooked modality-specific attributes. Next, the Semantic Distillation Cascade Enhancement (SDCE) module distills identity-relevant knowledge from these shallow aggregated features to enhance the learning of modality-invariant features. Additionally, an Identity Clues Guided (ICG) Loss is proposed to mitigate modality discrepancies in the enhanced features and encourage a more diverse representation space. Extensive experiments validate the effectiveness of the ICRE framework, demonstrating its superior performance over state-of-the-art methods across multiple publicly available datasets. This work highlights the value of integrating modality-specific identity clues alongside invariant features for robust VI-ReID. <div>
arXiv:2512.04522v1 Announce Type: new 
Abstract: Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto3R: Automated 3D Reconstruction and Scanning via Data-driven Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2512.04528</link>
<guid>https://arxiv.org/abs/2512.04528</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scanning, uncertainty quantification, autonomous reconstruction, non-lambertian materials, robotic digitization  

<br /><br />Summary:  
This article presents Auto3R, a novel data-driven uncertainty quantification model designed to automate the 3D scanning and reconstruction process for both scenes and objects. The motivation stems from the traditionally labor-intensive nature of high-quality 3D scanning, which requires human planning, and the emerging need for fully autonomous systems using drones and robots. Auto3R specifically addresses challenges posed by objects with complex surface properties such as non-lambertian and specular materials, which are difficult to scan accurately. The model operates in an iterative loop, predicting the uncertainty distribution of potential scanning viewpoints without prior knowledge of the ground truth geometry or appearance, enabling efficient and precise viewpoint selection. Extensive experiments demonstrate that Auto3R significantly outperforms existing state-of-the-art methods in accuracy and efficiency. Moreover, the approach is validated in practical deployment by integrating Auto3R onto a robotic arm equipped with a camera, successfully digitizing real-world objects to create photorealistic digital assets ready for use. The contribution facilitates fully automated, high-quality 3D digitization, advancing the capabilities of embodied systems in autonomous perception and reconstruction tasks. Additional resources and code are available on the project’s homepage for further exploration and application. <div>
arXiv:2512.04528v1 Announce Type: new 
Abstract: Traditional high-quality 3D scanning and reconstruction typically relies on human labor to plan the scanning procedure. With the rapid development of embodied systems such as drones and robots, there is a growing demand of performing accurate 3D scanning and reconstruction in an fully automated manner. We introduce Auto3R, a data-driven uncertainty quantification model that is designed to automate the 3D scanning and reconstruction of scenes and objects, including objects with non-lambertian and specular materials. Specifically, in a process of iterative 3D reconstruction and scanning, Auto3R can make efficient and accurate prediction of uncertainty distribution over potential scanning viewpoints, without knowing the ground truth geometry and appearance. Through extensive experiments, Auto3R achieves superior performance that outperforms the state-of-the-art methods by a large margin. We also deploy Auto3R on a robot arm equipped with a camera and demonstrate that Auto3R can be used to effectively digitize real-world 3D objects and delivers ready-to-use and photorealistic digital assets. Our homepage: https://tomatoma00.github.io/auto3r.github.io .
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement</title>
<link>https://arxiv.org/abs/2512.04532</link>
<guid>https://arxiv.org/abs/2512.04532</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, physical dynamics, Neural ODE, motion modeling, self-supervised learning<br /><br />Summary:<br /><br />Video Large Language Models (Video LLMs) excel at video-language tasks but struggle with deep physical dynamics understanding due to their reliance on appearance-based matching. To overcome this, the authors propose PhyVLLM, a framework designed to explicitly incorporate physical motion into Video LLMs. PhyVLLM addresses three main challenges: disentangling motion signals from appearance variations, modeling continuous-time physical dynamics, and avoiding the need for costly physical annotations. The approach utilizes a dual-branch encoder to separate visual appearance from object motion effectively. A Neural Ordinary Differential Equation (Neural ODE) module is integrated to model the temporal evolution of physical dynamics, producing differentiable motion-aware representations. These representations are then projected into the token space of a pretrained Large Language Model, allowing physics reasoning without sacrificing general multimodal capabilities. Importantly, PhyVLLM uses self-supervised learning to capture continuous motion evolution, thereby eliminating the requirement for explicit physical labels. Experimental results show that PhyVLLM significantly outperforms state-of-the-art Video LLMs in physical reasoning and overall video understanding, validating the benefits of embedding explicit physical modeling into video-language frameworks. <div>
arXiv:2512.04532v1 Announce Type: new 
Abstract: Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refa\c{c}ade: Editing Object with Given Reference Texture</title>
<link>https://arxiv.org/abs/2512.04534</link>
<guid>https://arxiv.org/abs/2512.04534</guid>
<content:encoded><![CDATA[
arXiv:2512.04534v1 Announce Type: new 
Abstract: Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored. In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos. To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture. However, this approach suffers from limited controllability for two reasons: conditioning on the raw reference image introduces unwanted structural information, and it fails to disentangle the visual texture and structure information of the source. To address this problem, we propose Refa\c{c}ade, a method that consists of two key designs to achieve precise and controllable texture transfer in both images and videos. First, we employ a texture remover trained on paired textured/untextured 3D mesh renderings to remove appearance information while preserving the geometry and motion of source videos. Second, we disrupt the reference global layout using a jigsaw permutation, encouraging the model to focus on local texture statistics rather than the global layout of the object. Extensive experiments demonstrate superior visual quality, precise editing, and controllability, outperforming strong baselines in both quantitative and human evaluations. Code is available at https://github.com/fishZe233/Refacade.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model</title>
<link>https://arxiv.org/abs/2512.04536</link>
<guid>https://arxiv.org/abs/2512.04536</guid>
<content:encoded><![CDATA[
arXiv:2512.04536v1 Announce Type: new 
Abstract: Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</title>
<link>https://arxiv.org/abs/2512.04537</link>
<guid>https://arxiv.org/abs/2512.04537</guid>
<content:encoded><![CDATA[
arXiv:2512.04537v1 Announce Type: new 
Abstract: The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management</title>
<link>https://arxiv.org/abs/2512.04540</link>
<guid>https://arxiv.org/abs/2512.04540</guid>
<content:encoded><![CDATA[
arXiv:2512.04540v1 Announce Type: new 
Abstract: Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</title>
<link>https://arxiv.org/abs/2512.04542</link>
<guid>https://arxiv.org/abs/2512.04542</guid>
<content:encoded><![CDATA[
arXiv:2512.04542v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\&amp;T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\&amp;T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.04554</link>
<guid>https://arxiv.org/abs/2512.04554</guid>
<content:encoded><![CDATA[
arXiv:2512.04554v1 Announce Type: new 
Abstract: Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.04563</link>
<guid>https://arxiv.org/abs/2512.04563</guid>
<content:encoded><![CDATA[
arXiv:2512.04563v1 Announce Type: new 
Abstract: Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset creation for supervised deep learning-based analysis of microscopic images - review of important considerations and recommendations</title>
<link>https://arxiv.org/abs/2512.04564</link>
<guid>https://arxiv.org/abs/2512.04564</guid>
<content:encoded><![CDATA[
arXiv:2512.04564v1 Announce Type: new 
Abstract: Supervised deep learning (DL) receives great interest for automated analysis of microscopic images with an increasing body of literature supporting its potential. The development and validation of those DL models relies heavily on the availability of high-quality, large-scale datasets. However, creating such datasets is a complex and resource-intensive process, often hindered by challenges such as time constraints, domain variability, and risks of bias in image collection and label creation. This review provides a comprehensive guide to the critical steps in dataset creation, including: 1) image acquisition, 2) selection of annotation software, and 3) annotation creation. In addition to ensuring a sufficiently large number of images, it is crucial to address sources of image variability (domain shifts) - such as those related to slide preparation and digitization - that could lead to algorithmic errors if not adequately represented in the training data. Key quality criteria for annotations are the three "C"s: correctness, completeness, and consistency. This review explores methods to enhance annotation quality through the use of advanced techniques that mitigate the limitations of single annotators. To support dataset creators, a standard operating procedure (SOP) is provided as supplemental material, outlining best practices for dataset development. Furthermore, the article underscores the importance of open datasets in driving innovation and enhancing reproducibility of DL research. By addressing the challenges and offering practical recommendations, this review aims to advance the creation of and availability to high-quality, large-scale datasets, ultimately contributing to the development of generalizable and robust DL models for pathology applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt2Craft: Generating Functional Craft Assemblies with LLMs</title>
<link>https://arxiv.org/abs/2512.04568</link>
<guid>https://arxiv.org/abs/2512.04568</guid>
<content:encoded><![CDATA[
arXiv:2512.04568v1 Announce Type: new 
Abstract: Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labeled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</title>
<link>https://arxiv.org/abs/2512.04576</link>
<guid>https://arxiv.org/abs/2512.04576</guid>
<content:encoded><![CDATA[
arXiv:2512.04576v1 Announce Type: new 
Abstract: Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the "missing modality" problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. TARDis explicitly disentangles the latent feature space into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to hallucinate missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale private abdominal CT dataset (2,282 cases) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.04581</link>
<guid>https://arxiv.org/abs/2512.04581</guid>
<content:encoded><![CDATA[
arXiv:2512.04581v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM3-I: Segment Anything with Instructions</title>
<link>https://arxiv.org/abs/2512.04585</link>
<guid>https://arxiv.org/abs/2512.04585</guid>
<content:encoded><![CDATA[
arXiv:2512.04585v1 Announce Type: new 
Abstract: Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering</title>
<link>https://arxiv.org/abs/2512.04597</link>
<guid>https://arxiv.org/abs/2512.04597</guid>
<content:encoded><![CDATA[
arXiv:2512.04597v1 Announce Type: new 
Abstract: Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</title>
<link>https://arxiv.org/abs/2512.04599</link>
<guid>https://arxiv.org/abs/2512.04599</guid>
<content:encoded><![CDATA[
arXiv:2512.04599v1 Announce Type: new 
Abstract: Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence</title>
<link>https://arxiv.org/abs/2512.04619</link>
<guid>https://arxiv.org/abs/2512.04619</guid>
<content:encoded><![CDATA[
arXiv:2512.04619v1 Announce Type: new 
Abstract: In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding</title>
<link>https://arxiv.org/abs/2512.04643</link>
<guid>https://arxiv.org/abs/2512.04643</guid>
<content:encoded><![CDATA[
arXiv:2512.04643v1 Announce Type: new 
Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models</title>
<link>https://arxiv.org/abs/2512.04660</link>
<guid>https://arxiv.org/abs/2512.04660</guid>
<content:encoded><![CDATA[
arXiv:2512.04660v1 Announce Type: new 
Abstract: Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose \textbf{I2I-Bench}, a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
<link>https://arxiv.org/abs/2512.04677</link>
<guid>https://arxiv.org/abs/2512.04677</guid>
<content:encoded><![CDATA[
arXiv:2512.04677v1 Announce Type: new 
Abstract: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation</title>
<link>https://arxiv.org/abs/2512.04678</link>
<guid>https://arxiv.org/abs/2512.04678</guid>
<content:encoded><![CDATA[
arXiv:2512.04678v1 Announce Type: new 
Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Cross-View Point Correspondence in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04686</link>
<guid>https://arxiv.org/abs/2512.04686</guid>
<content:encoded><![CDATA[
arXiv:2512.04686v1 Announce Type: new 
Abstract: Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.04699</link>
<guid>https://arxiv.org/abs/2512.04699</guid>
<content:encoded><![CDATA[
arXiv:2512.04699v1 Announce Type: new 
Abstract: Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild</title>
<link>https://arxiv.org/abs/2512.04728</link>
<guid>https://arxiv.org/abs/2512.04728</guid>
<content:encoded><![CDATA[
arXiv:2512.04728v1 Announce Type: new 
Abstract: Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.04733</link>
<guid>https://arxiv.org/abs/2512.04733</guid>
<content:encoded><![CDATA[
arXiv:2512.04733v1 Announce Type: new 
Abstract: End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title>
<link>https://arxiv.org/abs/2512.04734</link>
<guid>https://arxiv.org/abs/2512.04734</guid>
<content:encoded><![CDATA[
arXiv:2512.04734v1 Announce Type: new 
Abstract: Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: 3D Shape Generation from Sequential VR Sketches</title>
<link>https://arxiv.org/abs/2512.04761</link>
<guid>https://arxiv.org/abs/2512.04761</guid>
<content:encoded><![CDATA[
arXiv:2512.04761v1 Announce Type: new 
Abstract: VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling</title>
<link>https://arxiv.org/abs/2512.04784</link>
<guid>https://arxiv.org/abs/2512.04784</guid>
<content:encoded><![CDATA[
arXiv:2512.04784v1 Announce Type: new 
Abstract: Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaFiTe: A Generative Latent Field for 3D Native Texturing</title>
<link>https://arxiv.org/abs/2512.04786</link>
<guid>https://arxiv.org/abs/2512.04786</guid>
<content:encoded><![CDATA[
arXiv:2512.04786v1 Announce Type: new 
Abstract: Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title>
<link>https://arxiv.org/abs/2512.04810</link>
<guid>https://arxiv.org/abs/2512.04810</guid>
<content:encoded><![CDATA[
arXiv:2512.04810v1 Announce Type: new 
Abstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS</title>
<link>https://arxiv.org/abs/2512.04815</link>
<guid>https://arxiv.org/abs/2512.04815</guid>
<content:encoded><![CDATA[
arXiv:2512.04815v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.04821</link>
<guid>https://arxiv.org/abs/2512.04821</guid>
<content:encoded><![CDATA[
arXiv:2512.04821v1 Announce Type: new 
Abstract: Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis</title>
<link>https://arxiv.org/abs/2512.04830</link>
<guid>https://arxiv.org/abs/2512.04830</guid>
<content:encoded><![CDATA[
arXiv:2512.04830v1 Announce Type: new 
Abstract: Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Buildings: A Transformer for Layout Synthesis</title>
<link>https://arxiv.org/abs/2512.04832</link>
<guid>https://arxiv.org/abs/2512.04832</guid>
<content:encoded><![CDATA[
arXiv:2512.04832v1 Announce Type: new 
Abstract: We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World</title>
<link>https://arxiv.org/abs/2512.04837</link>
<guid>https://arxiv.org/abs/2512.04837</guid>
<content:encoded><![CDATA[
arXiv:2512.04837v1 Announce Type: new 
Abstract: Existing methods for deepfake detection aim to develop generalizable detectors. Although "generalizable" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens</title>
<link>https://arxiv.org/abs/2512.04857</link>
<guid>https://arxiv.org/abs/2512.04857</guid>
<content:encoded><![CDATA[
arXiv:2512.04857v1 Announce Type: new 
Abstract: Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing</title>
<link>https://arxiv.org/abs/2512.04862</link>
<guid>https://arxiv.org/abs/2512.04862</guid>
<content:encoded><![CDATA[
arXiv:2512.04862v1 Announce Type: new 
Abstract: Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection</title>
<link>https://arxiv.org/abs/2512.04875</link>
<guid>https://arxiv.org/abs/2512.04875</guid>
<content:encoded><![CDATA[
arXiv:2512.04875v1 Announce Type: new 
Abstract: Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms</title>
<link>https://arxiv.org/abs/2512.04883</link>
<guid>https://arxiv.org/abs/2512.04883</guid>
<content:encoded><![CDATA[
arXiv:2512.04883v1 Announce Type: new 
Abstract: Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Train Once (YOTO): A Retraining-Free Object Detection Framework</title>
<link>https://arxiv.org/abs/2512.04888</link>
<guid>https://arxiv.org/abs/2512.04888</guid>
<content:encoded><![CDATA[
arXiv:2512.04888v1 Announce Type: new 
Abstract: Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI</title>
<link>https://arxiv.org/abs/2512.04890</link>
<guid>https://arxiv.org/abs/2512.04890</guid>
<content:encoded><![CDATA[
arXiv:2512.04890v1 Announce Type: new 
Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching</title>
<link>https://arxiv.org/abs/2512.04904</link>
<guid>https://arxiv.org/abs/2512.04904</guid>
<content:encoded><![CDATA[
arXiv:2512.04904v1 Announce Type: new 
Abstract: Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion</title>
<link>https://arxiv.org/abs/2512.04926</link>
<guid>https://arxiv.org/abs/2512.04926</guid>
<content:encoded><![CDATA[
arXiv:2512.04926v1 Announce Type: new 
Abstract: Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting</title>
<link>https://arxiv.org/abs/2512.04927</link>
<guid>https://arxiv.org/abs/2512.04927</guid>
<content:encoded><![CDATA[
arXiv:2512.04927v1 Announce Type: new 
Abstract: The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</title>
<link>https://arxiv.org/abs/2512.04939</link>
<guid>https://arxiv.org/abs/2512.04939</guid>
<content:encoded><![CDATA[
arXiv:2512.04939v1 Announce Type: new 
Abstract: 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition</title>
<link>https://arxiv.org/abs/2512.04943</link>
<guid>https://arxiv.org/abs/2512.04943</guid>
<content:encoded><![CDATA[
arXiv:2512.04943v1 Announce Type: new 
Abstract: This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization</title>
<link>https://arxiv.org/abs/2512.04952</link>
<guid>https://arxiv.org/abs/2512.04952</guid>
<content:encoded><![CDATA[
arXiv:2512.04952v1 Announce Type: new 
Abstract: Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>