<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement</title>
<link>https://arxiv.org/abs/2505.04207</link>
<guid>https://arxiv.org/abs/2505.04207</guid>
<content:encoded><![CDATA[
<div> Keywords: pothole detection, RGB-D images, YOLOv8-based model, depth measurement, intelligent transportation solutions

Summary: 
Potholes on roads pose safety and economic concerns, necessitating accurate detection methods. This study introduces a dataset of RGB-D images (PothRGBD) and an improved YOLOv8-based model for pothole detection and analysis. By utilizing the Intel RealSense D415 depth camera, 1000 images were collected and labeled for segmentation. The proposed YOLO model, enhanced with DSConv, SimAM, and GELU, demonstrated improved accuracy in segmenting irregular pothole edges and measuring perimeter and depth in depth maps. Compared to the standard YOLOv8n-seg model, the proposed model achieved higher precision, recall, and mAP values. With a 1.96% increase in precision, 6.13% in recall, and 2.07% in mAP, the model proved effective for real-time applications in intelligent transportation solutions. This lightweight and accurate model offers a valuable tool for deep learning-based pothole detection and analysis. 

<br /><br />Summary: <div>
arXiv:2505.04207v2 Announce Type: replace 
Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning</title>
<link>https://arxiv.org/abs/2505.10575</link>
<guid>https://arxiv.org/abs/2505.10575</guid>
<content:encoded><![CDATA[
<div> Keywords: Emotion recognition, Physiological signals, EEG, Continual learning, Bi-level framework

Summary:
The article introduces a novel bi-level self-supervised continual learning framework, SSOCL, for emotion recognition using physiological signals like EEG. The framework addresses challenges such as cross-subject variability and noisy labels in data. SSOCL utilizes a dynamic memory buffer to retain representative samples from continuous, unlabeled data streams, enhancing generalization for emotion recognition. Pseudo-label assignments are iteratively refined in the bi-level architecture to improve emotion prediction accuracy. Key components like a fast adaptation module and a cluster-mapping module enable robust learning and efficient handling of evolving data streams. Experimental results on EEG tasks demonstrate SSOCL's superior performance in adapting to continuous data streams and maintaining strong generalization across subjects, surpassing existing approaches. <div>
arXiv:2505.10575v1 Announce Type: new 
Abstract: Emotion recognition through physiological signals such as electroencephalogram (EEG) has become an essential aspect of affective computing and provides an objective way to capture human emotions. However, physiological data characterized by cross-subject variability and noisy labels hinder the performance of emotion recognition models. Existing domain adaptation and continual learning methods struggle to address these issues, especially under realistic conditions where data is continuously streamed and unlabeled. To overcome these limitations, we propose a novel bi-level self-supervised continual learning framework, SSOCL, based on a dynamic memory buffer. This bi-level architecture iteratively refines the dynamic buffer and pseudo-label assignments to effectively retain representative samples, enabling generalization from continuous, unlabeled physiological data streams for emotion recognition. The assigned pseudo-labels are subsequently leveraged for accurate emotion prediction. Key components of the framework, including a fast adaptation module and a cluster-mapping module, enable robust learning and effective handling of evolving data streams. Experimental validation on two mainstream EEG tasks demonstrates the framework's ability to adapt to continuous data streams while maintaining strong generalization across subjects, outperforming existing approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias and Generalizability of Foundation Models across Datasets in Breast Mammography</title>
<link>https://arxiv.org/abs/2505.10579</link>
<guid>https://arxiv.org/abs/2505.10579</guid>
<content:encoded><![CDATA[
<div> fairness, bias, computer-aided diagnosis, breast cancer, mammography<br />
Summary:<br />
- Computer-aided diagnosis tools for breast cancer face challenges in clinical adoption due to data variability and biases.<br />
- Foundation models (FMs) show impressive generalizability but can be affected by spurious correlations from image quality and sensitive patient attributes.<br />
- Fairness and bias of FMs for breast mammography classification are explored using diverse datasets, with modality-specific pre-training enhancing performance.<br />
- Classifiers trained on individual datasets struggle to generalize across domains, leading to disparities in underrepresented subgroups like extreme breast densities and age groups.<br />
- Fairness-aware techniques provide more stable and equitable performance across subgroups, highlighting the importance of incorporating fairness evaluations and mitigation strategies into FM-based models to promote inclusive and generalizable AI.<br />  <div>
arXiv:2505.10579v1 Announce Type: new 
Abstract: Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability and transfer learning capabilities by leveraging vast and diverse datasets, their performance can be undermined by spurious correlations that arise from variations in image quality, labeling uncertainty, and sensitive patient attributes. In this work, we explore the fairness and bias of FMs for breast mammography classification by leveraging a large pool of datasets from diverse sources-including data from underrepresented regions and an in-house dataset. Our extensive experiments show that while modality-specific pre-training of FMs enhances performance, classifiers trained on features from individual datasets fail to generalize across domains. Aggregating datasets improves overall performance, yet does not fully mitigate biases, leading to significant disparities across under-represented subgroups such as extreme breast densities and age groups. Furthermore, while domain-adaptation strategies can reduce these disparities, they often incur a performance trade-off. In contrast, fairness-aware techniques yield more stable and equitable performance across subgroups. These findings underscore the necessity of incorporating rigorous fairness evaluations and mitigation strategies into FM-based models to foster inclusive and generalizable AI.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10583</link>
<guid>https://arxiv.org/abs/2505.10583</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multimodal integration, common representations, machine teaching, visual-language models

Summary: 
The study investigates the integration of different modalities in large language models, such as text and images, using machine teaching to analyze the minimal examples needed to teach concepts. Evaluating the subset of objects from the Quick, Draw! dataset, the research compares teaching complexity between raw images as bitmaps and trace coordinates in TikZ format for visual-language models. The findings reveal that image-based representations generally require fewer segments and achieve higher accuracy compared to coordinate-based representations. Surprisingly, despite the differences in modalities, the teaching size rankings for concepts remain consistent. This suggests that the simplicity of concepts may be an inherent property that is not influenced by the modality used for representation.<br /><br />Summary: <div>
arXiv:2505.10583v1 Announce Type: new 
Abstract: Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to the similar area in the latent space as a textual description of the strokes that conform the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper we evaluate the complexity of teaching visual-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios</title>
<link>https://arxiv.org/abs/2505.10584</link>
<guid>https://arxiv.org/abs/2505.10584</guid>
<content:encoded><![CDATA[
<div> model architectures, video generation, data processing pipeline, infrastructure, marketing scenarios

Summary:
Aquarius is a family of industry-level video generation models designed for large-scale clusters and models with billions of parameters. It includes a distributed graph and video data processing pipeline for efficient processing, with plans to open-source the framework. Different model architectures cater to various scales, supporting multi-aspect ratios and resolutions. The high-performance infrastructure for training incorporates hybrid parallelism and memory optimization strategies. Multi-xPU parallel inference acceleration achieves significant speedups. The framework is applied to various marketing scenarios like image-to-video and video personalization, with plans for future applications and evaluation metrics. Aquarius aims to demystify and advance industrial-scale video generation systems. <br /><br />Summary: <div>
arXiv:2505.10584v1 Announce Type: new 
Abstract: This report introduces Aquarius, a family of industry-level video generation models for marketing scenarios designed for thousands-xPU clusters and models with hundreds of billions of parameters. Leveraging efficient engineering architecture and algorithmic innovation, Aquarius demonstrates exceptional performance in high-fidelity, multi-aspect-ratio, and long-duration video synthesis. By disclosing the framework's design details, we aim to demystify industrial-scale video generation systems and catalyze advancements in the generative video community. The Aquarius framework consists of five components: Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands of CPUs and thousands of xPUs via automated task distribution, enabling efficient video data processing. Additionally, we are about to open-source the entire data processing framework named "Aquarius-Datapipe". Model Architectures for Different Scales: Include a Single-DiT architecture for 2B models and a Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios, multi-resolution, and multi-duration video generation. High-Performance infrastructure designed for video generation model training: Incorporating hybrid parallelism and fine-grained memory optimization strategies, this infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference Acceleration: Utilizes diffusion cache and attention optimization to achieve a 2.35x inference speedup. Multiple marketing-scenarios applications: Including image-to-video, text-to-video (avatar), video inpainting and video personalization, among others. More downstream applications and multi-dimensional evaluation metrics will be added in the upcoming version updates.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Malicious UAV Detection Using Autoencoder-TSMamba Integration</title>
<link>https://arxiv.org/abs/2505.10585</link>
<guid>https://arxiv.org/abs/2505.10585</guid>
<content:encoded><![CDATA[
<div> Keywords: Malicious UAVs, Next-generation networks, Integrated classifier system, Spatial relationships, Residual values<br />
Summary:<br />
This paper introduces a novel approach for detecting malicious Unmanned Aerial Vehicles (UAVs) in next-generation networks (NGNs). The proposed system utilizes an integrated (AE)-classifier system, combining a Tri-orientated Spatial Mamba (TSMamba) autoencoder (AE) and a ResNet-based classifier. By capturing complex spatial relationships, the system effectively identifies malicious UAV activities. Experimental results show significant improvements in binary and multi-class classification, achieving up to 99.8% recall. The approach not only enhances accuracy but also reduces computational complexity, making it suitable for large-scale deployment in NGN environments. The robustness and scalability of the method make it a promising solution for combating the threats posed by malicious UAVs. <br />Summary: <div>
arXiv:2505.10585v1 Announce Type: new 
Abstract: Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to next-generation networks (NGNs), posing risks such as unauthorized surveillance, data theft, and the delivery of hazardous materials. This paper proposes an integrated (AE)-classifier system to detect malicious UAVs. The proposed AE, based on a 4-layer Tri-orientated Spatial Mamba (TSMamba) architecture, effectively captures complex spatial relationships crucial for identifying malicious UAV activities. The first phase involves generating residual values through the AE, which are subsequently processed by a ResNet-based classifier. This classifier leverages the residual values to achieve lower complexity and higher accuracy. Our experiments demonstrate significant improvements in both binary and multi-class classification scenarios, achieving up to 99.8 % recall compared to 96.7 % in the benchmark. Additionally, our method reduces computational complexity, making it more suitable for large-scale deployment. These results highlight the robustness and scalability of our approach, offering an effective solution for malicious UAV detection in NGN environments.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Super-Resolution Generative Adversarial Networks based Video Enhancement</title>
<link>https://arxiv.org/abs/2505.10589</link>
<guid>https://arxiv.org/abs/2505.10589</guid>
<content:encoded><![CDATA[
<div> Keywords: video super-resolution, spatio-temporal data, SRGAN, 3D Non-Local Blocks, patch-wise learning

Summary:<br />
This study introduces an enhanced approach to video super-resolution by extending Single-Image Super-Resolution (SISR) with a modified SRGAN structure that incorporates 3D Non-Local Blocks to capture spatial and temporal relationships. An experimental training pipeline is developed to simulate real-world video conditions and learn from both local and global structures. Two model variants are presented to explore performance versus efficiency trade-offs. The results show improved temporal coherence, sharper textures, and fewer visual artifacts compared to traditional single-image methods. This work contributes to practical, learning-based solutions for video enhancement tasks with applications in streaming, gaming, and digital restoration. 

Summary: <div>
arXiv:2505.10589v1 Announce Type: new 
Abstract: This study introduces an enhanced approach to video super-resolution by extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution Generative Adversarial Network (SRGAN) structure to handle spatio-temporal data. While SRGAN has proven effective for single-image enhancement, its design does not account for the temporal continuity required in video processing. To address this, a modified framework that incorporates 3D Non-Local Blocks is proposed, which is enabling the model to capture relationships across both spatial and temporal dimensions. An experimental training pipeline is developed, based on patch-wise learning and advanced data degradation techniques, to simulate real-world video conditions and learn from both local and global structures and details. This helps the model generalize better and maintain stability across varying video content while maintaining the general structure besides the pixel-wise correctness. Two model variants-one larger and one more lightweight-are presented to explore the trade-offs between performance and efficiency. The results demonstrate improved temporal coherence, sharper textures, and fewer visual artifacts compared to traditional single-image methods. This work contributes to the development of practical, learning-based solutions for video enhancement tasks, with potential applications in streaming, gaming, and digital restoration.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2505.10595</link>
<guid>https://arxiv.org/abs/2505.10595</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared small target detection, deep learning, adaptive receptive field convolution, wavelet-attentive hierarchical network, feature enhancement

Summary:
The article introduces a novel approach for improving infrared small target detection using deep learning. The proposed Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network (ARFC-WAHNet) addresses the challenges faced in accurately detecting targets in infrared images, such as limited texture and structural information. The network incorporates a Multi-Receptive Field Feature Interaction Convolution module to extract discriminative features adaptively. It also utilizes a Wavelet Frequency Enhancement Downsampling module to enhance target features and suppress background noise. The integration of a High-Low Feature Fusion module and a Global Median Enhancement Attention module further improves feature diversity and expressiveness. Experimental results on public datasets show that ARFC-WAHNet outperforms existing state-of-the-art methods in terms of detection accuracy and robustness, especially in complex backgrounds. The code for the network is available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2505.10595v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) is critical in both civilian and military applications. However, the limited texture and structural information in infrared images makes accurate detection particularly challenging. Although recent deep learning-based methods have improved performance, their use of conventional convolution kernels limits adaptability to complex scenes and diverse targets. Moreover, pooling operations often cause feature loss and insufficient exploitation of image information. To address these issues, we propose an adaptive receptive field convolution and wavelet-attentive hierarchical network for infrared small target detection (ARFC-WAHNet). This network incorporates a multi-receptive field feature interaction convolution (MRFFIConv) module to adaptively extract discriminative features by integrating multiple convolutional branches with a gated unit. A wavelet frequency enhancement downsampling (WFED) module leverages Haar wavelet transform and frequency-domain reconstruction to enhance target features and suppress background noise. Additionally, we introduce a high-low feature fusion (HLFF) module for integrating low-level details with high-level semantics, and a global median enhancement attention (GMEA) module to improve feature diversity and expressiveness via global attention. Experiments on public datasets SIRST, NUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent state-of-the-art methods in both detection accuracy and robustness, particularly under complex backgrounds. The code is available at https://github.com/Leaf2001/ARFC-WAHNet.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2505.10601</link>
<guid>https://arxiv.org/abs/2505.10601</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, point cloud, super-resolution, sparse scenes, SRMamba

Summary: 
SRMamba is a novel method for super-resolution of LiDAR point clouds in sparse scenes. It addresses the challenge of recovering the 3D spatial structure of point clouds from novel views by implementing projection techniques based on Hough Voting and Hole Compensation strategy. The Visual State Space model and Multi-Directional Scanning mechanism are used to improve long-distance dependencies and focus on potential geometric features. An asymmetric U-Net network enables super-resolution reconstruction for multi-beam point clouds. Experimental results on challenging LiDAR datasets show SRMamba's significant superiority over other algorithms in both qualitative and quantitative evaluations. <div>
arXiv:2505.10601v1 Announce Type: new 
Abstract: In recent years, range-view-based LiDAR point cloud super-resolution techniques attract significant attention as a low-cost method for generating higher-resolution point cloud data. However, due to the sparsity and irregular structure of LiDAR point clouds, the point cloud super-resolution problem remains a challenging topic, especially for point cloud upsampling under novel views. In this paper, we propose SRMamba, a novel method for super-resolution of LiDAR point clouds in sparse scenes, addressing the key challenge of recovering the 3D spatial structure of point clouds from novel views. Specifically, we implement projection technique based on Hough Voting and Hole Compensation strategy to eliminate horizontally linear holes in range image. To improve the establishment of long-distance dependencies and to focus on potential geometric features in vertical 3D space, we employ Visual State Space model and Multi-Directional Scanning mechanism to mitigate the loss of 3D spatial structural information due to the range image. Additionally, an asymmetric U-Net network adapts to the input characteristics of LiDARs with different beam counts, enabling super-resolution reconstruction for multi-beam point clouds. We conduct a series of experiments on multiple challenging public LiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates significant superiority over other algorithms in both qualitative and quantitative evaluations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence</title>
<link>https://arxiv.org/abs/2505.10604</link>
<guid>https://arxiv.org/abs/2505.10604</guid>
<content:encoded><![CDATA[
<div> Counting, Relation, Object recognition, Spatial reasoning, Benchmark
Summary:
MIRAGE is a new multi-modal benchmark designed to evaluate models' abilities in object attribute recognition, spatial relational reasoning, and dynamic reasoning. The benchmark highlights gaps in current models' capabilities and emphasizes the need for improved representations and reasoning frameworks. Through diverse scenarios requiring fine-grained recognition and reasoning, MIRAGE challenges state-of-the-art models and provides a pathway towards spatiotemporal reasoning in future research. <div>
arXiv:2505.10604v1 Announce Type: new 
Abstract: Spatial perception and reasoning are core components of human cognition, encompassing object recognition, spatial relational understanding, and dynamic reasoning. Despite progress in computer vision, existing benchmarks reveal significant gaps in models' abilities to accurately recognize object attributes and reason about spatial relationships, both essential for dynamic reasoning. To address these limitations, we propose MIRAGE, a multi-modal benchmark designed to evaluate models' capabilities in Counting (object attribute recognition), Relation (spatial relational reasoning), and Counting with Relation. Through diverse and complex scenarios requiring fine-grained recognition and reasoning, MIRAGE highlights critical limitations in state-of-the-art models, underscoring the need for improved representations and reasoning frameworks. By targeting these foundational abilities, MIRAGE provides a pathway toward spatiotemporal reasoning in future research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly</title>
<link>https://arxiv.org/abs/2505.10610</link>
<guid>https://arxiv.org/abs/2505.10610</guid>
<content:encoded><![CDATA[
<div> benchmarking, long-context vision-language models, MMLongBench, downstream tasks, image types

Summary: 
The article introduces MMLongBench, a benchmark designed to evaluate long-context vision-language models (LCVLMs). It covers a diverse set of tasks with 13,331 examples across five categories, including Visual RAG and Many-Shot ICL. The benchmark includes various image types and standardized input lengths to assess model robustness. Evaluation of 46 models reveals that performance on a single task is not indicative of overall long-context capability. Both closed-source and open-source models face challenges in long-context vision-language tasks, signaling room for improvement. Models with stronger reasoning skills tend to excel in long-context performance. MMLongBench provides a comprehensive foundation for diagnosing and advancing LCVLMs. 

<br /><br />Summary: <div>
arXiv:2505.10610v1 Announce Type: new 
Abstract: The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding</title>
<link>https://arxiv.org/abs/2505.10634</link>
<guid>https://arxiv.org/abs/2505.10634</guid>
<content:encoded><![CDATA[
<div> language priors, Large Vision-Language Models (LVLMs), Cross-Image Contrastive Decoding (CICD), hallucinations, training-free method<br />
<br />
Summary:
Large Vision-Language Models (LVLMs) often generate visually inconsistent content due to language priors inherited from their pre-trained Large Language Model (LLM) backbone. To address this issue, a new method called Cross-Image Contrastive Decoding (CICD) is introduced. CICD identifies and eliminates detrimental language priors using contrastive decoding, thereby reducing hallucinations while maintaining textual coherence. The approach is effective across multiple benchmarks and LVLMs, particularly improving image captioning tasks. By leveraging the limited overlap of information between images, CICD successfully mitigates language priors without compromising visual information. Once accepted, the code for CICD will be made available for further research and application. <br /><br />Summary: <div>
arXiv:2505.10634v1 Announce Type: new 
Abstract: Language priors constitute one of the primary causes of hallucinations in Large Vision-Language Models (LVLMs), driving the models to generate linguistically plausible yet visually inconsistent content. The language priors in LVLMs originate from the linguistic knowledge inherited from their pre-trained Large Language Model (LLM) backbone. Consequently, this characteristic is an intrinsic property of the model that remains independent of visual inputs. Inspired by the finding that language priors are consistent across images, we propose Cross-Image Contrastive Decoding (CICD), a simple yet effective training-free method to alleviate language priors in LVLMs. CICD first identifies essential and detrimental priors, and then employs contrastive decoding to eliminate the detrimental ones. This approach simultaneously prevents LVLMs from generating hallucinated content while maintaining textual fluency and coherence. Furthermore, the limited information overlap between images helps prevent visual information loss during contrastive decoding. We validate the effectiveness of CICD on four benchmarks with six LVLMs. Our experiments demonstrate that CICD performs remarkably well in mitigating language priors, especially in the image captioning task, where such priors are most pronounced. Code will be released once accepted.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging</title>
<link>https://arxiv.org/abs/2505.10649</link>
<guid>https://arxiv.org/abs/2505.10649</guid>
<content:encoded><![CDATA[
<div> medical imaging, deep learning, whole slide image, multiple instance learning, continual learning

Summary:
This paper explores the use of continual learning in whole slide image analysis, focusing on attention MIL models. The study identifies that model forgetting mainly occurs in the attention layers of MIL models. To address this issue, the authors propose two components: Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD aims to retain attention layer knowledge between learning sessions, while PMP selectively stores informative patches from WSIs to reduce memory footprint. Experimental results show that the proposed method improves accuracy and memory efficiency on diverse WSI datasets, outperforming current CL methods. By enabling adaptability and resilience in diagnostic models, this research lays the groundwork for continual learning in large-scale, weakly annotated clinical datasets. 

<br /><br />Summary: <div>
arXiv:2505.10649v1 Announce Type: new 
Abstract: Advances in medical imaging and deep learning have propelled progress in whole slide image (WSI) analysis, with multiple instance learning (MIL) showing promise for efficient and accurate diagnostics. However, conventional MIL models often lack adaptability to evolving datasets, as they rely on static training that cannot incorporate new information without extensive retraining. Applying continual learning (CL) to MIL models is a possible solution, but often sees limited improvements. In this paper, we analyze CL in the context of attention MIL models and find that the model forgetting is mainly concentrated in the attention layers of the MIL model. Using the results of this analysis we propose two components for improving CL on MIL: Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates catastrophic forgetting by focusing on retaining attention layer knowledge between learning sessions, while PMP reduces the memory footprint by selectively storing only the most informative patches, or ``pseudo-bags'' from WSIs. Experimental evaluations demonstrate that our method significantly improves both accuracy and memory efficiency on diverse WSI datasets, outperforming current state-of-the-art CL methods. This work provides a foundation for CL in large-scale, weakly annotated clinical datasets, paving the way for more adaptable and resilient diagnostic models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier</title>
<link>https://arxiv.org/abs/2505.10664</link>
<guid>https://arxiv.org/abs/2505.10664</guid>
<content:encoded><![CDATA[
<div> AI-generated images, CLIP embeddings, image classification, CIFAKE benchmark, challenges <br />
Summary:
This study addresses the issue of verifying the authenticity of AI-generated images on social media using vision-language models, specifically CLIP embeddings. The research explores whether CLIP embeddings inherently contain information indicative of AI generation, and proposes a pipeline for extracting visual embeddings and fine-tuning classifiers. The experiments on the CIFAKE benchmark demonstrate high accuracy of 95% without language reasoning, and 85% accuracy with few-shot adaptation to custom data. Comparison with a closed-source baseline highlights challenges in classifying specific AI-generated image types, such as wide-angle photographs and oil paintings. The study raises new questions about the classification of AI-generated images and suggests the need for further investigation in this domain. <br /><br />Summary: <div>
arXiv:2505.10664v1 Announce Type: new 
Abstract: Verifying the authenticity of AI-generated images presents a growing challenge on social media platforms these days. While vision-language models (VLMs) like CLIP outdo in multimodal representation, their capacity for AI-generated image classification is underexplored due to the absence of such labels during the pre-training process. This work investigates whether CLIP embeddings inherently contain information indicative of AI generation. A proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds its embeddings to lightweight networks, and fine-tunes only the final classifier. Experiments on the public CIFAKE benchmark show the performance reaches 95% accuracy without language reasoning. Few-shot adaptation to curated custom with 20% of the data results in performance to 85%. A closed-source baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific styles. Notably, some specific image types, such as wide-angle photographs and oil paintings, pose significant challenges to classification. These results indicate previously unexplored difficulties in classifying certain types of AI-generated images, revealing new and more specific questions in this domain that are worth further investigation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding</title>
<link>https://arxiv.org/abs/2505.10671</link>
<guid>https://arxiv.org/abs/2505.10671</guid>
<content:encoded><![CDATA[
<div> 3D gaze estimation, spatial relationships, unconstrained settings, 2D appearance, 3D poses<br />
<br />
Summary: 
The proposed approach, GA3CE, focuses on 3D gaze estimation in unconstrained settings where close-up views of the subject's eyes are not available. By leveraging spatial relationships between the subject and objects in the scene, the method outputs 3D gaze direction, addressing the challenges posed by variations in subject pose, scene layout, and gaze direction. GA3CE utilizes 3D poses and object positions as 3D context to learn spatial relationships, aligning them in an egocentric space to simplify spatial complexity. Additionally, D$^3$ positional encoding is introduced to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experimental results show significant improvements in reducing mean angle error compared to existing baselines on benchmark datasets in single-frame settings. <div>
arXiv:2505.10671v1 Announce Type: new 
Abstract: We propose a novel 3D gaze estimation approach that learns spatial relationships between the subject and objects in the scene, and outputs 3D gaze direction. Our method targets unconstrained settings, including cases where close-up views of the subject's eyes are unavailable, such as when the subject is distant or facing away. Previous approaches typically rely on either 2D appearance alone or incorporate limited spatial cues using depth maps in the non-learnable post-processing step. Estimating 3D gaze direction from 2D observations in these scenarios is challenging; variations in subject pose, scene layout, and gaze direction, combined with differing camera poses, yield diverse 2D appearances and 3D gaze directions even when targeting the same 3D scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding. Our method represents subject and scene using 3D poses and object positions, treating them as 3D context to learn spatial relationships in 3D space. Inspired by human vision, we align this context in an egocentric space, significantly reducing spatial complexity. Furthermore, we propose D$^3$ (direction-distance-decomposed) positional encoding to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experiments demonstrate substantial improvements, reducing mean angle error by 13%-37% compared to leading baselines on benchmark datasets in single-frame settings.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?</title>
<link>https://arxiv.org/abs/2505.10679</link>
<guid>https://arxiv.org/abs/2505.10679</guid>
<content:encoded><![CDATA[
<div> Keywords: ST-GCNs, human action recognition, sparse architecture, multi-level sparsity, parameter reduction <br />
Summary: <br />
Spatial-temporal graph convolutional networks (ST-GCNs) have achieved impressive results in human action recognition (HAR) but are often over-parameterized. Experiments confirmed this through the lottery ticket hypothesis. A novel sparse ST-GCNs generator was proposed to train a sparse architecture from a dense network while maintaining performance levels. Multi-level sparsity ST-GCNs were created by integrating sparse structures at varying levels, leading to significant improvements in HAR performance. Experiments on multiple datasets showed that sparse ST-GCNs achieved comparable performance to dense counterparts with significantly fewer parameters. With 95% fewer parameters, sparse ST-GCNs only suffered a minor degradation in accuracy. Multi-level sparsity ST-GCNs, requiring 66% fewer parameters, demonstrated an improvement in accuracy of over 1%. The code for this work is available on GitHub for further exploration. <br /> <div>
arXiv:2505.10679v1 Announce Type: new 
Abstract: Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive performance in skeleton-based human action recognition (HAR). However, despite the development of numerous models, their recognition performance does not differ significantly after aligning the input settings. With this observation, we hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture subsequently confirmed through experiments employing the lottery ticket hypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which trains a sparse architecture from a randomly initialized dense network while maintaining comparable performance levels to the dense components. Moreover, we generate multi-level sparsity ST-GCNs by integrating sparse structures at various sparsity levels and demonstrate that the assembled model yields a significant enhancement in HAR performance. Thorough experiments on four datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate that the proposed sparse ST-GCNs can achieve comparable performance to their dense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a degradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs, demonstrate an improvement of >1% in top-1 accuracy. The code is available at https://github.com/davelailai/Sparse-ST-GCN.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention</title>
<link>https://arxiv.org/abs/2505.10685</link>
<guid>https://arxiv.org/abs/2505.10685</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic occupancy prediction, LiDAR-camera fusion, Gaussian-based representation, 3D deformable attention, multi-modal fusion<br />
Summary: <br />
3D semantic occupancy prediction is crucial for autonomous driving safety. By combining LiDAR and camera data, multi-modal approaches can offer more accurate predictions. This study introduces GaussianFormer3D, a framework using a Gaussian-based representation with 3D deformable attention. A voxel-to-Gaussian initialization strategy provides geometry priors from LiDAR, while a LiDAR-guided deformable attention mechanism refines predictions with fusion features. Experiments on various datasets show that GaussianFormer3D achieves high accuracy comparable to state-of-the-art methods, with lower memory usage and improved efficiency. <div>
arXiv:2505.10685v1 Announce Type: new 
Abstract: 3D semantic occupancy prediction is critical for achieving safe and reliable autonomous driving. Compared to camera-only perception systems, multi-modal pipelines, especially LiDAR-camera fusion methods, can produce more accurate and detailed predictions. Although most existing works utilize a dense grid-based representation, in which the entire 3D space is uniformly divided into discrete voxels, the emergence of 3D Gaussians provides a compact and continuous object-centric representation. In this work, we propose a multi-modal Gaussian-based semantic occupancy prediction framework utilizing 3D deformable attention, named as GaussianFormer3D. We introduce a voxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry priors from LiDAR data, and design a LiDAR-guided 3D deformable attention mechanism for refining 3D Gaussians with LiDAR-camera fusion features in a lifted 3D space. We conducted extensive experiments on both on-road and off-road datasets, demonstrating that our GaussianFormer3D achieves high prediction accuracy that is comparable to state-of-the-art multi-modal fusion-based methods with reduced memory consumption and improved efficiency.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Detection of Salvin's Albatrosses: Improving Deep Learning Tools for Aerial Wildlife Surveys</title>
<link>https://arxiv.org/abs/2505.10737</link>
<guid>https://arxiv.org/abs/2505.10737</guid>
<content:encoded><![CDATA[
<div> detection model, BirdDetector, breeding population, Salvin's albatross, Bounty Islands, New Zealand, unmanned aerial vehicles, UAVs, deep learning, wildlife monitoring

Summary:
The study evaluates BirdDetector, a general-purpose avian detection model, for estimating the breeding population of Salvin's albatross on the Bounty Islands, New Zealand, using drone-derived imagery. The effectiveness of the model is assessed in both zero-shot and fine-tuned settings, with improved detection accuracy seen after fine-tuning with target domain annotations and stronger image augmentation techniques. Results show that leveraging pre-trained deep-learning models like BirdDetector can enhance species-specific monitoring in challenging environments, particularly for densely populated seabird colonies. This research demonstrates the potential of incorporating advanced inference techniques and augmentation methods to improve wildlife monitoring capabilities using UAVs and deep learning technology. 

<br /><br />Summary: <div>
arXiv:2505.10737v1 Announce Type: new 
Abstract: Recent advancements in deep learning and aerial imaging have transformed wildlife monitoring, enabling researchers to survey wildlife populations at unprecedented scales. Unmanned Aerial Vehicles (UAVs) provide a cost-effective means of capturing high-resolution imagery, particularly for monitoring densely populated seabird colonies. In this study, we assess the performance of a general-purpose avian detection model, BirdDetector, in estimating the breeding population of Salvin's albatross (Thalassarche salvini) on the Bounty Islands, New Zealand. Using drone-derived imagery, we evaluate the model's effectiveness in both zero-shot and fine-tuned settings, incorporating enhanced inference techniques and stronger augmentation methods. Our findings indicate that while applying the model in a zero-shot setting offers a strong baseline, fine-tuning with annotations from the target domain and stronger image augmentation leads to marked improvements in detection accuracy. These results highlight the potential of leveraging pre-trained deep-learning models for species-specific monitoring in remote and challenging environments.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation</title>
<link>https://arxiv.org/abs/2505.10743</link>
<guid>https://arxiv.org/abs/2505.10743</guid>
<content:encoded><![CDATA[
<div> attention weights, U-Net, fine-tuning, Stable Diffusion XL, LoRA<br />
Summary:<br />
The article introduces a two-stage pipeline that enhances text-to-image diffusion models for personalized image generation. By leveraging LoRA-based fine-tuning on attention weights within the U-Net of the Stable Diffusion XL (SDXL) model, the method addresses challenges in representing novel subjects based on a few reference images. Initially, a generic scene is generated using the unmodified SDXL model by replacing the subject with its class label. Subsequently, the personalized subject is selectively inserted using a segmentation-driven image-to-image pipeline that utilizes trained LoRA weights. This approach isolates the subject encoding while preserving SDXL's broader generative capabilities, achieving a high DINO similarity score of 0.789 on SDXL and surpassing existing personalized text-to-image methods. <br />Summary: <div>
arXiv:2505.10743v1 Announce Type: new 
Abstract: Recent advances in text-to-image diffusion models, particularly Stable Diffusion, have enabled the generation of highly detailed and semantically rich images. However, personalizing these models to represent novel subjects based on a few reference images remains challenging. This often leads to catastrophic forgetting, overfitting, or large computational overhead.We propose a two-stage pipeline that addresses these limitations by leveraging LoRA-based fine-tuning on the attention weights within the U-Net of the Stable Diffusion XL (SDXL) model. First, we use the unmodified SDXL to generate a generic scene by replacing the subject with its class label. Then, we selectively insert the personalized subject through a segmentation-driven image-to-image (Img2Img) pipeline that uses the trained LoRA weights.This framework isolates the subject encoding from the overall composition, thus preserving SDXL's broader generative capabilities while integrating the new subject in a high-fidelity manner. Our method achieves a DINO similarity score of 0.789 on SDXL, outperforming existing personalized text-to-image approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis</title>
<link>https://arxiv.org/abs/2505.10751</link>
<guid>https://arxiv.org/abs/2505.10751</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, point cloud datasets, Structure From Motion algorithms, forest environments, deep learning models

Summary: 
Remote sensing technologies for monitoring forests face challenges due to the scarcity of publicly available annotated datasets generated through Structure From Motion (SfM) algorithms applied to imagery. This work presents a novel pipeline for generating semantically segmented point clouds of forest environments. By utilizing a custom-built forest simulator to generate realistic RGB images and their corresponding semantic segmentation masks, and modifying open-source SfM software, accurate and detailed point clouds are produced. These point clouds combine geometric and semantic information, serving as a valuable resource for training and evaluating deep learning models aimed at segmenting real forest point clouds obtained via SfM. The approach addresses the limitations of existing datasets and contributes to enhancing the monitoring and analysis of forested environments. 

<br /><br />Summary: <div>
arXiv:2505.10751v1 Announce Type: new 
Abstract: Although the use of remote sensing technologies for monitoring forested environments has gained increasing attention, publicly available point cloud datasets remain scarce due to the high costs, sensor requirements, and time-intensive nature of their acquisition. Moreover, as far as we are aware, there are no public annotated datasets generated through Structure From Motion (SfM) algorithms applied to imagery, which may be due to the lack of SfM algorithms that can map semantic segmentation information into an accurate point cloud, especially in a challenging environment like forests.
  In this work, we present a novel pipeline for generating semantically segmented point clouds of forest environments. Using a custom-built forest simulator, we generate realistic RGB images of diverse forest scenes along with their corresponding semantic segmentation masks. These labeled images are then processed using modified open-source SfM software capable of preserving semantic information during 3D reconstruction. The resulting point clouds provide both geometric and semantic detail, offering a valuable resource for training and evaluating deep learning models aimed at segmenting real forest point clouds obtained via SfM.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities</title>
<link>https://arxiv.org/abs/2505.10764</link>
<guid>https://arxiv.org/abs/2505.10764</guid>
<content:encoded><![CDATA[
<div> Keywords: Minimally Invasive Surgery, Machine Learning, Vision-Language Models, Surgical Understanding, Benchmarking 

Summary: 
Minimally Invasive Surgery (MIS) poses challenges in visual and technical aspects, such as instrument classification and understanding surgical actions. Current machine learning methods for surgical understanding often rely on small annotated datasets. Vision-Language Models (VLMs), trained on large image-text pairs, show adaptability across diverse visual data and tasks. However, there is a gap in these models' ability to consistently connect language to specific regions in surgical scenes. The study benchmarks various VLMs on different surgical datasets, highlighting their strengths and limitations in the surgical domain. This research raises important questions about the performance of general-purpose VLMs in surgical applications and identifies areas for improvement in linking language to surgical actions accurately. <br /><br />Summary: <div>
arXiv:2505.10764v1 Announce Type: new 
Abstract: Minimally invasive surgery (MIS) presents significant visual and technical challenges, including surgical instrument classification and understanding surgical action involving instruments, verbs, and anatomical targets. While many machine learning-based methods have been developed for surgical understanding, they typically rely on procedure- and task-specific models trained on small, manually annotated datasets. In contrast, the recent success of vision-language models (VLMs) trained on large volumes of raw image-text pairs has demonstrated strong adaptability to diverse visual data and a range of downstream tasks. This opens meaningful research questions: how well do these general-purpose VLMs perform in the surgical domain? In this work, we explore those questions by benchmarking several VLMs across diverse surgical datasets, including general laparoscopic procedures and endoscopic submucosal dissection, to assess their current capabilities and limitations. Our benchmark reveals key gaps in the models' ability to consistently link language to the correct regions in surgical scenes.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Segment Anything in Microscopy with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2505.10769</link>
<guid>https://arxiv.org/abs/2505.10769</guid>
<content:encoded><![CDATA[
<div> Keywords: segmentation, biomedical images, vision-language knowledge, multimodal large language models, microscopy

Summary: 
Segmentation of regions of interest in biomedical images is crucial for image analysis. Existing models often struggle to generalize to unseen domain data due to a lack of vision-language knowledge. This paper introduces a method, uLLSAM, which leverages Multimodal Large Language Models (MLLMs) to guide the Segment Anything Model (SAM) in learning microscopy data. The Vision-Language Semantic Alignment (VLSA) module injects vision-language knowledge, leading to significant performance improvements, particularly in boundary contour perception. The proposed Semantic Boundary Regularization (SBR) further enhances SAM's performance. uLLSAM achieves state-of-the-art results on in-domain microscopy datasets, with 7.71% improvement in Dice and 12.10% improvement in SA. It also demonstrates strong generalization capabilities on out-of-domain datasets, with 6.79% improvement in Dice and 10.08% improvement in SA. The code is available on GitHub for reproducibility. 

<br /><br />Summary: <div>
arXiv:2505.10769v1 Announce Type: new 
Abstract: Accurate segmentation of regions of interest in biomedical images holds substantial value in image analysis. Although several foundation models for biomedical segmentation have currently achieved excellent performance on certain datasets, they typically demonstrate sub-optimal performance on unseen domain data. We owe the deficiency to lack of vision-language knowledge before segmentation. Multimodal Large Language Models (MLLMs) bring outstanding understanding and reasoning capabilities to multimodal tasks, which inspires us to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling vision models to demonstrate superior generalization capabilities on cross-domain datasets. In this paper, we propose using MLLMs to guide SAM in learning microscopy crose-domain data, unifying Segment Anything in Microscopy, named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment (VLSA) module, which injects VLK into Segment Anything Model (SAM). We find that after SAM receives global VLK prompts, its performance improves significantly, but there are deficiencies in boundary contour perception. Therefore, we further propose Semantic Boundary Regularization (SBR) to prompt SAM. Our method achieves performance improvements of 7.71% in Dice and 12.10% in SA across 9 in-domain microscopy datasets, achieving state-of-the-art performance. Our method also demonstrates improvements of 6.79% in Dice and 10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization capabilities. Code is available at https://github.com/ieellee/uLLSAM.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.10781</link>
<guid>https://arxiv.org/abs/2505.10781</guid>
<content:encoded><![CDATA[
<div> Keyword: weakly supervised, class-incremental learning, semantic segmentation, pseudo-labels, data augmentation

Summary:
This work introduces a completely weakly supervised class-incremental learning approach for semantic segmentation, allowing for the learning of segmentation for both base and novel classes using only image-level labels. The method generates robust pseudo-labels by combining labels from a localizer and a sequence of foundation models based on uncertainty. To prevent catastrophic forgetting, an exemplar-guided data augmentation technique is introduced to generate diverse images containing both previous and novel classes. Experimental results on common settings show that the proposed method outperforms partially weakly supervised methods in some scenarios and achieves competitive accuracy in others. This novel approach addresses the challenges of class-incremental semantic segmentation without the need for expensive pixel-level annotations. 

<br /><br />Summary: <div>
arXiv:2505.10781v1 Announce Type: new 
Abstract: This work addresses the task of completely weakly supervised class-incremental learning for semantic segmentation to learn segmentation for both base and additional novel classes using only image-level labels. While class-incremental semantic segmentation (CISS) is crucial for handling diverse and newly emerging objects in the real world, traditional CISS methods require expensive pixel-level annotations for training. To overcome this limitation, partially weakly-supervised approaches have recently been proposed. However, to the best of our knowledge, this is the first work to introduce a completely weakly-supervised method for CISS. To achieve this, we propose to generate robust pseudo-labels by combining pseudo-labels from a localizer and a sequence of foundation models based on their uncertainty. Moreover, to mitigate catastrophic forgetting, we introduce an exemplar-guided data augmentation method that generates diverse images containing both previous and novel classes with guidance. Finally, we conduct experiments in three common experimental settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint and overlap. The experimental results demonstrate that our completely weakly supervised method outperforms even partially weakly supervised methods in the 15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the COCO-to-VOC setting.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios</title>
<link>https://arxiv.org/abs/2505.10784</link>
<guid>https://arxiv.org/abs/2505.10784</guid>
<content:encoded><![CDATA[
<div> Dataset, railway safety, obstacle detection, synthetic data, diffusion models

Summary: 
The article introduces SynRailObs, a synthetic dataset created to improve obstacle detection in railway environments. The dataset is designed to represent a diverse range of weather conditions and geographical features, including rare and difficult-to-capture obstacles. Experiments conducted in real-world railway settings show that models trained on SynRailObs perform consistently well across different distances and environmental conditions. Additionally, the dataset enables zero-shot capabilities, which are crucial for security-sensitive applications. SynRailObs addresses the current lack of large-scale, high-quality datasets in railway safety research, providing a valuable resource for advancing obstacle detection technology in railway environments. The data is publicly available on Kaggle for further research and development purposes.<br /><br />Summary: <div>
arXiv:2505.10784v1 Announce Type: new 
Abstract: Detecting potential obstacles in railway environments is critical for preventing serious accidents. Identifying a broad range of obstacle categories under complex conditions requires large-scale datasets with precisely annotated, high-quality images. However, existing publicly available datasets fail to meet these requirements, thereby hindering progress in railway safety research. To address this gap, we introduce SynRailObs, a high-fidelity synthetic dataset designed to represent a diverse range of weather conditions and geographical features. Furthermore, diffusion models are employed to generate rare and difficult-to-capture obstacles that are typically challenging to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs, we perform experiments in real-world railway environments, testing on both ballasted and ballastless tracks across various weather conditions. The results demonstrate that SynRailObs holds substantial potential for advancing obstacle detection in railway safety applications. Models trained on this dataset show consistent performance across different distances and environmental conditions. Moreover, the model trained on SynRailObs exhibits zero-shot capabilities, which are essential for applications in security-sensitive domains. The data is available in https://www.kaggle.com/datasets/qiushi910/synrailobs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes</title>
<link>https://arxiv.org/abs/2505.10787</link>
<guid>https://arxiv.org/abs/2505.10787</guid>
<content:encoded><![CDATA[
<div> mesh structure, Gaussian pruning, densification strategy, vector quantization, real-time rendering<br />
<br />
Summary:<br />
The article introduces EA-3DGS, a method for efficient and high-quality real-time rendering of outdoor scenes. It utilizes a mesh structure to initialize Gaussian components and adaptively captures geometric structures. A Gaussian pruning strategy evaluates and prunes Gaussian points for efficient rendering. A densification strategy retains geometry-critical points by densifying in low-curvature regions. Vector quantization reduces disk space requirements without significantly affecting rendering quality. Experiments on various scenes demonstrate the effectiveness of the proposed method. <br /> <div>
arXiv:2505.10787v1 Announce Type: new 
Abstract: Efficient scene representations are essential for many real-world applications, especially those involving spatial measurement. Although current NeRF-based methods have achieved impressive results in reconstructing building-scale scenes, they still suffer from slow training and inference speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent performance with its high-quality rendering and real-time speed, especially for objects and small-scale scenes. However, in outdoor scenes, its point-based explicit representation lacks an effective adjustment mechanism, and the millions of Gaussian points required often lead to memory constraints during training. To address these challenges, we propose EA-3DGS, a high-quality real-time rendering method designed for outdoor scenes. First, we introduce a mesh structure to regulate the initialization of Gaussian components by leveraging an adaptive tetrahedral mesh that partitions the grid and initializes Gaussian components on each face, effectively capturing geometric structures in low-texture regions. Second, we propose an efficient Gaussian pruning strategy that evaluates each 3D Gaussian's contribution to the view and prunes accordingly. To retain geometry-critical Gaussian points, we also present a structure-aware densification strategy that densifies Gaussian points in low-curvature regions. Additionally, we employ vector quantization for parameter quantization of Gaussian components, significantly reducing disk space requirements with only a minimal impact on rendering quality. Extensive experiments on 13 scenes, including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \& Temples, WHU) and five self-collected scenes acquired through UAV photogrammetry measurement from SCUT-CA and plateau regions, further demonstrate the superiority of our method.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</title>
<link>https://arxiv.org/abs/2505.10810</link>
<guid>https://arxiv.org/abs/2505.10810</guid>
<content:encoded><![CDATA[
<div> Keywords: Human motion generation, MoCLIP, contrastive learning, motion-aware representations, text-to-motion alignment<br />
Summary:<br />
Human motion generation is crucial for applications such as animation, robotics, and virtual reality. Traditional approaches rely on CLIP-based text encoders but struggle to capture motion dynamics effectively. The MoCLIP model, introduced in this work, enhances motion fidelity by incorporating motion-aware representations through contrastive learning and tethering loss. This fine-tuned CLIP model with a motion encoding head improves accuracy metrics while maintaining competitive FID scores, leading to better text-to-motion alignment results. MoCLIP seamlessly integrates into existing CLIP-based pipelines and methods, making it a versatile and effective framework for enhancing motion generation in various applications. <div>
arXiv:2505.10810v1 Announce Type: new 
Abstract: Human motion generation is essential for fields such as animation, robotics, and virtual reality, requiring models that effectively capture motion dynamics from text descriptions. Existing approaches often rely on Contrastive Language-Image Pretraining (CLIP)-based text encoders, but their training on text-image pairs constrains their ability to understand temporal and kinematic structures inherent in motion and motion generation. This work introduces MoCLIP, a fine-tuned CLIP model with an additional motion encoding head, trained on motion sequences using contrastive learning and tethering loss. By explicitly incorporating motion-aware representations, MoCLIP enhances motion fidelity while remaining compatible with existing CLIP-based pipelines and seamlessly integrating into various CLIP-based methods. Experiments demonstrate that MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining competitive FID, leading to improved text-to-motion alignment results. These results highlight MoCLIP's versatility and effectiveness, establishing it as a robust framework for enhancing motion generation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification</title>
<link>https://arxiv.org/abs/2505.10823</link>
<guid>https://arxiv.org/abs/2505.10823</guid>
<content:encoded><![CDATA[
<div> embeddings, adapter models, radiography classification, foundation models, medical imaging

Summary: 
This study evaluates the use of embeddings from general-purpose and medical domain-specific foundation models for training lightweight adapter models in multi-class radiography classification, focusing on tube placement assessment. Six foundation models were employed to extract embeddings, with MedImageInsight yielding the highest mean area under the curve (mAUC) at 93.8%. Most adapter models achieved computational efficiency, with training within one minute and inference within seconds on CPU. Fairness analyses on adapters trained on MedImageInsight-derived embeddings showed minimal disparities, with gender performance differences within 2% and standard deviations across age groups not exceeding 3%. These findings confirm the effectiveness of foundation model embeddings, particularly from MedImageInsight, for accurate, efficient, and equitable diagnostic classification using lightweight adapters for radiographic image analysis. 

<br /><br />Summary: <div>
arXiv:2505.10823v1 Announce Type: new 
Abstract: Foundation models, pretrained on extensive datasets, have significantly advanced machine learning by providing robust and transferable embeddings applicable to various domains, including medical imaging diagnostics. This study evaluates the utility of embeddings derived from both general-purpose and medical domain-specific foundation models for training lightweight adapter models in multi-class radiography classification, focusing specifically on tube placement assessment. A dataset comprising 8842 radiographs classified into seven distinct categories was employed to extract embeddings using six foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight, Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using classical machine learning algorithms. Among these combinations, MedImageInsight embeddings paired with an support vector machine adapter yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%, respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%. Notably, most adapter models demonstrated computational efficiency, achieving training within one minute and inference within seconds on CPU, underscoring their practicality for clinical applications. Furthermore, fairness analyses on adapters trained on MedImageInsight-derived embeddings indicated minimal disparities, with gender differences in performance within 2% and standard deviations across age groups not exceeding 3%. These findings confirm that foundation model embeddings-especially those from MedImageInsight-facilitate accurate, computationally efficient, and equitable diagnostic classification using lightweight adapters for radiographic image analysis.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation</title>
<link>https://arxiv.org/abs/2505.10825</link>
<guid>https://arxiv.org/abs/2505.10825</guid>
<content:encoded><![CDATA[
<div> Thermal Infrared, Object Detection, Feature Attention, CRT-YOLO, EMA Modules <br />
<br />
Summary: 
The article introduces a new thermal infrared object detection framework called CRT-YOLO, which utilizes centralized feature regulation to enhance global-range interaction in TIR information. The model incorporates efficient multi-scale attention (EMA) modules to capture long-range dependencies with minimal computational cost and integrates the Centralized Feature Pyramid (CFP) network for global regulation of TIR features. Extensive experiments on benchmark datasets show that CRT-YOLO outperforms traditional methods for object detection in thermal infrared images. An ablation study confirms the effectiveness of the proposed modules, highlighting the potential impact of this approach on advancing thermal infrared object detection research. <div>
arXiv:2505.10825v1 Announce Type: new 
Abstract: Thermal Infrared (TIR) technology involves the use of sensors to detect and measure infrared radiation emitted by objects, and it is widely utilized across a broad spectrum of applications. The advancements in object detection methods utilizing TIR images have sparked significant research interest. However, most traditional methods lack the capability to effectively extract and fuse local-global information, which is crucial for TIR-domain feature attention. In this study, we present a novel and efficient thermal infrared object detection framework, known as CRT-YOLO, that is based on centralized feature regulation, enabling the establishment of global-range interaction on TIR information. Our proposed model integrates efficient multi-scale attention (EMA) modules, which adeptly capture long-range dependencies while incurring minimal computational overhead. Additionally, it leverages the Centralized Feature Pyramid (CFP) network, which offers global regulation of TIR features. Extensive experiments conducted on two benchmark datasets demonstrate that our CRT-YOLO model significantly outperforms conventional methods for TIR image object detection. Furthermore, the ablation study provides compelling evidence of the effectiveness of our proposed modules, reinforcing the potential impact of our approach on advancing the field of thermal infrared object detection.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits</title>
<link>https://arxiv.org/abs/2505.10827</link>
<guid>https://arxiv.org/abs/2505.10827</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit surface, neural implicit surfaces, text-guided editing, geometry-aware distillation, NeuSEditor<br />
Summary:<br />
- NeuSEditor is a new method for editing neural implicit surfaces derived from multi-view images. It addresses challenges in preserving identity and maintaining geometric consistency during editing.<br />
- The method introduces an identity-preserving architecture that separates scenes into foreground and background, allowing precise modifications without altering scene-specific elements.<br />
- A geometry-aware distillation loss improves rendering and geometric quality, simplifying the editing workflow without needing continuous dataset updates or source prompting.<br />
- NeuSEditor surpasses existing state-of-the-art methods like PDS and InstructNeRF2NeRF, delivering superior quantitative and qualitative results.<br />
- For more detailed visual results, the website neuseditor.github.io provides additional information. <br /> 

Summary: <br />
NeuSEditor is a novel method that allows for efficient text-guided editing of neural implicit surfaces, separating scenes for precise modifications while maintaining identity and geometric consistency. Its pioneering geometry-aware distillation loss enhances rendering quality and simplifies the editing process by eliminating the need for continuous dataset updates or source prompting. Outperforming current methods like PDS and InstructNeRF2NeRF, NeuSEditor provides superior quantitative and qualitative results, making it a valuable tool for implicit surface editing. More visual outputs and details can be found on neuseditor.github.io. <div>
arXiv:2505.10827v1 Announce Type: new 
Abstract: Implicit surface representations are valued for their compactness and continuity, but they pose significant challenges for editing. Despite recent advancements, existing methods often fail to preserve identity and maintain geometric consistency during editing. To address these challenges, we present NeuSEditor, a novel method for text-guided editing of neural implicit surfaces derived from multi-view images. NeuSEditor introduces an identity-preserving architecture that efficiently separates scenes into foreground and background, enabling precise modifications without altering the scene-specific elements. Our geometry-aware distillation loss significantly enhances rendering and geometric quality. Our method simplifies the editing workflow by eliminating the need for continuous dataset updates and source prompting. NeuSEditor outperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF, delivering superior quantitative and qualitative results. For more visual results, visit: neuseditor.github.io.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects</title>
<link>https://arxiv.org/abs/2505.10841</link>
<guid>https://arxiv.org/abs/2505.10841</guid>
<content:encoded><![CDATA[
<div> Reference image, geometric correspondence, object pose estimation, correlation volume, BOP benchmark

Summary:
RefPose is a novel approach for estimating the 6D pose of unseen objects from monocular RGB images. It utilizes a reference image and geometric correspondence to guide the pose estimation process. By first predicting an initial pose using object templates and establishing geometric correspondence, RefPose then refines the pose through a render-and-compare approach. A correlation volume-guided attention mechanism enhances the estimation by capturing correlations between query and reference images. Unlike traditional methods, RefPose dynamically adapts to new object shapes, resulting in robust performance across previously unseen objects. Extensive evaluation on the BOP benchmark datasets demonstrates that RefPose achieves state-of-the-art results while maintaining a competitive runtime. <br /><br />Summary: <div>
arXiv:2505.10841v1 Announce Type: new 
Abstract: Estimating the 6D pose of unseen objects from monocular RGB images remains a challenging problem, especially due to the lack of prior object-specific knowledge. To tackle this issue, we propose RefPose, an innovative approach to object pose estimation that leverages a reference image and geometric correspondence as guidance. RefPose first predicts an initial pose by using object templates to render the reference image and establish the geometric correspondence needed for the refinement stage. During the refinement stage, RefPose estimates the geometric correspondence of the query based on the generated references and iteratively refines the pose through a render-and-compare approach. To enhance this estimation, we introduce a correlation volume-guided attention mechanism that effectively captures correlations between the query and reference images. Unlike traditional methods that depend on pre-defined object models, RefPose dynamically adapts to new object shapes by leveraging a reference image and geometric correspondence. This results in robust performance across previously unseen objects. Extensive evaluation on the BOP benchmark datasets shows that RefPose achieves state-of-the-art results while maintaining a competitive runtime.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination</title>
<link>https://arxiv.org/abs/2505.10869</link>
<guid>https://arxiv.org/abs/2505.10869</guid>
<content:encoded><![CDATA[
<div> Keywords: velocity patterns, body parts, walking, gait symmetry, intersegmental coordination <br />
Summary: <br />
This study examines the velocity patterns of different body parts during walking and introduces a new method for assessing gait symmetry. Unlike previous studies that focused on EMG signals or acceleration, this research utilizes an LTI system to model intersegmental coordination. The proposed dissimilarity metric offers a unique way to evaluate symmetry in gait. The method was tested on five subjects with varying levels of gait symmetry. By analyzing the differences in velocity patterns and utilizing the proposed metric, the study aims to provide a more comprehensive understanding of gait symmetry and potentially improve evaluations and treatments for gait-related issues.<br /> <div>
arXiv:2505.10869v1 Announce Type: new 
Abstract: This study focuses on the velocity patterns of various body parts during walking and proposes a method for evaluating gait symmetry. Traditional motion analysis studies have assessed gait symmetry based on differences in electromyographic (EMG) signals or acceleration between the left and right sides. In contrast, this paper models intersegmental coordination using an LTI system and proposes a dissimilarity metric to evaluate symmetry. The method was tested on five subjects with both symmetric and asymmetric gait.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision</title>
<link>https://arxiv.org/abs/2505.10875</link>
<guid>https://arxiv.org/abs/2505.10875</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial reasoning, multi-modal large language model, visually impaired, navigation, object recognition

Summary: 
This paper introduces a novel approach for visually impaired individuals by enhancing multi-modal large language models with spatial reasoning capabilities and integrating them into a user-friendly hardware attachment for glasses. The system aims to improve environmental understanding for navigation and object recognition tasks. By fine-tuning the model, the proposed method shows significant improvements in accuracy and user experience, as demonstrated on the VizWiz dataset. The integration of advanced VLMs enables real-time, spatially aware feedback to assist visually impaired users in navigating their surroundings more effectively and independently. The system's effectiveness is evaluated through a comprehensive dataset designed for real-world scenarios, showcasing its potential as a robust solution for improving the independence and safety of visually impaired individuals.<br /><br />Summary: <div>
arXiv:2505.10875v1 Announce Type: new 
Abstract: People with blindness and low vision (pBLV) face significant challenges, struggling to navigate environments and locate objects due to limited visual cues. Spatial reasoning is crucial for these individuals, as it enables them to understand and interpret the spatial relationships in their surroundings, enhancing their ability to navigate and interact more safely and independently. Current multi-modal large language (MLLM) models for low vision people lack the spatial reasoning capabilities needed to effectively assist in these tasks. Moreover, there is a notable absence of lightweight, easy-to-use systems that allow pBLV to effectively perceive and interact with their surrounding environment. In this paper, we propose a novel spatial enhanced multi-modal large language model based approach for visually impaired individuals. By fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method significantly improves the understanding of environmental context, which is critical for navigation and object recognition. The innovation extends to a hardware component, designed as an attachment for glasses, ensuring increased accessibility and ease of use. This integration leverages advanced VLMs to interpret visual data and provide real-time, spatially aware feedback to the user. Our approach aims to bridge the gap between advanced machine learning models and practical, user-friendly assistive devices, offering a robust solution for visually impaired users to navigate their surroundings more effectively and independently. The paper includes an in-depth evaluation using the VizWiz dataset, demonstrating substantial improvements in accuracy and user experience. Additionally, we design a comprehensive dataset to evaluate our method's effectiveness in realworld situations, demonstrating substantial improvements in accuracy and user experience.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2505.10888</link>
<guid>https://arxiv.org/abs/2505.10888</guid>
<content:encoded><![CDATA[
<div> Keywords: human pose estimation, three-dimensional, cross-dataset evaluation, PoseBench3D, model generalization capabilities

Summary:
Pose estimation in three-dimensional space is crucial for real-world applications, requiring adaptation to varied environments and camera setups. Prior work often lacks robust evaluation across different datasets, impacting real-world performance. To address this, a standardized testing environment called PoseBench3D was developed to assess methods on multiple datasets, enabling fair cross-dataset comparisons. 18 existing methods were re-evaluated using Mean Per Joint Position Error (MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE), revealing novel insights into model performance. Analysis of pre-processing techniques and dataset parameters highlighted the impact on generalization capabilities, aiding in the assessment of model robustness. PoseBench3D serves as a unified framework to support future advancements in human pose estimation research. 

<br /><br />Summary: The research focuses on improving three-dimensional human pose estimation by introducing PoseBench3D, a standardized evaluation framework for cross-dataset analysis. By re-evaluating existing methods across multiple datasets and comparing performance metrics, the study sheds light on generalization capabilities and the impact of dataset preparation. The findings offer valuable insights for enhancing real-world applications of pose estimation technologies. <div>
arXiv:2505.10888v1 Announce Type: new 
Abstract: Reliable three-dimensional human pose estimation is becoming increasingly important for real-world applications, yet much of prior work has focused solely on the performance within a single dataset. In practice, however, systems must adapt to diverse viewpoints, environments, and camera setups -- conditions that differ significantly from those encountered during training, which is often the case in real-world scenarios. To address these challenges, we present a standardized testing environment in which each method is evaluated on a variety of datasets, ensuring consistent and fair cross-dataset comparisons -- allowing for the analysis of methods on previously unseen data. Therefore, we propose PoseBench3D, a unified framework designed to systematically re-evaluate prior and future models across four of the most widely used datasets for human pose estimation -- with the framework able to support novel and future datasets as the field progresses. Through a unified interface, our framework provides datasets in a pre-configured yet easily modifiable format, ensuring compatibility with diverse model architectures. We re-evaluated the work of 18 methods, either trained or gathered from existing literature, and reported results using both Mean Per Joint Position Error (MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE) metrics, yielding more than 100 novel cross-dataset evaluation results. Additionally, we analyze performance differences resulting from various pre-processing techniques and dataset preparation parameters -- offering further insight into model generalization capabilities.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach</title>
<link>https://arxiv.org/abs/2505.10902</link>
<guid>https://arxiv.org/abs/2505.10902</guid>
<content:encoded><![CDATA[
<div> 4D-CTA, dynamic cardiac model, digital twin technology, interventional cardiology, virtual angiography
Summary:
This study introduces a novel framework for creating dynamic cardiac models using 4D-CTA data and digital twin technology. The system accurately simulates vessel deformation across cardiac phases, allowing for precise anatomical and physiological representation. Physical models manufactured from medical-grade silicone provide tactile feedback for training in coronary interventions. Virtual angiography and guidewire reconstruction systems demonstrate high accuracy in replicating real-life procedures. The transparent physical model enhances training for coronary artery bypass grafting by simulating challenges encountered during surgery. Overall, this patient-specific digital-physical twin approach offers a dynamic educational tool with visual and tactile feedback, beneficial for physician training and preoperative planning in interventional cardiology. 
<br /><br />Summary: <div>
arXiv:2505.10902v1 Announce Type: new 
Abstract: Background and Objective: Precise preoperative planning and effective physician training for coronary interventions are increasingly important. Despite advances in medical imaging technologies, transforming static or limited dynamic imaging data into comprehensive dynamic cardiac models remains challenging. Existing training systems lack accurate simulation of cardiac physiological dynamics. This study develops a comprehensive dynamic cardiac model research framework based on 4D-CTA, integrating digital twin technology, computer vision, and physical model manufacturing to provide precise, personalized tools for interventional cardiology. Methods: Using 4D-CTA data from a 60-year-old female with three-vessel coronary stenosis, we segmented cardiac chambers and coronary arteries, constructed dynamic models, and implemented skeletal skinning weight computation to simulate vessel deformation across 20 cardiac phases. Transparent vascular physical models were manufactured using medical-grade silicone. We developed cardiac output analysis and virtual angiography systems, implemented guidewire 3D reconstruction using binocular stereo vision, and evaluated the system through angiography validation and CABG training applications. Results: Morphological consistency between virtual and real angiography reached 80.9%. Dice similarity coefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory errors below 1.1 mm. The transparent model demonstrated advantages in CABG training, allowing direct visualization while simulating beating heart challenges. Conclusion: Our patient-specific digital-physical twin approach effectively reproduces both anatomical structures and dynamic characteristics of coronary vasculature, offering a dynamic environment with visual and tactile feedback valuable for education and clinical planning.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross-Modal Mutual Information Maximization</title>
<link>https://arxiv.org/abs/2505.10917</link>
<guid>https://arxiv.org/abs/2505.10917</guid>
<content:encoded><![CDATA[
<div> alignment, cross-entropy loss, multimodal, information fusion, VISTA

Summary: 
The paper addresses the challenge of modality alignment in multimodal large language models (MLLMs), focusing on the bias towards textual information. The analysis of the cross-entropy loss used in MLLMs uncovers limitations in cross-modal alignment as text sequence length increases. To overcome these limitations, the paper introduces VISTA, a novel approach with an explicit alignment objective aimed at maximizing cross-modal mutual information. VISTA improves visual alignment without the need for additional trainable modules or extra training data. The method surpasses baseline models on multiple benchmark datasets, including VQAv2, MMStar, and MME, demonstrating enhanced visual understanding capabilities in MLLMs. This theoretical and practical advancement opens avenues for further research in modal alignment for MLLMs. 

<br /><br />Summary: <div>
arXiv:2505.10917v1 Announce Type: new 
Abstract: Current multimodal large language models (MLLMs) face a critical challenge in modality alignment, often exhibiting a bias towards textual information at the expense of other modalities like vision. This paper conducts a systematic information-theoretic analysis of the widely used cross-entropy loss in MLLMs, uncovering its implicit alignment objective. Our theoretical investigation reveals that this implicit objective has inherent limitations, leading to a degradation of cross-modal alignment as text sequence length increases, thereby hindering effective multimodal information fusion. To overcome these drawbacks, we propose Vision-Text Alignment (VISTA), a novel approach guided by our theoretical insights. VISTA introduces an explicit alignment objective designed to maximize cross-modal mutual information, preventing the degradation of visual alignment. Notably, VISTA enhances the visual understanding capabilities of existing MLLMs without requiring any additional trainable modules or extra training data, making it both efficient and practical. Our method significantly outperforms baseline models across more than a dozen benchmark datasets, including VQAv2, MMStar, and MME, paving the way for new directions in MLLM modal alignment research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cross-modal Retrieval in Chinese Cultural Heritage Documents: Dataset and Solution</title>
<link>https://arxiv.org/abs/2505.10921</link>
<guid>https://arxiv.org/abs/2505.10921</guid>
<content:encoded><![CDATA[
<div> dataset, cross-modal retrieval, Chinese cultural heritage, local alignment, LACLIP<br />
Summary:<br />
The article introduces a new multimodal dataset called CulTi specifically designed for Chinese cultural heritage. The dataset contains image-text pairs related to ancient Chinese silk patterns and Dunhuang murals. The challenge in cross-modal retrieval lies in the intricate alignment between visual motifs and textual descriptions. To address this issue, a training-free local alignment strategy named LACLIP is proposed, enhancing the matching of global textual descriptions with local visual regions. Experimental results show that LACLIP outperforms existing models in cross-modal retrieval, especially in capturing fine-grained semantic associations within Chinese cultural heritage. This research fills a critical gap in the field of multimodal learning for Chinese cultural heritage and provides a valuable resource for future studies in this area.<br /> 
Summary: <div>
arXiv:2505.10921v1 Announce Type: new 
Abstract: China has a long and rich history, encompassing a vast cultural heritage that includes diverse multimodal information, such as silk patterns, Dunhuang murals, and their associated historical narratives. Cross-modal retrieval plays a pivotal role in understanding and interpreting Chinese cultural heritage by bridging visual and textual modalities to enable accurate text-to-image and image-to-text retrieval. However, despite the growing interest in multimodal research, there is a lack of specialized datasets dedicated to Chinese cultural heritage, limiting the development and evaluation of cross-modal learning models in this domain. To address this gap, we propose a multimodal dataset named CulTi, which contains 5,726 image-text pairs extracted from two series of professional documents, respectively related to ancient Chinese silk and Dunhuang murals. Compared to existing general-domain multimodal datasets, CulTi presents a challenge for cross-modal retrieval: the difficulty of local alignment between intricate decorative motifs and specialized textual descriptions. To address this challenge, we propose LACLIP, a training-free local alignment strategy built upon a fine-tuned Chinese-CLIP. LACLIP enhances the alignment of global textual descriptions with local visual regions by computing weighted similarity scores during inference. Experimental results on CulTi demonstrate that LACLIP significantly outperforms existing models in cross-modal retrieval, particularly in handling fine-grained semantic associations within Chinese cultural heritage.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection</title>
<link>https://arxiv.org/abs/2505.10931</link>
<guid>https://arxiv.org/abs/2505.10931</guid>
<content:encoded><![CDATA[
<div> Dataset, Optical-SAR fusion, Object detection, Multi-source fusion methods, End-to-end framework  
Summary:  
- The article introduces a new dataset, M4-SAR, for optical-SAR fusion object detection, addressing challenges in complex environments by combining the advantages of optical and SAR images.  
- M4-SAR contains precisely aligned image pairs, labeled instances with arbitrary orientations, and spans six key categories to facilitate standardized evaluation.  
- A unified benchmarking toolkit integrating six state-of-the-art multi-source fusion methods is developed for evaluation purposes.  
- The proposed end-to-end multi-source fusion detection framework, E2E-OSDet, mitigates cross-domain discrepancies and establishes a robust baseline for future studies.  
- Experimental results on M4-SAR demonstrate that fusing optical and SAR data can improve mean Average Precision (mAP) by 5.7% over single-source inputs, with significant gains in complex environments.  
<br /><br />Summary: <div>
arXiv:2505.10931v1 Announce Type: new 
Abstract: Single-source remote sensing object detection using optical or SAR images struggles in complex environments. Optical images offer rich textural details but are often affected by low-light, cloud-obscured, or low-resolution conditions, reducing the detection performance. SAR images are robust to weather, but suffer from speckle noise and limited semantic expressiveness. Optical and SAR images provide complementary advantages, and fusing them can significantly improve the detection accuracy. However, progress in this field is hindered by the lack of large-scale, standardized datasets. To address these challenges, we propose the first comprehensive dataset for optical-SAR fusion object detection, named Multi-resolution, Multi-polarization, Multi-scene, Multi-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image pairs and nearly one million labeled instances with arbitrary orientations, spanning six key categories. To enable standardized evaluation, we develop a unified benchmarking toolkit that integrates six state-of-the-art multi-source fusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end multi-source fusion detection framework that mitigates cross-domain discrepancies and establishes a robust baseline for future studies. Extensive experiments on M4-SAR demonstrate that fusing optical and SAR data can improve $mAP$ by 5.7\% over single-source inputs, with particularly significant gains in complex environments. The dataset and code are publicly available at https://github.com/wchao0601/M4-SAR.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Anomaly Detection under Complex View-Illumination Interplay: A Large-Scale Benchmark</title>
<link>https://arxiv.org/abs/2505.10996</link>
<guid>https://arxiv.org/abs/2505.10996</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Anomaly Detection, Multi-View, Multi-Illumination, Benchmark, Robustness

Summary: 
The article introduces a new benchmark called Multi-View Multi-Illumination Anomaly Detection (M2AD) designed to assess the robustness of Visual Anomaly Detection (VAD) systems against viewpoint and illumination variations. The benchmark comprises a large dataset of high-resolution images captured from 999 specimens across 10 categories using 12 synchronized views and 10 illumination settings. Two evaluation protocols, M2AD-Synergy, and M2AD-Invariant, are established to test the ability of VAD methods to fuse information across diverse configurations and to evaluate single-image robustness against view-illumination effects. The benchmarking results show that current VAD methods struggle significantly on M2AD, highlighting the challenges posed by view-illumination interplay. The M2AD benchmark will be released to facilitate the development and validation of VAD methods capable of overcoming real-world complexities. 

<br /><br />Summary: <div>
arXiv:2505.10996v1 Announce Type: new 
Abstract: The practical deployment of Visual Anomaly Detection (VAD) systems is hindered by their sensitivity to real-world imaging variations, particularly the complex interplay between viewpoint and illumination which drastically alters defect visibility. Current benchmarks largely overlook this critical challenge. We introduce Multi-View Multi-Illumination Anomaly Detection (M2AD), a new large-scale benchmark comprising 119,880 high-resolution images designed explicitly to probe VAD robustness under such interacting conditions. By systematically capturing 999 specimens across 10 categories using 12 synchronized views and 10 illumination settings (120 configurations total), M2AD enables rigorous evaluation. We establish two evaluation protocols: M2AD-Synergy tests the ability to fuse information across diverse configurations, and M2AD-Invariant measures single-image robustness against realistic view-illumination effects. Our extensive benchmarking shows that state-of-the-art VAD methods struggle significantly on M2AD, demonstrating the profound challenge posed by view-illumination interplay. This benchmark serves as an essential tool for developing and validating VAD methods capable of overcoming real-world complexities. Our full dataset and test suite will be released at https://hustcyq.github.io/M2AD to facilitate the field.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning</title>
<link>https://arxiv.org/abs/2505.10999</link>
<guid>https://arxiv.org/abs/2505.10999</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, image synthesis, self-conditioning, generative pre-training, representation learners

Summary: 
This study explores the use of self-conditioning to enhance the training of diffusion models in image synthesis. The approach leverages the rich semantics within denoising networks to guide decoding layers, creating a tighter bottleneck that improves generation quality without compromising on recognition accuracy. The results demonstrate a boost in generation FID and recognition accuracy with minimal computational overhead, and the method can be applied to various diffusion architectures. By incorporating discriminative techniques like contrastive self-distillation, the enhanced diffusion models, particularly UViT and DiT, outperform traditional self-supervised models as representation learners. Linear evaluations on both pixel-space and latent-space datasets show the effectiveness of this approach in producing high-quality generative outputs while enhancing feature quality for downstream tasks. Overall, self-conditioning offers a promising avenue for improving the training and performance of diffusion models in image synthesis. 

<br /><br /> <div>
arXiv:2505.10999v1 Announce Type: new 
Abstract: While diffusion models have gained prominence in image synthesis, their generative pre-training has been shown to yield discriminative representations, paving the way towards unified visual generation and understanding. However, two key questions remain: 1) Can these representations be leveraged to improve the training of diffusion models themselves, rather than solely benefiting downstream tasks? 2) Can the feature quality be enhanced to rival or even surpass modern self-supervised learners, without compromising generative capability? This work addresses these questions by introducing self-conditioning, a straightforward yet effective mechanism that internally leverages the rich semantics inherent in denoising network to guide its own decoding layers, forming a tighter bottleneck that condenses high-level semantics to improve generation. Results are compelling: our method boosts both generation FID and recognition accuracy with 1% computational overhead and generalizes across diverse diffusion architectures. Crucially, self-conditioning facilitates an effective integration of discriminative techniques, such as contrastive self-distillation, directly into diffusion models without sacrificing generation quality. Extensive experiments on pixel-space and latent-space datasets show that in linear evaluations, our enhanced diffusion models, particularly UViT and DiT, serve as strong representation learners, surpassing various self-supervised models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForensicHub: A Unified Benchmark &amp; Codebase for All-Domain Fake Image Detection and Localization</title>
<link>https://arxiv.org/abs/2505.11003</link>
<guid>https://arxiv.org/abs/2505.11003</guid>
<content:encoded><![CDATA[
<div> unified benchmark, fake image detection, localization, ForensicHub, domain silos
<br />
ForensicHub is introduced as a unified benchmark and codebase for all-domain fake image detection and localization, aiming to address the fragmented nature of the field. By offering a modular and configuration-driven architecture, it allows for flexible composition across datasets, transforms, models, and evaluators, breaking down domain silos. The platform includes baseline models, backbones, and benchmarks for various domains within fake image detection and localization, facilitating cross-domain comparisons and development. Through an adapter-based design, existing benchmarks like DeepfakeBench and IMDLBenCo are integrated into ForensicHub. The platform also enables in-depth analysis, providing key insights into FIDL model architecture, dataset characteristics, and evaluation standards. ForensicHub's comprehensive approach marks a significant advancement in unifying the FIDL field and fostering future advancements.
<br /><br />Summary: <div>
arXiv:2505.11003v1 Announce Type: new 
Abstract: The field of Fake Image Detection and Localization (FIDL) is highly fragmented, encompassing four domains: deepfake detection (Deepfake), image manipulation detection and localization (IMDL), artificial intelligence-generated image detection (AIGC), and document image manipulation localization (Doc). Although individual benchmarks exist in some domains, a unified benchmark for all domains in FIDL remains blank. The absence of a unified benchmark results in significant domain silos, where each domain independently constructs its datasets, models, and evaluation protocols without interoperability, preventing cross-domain comparisons and hindering the development of the entire FIDL field. To close the domain silo barrier, we propose ForensicHub, the first unified benchmark & codebase for all-domain fake image detection and localization. Considering drastic variations on dataset, model, and evaluation configurations across all domains, as well as the scarcity of open-sourced baseline models and the lack of individual benchmarks in some domains, ForensicHub: i) proposes a modular and configuration-driven architecture that decomposes forensic pipelines into interchangeable components across datasets, transforms, models, and evaluators, allowing flexible composition across all domains; ii) fully implements 10 baseline models, 6 backbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing benchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii) conducts indepth analysis based on the ForensicHub, offering 8 key actionable insights into FIDL model architecture, dataset characteristics, and evaluation standards. ForensicHub represents a significant leap forward in breaking the domain silos in the FIDL field and inspiring future breakthroughs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</title>
<link>https://arxiv.org/abs/2505.11013</link>
<guid>https://arxiv.org/abs/2505.11013</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D human motion, text-to-motion generation, VQVAE, diffusion processes, keyframe control

Summary: 
The article introduces a new framework, MoMADiff, for generating 3D human motion from text descriptions. This framework addresses challenges faced by existing methods, such as the inability to represent novel motions faithfully using discrete tokens and the lack of fine-grained control over individual frames. MoMADiff combines masked modeling with diffusion processes to generate motion using frame-level continuous representations, allowing for flexible keyframe specification for precise control over motion synthesis. The framework demonstrates strong generalization capability on novel text-to-motion datasets, outperforming existing models in terms of motion quality, instruction fidelity, and keyframe adherence. Extensive experiments on held-out datasets and standard benchmarks validate the effectiveness of MoMADiff in generating realistic and diverse human motion sequences. <div>
arXiv:2505.11013v1 Announce Type: new 
Abstract: Generating 3D human motion from text descriptions remains challenging due to the diverse and complex nature of human motion. While existing methods excel within the training distribution, they often struggle with out-of-distribution motions, limiting their applicability in real-world scenarios. Existing VQVAE-based methods often fail to represent novel motions faithfully using discrete tokens, which hampers their ability to generalize beyond seen data. Meanwhile, diffusion-based methods operating on continuous representations often lack fine-grained control over individual frames. To address these challenges, we propose a robust motion generation framework MoMADiff, which combines masked modeling with diffusion processes to generate motion using frame-level continuous representations. Our model supports flexible user-provided keyframe specification, enabling precise control over both spatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong generalization capability on novel text-to-motion datasets with sparse keyframes as motion prompts. Extensive experiments on two held-out datasets and two standard benchmarks show that our method consistently outperforms state-of-the-art models in motion quality, instruction fidelity, and keyframe adherence.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?</title>
<link>https://arxiv.org/abs/2505.11015</link>
<guid>https://arxiv.org/abs/2505.11015</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Document Understanding, WildDoc, Benchmark, Model Robustness

Summary:
The article introduces a new benchmark called WildDoc, specifically designed to assess document understanding in natural environments. This benchmark includes a diverse set of manually captured document images that reflect real-world conditions, such as variable illumination and physical distortions. WildDoc aims to address the limitations of existing benchmarks like DocVQA and ChartQA, which primarily focus on scanned or digital documents. Each document in WildDoc is captured four times under different conditions to evaluate model robustness thoroughly. State-of-the-art Multimodal Large Language Models (MLLMs) were evaluated on WildDoc, revealing significant performance declines and highlighting the models' lack of robustness compared to traditional benchmarks. The project homepage for WildDoc is https://bytedance.github.io/WildDoc. 

<br /><br />Summary: 
- Introduction of WildDoc benchmark for document understanding in natural environments
- Diverse set of manually captured document images with real-world conditions
- Comparison with existing benchmarks like DocVQA and ChartQA
- Evaluation of model robustness through multiple captures of each document
- Performance declines of state-of-the-art MLLMs on WildDoc compared to traditional benchmarks <div>
arXiv:2505.11015v1 Announce Type: new 
Abstract: The rapid advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced capabilities in Document Understanding. However, prevailing benchmarks like DocVQA and ChartQA predominantly comprise \textit{scanned or digital} documents, inadequately reflecting the intricate challenges posed by diverse real-world scenarios, such as variable illumination and physical distortions. This paper introduces WildDoc, the inaugural benchmark designed specifically for assessing document understanding in natural environments. WildDoc incorporates a diverse set of manually captured document images reflecting real-world conditions and leverages document sources from established benchmarks to facilitate comprehensive comparisons with digital or scanned documents. Further, to rigorously evaluate model robustness, each document is captured four times under different conditions. Evaluations of state-of-the-art MLLMs on WildDoc expose substantial performance declines and underscore the models' inadequate robustness compared to traditional benchmarks, highlighting the unique challenges posed by real-world document understanding. Our project homepage is available at https://bytedance.github.io/WildDoc.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning</title>
<link>https://arxiv.org/abs/2505.11018</link>
<guid>https://arxiv.org/abs/2505.11018</guid>
<content:encoded><![CDATA[
<div> mean teacher strategy, semi-supervised learning, medical image segmentation, self-paced learning, dual teacher-student learning

Summary:
The paper introduces a novel framework called dual teacher-student learning (DTSL) for semi-supervised medical image segmentation. The proposed method reinterprets the mean teacher strategy as a form of self-paced learning, utilizing output agreement between teacher and student models. DTSL incorporates two groups of teacher-student models with different architectures, leveraging output agreement between cross-group models for pseudo-label generation. This consensus label generator (CLG) uses Jensen-Shannon divergence to produce pseudo-labels for unlabeled data. Experimental results on popular datasets show that DTSL consistently outperforms existing state-of-the-art approaches. Ablation studies confirm the effectiveness of the proposed modules, highlighting the potential of DTSL for improving segmentation accuracy in medical imaging tasks.<br /><br />Summary: <div>
arXiv:2505.11018v1 Announce Type: new 
Abstract: Semi-supervised medical image segmentation has attracted significant attention due to its potential to reduce manual annotation costs. The mean teacher (MT) strategy, commonly understood as introducing smoothed, temporally lagged consistency regularization, has demonstrated strong performance across various tasks in this field. In this work, we reinterpret the MT strategy on supervised data as a form of self-paced learning, regulated by the output agreement between the temporally lagged teacher model and the ground truth labels. This idea is further extended to incorporate agreement between a temporally lagged model and a cross-architectural model, which offers greater flexibility in regulating the learning pace and enables application to unlabeled data. Specifically, we propose dual teacher-student learning (DTSL), a framework that introduces two groups of teacher-student models with different architectures. The output agreement between the cross-group teacher and student models is used as pseudo-labels, generated via a Jensen-Shannon divergence-based consensus label generator (CLG). Extensive experiments on popular datasets demonstrate that the proposed method consistently outperforms existing state-of-the-art approaches. Ablation studies further validate the effectiveness of the proposed modules.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifying Shelf Life Quality of Pineapples by Combining Audio and Visual Features</title>
<link>https://arxiv.org/abs/2505.11020</link>
<guid>https://arxiv.org/abs/2505.11020</guid>
<content:encoded><![CDATA[
<div> Dataset, Classification model, Pineapples, Quality levels, Non-destructive methods<br />
Summary:<br />
- A multimodal and multiview classification model was developed to evaluate the shelf life quality of pineapples using audio and visual features.
- The PQC500 dataset consisting of 500 pineapples was compiled and released for research purposes.
- The classification model utilized cross-modal learning and was trained on audiovisual pairs using a modified contrastive audiovisual masked autoencoder.
- A compact size of training data was sampled to improve computation efficiency.
- The cross-modal model, trained using audio-major sampling, achieved 84% accuracy, outperforming unimodal models of only audio and only visual by 6% and 18%, respectively.<br /> <div>
arXiv:2505.11020v1 Announce Type: new 
Abstract: Determining the shelf life quality of pineapples using non-destructive methods is a crucial step to reduce waste and increase income. In this paper, a multimodal and multiview classification model was constructed to classify pineapples into four quality levels based on audio and visual characteristics. For research purposes, we compiled and released the PQC500 dataset consisting of 500 pineapples with two modalities: one was tapping pineapples to record sounds by multiple microphones and the other was taking pictures by multiple cameras at different locations, providing multimodal and multi-view audiovisual features. We modified the contrastive audiovisual masked autoencoder to train the cross-modal-based classification model by abundant combinations of audio and visual pairs. In addition, we proposed to sample a compact size of training data for efficient computation. The experiments were evaluated under various data and model configurations, and the results demonstrated that the proposed cross-modal model trained using audio-major sampling can yield 84% accuracy, outperforming the unimodal models of only audio and only visual by 6% and 18%, respectively.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanPatrick: A Benchmark for Image Data Cleaning</title>
<link>https://arxiv.org/abs/2505.11034</link>
<guid>https://arxiv.org/abs/2505.11034</guid>
<content:encoded><![CDATA[
<div> Keywords: data cleaning, image domain, benchmark, anomaly detection, medical classification

Summary:
CleanPatrick is introduced as a large-scale benchmark for data cleaning in the image domain, utilizing the Fitzpatrick17k dermatology dataset. The dataset underwent rigorous analysis by collecting binary annotations from medical crowd workers to identify off-topic samples, near-duplicates, and label errors. An aggregation model and expert review were employed to ensure high-quality ground truth. The benchmark formalizes issue detection as a ranking task, evaluating various data cleaning methods including classical anomaly detectors, perceptual hashing, and self-supervised representations. Results show that self-supervised representations excel at near-duplicate detection, classical methods perform well under constrained review budgets for off-topic detection, but label-error detection remains a challenge in fine-grained medical classification. CleanPatrick's release of the dataset and evaluation framework allows for systematic comparison of image-cleaning strategies for more reliable artificial intelligence. 

<br /><br />Summary: <div>
arXiv:2505.11034v1 Announce Type: new 
Abstract: Robust machine learning depends on clean data, yet current image data cleaning benchmarks rely on synthetic noise or narrow human studies, limiting comparison and real-world relevance. We introduce CleanPatrick, the first large-scale benchmark for data cleaning in the image domain, built upon the publicly available Fitzpatrick17k dermatology dataset. We collect 496,377 binary annotations from 933 medical crowd workers, identify off-topic samples (4%), near-duplicates (21%), and label errors (22%), and employ an aggregation model inspired by item-response theory followed by expert review to derive high-quality ground truth. CleanPatrick formalizes issue detection as a ranking task and adopts typical ranking metrics mirroring real audit workflows. Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick, self-supervised representations excel at near-duplicate detection, classical methods achieve competitive off-topic detection under constrained review budgets, and label-error detection remains an open challenge for fine-grained medical classification. By releasing both the dataset and the evaluation framework, CleanPatrick enables a systematic comparison of image-cleaning strategies and paves the way for more reliable data-centric artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artifacts of Idiosyncracy in Global Street View Data</title>
<link>https://arxiv.org/abs/2505.11046</link>
<guid>https://arxiv.org/abs/2505.11046</guid>
<content:encoded><![CDATA[
<div> Keywords: street view data, machine learning datasets, biases, city coverage, idiosyncracies 

Summary: 
The study examines the use of street view data in computer vision applications and highlights biases in coverage across 28 global cities. Despite dense sampling, discrepancies in coverage exist due to city-specific idiosyncracies. The research proposes a method for evaluating coverage distributions to identify these biases and improve dataset representation. A case study of Amsterdam emphasizes how collection process idiosyncrasies impact coverage, showcasing the need to address biases at the source through interviews and analysis. This study sheds light on the importance of understanding and mitigating biases in street view data to enhance the systematic representation of cities in machine learning applications. 

<br /><br />Summary: <div>
arXiv:2505.11046v1 Announce Type: new 
Abstract: Street view data is increasingly being used in computer vision applications in recent years. Machine learning datasets are collected for these applications using simple sampling techniques. These datasets are assumed to be a systematic representation of cities, especially when densely sampled. Prior works however, show that there are clear gaps in coverage, with certain cities or regions being covered poorly or not at all. Here we demonstrate that a cities' idiosyncracies, such as city layout, may lead to biases in street view data for 28 cities across the globe, even when they are densely covered. We quantitatively uncover biases in the distribution of coverage of street view data and propose a method for evaluation of such distributions to get better insight in idiosyncracies in a cities' coverage. In addition, we perform a case study of Amsterdam with semi-structured interviews, showing how idiosyncracies of the collection process impact representation of cities and regions and allowing us to address biases at their source.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs</title>
<link>https://arxiv.org/abs/2505.11060</link>
<guid>https://arxiv.org/abs/2505.11060</guid>
<content:encoded><![CDATA[
<div> Concept embeddings, Bias identification, Deep vision models, Interpretability, Unsupervised learning <br />
Summary: <br />
The paper introduces CUBIC, a novel method for automatically identifying biases in deep vision models. Unlike existing approaches, CUBIC does not require predefined bias candidates or examples of model failures. It leverages image-text latent space and linear classifier probes to analyze shifts in superclass label representations influenced by specific concepts. By comparing these shifts against the classifier's decision boundary, CUBIC detects concepts significantly impacting model predictions. The method successfully uncovers hidden biases in Vision-Language Models without prior knowledge or specific dataset samples where the classifier fails. This approach offers a more efficient and interpretable way to uncover biases learned from spurious correlations in datasets, enhancing the fairness and transparency of deep vision models. <br /> <div>
arXiv:2505.11060v1 Announce Type: new 
Abstract: Deep vision models often rely on biases learned from spurious correlations in datasets. To identify these biases, methods that interpret high-level, human-understandable concepts are more effective than those relying primarily on low-level features like heatmaps. A major challenge for these concept-based methods is the lack of image annotations indicating potentially bias-inducing concepts, since creating such annotations requires detailed labeling for each dataset and concept, which is highly labor-intensive. We present CUBIC (Concept embeddings for Unsupervised Bias IdentifiCation), a novel method that automatically discovers interpretable concepts that may bias classifier behavior. Unlike existing approaches, CUBIC does not rely on predefined bias candidates or examples of model failures tied to specific biases, as such information is not always available. Instead, it leverages image-text latent space and linear classifier probes to examine how the latent representation of a superclass label$\unicode{x2014}$shared by all instances in the dataset$\unicode{x2014}$is influenced by the presence of a given concept. By measuring these shifts against the normal vector to the classifier's decision boundary, CUBIC identifies concepts that significantly influence model predictions. Our experiments demonstrate that CUBIC effectively uncovers previously unknown biases using Vision-Language Models (VLMs) without requiring the samples in the dataset where the classifier underperforms or prior knowledge of potential biases.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSRMamba: Efficient Wavelet Stripe State Space Model for Hyperspectral Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.11062</link>
<guid>https://arxiv.org/abs/2505.11062</guid>
<content:encoded><![CDATA[
<div> Keywords: SHSR, hyperspectral image, super-resolution, Visual Mamba, HSRMamba

Summary:
HSRMamba is a new model for single hyperspectral image super-resolution that aims to improve upon the Visual Mamba model in terms of artifact reduction and super-resolution performance. The proposed HSRMamba model utilizes a strip-based scanning scheme to mitigate potential artifacts caused by global unidirectional scanning. Additionally, wavelet decomposition is implemented to address conflicts between high-frequency spatial features and low-frequency spectral features, resulting in enhanced super-resolution quality. HSRMamba maintains computational efficiency while outperforming existing methods in terms of reducing computational load, model size, and achieving state-of-the-art results in hyperspectral image super-resolution tasks. This new approach shows promise in advancing the field of hyperspectral imaging and super-resolution techniques.<br /><br />Summary: <div>
arXiv:2505.11062v1 Announce Type: new 
Abstract: Single hyperspectral image super-resolution (SHSR) aims to restore high-resolution images from low-resolution hyperspectral images. Recently, the Visual Mamba model has achieved an impressive balance between performance and computational efficiency. However, due to its 1D scanning paradigm, the model may suffer from potential artifacts during image generation. To address this issue, we propose HSRMamba. While maintaining the computational efficiency of Visual Mamba, we introduce a strip-based scanning scheme to effectively reduce artifacts from global unidirectional scanning. Additionally, HSRMamba uses wavelet decomposition to alleviate modal conflicts between high-frequency spatial features and low-frequency spectral features, further improving super-resolution performance. Extensive experiments show that HSRMamba not only excels in reducing computational load and model size but also outperforms existing methods, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Self-Improvement of Diffusion Models via Group Preference Optimization</title>
<link>https://arxiv.org/abs/2505.11070</link>
<guid>https://arxiv.org/abs/2505.11070</guid>
<content:encoded><![CDATA[
<div> diffusion, text-to-image, preference optimization, group preference optimization, performance gains <br />
<br />
Summary: <br />
The article discusses the challenges faced in aligning text-to-image diffusion models with Direct Preference Optimization (DPO) due to the sensitivity of DPO to preference pairs and the labor-intensive data collection process. It highlights the issue of misclassification of samples with marginal differences by DPO and proposes extending DPO from pairwise to groupwise, incorporating reward standardization for reweighting, and introducing Group Preference Optimization (GPO). The GPO method improves performance without requiring external data and is shown to be effective across various diffusion models and tasks. Experimental results demonstrate a 20 percentage point improvement in accurate counting and text rendering capabilities when combining GPO with computer vision models such as YOLO and OCR in the Stable Diffusion 3.5 Medium model. GPO is described as a plug-and-play method that enhances performance without introducing extra overhead during inference. <br /> <div>
arXiv:2505.11070v1 Announce Type: new 
Abstract: Aligning text-to-image (T2I) diffusion models with Direct Preference Optimization (DPO) has shown notable improvements in generation quality. However, applying DPO to T2I faces two challenges: the sensitivity of DPO to preference pairs and the labor-intensive process of collecting and annotating high-quality data. In this work, we demonstrate that preference pairs with marginal differences can degrade DPO performance. Since DPO relies exclusively on relative ranking while disregarding the absolute difference of pairs, it may misclassify losing samples as wins, or vice versa. We empirically show that extending the DPO from pairwise to groupwise and incorporating reward standardization for reweighting leads to performance gains without explicit data selection. Furthermore, we propose Group Preference Optimization (GPO), an effective self-improvement method that enhances performance by leveraging the model's own capabilities without requiring external data. Extensive experiments demonstrate that GPO is effective across various diffusion models and tasks. Specifically, combining with widely used computer vision models, such as YOLO and OCR, the GPO improves the accurate counting and text rendering capabilities of the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a plug-and-play method, no extra overhead is introduced during inference.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo-Label Quality Decoupling and Correction for Semi-Supervised Instance Segmentation</title>
<link>https://arxiv.org/abs/2505.11075</link>
<guid>https://arxiv.org/abs/2505.11075</guid>
<content:encoded><![CDATA[
<div> instance segmentation, semi-supervised learning, pseudo-labeling, quality decoupling, mask uncertainty-aware

Summary:
- The paper introduces a novel framework called Pseudo-Label Quality Decoupling and Correction (PL-DC) for Semi-Supervised Instance Segmentation (SSIS).
- PL-DC addresses challenges in SSIS by implementing a dual-threshold filtering mechanism to separate class and mask quality estimations for instance-level pseudo-labels.
- A dynamic instance category correction module is introduced to alleviate category confusion.
- A mask uncertainty-aware mechanism is implemented at the pixel level to re-weight the mask loss for different pixels, reducing the impact of noise from pixel-level mask pseudo-labels.
- Extensive experiments on COCO and Cityscapes datasets demonstrate that PL-DC achieves significant performance improvements, setting new state-of-the-art results for SSIS even with minimal labeled data. A remarkable improvement of +11.6 mAP with just 1% COCO labeled data and +15.5 mAP with 5% Cityscapes labeled data is achieved. The code will be made public.

<br /><br />Summary: <div>
arXiv:2505.11075v1 Announce Type: new 
Abstract: Semi-Supervised Instance Segmentation (SSIS) involves classifying and grouping image pixels into distinct object instances using limited labeled data. This learning paradigm usually faces a significant challenge of unstable performance caused by noisy pseudo-labels of instance categories and pixel masks. We find that the prevalent practice of filtering instance pseudo-labels assessing both class and mask quality with a single score threshold, frequently leads to compromises in the trade-off between the qualities of class and mask labels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and Correction (PL-DC) framework for SSIS to tackle the above challenges. Firstly, at the instance level, a decoupled dual-threshold filtering mechanism is designed to decouple class and mask quality estimations for instance-level pseudo-labels, thereby independently controlling pixel classifying and grouping qualities. Secondly, at the category level, we introduce a dynamic instance category correction module to dynamically correct the pseudo-labels of instance categories, effectively alleviating category confusion. Lastly, we introduce a pixel-level mask uncertainty-aware mechanism at the pixel level to re-weight the mask loss for different pixels, thereby reducing the impact of noise introduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO and Cityscapes datasets demonstrate that the proposed PL-DC achieves significant performance improvements, setting new state-of-the-art results for SSIS. Notably, our PL-DC shows substantial gains even with minimal labeled data, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and +15.5 mAP with 5% Cityscapes labeled data. The code will be public.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification</title>
<link>https://arxiv.org/abs/2505.11099</link>
<guid>https://arxiv.org/abs/2505.11099</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud classification, Mamba architecture, geometric feature extraction, bidirectional model, classification accuracy

Summary: 
The Mamba architecture addresses the challenges of efficiently extracting local geometric features in point cloud classification tasks by utilizing state space models (SSMs) with linear complexity. However, its unidirectional dependency limits spatial correlation modeling in local neighborhoods. The proposed Hybrid-Emba3D model enhances the Mamba architecture by incorporating geometry-feature coupling and cross-path feature hybridization. The Local geometric pooling mechanism increases local feature discriminative power through coordinated propagation of geometric information. The Collaborative feature enhancer handles local mutations and sparse key signals effectively. Experimental results show that the Hybrid-Emba3D model achieves a new state-of-the-art classification accuracy of 95.99% on ModelNet40 with minimal additional parameters. <br /><br />Summary: <div>
arXiv:2505.11099v1 Announce Type: new 
Abstract: The point cloud classification tasks face the dual challenge of efficiently extracting local geometric features while maintaining model complexity. The Mamba architecture utilizes the linear complexity advantage of state space models (SSMs) to overcome the computational bottleneck of Transformers while balancing global modeling capabilities. However, the inherent contradiction between its unidirectional dependency and the unordered nature of point clouds impedes modeling spatial correlation in local neighborhoods, thus constraining geometric feature extraction. This paper proposes Hybrid-Emba3D, a bidirectional Mamba model enhanced by geometry-feature coupling and cross-path feature hybridization. The Local geometric pooling with geometry-feature coupling mechanism significantly enhances local feature discriminative power via coordinated propagation and dynamic aggregation of geometric information between local center points and their neighborhoods, without introducing additional parameters. The designed Collaborative feature enhancer adopts dual-path hybridization, effectively handling local mutations and sparse key signals, breaking through the limitations of traditional SSM long-range modeling. Experimental results demonstrate that the proposed model achieves a new SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M additional.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark</title>
<link>https://arxiv.org/abs/2505.11109</link>
<guid>https://arxiv.org/abs/2505.11109</guid>
<content:encoded><![CDATA[
arXiv:2505.11109v1 Announce Type: new 
Abstract: We present the first large-scale open-set benchmark for multilingual audio-video deepfake detection. Our dataset comprises over 250 hours of real and fake videos across eight languages, with 60% of data being generated. For each language, the fake videos are generated with seven distinct deepfake generation models, selected based on the quality of the generated content. We organize the training, validation and test splits such that only a subset of the chosen generative models and languages are available during training, thus creating several challenging open-set evaluation setups. We perform experiments with various pre-trained and fine-tuned deepfake detectors proposed in recent literature. Our results show that state-of-the-art detectors are not currently able to maintain their performance levels when tested in our open-set scenarios. We publicly release our data and code at: https://huggingface.co/datasets/unibuc-cs/MAVOS-DD.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation</title>
<link>https://arxiv.org/abs/2505.11110</link>
<guid>https://arxiv.org/abs/2505.11110</guid>
<content:encoded><![CDATA[
arXiv:2505.11110v1 Announce Type: new 
Abstract: Synthetic media generated by Generative Adversarial Networks (GANs) pose significant challenges in verifying authenticity and tracing dataset origins, raising critical concerns in copyright enforcement, privacy protection, and legal compliance. This paper introduces a novel forensic framework for identifying the training dataset (e.g., CelebA or FFHQ) of GAN-generated images through interpretable feature analysis. By integrating spectral transforms (Fourier/DCT), color distribution metrics, and local feature descriptors (SIFT), our pipeline extracts discriminative statistical signatures embedded in synthetic outputs. Supervised classifiers (Random Forest, SVM, XGBoost) achieve 98-99% accuracy in binary classification (real vs. synthetic) and multi-class dataset attribution across diverse GAN architectures (StyleGAN, AttGAN, GDWCT, StarGAN, and StyleGAN2). Experimental results highlight the dominance of frequency-domain features (DCT/FFT) in capturing dataset-specific artifacts, such as upsampling patterns and spectral irregularities, while color histograms reveal implicit regularization strategies in GAN training. We further examine legal and ethical implications, showing how dataset attribution can address copyright infringement, unauthorized use of personal data, and regulatory compliance under frameworks like GDPR and California's AB 602. Our framework advances accountability and governance in generative modeling, with applications in digital forensics, content moderation, and intellectual property litigation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing</title>
<link>https://arxiv.org/abs/2505.11121</link>
<guid>https://arxiv.org/abs/2505.11121</guid>
<content:encoded><![CDATA[
arXiv:2505.11121v1 Announce Type: new 
Abstract: The development of foundation models through pretraining of vision-language models (VLMs) has recently attracted great attention in remote sensing (RS). VLM pretraining aims to learn image and language alignments from a large number of image-text pairs. Each pretraining image is often associated with multiple captions containing redundant information due to repeated or semantically similar phrases, resulting in increased pretraining and inference time. To overcome this, we introduce a weighted feature aggregation (WFA) strategy for VLM pretraining in RS. Our strategy aims to extract and exploit complementary information from multiple captions per image while reducing redundancies through feature aggregation with importance weighting. To calculate adaptive importance weights for different captions of each image, we propose two techniques: (i) non-parametric uniqueness and (ii) learning-based attention. In the first technique, importance weights are calculated based on the bilingual evaluation understudy (BLEU) scores of the captions to emphasize unique sentences and reduce the influence of repetitive ones. In the second technique, importance weights are learned through an attention mechanism instead of relying on hand-crafted features. The effectiveness of the proposed WFA strategy with the two techniques is analyzed in terms of downstream performance on text-to-image retrieval in RS. Experimental results show that the proposed strategy enables efficient and effective pretraining of VLMs in RS. Based on the experimental analysis, we derive guidelines for selecting appropriate techniques depending on downstream task requirements and resource constraints. The code of this work is publicly available at https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video</title>
<link>https://arxiv.org/abs/2505.11129</link>
<guid>https://arxiv.org/abs/2505.11129</guid>
<content:encoded><![CDATA[
arXiv:2505.11129v1 Announce Type: new 
Abstract: Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation. Our model leverages variational inference to learn robust visual representations from continuous input streams, similar to human visual processing. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation models, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a significant step toward more biologically plausible computer vision systems that process visual information in a manner more closely aligned with human cognitive processes.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework</title>
<link>https://arxiv.org/abs/2505.11131</link>
<guid>https://arxiv.org/abs/2505.11131</guid>
<content:encoded><![CDATA[
arXiv:2505.11131v1 Announce Type: new 
Abstract: Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at https://github.com/Ferry-Li/Co-Erasing.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans</title>
<link>https://arxiv.org/abs/2505.11141</link>
<guid>https://arxiv.org/abs/2505.11141</guid>
<content:encoded><![CDATA[
arXiv:2505.11141v1 Announce Type: new 
Abstract: The goal of achieving Artificial General Intelligence (AGI) is to imitate humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have demonstrated that large language models (LLMs) with human-like reasoning capabilities exhibit exceptional performance and are being gradually integrated into multimodal large language models (MLLMs). However, whether these models possess capabilities comparable to humans in handling reasoning tasks remains unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark for fine-grained alignment of multimodal reasoning with human performance. Specifically, we collected 9,794 multimodal questions that solely rely on contextual reasoning, including bilingual (Chinese and English) multimodal questions and pure text-based questions, encompassing four question types: visual reasoning, definition judgment, analogical reasoning, and logical judgment. More importantly, each question is accompanied by human success rates and options that humans are prone to choosing incorrectly. Extensive experiments on the Human-Aligned Bench reveal notable differences between the performance of current MLLMs in multimodal reasoning and human performance. The findings on our benchmark provide insights into the development of the next-generation models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dense Hand Contact Estimation from Imbalanced Data</title>
<link>https://arxiv.org/abs/2505.11152</link>
<guid>https://arxiv.org/abs/2505.11152</guid>
<content:encoded><![CDATA[
arXiv:2505.11152v1 Announce Type: new 
Abstract: Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer</title>
<link>https://arxiv.org/abs/2505.11168</link>
<guid>https://arxiv.org/abs/2505.11168</guid>
<content:encoded><![CDATA[
arXiv:2505.11168v1 Announce Type: new 
Abstract: The automatic diagnosis of chest diseases is a popular and challenging task. Most current methods are based on convolutional neural networks (CNNs), which focus on local features while neglecting global features. Recently, self-attention mechanisms have been introduced into the field of computer vision, demonstrating superior performance. Therefore, this paper proposes an effective model, CheX-DS, for classifying long-tail multi-label data in the medical field of chest X-rays. The model is based on the excellent CNN model DenseNet for medical imaging and the newly popular Swin Transformer model, utilizing ensemble deep learning techniques to combine the two models and leverage the advantages of both CNNs and Transformers. The loss function of CheX-DS combines weighted binary cross-entropy loss with asymmetric loss, effectively addressing the issue of data imbalance. The NIH ChestX-ray14 dataset is selected to evaluate the model's effectiveness. The model outperforms previous studies with an excellent average AUC score of 83.76\%, demonstrating its superior performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback</title>
<link>https://arxiv.org/abs/2505.11178</link>
<guid>https://arxiv.org/abs/2505.11178</guid>
<content:encoded><![CDATA[
arXiv:2505.11178v1 Announce Type: new 
Abstract: State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest's feedback as preference signals to improve diffusion models' compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning</title>
<link>https://arxiv.org/abs/2505.11182</link>
<guid>https://arxiv.org/abs/2505.11182</guid>
<content:encoded><![CDATA[
arXiv:2505.11182v1 Announce Type: new 
Abstract: In incomplete multi-view clustering (IMVC), missing data induce prototype shifts within views and semantic inconsistencies across views. A feasible solution is to explore cross-view consistency in paired complete observations, further imputing and aligning the similarity relationships inherently shared across views. Nevertheless, existing methods are constrained by two-tiered limitations: (1) Neither instance- nor cluster-level consistency learning construct a semantic space shared across views to learn consensus semantics. The former enforces cross-view instances alignment, and wrongly regards unpaired observations with semantic consistency as negative pairs; the latter focuses on cross-view cluster counterparts while coarsely handling fine-grained intra-cluster relationships within views. (2) Excessive reliance on consistency results in unreliable imputation and alignment without incorporating view-specific cluster information. Thus, we propose an IMVC framework, imputation- and alignment-free for consensus semantics learning (FreeCSL). To bridge semantic gaps across all observations, we learn consensus prototypes from available data to discover a shared space, where semantically similar observations are pulled closer for consensus semantics learning. To capture semantic relationships within specific views, we design a heuristic graph clustering based on modularity to recover cluster structure with intra-cluster compactness and inter-cluster separation for cluster semantics enhancement. Extensive experiments demonstrate, compared to state-of-the-art competitors, FreeCSL achieves more confident and robust assignments on IMVC task.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2505.11192</link>
<guid>https://arxiv.org/abs/2505.11192</guid>
<content:encoded><![CDATA[
arXiv:2505.11192v1 Announce Type: new 
Abstract: False negatives pose a critical challenge in vision-language pretraining (VLP) due to the many-to-many correspondence between images and texts in large-scale datasets. These false negatives introduce conflicting supervision signals that degrade the learned embedding space and diminish the effectiveness of hard negative sampling. In this paper, we propose FALCON (False-negative Aware Learning of COntrastive Negatives), a learning-based mini-batch construction strategy that adaptively balances the trade-off between hard and false negatives during VLP. Rather than relying on fixed heuristics, FALCON employs a negative mining scheduler that dynamically selects negative samples of appropriate hardness for each anchor instance during mini-batch construction, guided by a proxy for cross-modal alignment improvement. Experimental results demonstrate that FALCON significantly improves performance across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of downstream tasks and evaluation settings, underscoring its effectiveness and robustness in mitigating the impact of false negatives.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling</title>
<link>https://arxiv.org/abs/2505.11196</link>
<guid>https://arxiv.org/abs/2505.11196</guid>
<content:encoded><![CDATA[
arXiv:2505.11196v1 Announce Type: new 
Abstract: Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256x256-without any additional supervision during training. Code: https://github.com/shallowdream204/DiCo.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoMM: On Geodesic Perspective for Multi-modal Learning</title>
<link>https://arxiv.org/abs/2505.11216</link>
<guid>https://arxiv.org/abs/2505.11216</guid>
<content:encoded><![CDATA[
arXiv:2505.11216v1 Announce Type: new 
Abstract: Geodesic distance serves as a reliable means of measuring distance in nonlinear spaces, and such nonlinear manifolds are prevalent in the current multimodal learning. In these scenarios, some samples may exhibit high similarity, yet they convey different semantics, making traditional distance metrics inadequate for distinguishing between positive and negative samples. This paper introduces geodesic distance as a novel distance metric in multi-modal learning for the first time, to mine correlations between samples, aiming to address the limitations of common distance metric. Our approach incorporates a comprehensive series of strategies to adapt geodesic distance for the current multimodal learning. Specifically, we construct a graph structure to represent the adjacency relationships among samples by thresholding distances between them and then apply the shortest-path algorithm to obtain geodesic distance within this graph. To facilitate efficient computation, we further propose a hierarchical graph structure through clustering and combined with incremental update strategies for dynamic status updates. Extensive experiments across various downstream tasks validate the effectiveness of our proposed method, demonstrating its capability to capture complex relationships between samples and improve the performance of multimodal learning models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition</title>
<link>https://arxiv.org/abs/2505.11232</link>
<guid>https://arxiv.org/abs/2505.11232</guid>
<content:encoded><![CDATA[
arXiv:2505.11232v1 Announce Type: new 
Abstract: Event cameras, which capture brightness changes with high temporal resolution, inherently generate a significant amount of redundant and noisy data beyond essential object structures. The primary challenge in event-based object recognition lies in effectively removing this noise without losing critical spatial-temporal information. To address this, we propose an Adaptive Graph-based Noisy Data Removal framework for Event-based Object Recognition. Specifically, our approach integrates adaptive event segmentation based on normalized density analysis, a multifactorial edge-weighting mechanism, and adaptive graph-based denoising strategies. These innovations significantly enhance the integration of spatiotemporal information, effectively filtering noise while preserving critical structural features for robust recognition. Experimental evaluations on four challenging datasets demonstrate that our method achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and 96.89%, surpassing existing graph-based methods by up to 8.79%, and improving noise reduction performance by up to 19.57%, with an additional accuracy gain of 6.26% compared to traditional Euclidean-based techniques.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models</title>
<link>https://arxiv.org/abs/2505.11245</link>
<guid>https://arxiv.org/abs/2505.11245</guid>
<content:encoded><![CDATA[
arXiv:2505.11245v1 Announce Type: new 
Abstract: Diffusion models have made substantial advances in image generation, yet models trained on large, unfiltered datasets often yield outputs misaligned with human preferences. Numerous methods have been proposed to fine-tune pre-trained diffusion models, achieving notable improvements in aligning generated outputs with human preferences. However, we argue that existing preference alignment methods neglect the critical role of handling unconditional/negative-conditional outputs, leading to a diminished capacity to avoid generating undesirable outcomes. This oversight limits the efficacy of classifier-free guidance~(CFG), which relies on the contrast between conditional generation and unconditional/negative-conditional generation to optimize output quality. In response, we propose a straightforward but versatile effective approach that involves training a model specifically attuned to negative preferences. This method does not require new training strategies or datasets but rather involves minor modifications to existing techniques. Our approach integrates seamlessly with models such as SD1.5, SDXL, video diffusion models and models that have undergone preference optimization, consistently enhancing their alignment with human preferences.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Driven Genetic Optimization for Deep-Feature-Guided Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2505.11246</link>
<guid>https://arxiv.org/abs/2505.11246</guid>
<content:encoded><![CDATA[
arXiv:2505.11246v1 Announce Type: new 
Abstract: Image enhancement methods often prioritize pixel level information, overlooking the semantic features. We propose a novel, unsupervised, fuzzy-inspired image enhancement framework guided by NSGA-II algorithm that optimizes image brightness, contrast, and gamma parameters to achieve a balance between visual quality and semantic fidelity. Central to our proposed method is the use of a pre trained deep neural network as a feature extractor. To find the best enhancement settings, we use a GPU-accelerated NSGA-II algorithm that balances multiple objectives, namely, increasing image entropy, improving perceptual similarity, and maintaining appropriate brightness. We further improve the results by applying a local search phase to fine-tune the top candidates from the genetic algorithm. Our approach operates entirely without paired training data making it broadly applicable across domains with limited or noisy labels. Quantitatively, our model achieves excellent performance with average BRISQUE and NIQE scores of 19.82 and 3.652, respectively, in all unpaired datasets. Qualitatively, enhanced images by our model exhibit significantly improved visibility in shadowed regions, natural balance of contrast and also preserve the richer fine detail without introducing noticable artifacts. This work opens new directions for unsupervised image enhancement where semantic consistency is critical.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models</title>
<link>https://arxiv.org/abs/2505.11257</link>
<guid>https://arxiv.org/abs/2505.11257</guid>
<content:encoded><![CDATA[
arXiv:2505.11257v1 Announce Type: new 
Abstract: The remarkable ease of use of diffusion models for image generation has led to a proliferation of synthetic content online. While these models are often employed for legitimate purposes, they are also used to generate fake images that support misinformation and hate speech. Consequently, it is crucial to develop robust tools capable of detecting whether an image has been generated by such models. Many current detection methods, however, require large volumes of sample images for training. Unfortunately, due to the rapid evolution of the field, existing datasets often cover only a limited range of models and quickly become outdated. In this work, we introduce DRAGON, a comprehensive dataset comprising images from 25 diffusion models, spanning both recent advancements and older, well-established architectures. The dataset contains a broad variety of images representing diverse subjects. To enhance image realism, we propose a simple yet effective pipeline that leverages a large language model to expand input prompts, thereby generating more diverse and higher-quality outputs, as evidenced by improvements in standard quality metrics. The dataset is provided in multiple sizes (ranging from extra-small to extra-large) to accomodate different research scenarios. DRAGON is designed to support the forensic community in developing and evaluating detection and attribution techniques for synthetic content. Additionally, the dataset is accompanied by a dedicated test set, intended to serve as a benchmark for assessing the performance of newly developed methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-view dense image matching with similarity learning and geometry priors</title>
<link>https://arxiv.org/abs/2505.11264</link>
<guid>https://arxiv.org/abs/2505.11264</guid>
<content:encoded><![CDATA[
arXiv:2505.11264v1 Announce Type: new 
Abstract: We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks designed for multi-view similarity learning, leveraging epipolar geometry for training. Our approach incorporates an online geometry prior to characterize pixel relationships, either along the epipolar line or through homography rectification. This enables the generation of geometry-aware features from native images, which are then projected across candidate depth hypotheses using plane sweeping. Our method geometric preconditioning effectively adapts epipolar-based features for enhanced multi-view reconstruction, without requiring the laborious multi-view training dataset creation. By aggregating learned similarities, we construct and regularize the cost volume, leading to improved multi-view surface reconstruction over traditional dense matching approaches. MV-DeepSimNets demonstrates superior performance against leading similarity learning networks and end-to-end regression models, especially in terms of generalization capabilities across both aerial and satellite imagery with varied ground sampling distances. Our pipeline is integrated into MicMac software and can be readily adopted in standard multi-resolution image matching pipelines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity</title>
<link>https://arxiv.org/abs/2505.11267</link>
<guid>https://arxiv.org/abs/2505.11267</guid>
<content:encoded><![CDATA[
arXiv:2505.11267v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) representation is fundamentally challenged by pervasive non-uniformity, where spectral dependencies, spatial continuity, and feature efficiency exhibit complex and often conflicting behaviors. Most existing models rely on a unified processing paradigm that assumes homogeneity across dimensions, leading to suboptimal performance and biased representations. To address this, we propose FairHyp, a fairness-directed framework that explicitly disentangles and resolves the threefold non-uniformity through cooperative yet specialized modules. We introduce a Runge-Kutta-inspired spatial variability adapter to restore spatial coherence under resolution discrepancies, a multi-receptive field convolution module with sparse-aware refinement to enhance discriminative features while respecting inherent sparsity, and a spectral-context state space model that captures stable and long-range spectral dependencies via bidirectional Mamba scanning and statistical aggregation. Unlike one-size-fits-all solutions, FairHyp achieves dimension-specific adaptation while preserving global consistency and mutual reinforcement. This design is grounded in the view that non-uniformity arises from the intrinsic structure of HSI representations, rather than any particular task setting. To validate this, we apply FairHyp across four representative tasks including classification, denoising, super-resolution, and inpaintin, demonstrating its effectiveness in modeling a shared structural flaw. Extensive experiments show that FairHyp consistently outperforms state-of-the-art methods under varied imaging conditions. Our findings redefine fairness as a structural necessity in HSI modeling and offer a new paradigm for balancing adaptability, efficiency, and fidelity in high-dimensional vision tasks.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection</title>
<link>https://arxiv.org/abs/2505.11282</link>
<guid>https://arxiv.org/abs/2505.11282</guid>
<content:encoded><![CDATA[
arXiv:2505.11282v1 Announce Type: new 
Abstract: Mobile robots are reaching unprecedented speeds, with platforms like Unitree B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s. However, effectively utilizing such speeds remains a challenge due to the limitations of RGB cameras, which suffer from motion blur and fail to provide real-time responsiveness. Event cameras, with their asynchronous operation, and low-latency sensing, offer a promising alternative for high-speed robotic perception. In this work, we introduce MTevent, a dataset designed for 6D pose estimation and moving object detection in highly dynamic environments with large detection distances. Our setup consists of a stereo-event camera and an RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16 unique objects under challenging conditions such as extreme viewing angles, varying lighting, and occlusions. MTevent is the first dataset to combine high-speed motion, long-range perception, and real-world object interactions, making it a valuable resource for advancing event-based vision in robotics. To establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's FoundationPose on RGB images, achieving an Average Recall of 0.22 with ground-truth masks, highlighting the limitations of RGB-based approaches in such dynamic settings. With MTevent, we provide a novel resource to improve perception models and foster further research in high-speed robotic vision. The dataset is available for download https://huggingface.co/datasets/anas-gouda/MTevent
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining</title>
<link>https://arxiv.org/abs/2505.11293</link>
<guid>https://arxiv.org/abs/2505.11293</guid>
<content:encoded><![CDATA[
arXiv:2505.11293v1 Announce Type: new 
Abstract: Contrastive learning (CL) is a prevalent technique for training embedding models, which pulls semantically similar examples (positives) closer in the representation space while pushing dissimilar ones (negatives) further apart. A key source of negatives are 'in-batch' examples, i.e., positives from other examples in the batch. Effectiveness of such models is hence strongly influenced by the size and quality of training batches. In this work, we propose 'Breaking the Batch Barrier' (B3), a novel batch construction strategy designed to curate high-quality batches for CL. Our approach begins by using a pretrained teacher embedding model to rank all examples in the dataset, from which a sparse similarity graph is constructed. A community detection algorithm is then applied to this graph to identify clusters of examples that serve as strong negatives for one another. The clusters are then used to construct batches that are rich in in-batch negatives. Empirical results on the MMEB multimodal embedding benchmark (36 tasks) demonstrate that our method sets a new state of the art, outperforming previous best methods by +1.3 and +2.9 points at the 7B and 2B model scales, respectively. Notably, models trained with B3 surpass existing state-of-the-art results even with a batch size as small as 64, which is 4-16x smaller than that required by other methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks</title>
<link>https://arxiv.org/abs/2505.11314</link>
<guid>https://arxiv.org/abs/2505.11314</guid>
<content:encoded><![CDATA[
arXiv:2505.11314v1 Announce Type: new 
Abstract: The assessment of evaluation metrics (meta-evaluation) is crucial for determining the suitability of existing metrics in text-to-image (T2I) generation tasks. Human-based meta-evaluation is costly and time-intensive, and automated alternatives are scarce. We address this gap and propose CROC: a scalable framework for automated Contrastive Robustness Checks that systematically probes and quantifies metric robustness by synthesizing contrastive test cases across a comprehensive taxonomy of image properties. With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one million contrastive prompt-image pairs to enable a fine-grained comparison of evaluation metrics. We also use the dataset to train CROCScore, a new metric that achieves state-of-the-art performance among open-source methods, demonstrating an additional key application of our framework. To complement this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$) targeting especially challenging categories. Our results highlight robustness issues in existing metrics: for example, many fail on prompts involving negation, and all tested open-source metrics fail on at least 25% of cases involving correct identification of body parts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.11326</link>
<guid>https://arxiv.org/abs/2505.11326</guid>
<content:encoded><![CDATA[
arXiv:2505.11326v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have shown remarkable progress in offline tasks such as image captioning and video question answering. However, real-time interactive environments impose new demands on VLMs, requiring them to generate utterances that are not only semantically accurate but also precisely timed. We identify two core capabilities necessary for such settings -- $\textit{perceptual updating}$ and $\textit{contingency awareness}$ -- and propose a new benchmark task, $\textbf{Temporally-Grounded Language Generation (TGLG)}$, to evaluate them. TGLG requires models to generate utterances in response to streaming video such that both content and timing align with dynamic visual input. To support this benchmark, we curate evaluation datasets from sports broadcasting and egocentric human interaction domains, and introduce a new metric, $\textbf{TRACE}$, to evaluate TGLG by jointly measuring semantic similarity and temporal alignment. Finally, we present $\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$, a model that interleaves visual and linguistic tokens in a time-synchronized manner, enabling real-time language generation without relying on turn-based assumptions. Experimental results show that VLM-TSI significantly outperforms a strong baseline, yet overall performance remains modest -- highlighting the difficulty of TGLG and motivating further research in real-time VLMs. Code and data available $\href{https://github.com/yukw777/tglg}{here}$.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARRS: Masked Autoregressive Unit-based Reaction Synthesis</title>
<link>https://arxiv.org/abs/2505.11334</link>
<guid>https://arxiv.org/abs/2505.11334</guid>
<content:encoded><![CDATA[
arXiv:2505.11334v1 Announce Type: new 
Abstract: This work aims at a challenging task: human action-reaction synthesis, i.e., generating human reactions based on the action sequence of the other as conditions. Currently, autoregressive modeling approaches have achieved remarkable performance in motion generation tasks, e.g. text-to-motion. However, vector quantization (VQ) accompanying autoregressive generation has inherent disadvantages, including loss of quantization information, low codebook utilization, etc. Moreover, unlike text-to-motion, which focuses solely on the movement of body joints, human action-reaction synthesis also encompasses fine-grained hand movements. In this work, we propose MARRS, a novel framework designed to generate coordinated and fine-grained reaction motions in continuous representations. Initially, we present the Unit-distinguished Motion Variational AutoEncoder (UD-VAE), which segments the entire body into distinct body and hand units, encoding them independently. Subsequently, we propose Action-Conditioned Fusion (ACF), which involves randomly masking a subset of reactive tokens and extracting specific information about the body and hands from the active tokens. Furthermore, we introduce Adaptive Unit Modulation (AUM) to facilitate interaction between body and hand units by using the information from one unit to adaptively modulate the other. Finally, for the diffusion model, we employ a compact MLP as a noise predictor for each distinct body unit and incorporate the diffusion loss to model the probability distribution of each token. Quantitative and qualitative results demonstrate that our method achieves superior performance. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Base model Shift for Delta Compression</title>
<link>https://arxiv.org/abs/2505.11344</link>
<guid>https://arxiv.org/abs/2505.11344</guid>
<content:encoded><![CDATA[
arXiv:2505.11344v1 Announce Type: new 
Abstract: Transformer-based models with the pretrain-finetune paradigm bring about significant progress, along with the heavy storage and deployment costs of finetuned models on multiple tasks. Delta compression attempts to lower the costs by reducing the redundancy of delta parameters (i.e., the difference between the finetuned and pre-trained model weights) through pruning or quantization. However, existing methods by default employ the pretrained model as the base model and compress the delta parameters for every task, which may causes significant performance degradation, especially when the compression rate is extremely high. To tackle this issue, we investigate the impact of different base models on the performance of delta compression and find that the pre-trained base model can hardly be optimal. To this end, we propose Dynamic Base Model Shift (DBMS), which dynamically adapts the base model to the target task before performing delta compression. Specifically, we adjust two parameters, which respectively determine the magnitude of the base model shift and the overall scale of delta compression, to boost the compression performance on each task. Through low-cost learning of these two parameters, our DBMS can maintain most of the finetuned model's performance even under an extremely high compression ratio setting, significantly surpassing existing methods. Moreover, our DBMS is orthogonal and can be integrated with a variety of other methods, and it has been evaluated across different types of models including language, vision transformer, and multi-modal models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2505.11383</link>
<guid>https://arxiv.org/abs/2505.11383</guid>
<content:encoded><![CDATA[
arXiv:2505.11383v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory</title>
<link>https://arxiv.org/abs/2505.11386</link>
<guid>https://arxiv.org/abs/2505.11386</guid>
<content:encoded><![CDATA[
arXiv:2505.11386v1 Announce Type: new 
Abstract: This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field (NeRF) performance under limited samples using Mutual Information Theory. While NeRF excels in 3D scene synthesis, challenges arise with limited data and existing methods that aim to introduce prior knowledge lack theoretical support in a unified framework. We introduce a simple but theoretically robust concept, Mutual Information, as a metric to uniformly measure the correlation between images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints containing more non-overlapping scene information by minimizing mutual information without knowing ground truth images beforehand. Our framework employs a greedy algorithm, offering a near-optimal solution.
  For few-shot view synthesis, we maximize the mutual information between inferred images and ground truth, expecting inferred images to gain more relevant information from known images. This is achieved by incorporating efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over state-of-the-art baselines in different settings, affirming the efficacy of our framework.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner</title>
<link>https://arxiv.org/abs/2505.11404</link>
<guid>https://arxiv.org/abs/2505.11404</guid>
<content:encoded><![CDATA[
arXiv:2505.11404v1 Announce Type: new 
Abstract: Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.11405</link>
<guid>https://arxiv.org/abs/2505.11405</guid>
<content:encoded><![CDATA[
arXiv:2505.11405v1 Announce Type: new 
Abstract: Emotion understanding is a critical yet challenging task. Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities in this area. However, MLLMs often suffer from hallucinations, generating irrelevant or nonsensical content. To the best of our knowledge, despite the importance of this issue, there has been no dedicated effort to evaluate emotion-related hallucinations in MLLMs. In this work, we introduce EmotionHallucer, the first benchmark for detecting and analyzing emotion hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from the interplay of biology and social learning, MLLMs rely solely on data-driven learning and lack innate emotional instincts. Fortunately, emotion psychology provides a solid foundation of knowledge about human emotions. Building on this, we assess emotion hallucinations from two dimensions: emotion psychology knowledge and real-world multimodal perception. To support robust evaluation, we utilize an adversarial binary question-answer (QA) framework, which employs carefully crafted basic and hallucinated pairs to assess the emotion hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on EmotionHallucer, we reveal that: i) most current models exhibit substantial issues with emotion hallucinations; ii) closed-source models outperform open-source ones in detecting emotion hallucinations, and reasoning capability provides additional advantages; iii) existing models perform better in emotion psychology knowledge than in multimodal emotion perception. As a byproduct, these findings inspire us to propose the PEP-MEK framework, which yields an average improvement of 9.90% in emotion hallucination detection across selected models. Resources will be available at https://github.com/xxtars/EmotionHallucer.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study</title>
<link>https://arxiv.org/abs/2505.11424</link>
<guid>https://arxiv.org/abs/2505.11424</guid>
<content:encoded><![CDATA[
arXiv:2505.11424v1 Announce Type: new 
Abstract: This study evaluated the performance of a YOLOv8-based segmentation model for detecting and segmenting wrinkles in facial images.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Face Consistency Benchmark for GenAI Video</title>
<link>https://arxiv.org/abs/2505.11425</link>
<guid>https://arxiv.org/abs/2505.11425</guid>
<content:encoded><![CDATA[
arXiv:2505.11425v1 Announce Type: new 
Abstract: Video generation driven by artificial intelligence has advanced significantly, enabling the creation of dynamic and realistic content. However, maintaining character consistency across video sequences remains a major challenge, with current models struggling to ensure coherence in appearance and attributes. This paper introduces the Face Consistency Benchmark (FCB), a framework for evaluating and comparing the consistency of characters in AI-generated videos. By providing standardized metrics, the benchmark highlights gaps in existing solutions and promotes the development of more reliable approaches. This work represents a crucial step toward improving character consistency in AI video generation technologies.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision</title>
<link>https://arxiv.org/abs/2505.11439</link>
<guid>https://arxiv.org/abs/2505.11439</guid>
<content:encoded><![CDATA[
arXiv:2505.11439v1 Announce Type: new 
Abstract: Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery (RMIS) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, supervised learning methods require extensive training on annotated datasets, limiting their adaptability to new tools. Despite their success in other domains, zero-shot pose estimation models remain unexplored in RMIS for pose estimation of surgical instruments, creating a gap in generalising to unseen surgical tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D models like the FoundationPose and SAM-6D. We advanced these models by incorporating vision-based depth estimation using the RAFT-Stereo method, for robust depth estimation in reflective and textureless environments. Additionally, we enhanced SAM-6D by replacing its instance segmentation module, Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly boosting segmentation accuracy in occluded and complex conditions. Extensive validation reveals that our enhanced SAM-6D surpasses FoundationPose in zero-shot pose estimation of unseen surgical instruments, setting a new benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the generalisability of pose estimation for unseen objects and pioneers the application of RGB-D zero-shot methods in RMIS.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation</title>
<link>https://arxiv.org/abs/2505.11454</link>
<guid>https://arxiv.org/abs/2505.11454</guid>
<content:encoded><![CDATA[
arXiv:2505.11454v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment</title>
<link>https://arxiv.org/abs/2505.11468</link>
<guid>https://arxiv.org/abs/2505.11468</guid>
<content:encoded><![CDATA[
arXiv:2505.11468v1 Announce Type: new 
Abstract: Diffusion models have made remarkable advancements in generating high-quality images from textual descriptions. Recent works like LayerDiffuse have extended the previous single-layer, unified image generation paradigm to transparent image layer generation. However, existing multi-layer generation methods fail to handle the interactions among multiple layers such as rational global layout, physics-plausible contacts and visual effects like shadows and reflections while maintaining high alpha quality. To solve this problem, we propose PSDiffusion, a unified diffusion framework for simultaneous multi-layer text-to-image generation. Our model can automatically generate multi-layer images with one RGB background and multiple RGBA foregrounds through a single feed-forward process. Unlike existing methods that combine multiple tools for post-decomposition or generate layers sequentially and separately, our method introduces a global-layer interactive mechanism that generates layered-images concurrently and collaboratively, ensuring not only high quality and completeness for each layer, but also spatial and visual interactions among layers for global coherence.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models</title>
<link>https://arxiv.org/abs/2505.11482</link>
<guid>https://arxiv.org/abs/2505.11482</guid>
<content:encoded><![CDATA[
arXiv:2505.11482v1 Announce Type: new 
Abstract: Diffusion models are widely used as priors in imaging inverse problems. However, their performance often degrades under distribution shifts between the training and test-time images. Existing methods for identifying and quantifying distribution shifts typically require access to clean test images, which are almost never available while solving inverse problems (at test time). We propose a fully unsupervised metric for estimating distribution shifts using only indirect (corrupted) measurements and score functions from diffusion models trained on different datasets. We theoretically show that this metric estimates the KL divergence between the training and test image distributions. Empirically, we show that our score-based metric, using only corrupted measurements, closely approximates the KL divergence computed from clean images. Motivated by this result, we show that aligning the out-of-distribution score with the in-distribution score -- using only corrupted measurements -- reduces the KL divergence and leads to improved reconstruction quality across multiple inverse problems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing</title>
<link>https://arxiv.org/abs/2505.11493</link>
<guid>https://arxiv.org/abs/2505.11493</guid>
<content:encoded><![CDATA[
arXiv:2505.11493v1 Announce Type: new 
Abstract: Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QVGen: Pushing the Limit of Quantized Video Generative Models</title>
<link>https://arxiv.org/abs/2505.11497</link>
<guid>https://arxiv.org/abs/2505.11497</guid>
<content:encoded><![CDATA[
arXiv:2505.11497v1 Announce Type: new 
Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\Phi$, we propose a rank-decay strategy that progressively eliminates $\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\mathbf{\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution</title>
<link>https://arxiv.org/abs/2505.10577</link>
<guid>https://arxiv.org/abs/2505.10577</guid>
<content:encoded><![CDATA[
arXiv:2505.10577v1 Announce Type: cross 
Abstract: Modern video super-resolution (VSR) systems based on convolutional neural networks (CNNs) require huge computational costs. The problem of feature redundancy is present in most models in many domains, but is rarely discussed in VSR. We experimentally observe that many features in VSR models are also similar to each other, so we propose to use "Ghost features" to reduce this redundancy. We also analyze the so-called "gradient disappearance" phenomenon generated by the conventional recurrent convolutional network (RNN) model, and combine the Ghost module with RNN to complete the modeling on time series. The current frame is used as input to the model together with the next frame, the output of the previous frame and the hidden state. Extensive experiments on several benchmark models and datasets show that the PSNR and SSIM of our proposed modality are improved to some extent. Some texture details in the video are also better preserved.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExploreGS: a vision-based low overhead framework for 3D scene reconstruction</title>
<link>https://arxiv.org/abs/2505.10578</link>
<guid>https://arxiv.org/abs/2505.10578</guid>
<content:encoded><![CDATA[
arXiv:2505.10578v1 Announce Type: cross 
Abstract: This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework integrates scene exploration and model reconstruction, and leverags a Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore, the 3D Gaussian Splatting (3DGS) training can be executed on-board. Comprehensive experiments in both simulation and real-world environments demonstrate the efficiency and applicability of the ExploreGS framework on resource-constrained devices, while maintaining reconstruction quality comparable to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation</title>
<link>https://arxiv.org/abs/2505.10672</link>
<guid>https://arxiv.org/abs/2505.10672</guid>
<content:encoded><![CDATA[
arXiv:2505.10672v1 Announce Type: cross 
Abstract: Efficient and accurate multi-organ segmentation from abdominal CT volumes is a fundamental challenge in medical image analysis. Existing 3D segmentation approaches are computationally and memory intensive, often processing entire volumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods suffer from class imbalance and lack cross-view contextual awareness. To address these limitations, we propose a novel, anatomically-aware slice selector pipeline that reduces input volume prior to segmentation. Our unified framework introduces a vision-language model (VLM) for cross-view organ presence detection using fused tri-slice (2.5D) representations from axial, sagittal, and coronal planes. Our proposed model acts as an "expert" in anatomical localization, reasoning over multi-view representations to selectively retain slices with high structural relevance. This enables spatially consistent filtering across orientations while preserving contextual cues. More importantly, since standard segmentation metrics such as Dice or IoU fail to measure the spatial precision of such slice selection, we introduce a novel metric, Slice Localization Concordance (SLC), which jointly captures anatomical coverage and spatial alignment with organ-centric reference slices. Unlike segmentation-specific metrics, SLC provides a model-agnostic evaluation of localization fidelity. Our model offers substantial improvement gains against several baselines across all organs, demonstrating both accurate and reliable organ-focused slice filtering. These results show that our method enables efficient and spatially consistent organ filtering, thereby significantly reducing downstream segmentation cost while maintaining high anatomical fidelity.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation</title>
<link>https://arxiv.org/abs/2505.10687</link>
<guid>https://arxiv.org/abs/2505.10687</guid>
<content:encoded><![CDATA[
arXiv:2505.10687v1 Announce Type: cross 
Abstract: The hippocampus, a critical brain structure involved in memory processing and various neurodegenerative and psychiatric disorders, comprises three key subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3 (CA3). Accurate segmentation of these subregions from histological tissue images is essential for advancing our understanding of disease mechanisms, developmental dynamics, and therapeutic interventions. However, no existing methods address the automated segmentation of hippocampal subregions from tissue images, particularly from immunohistochemistry (IHC) images. To bridge this gap, we introduce a novel set of four comprehensive murine hippocampal IHC datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing structural, neuronal activity, and plasticity associated information. Additionally, we propose ROIsGAN, a region-guided U-Net-based generative adversarial network tailored for hippocampal subregion segmentation. By leveraging adversarial learning, ROIsGAN enhances boundary delineation and structural detail refinement through a novel region-guided discriminator loss combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3 subregions, ROIsGAN consistently outperforms conventional segmentation models, achieving performance gains ranging from 1-10% in Dice score and up to 11% in Intersection over Union (IoU), particularly under challenging staining conditions. Our work establishes foundational datasets and methods for automated hippocampal segmentation, enabling scalable, high-precision analysis of tissue images in neuroscience research. Our generated datasets, proposed model as a standalone tool, and its corresponding source code are publicly available at: https://github.com/MehediAzim/ROIsGAN
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A probabilistic framework for dynamic quantization</title>
<link>https://arxiv.org/abs/2505.10689</link>
<guid>https://arxiv.org/abs/2505.10689</guid>
<content:encoded><![CDATA[
arXiv:2505.10689v1 Announce Type: cross 
Abstract: We propose a probabilistic framework for dynamic quantization of neural networks that allows for a computationally efficient input-adaptive rescaling of the quantization parameters. Our framework applies a probabilistic model to the network's pre-activations through a lightweight surrogate, enabling the adaptive adjustment of the quantization parameters on a per-input basis without significant memory overhead. We validate our approach on a set of popular computer vision tasks and models, observing only a negligible loss in performance. Our method strikes the best performance and computational overhead tradeoff compared to standard quantization strategies.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients</title>
<link>https://arxiv.org/abs/2505.10691</link>
<guid>https://arxiv.org/abs/2505.10691</guid>
<content:encoded><![CDATA[
arXiv:2505.10691v1 Announce Type: cross 
Abstract: While the acute phase of the COVID-19 pandemic has subsided, its long-term effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known as Long COVID. There remains substantial uncertainty regarding both its duration and optimal management strategies. PASC manifests as a diverse array of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and musculoskeletal abnormalities--that extend beyond the acute infection phase. This heterogeneous presentation poses substantial challenges for clinical assessment, diagnosis, and treatment planning. In this paper, we focus on imaging findings that may suggest fibrotic damage in the lungs, a critical manifestation characterized by scarring of lung tissue, which can potentially affect long-term respiratory function in patients with PASC. This study introduces a novel multi-center chest CT analysis framework that combines deep learning and radiomics for fibrosis prediction. Our approach leverages convolutional neural networks (CNNs) and interpretable feature extraction, achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate the effectiveness of Grad-CAM visualization and radiomics-based feature analysis in providing clinically relevant insights for PASC-related lung fibrosis prediction. Our findings highlight the potential of deep learning-driven computational methods for early detection and risk assessment of PASC-related lung fibrosis--presented for the first time in the literature.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</title>
<link>https://arxiv.org/abs/2505.10696</link>
<guid>https://arxiv.org/abs/2505.10696</guid>
<content:encoded><![CDATA[
arXiv:2505.10696v1 Announce Type: cross 
Abstract: We present TartanGround, a large-scale, multi-modal dataset to advance the perception and autonomy of ground robots operating in diverse environments. This dataset, collected in various photorealistic simulation environments includes multiple RGB stereo cameras for 360-degree coverage, along with depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, and occupancy maps with semantic labels. Data is collected using an integrated automatic pipeline, which generates trajectories mimicking the motion patterns of various ground robot platforms, including wheeled and legged robots. We collect 910 trajectories across 70 environments, resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM tasks reveal that state-of-the-art methods trained on existing datasets struggle to generalize across diverse scenes. TartanGround can serve as a testbed for training and evaluation of a broad range of learning-based tasks, including occupancy prediction, SLAM, neural scene representation, perception-based navigation, and more, enabling advancements in robotic perception and autonomy towards achieving robust models generalizable to more diverse scenarios. The dataset and codebase for data collection will be made publicly available upon acceptance. Webpage: https://tartanair.org/tartanground
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling</title>
<link>https://arxiv.org/abs/2505.10729</link>
<guid>https://arxiv.org/abs/2505.10729</guid>
<content:encoded><![CDATA[
arXiv:2505.10729v1 Announce Type: cross 
Abstract: Spatial transcriptomics (ST) is a promising technique that characterizes the spatial gene profiling patterns within the tissue context. Comprehensive ST analysis depends on consecutive slices for 3D spatial insights, whereas the missing intermediate tissue sections and high costs limit the practical feasibility of generating multi-slice ST. In this paper, we propose C2-STi, the first attempt for interpolating missing ST slices at arbitrary intermediate positions between adjacent ST slices. Despite intuitive, effective ST interpolation presents significant challenges, including 1) limited continuity across heterogeneous tissue sections, 2) complex intrinsic correlation across genes, and 3) intricate cellular structures and biological semantics within each tissue section. To mitigate these challenges, in C2-STi, we design 1) a distance-aware local structural modulation module to adaptively capture cross-slice deformations and enhance positional correlations between ST slices, 2) a pyramid gene co-expression correlation module to capture multi-scale biological associations among genes, and 3) a cross-modal alignment module that integrates the ST-paired hematoxylin and eosin (H&amp;E)-stained images to filter and align the essential cellular features across ST and H\&amp;E images. Extensive experiments on the public dataset demonstrate our superiority over state-of-the-art approaches on both single-slice and multi-slice ST interpolation. Codes are available at https://github.com/XiaofeiWang2018/C2-STi.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Textured mesh Quality Assessment using Geometry and Color Field Similarity</title>
<link>https://arxiv.org/abs/2505.10824</link>
<guid>https://arxiv.org/abs/2505.10824</guid>
<content:encoded><![CDATA[
arXiv:2505.10824v1 Announce Type: cross 
Abstract: Textured mesh quality assessment (TMQA) is critical for various 3D mesh applications. However, existing TMQA methods often struggle to provide accurate and robust evaluations. Motivated by the effectiveness of fields in representing both 3D geometry and color information, we propose a novel point-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes signed distance fields and a newly proposed color field named nearest surface point color field to realize effective mesh feature description. Four features related to visual perception are extracted from the geometry and color fields: geometry similarity, geometry gradient similarity, space color distribution similarity, and space color gradient similarity. Experimental results on three benchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA) TMQA metrics. Furthermore, FMQM exhibits low computational complexity, making it a practical and efficient solution for real-world applications in 3D graphics and visualization. Our code is publicly available at: https://github.com/yyyykf/FMQM.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs</title>
<link>https://arxiv.org/abs/2505.10836</link>
<guid>https://arxiv.org/abs/2505.10836</guid>
<content:encoded><![CDATA[
arXiv:2505.10836v1 Announce Type: cross 
Abstract: In this paper, we study the challenges of detecting events on social media, where traditional unimodal systems struggle due to the rapid and multimodal nature of data dissemination. We employ a range of models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o, and LLaVA. Additionally, we also study the effect of providing multimodal generative models (such as GPT-4o) with a single modality to assess their efficacy. Our results indicate that while multimodal approaches notably outperform unimodal counterparts, generative approaches despite having a large number of parameters, lag behind supervised methods in precision. Furthermore, we also found that they lag behind instruction-tuned models because of their inability to generate event classes correctly. During our error analysis, we discovered that common social media issues such as leet speak, text elongation, etc. are effectively handled by generative approaches but are hard to tackle using supervised approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers</title>
<link>https://arxiv.org/abs/2505.10855</link>
<guid>https://arxiv.org/abs/2505.10855</guid>
<content:encoded><![CDATA[
arXiv:2505.10855v1 Announce Type: cross 
Abstract: AI automated segmentations for radiation treatment planning (RTP) can deteriorate when applied in clinical cases with different characteristics than training dataset. Hence, we refined a pretrained transformer into a hybrid transformer convolutional network (HTN) to segment cardiac substructures lung and breast cancer patients acquired with varying imaging contrasts and patient scan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124 non-contrast CT (NCCT) scans from patients with non-small cell lung cancers acquired in supine position, was used to create oracle with all 180 training cases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models were evaluated on a held-out validation set of 60 cohort I patients and 66 patients with breast cancer from cohort II acquired in supine (n=45) and prone (n=21) positions. Accuracy was measured using DSC, HD95, and dose metrics. Publicly available TotalSegmentator served as the benchmark. The oracle and balanced models were similarly accurate (DSC Cohort I: 0.80 \pm 0.10 versus 0.81 \pm 0.10; Cohort II: 0.77 \pm 0.13 versus 0.80 \pm 0.12), outperforming TotalSegmentator. The balanced model, using half the training cases as oracle, produced similar dose metrics as manual delineations for all cardiac substructures. This model was robust to CT contrast in 6 out of 8 substructures and patient scan position variations in 5 out of 8 substructures and showed low correlations of accuracy to patient size and age. A HTN demonstrated robustly accurate (geometric and dose metrics) cardiac substructures segmentation from CTs with varying imaging and patient characteristics, one key requirement for clinical use. Moreover, the model combining pretraining with balanced distribution of NCCT and CECT scans was able to provide reliably accurate segmentations under varied conditions with far fewer labeled datasets compared to an oracle model.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hashing for Structure-based Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.10873</link>
<guid>https://arxiv.org/abs/2505.10873</guid>
<content:encoded><![CDATA[
arXiv:2505.10873v1 Announce Type: cross 
Abstract: We focus on the problem of identifying samples in a set that do not conform to structured patterns represented by low-dimensional manifolds. An effective way to solve this problem is to embed data in a high dimensional space, called Preference Space, where anomalies can be identified as the most isolated points. In this work, we employ Locality Sensitive Hashing to avoid explicit computation of distances in high dimensions and thus improve Anomaly Detection efficiency. Specifically, we present an isolation-based anomaly detection technique designed to work in the Preference Space which achieves state-of-the-art performance at a lower computational cost. Code is publicly available at https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection</title>
<link>https://arxiv.org/abs/2505.10874</link>
<guid>https://arxiv.org/abs/2505.10874</guid>
<content:encoded><![CDATA[
arXiv:2505.10874v1 Announce Type: cross 
Abstract: We address the problem of recovering multiple structures of different classes in a dataset contaminated by noise and outliers. In particular, we consider geometric structures defined by a mixture of underlying parametric models (e.g. planes and cylinders, homographies and fundamental matrices), and we tackle the robust fitting problem by preference analysis and clustering. We present a new algorithm, termed MultiLink, that simultaneously deals with multiple classes of models. MultiLink combines on-the-fly model fitting and model selection in a novel linkage scheme that determines whether two clusters are to be merged. The resulting method features many practical advantages with respect to methods based on preference analysis, being faster, less sensitive to the inlier threshold, and able to compensate limitations deriving from hypotheses sampling. Experiments on several public datasets demonstrate that Multi-Link favourably compares with state of the art alternatives, both in multi-class and single-class problems. Code is publicly made available for download.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Isolation Forest for Structure-based Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.10876</link>
<guid>https://arxiv.org/abs/2505.10876</guid>
<content:encoded><![CDATA[
arXiv:2505.10876v1 Announce Type: cross 
Abstract: We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most general solution, $ii$) RuzHash-iForest, that avoids explicit computation of distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a locality prior to improve efficiency and effectiveness.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting</title>
<link>https://arxiv.org/abs/2505.10894</link>
<guid>https://arxiv.org/abs/2505.10894</guid>
<content:encoded><![CDATA[
arXiv:2505.10894v1 Announce Type: cross 
Abstract: This paper proposes CTP, a novel deep learning framework that integrates convolutional neural network(CNN), Transformer architectures, and physics-informed neural network(PINN) for ocean front prediction. Ocean fronts, as dynamic interfaces between distinct water masses, play critical roles in marine biogeochemical and physical processes. Existing methods such as LSTM, ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and physical consistency over multi-step forecasts. CTP addresses these challenges by combining localized spatial encoding, long-range temporal attention, and physical constraint enforcement. Experimental results across south China sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP achieves state-of-the-art(SOTA) performance in both single-step and multi-step predictions, significantly outperforming baseline models in accuracy, $F_1$ score, and temporal stability.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats</title>
<link>https://arxiv.org/abs/2505.10923</link>
<guid>https://arxiv.org/abs/2505.10923</guid>
<content:encoded><![CDATA[
arXiv:2505.10923v1 Announce Type: cross 
Abstract: Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?</title>
<link>https://arxiv.org/abs/2505.10924</link>
<guid>https://arxiv.org/abs/2505.10924</guid>
<content:encoded><![CDATA[
arXiv:2505.10924v1 Announce Type: cross 
Abstract: Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2505.10993</link>
<guid>https://arxiv.org/abs/2505.10993</guid>
<content:encoded><![CDATA[
arXiv:2505.10993v1 Announce Type: cross 
Abstract: Generative modeling has emerged as a promising direction in computational pathology, offering capabilities such as data-efficient learning, synthetic data augmentation, and multimodal representation across diverse diagnostic tasks. This review provides a comprehensive synthesis of recent progress in the field, organized into four key domains: image generation, text generation, multimodal image-text generation, and other generative applications, including spatial simulation and molecular inference. By analyzing over 150 representative studies, we trace the evolution of generative architectures from early generative adversarial networks to recent advances in diffusion models and foundation models with generative capabilities. We further examine the datasets and evaluation protocols commonly used in this domain and highlight ongoing limitations, including challenges in generating high-fidelity whole slide images, clinical interpretability, and concerns related to the ethical and legal implications of synthetic data. The review concludes with a discussion of open challenges and prospective research directions, with an emphasis on developing unified, multimodal, and clinically deployable generative systems. This work aims to provide a foundational reference for researchers and practitioners developing and applying generative models in computational pathology.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy</title>
<link>https://arxiv.org/abs/2505.11032</link>
<guid>https://arxiv.org/abs/2505.11032</guid>
<content:encoded><![CDATA[
arXiv:2505.11032v1 Announce Type: cross 
Abstract: Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Performance of Analog Training for Transfer Learning</title>
<link>https://arxiv.org/abs/2505.11067</link>
<guid>https://arxiv.org/abs/2505.11067</guid>
<content:encoded><![CDATA[
arXiv:2505.11067v1 Announce Type: cross 
Abstract: Analog in-memory computing is a next-generation computing paradigm that promises fast, parallel, and energy-efficient deep learning training and transfer learning (TL). However, achieving this promise has remained elusive due to a lack of suitable training algorithms. Analog memory devices exhibit asymmetric and non-linear switching behavior in addition to device-to-device variation, meaning that most, if not all, of the current off-the-shelf training algorithms cannot achieve good training outcomes. Also, recently introduced algorithms have enjoyed limited attention, as they require bi-directionally switching devices of unrealistically high symmetry and precision and are highly sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which leverages the chopped technique to address many of the challenges mentioned above. In this paper, we assess the performance of the c-TTv2 algorithm for analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also investigate the robustness of our algorithm to changes in some device specifications, including weight transfer noise, symmetry point skew, and symmetry point variability
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planar Velocity Estimation for Fast-Moving Mobile Robots Using Event-Based Optical Flow</title>
<link>https://arxiv.org/abs/2505.11116</link>
<guid>https://arxiv.org/abs/2505.11116</guid>
<content:encoded><![CDATA[
arXiv:2505.11116v1 Announce Type: cross 
Abstract: Accurate velocity estimation is critical in mobile robotics, particularly for driver assistance systems and autonomous driving. Wheel odometry fused with Inertial Measurement Unit (IMU) data is a widely used method for velocity estimation; however, it typically requires strong assumptions, such as non-slip steering, or complex vehicle dynamics models that do not hold under varying environmental conditions like slippery surfaces. We introduce an approach to velocity estimation that is decoupled from wheel-to-surface traction assumptions by leveraging planar kinematics in combination with optical flow from event cameras pointed perpendicularly at the ground. The asynchronous micro-second latency and high dynamic range of event cameras make them highly robust to motion blur, a common challenge in vision-based perception techniques for autonomous driving. The proposed method is evaluated through in-field experiments on a 1:10 scale autonomous racing platform and compared to precise motion capture data, demonstrating not only performance on par with the state-of-the-art Event-VIO method but also a 38.3 % improvement in lateral error. Qualitative experiments at highway speeds of up to 32 m/s further confirm the effectiveness of our approach, indicating significant potential for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold</title>
<link>https://arxiv.org/abs/2505.11128</link>
<guid>https://arxiv.org/abs/2505.11128</guid>
<content:encoded><![CDATA[
arXiv:2505.11128v1 Announce Type: cross 
Abstract: Recent advances in diffusion models have demonstrated their remarkable ability to capture complex image distributions, but the geometric properties of the learned data manifold remain poorly understood. We address this gap by introducing a score-based Riemannian metric that leverages the Stein score function from diffusion models to characterize the intrinsic geometry of the data manifold without requiring explicit parameterization. Our approach defines a metric tensor in the ambient space that stretches distances perpendicular to the manifold while preserving them along tangential directions, effectively creating a geometry where geodesics naturally follow the manifold's contours. We develop efficient algorithms for computing these geodesics and demonstrate their utility for both interpolation between data points and extrapolation beyond the observed data distribution. Through experiments on synthetic data with known geometry, Rotated MNIST, and complex natural images via Stable Diffusion, we show that our score-based geodesics capture meaningful transformations that respect the underlying data distribution. Our method consistently outperforms baseline approaches on perceptual metrics (LPIPS) and distribution-level metrics (FID, KID), producing smoother, more realistic image transitions. These results reveal the implicit geometric structure learned by diffusion models and provide a principled way to navigate the manifold of natural images through the lens of Riemannian geometry.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection</title>
<link>https://arxiv.org/abs/2505.11134</link>
<guid>https://arxiv.org/abs/2505.11134</guid>
<content:encoded><![CDATA[
arXiv:2505.11134v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) process information via discrete spikes, enabling them to operate at remarkably low energy levels. However, our experimental observations reveal a striking vulnerability when SNNs are trained using the mainstream method--direct encoding combined with backpropagation through time (BPTT): even a single backward pass on data drawn from a slightly different distribution can lead to catastrophic network collapse. Our theoretical analysis attributes this vulnerability to the repeated inputs inherent in direct encoding and the gradient accumulation characteristic of BPTT, which together produce an exceptional large Hessian spectral radius. To address this challenge, we develop a hyperparameter-free method called Dominant Eigencomponent Projection (DEP). By orthogonally projecting gradients to precisely remove their dominant components, DEP effectively reduces the Hessian spectral radius, thereby preventing SNNs from settling into sharp minima. Extensive experiments demonstrate that DEP not only mitigates the vulnerability of SNNs to heterogeneous data poisoning, but also significantly enhances overall robustness compared to key baselines, providing strong support for safer and more reliable SNN deployment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Source Multi-Viewpoint Surgical Telerobotics</title>
<link>https://arxiv.org/abs/2505.11142</link>
<guid>https://arxiv.org/abs/2505.11142</guid>
<content:encoded><![CDATA[
arXiv:2505.11142v1 Announce Type: cross 
Abstract: As robots for minimally invasive surgery (MIS) gradually become more accessible and modular, we believe there is a great opportunity to rethink and expand the visualization and control paradigms that have characterized surgical teleoperation since its inception. We conjecture that introducing one or more additional adjustable viewpoints in the abdominal cavity would not only unlock novel visualization and collaboration strategies for surgeons but also substantially boost the robustness of machine perception toward shared autonomy. Immediate advantages include controlling a second viewpoint and teleoperating surgical tools from a different perspective, which would allow collaborating surgeons to adjust their views independently and still maneuver their robotic instruments intuitively. Furthermore, we believe that capturing synchronized multi-view 3D measurements of the patient's anatomy would unlock advanced scene representations. Accurate real-time intraoperative 3D perception will allow algorithmic assistants to directly control one or more robotic instruments and/or robotic cameras. Toward these goals, we are building a synchronized multi-viewpoint, multi-sensor robotic surgery system by integrating high-performance vision components and upgrading the da Vinci Research Kit control logic. This short paper reports a functional summary of our setup and elaborates on its potential impacts in research and future clinical practice. By fully open-sourcing our system, we will enable the research community to reproduce our setup, improve it, and develop powerful algorithms, effectively boosting clinical translation of cutting-edge research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model in Hyperspectral Image Processing and Analysis: A Review</title>
<link>https://arxiv.org/abs/2505.11158</link>
<guid>https://arxiv.org/abs/2505.11158</guid>
<content:encoded><![CDATA[
arXiv:2505.11158v1 Announce Type: cross 
Abstract: Hyperspectral image processing and analysis has important application value in remote sensing, agriculture and environmental monitoring, but its high dimensionality, data redundancy and noise interference etc. bring great challenges to the analysis. Traditional models have limitations in dealing with these complex data, and it is difficult to meet the increasing demand for analysis. In recent years, Diffusion Model, as an emerging generative model, has shown unique advantages in hyperspectral image processing. By simulating the diffusion process of data in time, the Diffusion Model can effectively process high-dimensional data, generate high-quality samples, and perform well in denoising and data enhancement. In this paper, we review the recent research advances in diffusion modeling for hyperspectral image processing and analysis, and discuss its applications in tasks such as high-dimensional data processing, noise removal, classification, and anomaly detection. The performance of diffusion-based models on image processing is compared and the challenges are summarized. It is shown that the diffusion model can significantly improve the accuracy and efficiency of hyperspectral image analysis, providing a new direction for future research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Asynchronicity in Event-based Neural Networks</title>
<link>https://arxiv.org/abs/2505.11165</link>
<guid>https://arxiv.org/abs/2505.11165</guid>
<content:encoded><![CDATA[
arXiv:2505.11165v1 Announce Type: cross 
Abstract: Event cameras deliver visual data with high temporal resolution, low latency, and minimal redundancy, yet their asynchronous, sparse sequential nature challenges standard tensor-based machine learning (ML). While the recent asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by asynchronously encoding events into learned representations for ML pipelines, existing A2S approaches often sacrifice representation expressivity and generalizability compared to dense, synchronous methods. This paper introduces EVA (EVent Asynchronous representation learning), a novel A2S framework to generate highly expressive and generalizable event-by-event representations. Inspired by the analogy between events and language, EVA uniquely adapts advances from language modeling in linear attention and self-supervised learning for its construction. In demonstration, EVA outperforms prior A2S methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the first A2S framework to successfully master demanding detection tasks, achieving a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's transformative potential for advancing real-time event-based vision applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization</title>
<link>https://arxiv.org/abs/2505.11217</link>
<guid>https://arxiv.org/abs/2505.11217</guid>
<content:encoded><![CDATA[
arXiv:2505.11217v1 Announce Type: cross 
Abstract: Imagine hearing a dog bark and turning toward the sound only to see a parked car, while the real, silent dog sits elsewhere. Such sensory conflicts test perception, yet humans reliably resolve them by prioritizing sound over misleading visuals. Despite advances in multimodal AI integrating vision and audio, little is known about how these systems handle cross-modal conflicts or whether they favor one modality. In this study, we systematically examine modality bias and conflict resolution in AI sound localization. We assess leading multimodal models and benchmark them against human performance in psychophysics experiments across six audiovisual conditions, including congruent, conflicting, and absent cues. Humans consistently outperform AI, demonstrating superior resilience to conflicting or missing visuals by relying on auditory information. In contrast, AI models often default to visual input, degrading performance to near chance levels. To address this, we finetune a state-of-the-art model using a stereo audio-image dataset generated via 3D simulations. Even with limited training data, the refined model surpasses existing benchmarks. Notably, it also mirrors human-like horizontal localization bias favoring left-right precision-likely due to the stereo audio structure reflecting human ear placement. These findings underscore how sensory input quality and system architecture shape multimodal representation accuracy.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fourier Space Perspective on Diffusion Models</title>
<link>https://arxiv.org/abs/2505.11278</link>
<guid>https://arxiv.org/abs/2505.11278</guid>
<content:encoded><![CDATA[
arXiv:2505.11278v1 Announce Type: cross 
Abstract: Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. In this work, we study the inductive bias of the forward process of diffusion models in Fourier space. We theoretically analyse and empirically demonstrate that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Our experiments show that this leads to degraded generation quality of high-frequency components. We then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</title>
<link>https://arxiv.org/abs/2505.11394</link>
<guid>https://arxiv.org/abs/2505.11394</guid>
<content:encoded><![CDATA[
arXiv:2505.11394v1 Announce Type: cross 
Abstract: Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced as a reliable technique providing a resolution in the micrometer range while allowing processing of series of complete brain sections. 3D-PLI acquisition is label-free and allows subsequent staining of sections after measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established within the same section. However, inevitable distortions introduced during the staining process make a nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. In a supervised setting, we build on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration methods. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method enables prediction of a Cresyl violet staining from 3D-PLI, matching individual cell instances.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Planning: Let's Think Only with Images</title>
<link>https://arxiv.org/abs/2505.11409</link>
<guid>https://arxiv.org/abs/2505.11409</guid>
<content:encoded><![CDATA[
arXiv:2505.11409v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views</title>
<link>https://arxiv.org/abs/2505.11467</link>
<guid>https://arxiv.org/abs/2505.11467</guid>
<content:encoded><![CDATA[
arXiv:2505.11467v1 Announce Type: cross 
Abstract: Vision based robot manipulation uses cameras to capture one or more images of a scene containing the objects to be manipulated. Taking multiple images can help if any object is occluded from one viewpoint but more visible from another viewpoint. However, the camera has to be moved to a sequence of suitable positions for capturing multiple images, which requires time and may not always be possible, due to reachability constraints. So while additional images can produce more accurate grasp poses due to the extra information available, the time-cost goes up with the number of additional views sampled. Scene representations like Gaussian Splatting are capable of rendering accurate photorealistic virtual images from user-specified novel viewpoints. In this work, we show initial results which indicate that novel view synthesis can provide additional context in generating grasp poses. Our experiments on the Graspnet-1billion dataset show that novel views contributed force-closure grasps in addition to the force-closure grasps obtained from sparsely sampled real views while also improving grasp coverage. In the future we hope this work can be extended to improve grasp extraction from radiance fields constructed with a single input image, using for example diffusion models or generalizable radiance fields.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI</title>
<link>https://arxiv.org/abs/2401.17207</link>
<guid>https://arxiv.org/abs/2401.17207</guid>
<content:encoded><![CDATA[
arXiv:2401.17207v2 Announce Type: replace 
Abstract: A comprehensive understanding of the organizational principles in the human brain requires, among other factors, well-quantifiable descriptors of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a microscopic imaging technique that enables insights into the fine-grained organization of myelinated nerve fibers with high resolution. Descriptors characterizing the fiber architecture observed in 3D-PLI would enable downstream analysis tasks such as multimodal correlation studies, clustering, and mapping. However, best practices for observer-independent characterization of fiber architecture in 3D-PLI are not yet available. To this end, we propose the application of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images using self-supervised representation learning. We introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning. We combine this sampling strategy with specifically designed image augmentations to gain robustness to typical variations in 3D-PLI parameter maps. The approach is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey brain. We show that extracted features are highly sensitive to different configurations of nerve fibers, yet robust to variations between consecutive brain sections arising from histological processing. We demonstrate their practical applicability for retrieving clusters of homogeneous fiber architecture, performing classification with minimal annotations, and query-based retrieval of characteristic components of fiber architecture such as U-fibers.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Deblur Polarized Images</title>
<link>https://arxiv.org/abs/2402.18134</link>
<guid>https://arxiv.org/abs/2402.18134</guid>
<content:encoded><![CDATA[
arXiv:2402.18134v2 Announce Type: replace 
Abstract: A polarization camera can capture four linear polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of linear polarization (DoLP) and the angle of linear polarization (AoLP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoLP and AoLP. Deblurring methods for conventional images often show degraded performance when handling the polarized images since they only focus on deblurring without considering the polarization constraints. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeA: Human-object Interaction Detection using Free Annotation Labels</title>
<link>https://arxiv.org/abs/2403.01840</link>
<guid>https://arxiv.org/abs/2403.01840</guid>
<content:encoded><![CDATA[
arXiv:2403.01840v2 Announce Type: replace 
Abstract: Recent human-object interaction (HOI) detection methods depend on extensively annotated image datasets, which require a significant amount of manpower. In this paper, we propose a novel self-adaptive, language-driven HOI detection method, termed FreeA. This method leverages the adaptability of the text-image model to generate latent HOI labels without requiring manual annotation. Specifically, FreeA aligns image features of human-object pairs with HOI text templates and employs a knowledge-based masking technique to decrease improbable interactions. Furthermore, FreeA implements a proposed method for matching interaction correlations to increase the probability of actions associated with a particular action, thereby improving the generated HOI labels. Experiments on two benchmark datasets showcase that FreeA achieves state-of-the-art performance among weakly supervised HOI competitors. Our proposal gets +\textbf{13.29} (\textbf{159\%$\uparrow$}) mAP and +\textbf{17.30} (\textbf{98\%$\uparrow$}) mAP than the newest ``Weakly'' supervised model, and +\textbf{7.19} (\textbf{28\%$\uparrow$}) mAP and +\textbf{14.69} (\textbf{34\%$\uparrow$}) mAP than the latest ``Weakly+'' supervised model, respectively, on HICO-DET and V-COCO datasets, more accurate in localizing and classifying the interactive actions. The source code will be made public.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning</title>
<link>https://arxiv.org/abs/2403.11083</link>
<guid>https://arxiv.org/abs/2403.11083</guid>
<content:encoded><![CDATA[
arXiv:2403.11083v2 Announce Type: replace 
Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, our objective is to develop a generic anomaly detection model that can be applied in multiple scenarios. To achieve this, we custom-build generic visual language foundation models that possess extensive knowledge and robust reasoning abilities as anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers diverse prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images, point clouds, and videos. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is publicly available at https://github.com/Xiaohao-Xu/Customizable-VLM
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Image to Video, what do we need in multimodal LLMs?</title>
<link>https://arxiv.org/abs/2404.11865</link>
<guid>https://arxiv.org/abs/2404.11865</guid>
<content:encoded><![CDATA[
arXiv:2404.11865v2 Announce Type: replace 
Abstract: Covering from Image LLMs to the more complex Video LLMs, the Multimodal Large Language Models (MLLMs) have demonstrated profound capabilities in comprehending cross-modal information as numerous studies have illustrated. Previous methods delve into designing comprehensive Video LLMs through integrating video foundation models with primitive LLMs. Despite its effectiveness, such paradigm renders Video LLM's structure verbose and typically requires substantial video data for pre-training. Crucially, it neglects leveraging the foundational contributions of ready-made Image LLMs. In this paper, we introduce RED-VILLM, a Resource-Efficient Development pipeline which builds robust Video LLMs through leveraging the prior knowledge of Image LLMs. Specifically, since a video is naturally a combination of images along the temporal dimension, we devise a temporal adaptation plug-and-play structure, endowing the backbone Image LLM with the capability to grasp temporal information. Moreover, through applying this pipeline, we achieve the first Video LLM within the Chinese-speaking community. Extensive experiments demonstrate that Video LLMs developed through our approach surpass conventional Video LLMs, requiring minimal instructional data and training resources. Our approach highlights the potential for a more cost-effective and scalable advancement in multimodal models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review on Discriminative Self-supervised Learning Methods in Computer Vision</title>
<link>https://arxiv.org/abs/2405.04969</link>
<guid>https://arxiv.org/abs/2405.04969</guid>
<content:encoded><![CDATA[
arXiv:2405.04969v2 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has rapidly emerged as a transformative approach in computer vision, enabling the extraction of rich feature representations from vast amounts of unlabeled data and reducing reliance on costly manual annotations. This review presents a comprehensive analysis of discriminative SSL methods, which focus on learning representations by solving pretext tasks that do not require human labels. The paper systematically categorizes discriminative SSL approaches into five main groups: contrastive methods, clustering methods, self-distillation methods, knowledge distillation methods, and feature decorrelation methods. For each category, the review details the underlying principles, architectural components, loss functions, and representative algorithms, highlighting their unique mechanisms and contributions to the field. Extensive comparative evaluations are provided, including linear and semi-supervised protocols on standard benchmarks such as ImageNet, as well as transfer learning performance across diverse downstream tasks. The review also discusses theoretical foundations, scalability, efficiency, and practical challenges, such as computational demands and accessibility. By synthesizing recent advancements and identifying key trends, open challenges, and future research directions, this work serves as a valuable resource for researchers and practitioners aiming to leverage discriminative SSL for robust and generalizable computer vision models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis</title>
<link>https://arxiv.org/abs/2407.03653</link>
<guid>https://arxiv.org/abs/2407.03653</guid>
<content:encoded><![CDATA[
arXiv:2407.03653v5 Announce Type: replace 
Abstract: This paper presents refined BigEarthNet (reBEN) that is a large-scale, multi-modal remote sensing dataset constructed to support deep learning (DL) studies for remote sensing image analysis. The reBEN dataset consists of 549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN, we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m. We apply atmospheric correction to the Sentinel-2 patches using the latest version of the sen2cor tool, resulting in higher-quality patches compared to those present in BigEarthNet. Each patch is then associated with a pixel-level reference map and scene-level multi-labels. This makes reBEN suitable for pixel- and scene-based learning tasks. The labels are derived from the most recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class nomenclature as in BigEarthNet. The use of the most recent CLC map results in overcoming the label noise present in BigEarthNet. Furthermore, we introduce a new geographical-based split assignment algorithm that significantly reduces the spatial correlation among the train, validation, and test sets with respect to those present in BigEarthNet. This increases the reliability of the evaluation of DL models. To minimize the DL model training time, we introduce software tools that convert the reBEN dataset into a DL-optimized data format. In our experiments, we show the potential of reBEN for multi-modal multi-label image classification problems by considering several state-of-the-art DL models. The pre-trained model weights, associated code, and complete dataset are available at https://bigearth.net.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</title>
<link>https://arxiv.org/abs/2408.01826</link>
<guid>https://arxiv.org/abs/2408.01826</guid>
<content:encoded><![CDATA[
arXiv:2408.01826v4 Announce Type: replace 
Abstract: Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation</title>
<link>https://arxiv.org/abs/2408.11518</link>
<guid>https://arxiv.org/abs/2408.11518</guid>
<content:encoded><![CDATA[
arXiv:2408.11518v3 Announce Type: replace 
Abstract: The creation of increasingly vivid 3D talking face has become a hot topic in recent years. Currently, most speech-driven works focus on lip synchronisation but neglect to effectively capture the correlations between emotions and facial motions. To address this problem, we propose a two-stream network called EmoFace, which consists of an emotion branch and a content branch. EmoFace employs a novel Mesh Attention mechanism to analyse and fuse the emotion features and content features. Particularly, a newly designed spatio-temporal graph-based convolution, SpiralConv3D, is used in Mesh Attention to learn potential temporal and spatial feature dependencies between mesh vertices. In addition, to the best of our knowledge, it is the first time to introduce a new self-growing training scheme with intermediate supervision to dynamically adjust the ratio of groundtruth adopted in the 3D face animation task. Comprehensive quantitative and qualitative evaluations on our high-quality 3D emotional facial animation dataset, 3D-RAVDESS ($4.8863\times 10^{-5}$mm for LVE and $0.9509\times 10^{-5}$mm for EVE), together with the public dataset VOCASET ($2.8669\times 10^{-5}$mm for LVE and $0.4664\times 10^{-5}$mm for EVE), demonstrate that our approach achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question-Answering Dense Video Events</title>
<link>https://arxiv.org/abs/2409.04388</link>
<guid>https://arxiv.org/abs/2409.04388</guid>
<content:encoded><![CDATA[
arXiv:2409.04388v5 Announce Type: replace 
Abstract: This paper presents question-answering on dense video events, a novel task that answers and grounds dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events over extended periods of time. To facilitate the study, we construct DeVE-QA -- a dataset featuring 78K questions about 26K events on 10.6K long videos. Our benchmarking shows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a notable increase of 4.8% and 2.1% for G(round)QA accuracy on DeVE-QA and NExT-GQA, respectively. Data and code are available at https://github.com/QHUni/DeVE-QA.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynCL: A Synergistic Training Strategy with Instance-Aware Contrastive Learning for End-to-End Multi-Camera 3D Tracking</title>
<link>https://arxiv.org/abs/2411.06780</link>
<guid>https://arxiv.org/abs/2411.06780</guid>
<content:encoded><![CDATA[
arXiv:2411.06780v3 Announce Type: replace 
Abstract: While existing query-based 3D end-to-end visual trackers integrate detection and tracking via the tracking-by-attention paradigm, these two chicken-and-egg tasks encounter optimization difficulties when sharing the same parameters. Our findings reveal that these difficulties arise due to two inherent constraints on the self-attention mechanism, i.e., over-deduplication for object queries and self-centric attention for track queries. In contrast, removing the self-attention mechanism not only minimally impacts regression predictions of the tracker, but also tends to generate more latent candidate boxes. Based on these analyses, we present SynCL, a novel plug-and-play synergistic training strategy designed to co-facilitate multi-task learning for detection and tracking. Specifically, we propose a Task-specific Hybrid Matching module for a weight-shared cross-attention-based decoder that matches the targets of track queries with multiple object queries to exploit promising candidates overlooked by the self-attention mechanism. To flexibly select optimal candidates for the one-to-many matching, we also design a Dynamic Query Filtering module controlled by model training status. Moreover, we introduce Instance-aware Contrastive Learning to break through the barrier of self-centric attention for track queries, effectively bridging the gap between detection and tracking. Without additional inference costs, SynCL consistently delivers improvements in various benchmarks and achieves state-of-the-art performance with $58.9\%$ AMOTA on the nuScenes dataset. Code and raw results will be publicly available.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation</title>
<link>https://arxiv.org/abs/2411.17141</link>
<guid>https://arxiv.org/abs/2411.17141</guid>
<content:encoded><![CDATA[
arXiv:2411.17141v2 Announce Type: replace 
Abstract: Simultaneously using multimodal inputs from multiple sensors to train segmentors is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where multimodal segmentors over rely on certain modalities, causing performance drops when others are missing, common in real world applications. To this end, we develop the first framework for learning robust segmentor that can handle any combinations of visual modalities. Specifically, we first introduce a parallel multimodal learning strategy for learning a strong teacher. The cross-modal and unimodal distillation is then achieved in the multi scale representation space by transferring the feature level knowledge from multimodal to anymodal segmentors, aiming at addressing the unimodal bias and avoiding over-reliance on specific modalities. Moreover, a prediction level modality agnostic semantic distillation is proposed to achieve semantic knowledge transferring for segmentation. Extensive experiments on both synthetic and real-world multi-sensor benchmarks demonstrate that our method achieves superior performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision-Language Models as Evaluators in Path Planning</title>
<link>https://arxiv.org/abs/2411.18711</link>
<guid>https://arxiv.org/abs/2411.18711</guid>
<content:encoded><![CDATA[
arXiv:2411.18711v4 Announce Type: replace 
Abstract: Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes</title>
<link>https://arxiv.org/abs/2412.01240</link>
<guid>https://arxiv.org/abs/2412.01240</guid>
<content:encoded><![CDATA[
arXiv:2412.01240v2 Announce Type: replace 
Abstract: As a foundational model, SAM has significantly influenced multiple fields within computer vision, and its upgraded version, SAM 2, enhances capabilities in video segmentation, poised to make a substantial impact once again. While SAMs (SAM and SAM 2) have demonstrated excellent performance in segmenting context-independent concepts like people, cars, and roads, they overlook more challenging context-dependent (CD) concepts, such as visual saliency, camouflage, product defects, and medical lesions. CD concepts rely heavily on global and local contextual information, making them susceptible to shifts in different contexts, which requires strong discriminative capabilities from the model. The lack of comprehensive evaluation of SAMs limits understanding of their performance boundaries, which may hinder the design of future models. In this paper, we conduct a thorough quantitative evaluation of SAMs on 11 CD concepts across 2D and 3D images and videos in various visual modalities within natural, medical, and industrial scenes. We develop a unified evaluation framework for SAM and SAM 2 that supports manual, automatic, and intermediate self-prompting, aided by our specific prompt generation and interaction strategies. We further explore the potential of SAM 2 for in-context learning and introduce prompt robustness testing to simulate real-world imperfect prompts. Finally, we analyze the benefits and limitations of SAMs in understanding CD concepts and discuss their future development in segmentation tasks. This work aims to provide valuable insights to guide future research in both context-independent and context-dependent concepts segmentation, potentially informing the development of the next version -- SAM 3.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model</title>
<link>https://arxiv.org/abs/2412.04729</link>
<guid>https://arxiv.org/abs/2412.04729</guid>
<content:encoded><![CDATA[
arXiv:2412.04729v3 Announce Type: replace 
Abstract: Recent advances in vision-language models (VLMs) have shown great promise in connecting images and text, but extending these models to long videos remains challenging due to the rapid growth in token counts. Models that compress videos by local aggregation in time or space have become popular for handling long-form inputs; however, these pooling-based projectors sacrifice the benefits of fixed-length representations that are crucial for streaming and efficient video understanding. We introduce $\texttt{Espresso}$, a new architecture that separately compresses spatial and temporal features into fixed-length sequences. $\texttt{Espresso}$ enables efficient video encoding while maintaining strong long-form reasoning capabilities. Experiments show that fixed-length compression combined with segment-wise processing offers a scalable and competitive alternative to pooling-based approaches. Our results demonstrate that fixed-length projectors, when properly designed and trained, remain a viable foundation for video-language modeling.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement</title>
<link>https://arxiv.org/abs/2412.09765</link>
<guid>https://arxiv.org/abs/2412.09765</guid>
<content:encoded><![CDATA[
arXiv:2412.09765v4 Announce Type: replace 
Abstract: The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
<link>https://arxiv.org/abs/2412.13303</link>
<guid>https://arxiv.org/abs/2412.13303</guid>
<content:encoded><![CDATA[
arXiv:2412.13303v2 Announce Type: replace 
Abstract: Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\times$1152), FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and DocVQA, using the same 0.5B LLM, but with 85$\times$ faster TTFT and a vision encoder that is 3.4$\times$ smaller. Code and models are available at https://github.com/apple/ml-fastvlm.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems</title>
<link>https://arxiv.org/abs/2412.16141</link>
<guid>https://arxiv.org/abs/2412.16141</guid>
<content:encoded><![CDATA[
arXiv:2412.16141v2 Announce Type: replace 
Abstract: Autonomous inspection of infrastructure on land and in water is a quickly growing market, with applications including surveying constructions, monitoring plants, and tracking environmental changes in on- and off-shore wind energy farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles overfitting of controllers to simulation conditions fundamentally leads to poor performance in the operation environment. There is a pressing need for more diverse and realistic test data that accurately represents the challenges faced by these systems. We address the challenge of generating perception test data for autonomous systems by leveraging Neural Radiance Fields to generate realistic and diverse test images, and integrating them into a metamorphic testing framework for vision components such as vSLAM and object detection. Our tool, N2R-Tester, allows training models of custom scenes and rendering test images from perturbed positions. An experimental evaluation of N2R-Tester on eight different vision components in AUVs and UAVs demonstrates the efficacy and versatility of the approach.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration for Image-Guided Liver Surgery with Patches-to-Partial Matching</title>
<link>https://arxiv.org/abs/2412.19328</link>
<guid>https://arxiv.org/abs/2412.19328</guid>
<content:encoded><![CDATA[
arXiv:2412.19328v2 Announce Type: replace 
Abstract: In image-guided liver surgery, the initial rigid alignment between preoperative and intraoperative data, often represented as point clouds, is crucial for providing sub-surface information from preoperative CT/MRI images to the surgeon during the procedure. Currently, this alignment is typically performed using semi-automatic methods, which, while effective to some extent, are prone to errors that demand manual correction. Point cloud correspondence-based registration methods are promising to serve as a fully automatic solution. However, they may struggle in scenarios with limited intraoperative surface visibility, a common challenge in liver surgery, particularly in laparoscopic procedures, which we refer to as complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating the performance of state-of-the-art learning-based point cloud registration methods on our carefully constructed in silico and in vitro datasets. Then, we propose a patches-to-partial matching strategy as a plug-and-play module to resolve the ambiguity, which can be seamlessly integrated into learning-based registration methods without disrupting their end-to-end structure. It has proven effective and efficient in improving registration performance for cases with limited intraoperative visibility. The constructed benchmark and the proposed module establish a solid foundation for advancing applications of point cloud correspondence-based registration methods in image-guided liver surgery.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Federated Learning Based on Explanation-Guided Pruning for Remote Sensing Image Classification</title>
<link>https://arxiv.org/abs/2501.11493</link>
<guid>https://arxiv.org/abs/2501.11493</guid>
<content:encoded><![CDATA[
arXiv:2501.11493v2 Announce Type: replace 
Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which multiple clients collaboratively train a global model by exchanging only model updates with the central server without sharing the local data of the clients. Due to the large volume of model updates required to be transmitted between clients and the central server, most FL systems are associated with high transfer costs (i.e., communication overhead). This issue is more critical for operational applications in remote sensing (RS), especially when large-scale RS data is processed and analyzed through FL systems with restricted communication bandwidth. To address this issue, we introduce an explanation-guided pruning strategy for communication-efficient FL in the context of RS image classification. Our pruning strategy is defined based on the layer-wise relevance propagation (LRP) driven explanations to: 1) efficiently and effectively identify the most relevant and informative model parameters (to be exchanged between clients and the central server); and 2) eliminate the non-informative ones to minimize the volume of model updates. The experimental results on the BigEarthNet-S2 dataset demonstrate that our strategy effectively reduces the number of shared model updates, while increasing the generalization ability of the global model. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-LRP.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language Models on Context-Aware Hazard Detection and Edge Case Evaluation</title>
<link>https://arxiv.org/abs/2502.00262</link>
<guid>https://arxiv.org/abs/2502.00262</guid>
<content:encoded><![CDATA[
arXiv:2502.00262v3 Announce Type: replace 
Abstract: Autonomous driving systems face significant challenges in handling unpredictable edge-case scenarios, such as adversarial pedestrian movements, dangerous vehicle maneuvers, and sudden environmental changes. Current end-to-end driving models struggle with generalization to these rare events due to limitations in traditional detection and prediction approaches. To address this, we propose INSIGHT (Integration of Semantic and Visual Inputs for Generalized Hazard Tracking), a hierarchical vision-language model (VLM) framework designed to enhance hazard detection and edge-case evaluation. By using multimodal data fusion, our approach integrates semantic and visual representations, enabling precise interpretation of driving scenarios and accurate forecasting of potential dangers. Through supervised fine-tuning of VLMs, we optimize spatial hazard localization using attention-based mechanisms and coordinate regression techniques. Experimental results on the BDD100K dataset demonstrate a substantial improvement in hazard prediction straightforwardness and accuracy over existing models, achieving a notable increase in generalization performance. This advancement enhances the robustness and safety of autonomous driving systems, ensuring improved situational awareness and potential decision-making in complex real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling CLIP for Multi-Object Perception</title>
<link>https://arxiv.org/abs/2502.02977</link>
<guid>https://arxiv.org/abs/2502.02977</guid>
<content:encoded><![CDATA[
arXiv:2502.02977v3 Announce Type: replace 
Abstract: Vision-language models like CLIP excel at recognizing the single, prominent object in a scene. However, they struggle in complex scenes containing multiple objects. We identify a fundamental reason behind this limitation: VLMs features space exhibits significant semantic entanglement, where features of one class contain substantial information about other unrelated classes, a phenomenon we term mutual feature information (MFI). This entanglement becomes evident during class-specific queries, as unrelated objects are activated alongside the queried class. To address this limitation, we propose DCLIP, a framework that disentangles CLIP features using two complementary objectives: a novel MFI Loss that orthogonalizes the text (class) features to reduce inter-class similarity, and the Asymmetric Loss (ASL) that aligns image features with the disentangled text features. Our experiment demonstrates that DCLIP reduces inter-class feature similarity by 30\% compared to CLIP, leading to significant performance gains on multi-label recognition (MLR) and zero-shot semantic segmentation (ZS3). In MLR, DCLIP outperforms SOTA approaches on VOC2007 and COCO-14 while using 75\% fewer parameters, and surpasses SOTA ZS3 methods by 3.4 mIoU on VOC2012 and 2.8 mIoU on COCO-17. These results establish feature disentanglement as a critical factor for effective multi-object perception in vision-language models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Model Merging for Enhancing the Robustness of Medical Imaging Classification Models</title>
<link>https://arxiv.org/abs/2502.20516</link>
<guid>https://arxiv.org/abs/2502.20516</guid>
<content:encoded><![CDATA[
arXiv:2502.20516v2 Announce Type: replace 
Abstract: Model merging is an effective strategy to merge multiple models for enhancing model performances, and more efficient than ensemble learning as it will not introduce extra computation into inference. However, limited research explores if the merging process can occur within one model and enhance the model's robustness, which is particularly critical in the medical image domain. In the paper, we are the first to propose in-model merging (InMerge), a novel approach that enhances the model's robustness by selectively merging similar convolutional kernels in the deep layers of a single convolutional neural network (CNN) during the training process for classification. We also analytically reveal important characteristics that affect how in-model merging should be performed, serving as an insightful reference for the community. We demonstrate the feasibility and effectiveness of this technique for different CNN architectures on 4 prevalent datasets. The proposed InMerge-trained model surpasses the typically-trained model by a substantial margin. The code will be made public.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Preference Optimization for Vision-Language Long-Horizon Task Planning</title>
<link>https://arxiv.org/abs/2502.20742</link>
<guid>https://arxiv.org/abs/2502.20742</guid>
<content:encoded><![CDATA[
arXiv:2502.20742v3 Announce Type: replace 
Abstract: Existing methods for vision-language task planning excel in short-horizon tasks but often fall short in complex, long-horizon planning within dynamic environments. These challenges primarily arise from the difficulty of effectively training models to produce high-quality reasoning processes for long-horizon tasks. To address this, we propose Structured Preference Optimization (SPO), which aims to enhance reasoning and action selection in long-horizon task planning through structured preference evaluation and optimized training strategies. Specifically, SPO introduces: 1) Preference-Based Scoring and Optimization, which systematically evaluates reasoning chains based on task relevance, visual grounding, and historical consistency; and 2) Curriculum-Guided Training, where the model progressively adapts from simple to complex tasks, improving its generalization ability in long-horizon scenarios and enhancing reasoning robustness. To advance research in vision-language long-horizon task planning, we introduce ExtendaBench, a comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat 2.0, categorized into ultra-short, short, medium, and long tasks. Experimental results demonstrate that SPO significantly improves reasoning quality and final decision accuracy, outperforming prior methods on long-horizon tasks and underscoring the effectiveness of preference-driven optimization in vision-language task planning. Specifically, SPO achieves a +5.98% GCR and +4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement in Habitat over the best-performing baselines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning</title>
<link>https://arxiv.org/abs/2503.04877</link>
<guid>https://arxiv.org/abs/2503.04877</guid>
<content:encoded><![CDATA[
arXiv:2503.04877v2 Announce Type: replace 
Abstract: Imitation Learning can train robots to perform complex and diverse manipulation tasks, but learned policies are brittle with observations outside of the training distribution. 3D scene representations that incorporate observations from calibrated RGBD cameras have been proposed as a way to mitigate this, but in our evaluations with unseen embodiments and camera viewpoints they show only modest improvement. To address those challenges, we propose Adapt3R, a general-purpose 3D observation encoder which synthesizes data from calibrated RGBD cameras into a vector that can be used as conditioning for arbitrary IL algorithms. The key idea is to use a pretrained 2D backbone to extract semantic information, using 3D only as a medium to localize this information with respect to the end-effector. We show across 93 simulated and 6 real tasks that when trained end-to-end with a variety of IL algorithms, Adapt3R maintains these algorithms' learning capacity while enabling zero-shot transfer to novel embodiments and camera poses.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Functioning as a Hook for Two-Stage Referring Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2503.07516</link>
<guid>https://arxiv.org/abs/2503.07516</guid>
<content:encoded><![CDATA[
arXiv:2503.07516v2 Announce Type: replace 
Abstract: Referring Multi-Object Tracking (RMOT) aims to localize target trajectories specified by natural language expressions in videos. Existing RMOT methods mainly follow two paradigms: one-stage strategies and two-stage ones. The former jointly trains tracking with referring but suffers from substantial computational overhead. Although the latter improves efficiency, it overlooks the inherent contextual aggregation capabilities of pre-trained visual backbones and takes a detour. Meanwhile, its fixed dual-tower architecture restricts compatibility with other visual / text backbones. To address these limitations, we propose JustHook, a novel hook-like framework for two-stage RMOT, which introduces two core components: (1) a Visual Feature Hook (VFH), enabling JustHook to extract context-rich local features directly from the original visual backbone like a hook; (2) a Parallel Combined Decoder (PCD), which transforms the passive cosine similarity measurement between independent modalities into active contrastive learning within the combined feature space. The proposed JustHook not only leverages the capabilities of pre-trained models but also breaks free from the constraints of inherent modality alignment, achieving strong scalability. Extensive experiments on Refer-KITTI and Refer-KITTI-V2 demonstrate that JustHook outperforms state-of-the-art methods across diverse encoder combinations, achieving a notable 7.77\% HOTA improvement on Refer-KITTI-V2. Code will be made available soon.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalized Matching Transformer</title>
<link>https://arxiv.org/abs/2503.17715</link>
<guid>https://arxiv.org/abs/2503.17715</guid>
<content:encoded><![CDATA[
arXiv:2503.17715v2 Announce Type: replace 
Abstract: We present a new state of the art approach for sparse keypoint matching between pairs of images. Our method consists of a fully deep learning based approach combining a visual backbone coupled with a SplineCNN graph neural network for feature processing and a normalized transformer decoder for decoding keypoint correspondences together with the Sinkhorn algorithm. Our method is trained using a contrastive and a hyperspherical loss for better feature representations. We additionally use data augmentation during training. This comparatively simple architecture combining extensive normalization and advanced losses outperforms current state of the art approaches on PascalVOC and SPair-71k datasets by $5.1\%$ and $2.2\%$ respectively compared to BBGM, ASAR, COMMON and GMTR while training for at least $1.7x$ fewer epochs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMP: Continual Multimodal Pre-training for Vision Foundation Models</title>
<link>https://arxiv.org/abs/2503.18931</link>
<guid>https://arxiv.org/abs/2503.18931</guid>
<content:encoded><![CDATA[
arXiv:2503.18931v2 Announce Type: replace 
Abstract: Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to accommodate visual inputs with different resolutions, and an Alignment Loss between visual and textual features for better cross-modal alignment. After continual pre-training, leading VFMs like DINOv2, SigLIP and AIMv2 achieve remarkable improvements not only in multimodal understanding tasks but also in generic classification and segmentation tasks. Remarkably, CoMP-AIMv2 achieves scores of 64.9 on ChartQA with a 0.5B LLM, while maintaining an 87.3% accuracy on ImageNet-1K and a 51.8 mIoU on ADE20K under frozen chunk evaluation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Plasticity-Aware Method for Continual Self-Supervised Learning in Remote Sensing</title>
<link>https://arxiv.org/abs/2503.24088</link>
<guid>https://arxiv.org/abs/2503.24088</guid>
<content:encoded><![CDATA[
arXiv:2503.24088v2 Announce Type: replace 
Abstract: Continual self-supervised learning (CSSL) methods have gained increasing attention in remote sensing (RS) due to their capability to learn new tasks sequentially from continuous streams of unlabeled data.
  Existing CSSL methods, while learning new tasks, focus on preventing catastrophic forgetting. To this end, most of them use regularization strategies to retain knowledge of previous tasks. This reduces the model's ability to adapt to the data of new tasks (i.e., learning plasticity), which can degrade performance. To address this problem, in this paper, we propose a novel CSSL method that aims to learn tasks sequentially, while achieving high learning plasticity. To this end, the proposed method uses a knowledge distillation strategy with an integrated decoupling mechanism. The decoupling is achieved by first dividing the feature dimensions into task-common and task-specific parts. Then, the task-common features are forced to be correlated to ensure memory stability while the task-specific features are forced to be de-correlated facilitating the learning of new features. Experimental results show the effectiveness of the proposed method compared to CaSSLe, which is a widely used CSSL framework, with improvements of up to 1.12% in average accuracy and 2.33% in intransigence in a task-incremental scenario, and 1.24% in average accuracy and 2.01% in intransigence in a class-incremental scenario.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration</title>
<link>https://arxiv.org/abs/2503.24121</link>
<guid>https://arxiv.org/abs/2503.24121</guid>
<content:encoded><![CDATA[
arXiv:2503.24121v3 Announce Type: replace 
Abstract: Image registration is fundamental in medical imaging, enabling precise alignment of anatomical structures for diagnosis, treatment planning, image-guided interventions, and longitudinal monitoring. This work introduces IMPACT (Image Metric with Pretrained model-Agnostic Comparison for Transmodality registration), a novel similarity metric designed for robust multimodal image registration. Rather than relying on raw intensities, handcrafted descriptors, or task-specific training, IMPACT defines a semantic similarity measure based on the comparison of deep features extracted from large-scale pretrained segmentation models. By leveraging representations from models such as TotalSegmentator, Segment Anything (SAM), and other foundation networks, IMPACT provides a task-agnostic, training-free solution that generalizes across imaging modalities. These features, originally trained for segmentation, offer strong spatial correspondence and semantic alignment capabilities, making them naturally suited for registration. The method integrates seamlessly into both algorithmic (Elastix) and learning-based (VoxelMorph) frameworks, leveraging the strengths of each. IMPACT was evaluated on five challenging 3D registration tasks involving thoracic CT/CBCT and pelvic MR/CT datasets. Quantitative metrics, including Target Registration Error and Dice Similarity Coefficient, demonstrated consistent improvements in anatomical alignment over baseline methods. Qualitative analyses further highlighted the robustness of the proposed metric in the presence of noise, artifacts, and modality variations. With its versatility, efficiency, and strong performance across diverse tasks, IMPACT offers a powerful solution for advancing multimodal image registration in both clinical and research settings.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-MAGE: A Game Evaluation Framework for Assessing Vision-Centric Capabilities in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.06148</link>
<guid>https://arxiv.org/abs/2504.06148</guid>
<content:encoded><![CDATA[
arXiv:2504.06148v2 Announce Type: replace 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in visual-text processing. However, existing static image-text benchmarks are insufficient for evaluating their dynamic perception and interactive reasoning abilities. We introduce Vision-centric Multiple Abilities Game Evaluation(V-MAGE), a novel game-based evaluation framework designed to systematically assess MLLMs' visual reasoning in interactive, continuous-space environments. V-MAGE features five distinct video games comprising over 30 carefully constructed evaluation scenarios. These scenarios are set in free-form, visually complex environments that require models to interpret dynamic game states and make decisions based solely on visual input, thereby closely reflecting the conditions encountered by human players. To ensure robust and interpretable comparisons across models, V-MAGE employs a dynamic Elo-based ranking system that accounts for varying difficulty levels and task diversity. Benchmarking state-of-the-art MLLMs against human baselines reveals that while leading models approach human-level performance in simple tasks, their performance drops significantly in complex scenarios requiring advanced reasoning and task orchestration. This persistent performance gap highlights fundamental limitations in current MLLMs' ability to perform real-time, vision-grounded interactions. Through extensive analyses, we demonstrate the utility of V-MAGE in uncovering these limitations and providing actionable insights for improving the visual and reasoning capabilities of MLLMs in dynamic, interactive settings. Code is publicly available at https://github.com/CSU-JPG/V-MAGE.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance</title>
<link>https://arxiv.org/abs/2504.06232</link>
<guid>https://arxiv.org/abs/2504.06232</guid>
<content:encoded><![CDATA[
arXiv:2504.06232v2 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. Recent approaches have investigated training-free strategies to enable high-resolution image synthesis with pre-trained models. However, these techniques often struggle with generating high-quality visuals and tend to exhibit artifacts or low-fidelity details, as they typically rely solely on the endpoint of the low-resolution sampling trajectory while neglecting intermediate states that are critical for preserving structure and synthesizing finer detail. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging such flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's capability in achieving superior high-resolution image quality over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone</title>
<link>https://arxiv.org/abs/2504.10400</link>
<guid>https://arxiv.org/abs/2504.10400</guid>
<content:encoded><![CDATA[
arXiv:2504.10400v2 Announce Type: replace 
Abstract: This work quantitatively evaluates the performance of event-based vision systems (EVS) against conventional RGB-based models for action prediction in collision avoidance on an FPGA accelerator. Our experiments demonstrate that the EVS model achieves a significantly higher effective frame rate (1 kHz) and lower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the RGB-based model, particularly when tested on out-of-distribution data. The EVS model also exhibits superior robustness in selecting optimal evasion maneuvers. In particular, in distinguishing between movement and stationary states, it achieves a 59 percentage point advantage in precision (78% vs. 19%) and a substantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility of the RGB model to overfitting. Further analysis in different combinations of spatial classes confirms the consistent performance of the EVS model in both test data sets. Finally, we evaluated the system end-to-end and achieved a latency of approximately 2.14 ms, with event aggregation (1 ms) and inference on the processing unit (0.94 ms) accounting for the largest components. These results underscore the advantages of event-based vision for real-time collision avoidance and demonstrate its potential for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
<link>https://arxiv.org/abs/2505.00254</link>
<guid>https://arxiv.org/abs/2505.00254</guid>
<content:encoded><![CDATA[
arXiv:2505.00254v3 Announce Type: replace 
Abstract: AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVAS, a VLM-powered system designed for open-ended, advanced video analytics. AVAS incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an accuracy of 75.8%.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking</title>
<link>https://arxiv.org/abs/2505.00752</link>
<guid>https://arxiv.org/abs/2505.00752</guid>
<content:encoded><![CDATA[
arXiv:2505.00752v2 Announce Type: replace 
Abstract: Nighttime UAV tracking presents significant challenges due to extreme illumination variations and viewpoint changes, which severely degrade tracking performance. Existing approaches either rely on light enhancers with high computational costs or introduce redundant domain adaptation mechanisms, failing to fully utilize the dynamic features in varying perspectives. To address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic \textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end tracking framework designed for nighttime UAV scenarios. DARTer leverages a Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime features from static and dynamic templates, enhancing representation robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates Vision Transformer layers based on extracted features, significantly improving efficiency by reducing redundant computations. Our model eliminates the need for complex multi-task loss functions, enabling a streamlined training process. Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate the superiority of DARTer over state-of-the-art trackers. These results confirm that DARTer effectively balances tracking accuracy and efficiency, making it a promising solution for real-world nighttime UAV tracking applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding</title>
<link>https://arxiv.org/abs/2505.01481</link>
<guid>https://arxiv.org/abs/2505.01481</guid>
<content:encoded><![CDATA[
arXiv:2505.01481v2 Announce Type: replace 
Abstract: Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMs' abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMs' abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-I-RAVEN and I-RAVEN-Mesh: Two New Benchmarks for Abstract Visual Reasoning</title>
<link>https://arxiv.org/abs/2406.11061</link>
<guid>https://arxiv.org/abs/2406.11061</guid>
<content:encoded><![CDATA[
arXiv:2406.11061v2 Announce Type: replace-cross 
Abstract: We study generalization and knowledge reuse capabilities of deep neural networks in the domain of abstract visual reasoning (AVR), employing Raven's Progressive Matrices (RPMs), a recognized benchmark task for assessing AVR abilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset are investigated. Firstly, inspired by generalization assessment capabilities of the PGM dataset and popularity of I-RAVEN, we introduce Attributeless-I-RAVEN (A-I-RAVEN), a benchmark with 10 generalization regimes that allow to systematically test generalization of abstract rules applied to held-out attributes at various levels of complexity (primary and extended regimes). In contrast to PGM, A-I-RAVEN features compositionality, a variety of figure configurations, and does not require substantial computational resources. Secondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs with a novel component structure comprising line-based patterns, facilitating assessment of progressive knowledge acquisition in transfer learning setting. We evaluate 13 strong models from the AVR literature on the introduced datasets, revealing their specific shortcomings in generalization and knowledge transfer.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers</title>
<link>https://arxiv.org/abs/2406.11624</link>
<guid>https://arxiv.org/abs/2406.11624</guid>
<content:encoded><![CDATA[
arXiv:2406.11624v5 Announce Type: replace-cross 
Abstract: Transformer-based models generate hidden states that are difficult to interpret. In this work, we analyze hidden states and modify them at inference, with a focus on motion forecasting. We use linear probing to analyze whether interpretable features are embedded in hidden states. Our experiments reveal high probing accuracy, indicating latent space regularities with functionally important directions. Building on this, we use the directions between hidden states with opposing features to fit control vectors. At inference, we add our control vectors to hidden states and evaluate their impact on predictions. Remarkably, such modifications preserve the feasibility of predictions. We further refine our control vectors using sparse autoencoders (SAEs). This leads to more linear changes in predictions when scaling control vectors. Our approach enables mechanistic interpretation as well as zero-shot generalization to unseen dataset characteristics with negligible computational overhead.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Encoder Graph Quantile Neural Networks for Geographic Data</title>
<link>https://arxiv.org/abs/2409.18865</link>
<guid>https://arxiv.org/abs/2409.18865</guid>
<content:encoded><![CDATA[
arXiv:2409.18865v2 Announce Type: replace-cross 
Abstract: Positional Encoder Graph Neural Networks (PE-GNNs) are among the most effective models for learning from continuous spatial data. However, their predictive distributions are often poorly calibrated, limiting their utility in applications that require reliable uncertainty quantification. We propose the Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel framework that combines PE-GNNs with Quantile Neural Networks, partially monotonic neural blocks, and post-hoc recalibration techniques. The PE-GQNN enables flexible and robust conditional density estimation with minimal assumptions about the target distribution, and it extends naturally to tasks beyond spatial data. Empirical results on benchmark datasets show that the PE-GQNN outperforms existing methods in both predictive accuracy and uncertainty quantification, without incurring additional computational cost. We also provide theoretical insights and identify important special cases arising from our formulation, including the PE-GNN.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminating image representations with principal distortions</title>
<link>https://arxiv.org/abs/2410.15433</link>
<guid>https://arxiv.org/abs/2410.15433</guid>
<content:encoded><![CDATA[
arXiv:2410.15433v2 Announce Type: replace-cross 
Abstract: Image representations (artificial or biological) are often compared in terms of their global geometric structure; however, representations with similar global structure can have strikingly different local geometries. Here, we propose a framework for comparing a set of image representations in terms of their local geometries. We quantify the local geometry of a representation using the Fisher information matrix, a standard statistical tool for characterizing the sensitivity to local stimulus distortions, and use this as a substrate for a metric on the local geometry in the vicinity of a base image. This metric may then be used to optimally differentiate a set of models, by finding a pair of "principal distortions" that maximize the variance of the models under this metric. As an example, we use this framework to compare a set of simple models of the early visual system, identifying a novel set of image distortions that allow immediate comparison of the models by visual inspection. In a second example, we apply our method to a set of deep neural network models and reveal differences in the local geometry that arise due to architecture and training types. These examples demonstrate how our framework can be used to probe for informative differences in local sensitivities between complex models, and suggest how it could be used to compare model representations with human perception.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Weight-Averaged Model-merging</title>
<link>https://arxiv.org/abs/2411.09263</link>
<guid>https://arxiv.org/abs/2411.09263</guid>
<content:encoded><![CDATA[
arXiv:2411.09263v4 Announce Type: replace-cross 
Abstract: Model-merging has emerged as a powerful approach in deep learning, capable of enhancing model performance without any training. However, the underlying mechanisms that explain its effectiveness remain largely unexplored. In this paper, we investigate this technique from three novel perspectives to empirically provide deeper insights into why and how weight-averaged model-merging~\cite{wortsman2022soups} works: (1) we examine the intrinsic patterns captured by the learning of the model weights, and we are the first to connect that these weights encode structured with why weight-averaged model merging can work; (2) we investigate averaging on weights versus averaging on features, providing analyses from the view of diverse architecture comparisons on multiple datasets; and (3) we explore the impact on model-merging prediction stability in terms of changing the parameter magnitude, revealing insights into the way of weight averaging works as regularization by showing the robustness across different parameter scales. The code is available at https://github.com/billhhh/Rethink-Merge.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Galaxy Morphology Evolution Through Cosmic Time via Redshift Conditioned Diffusion Models</title>
<link>https://arxiv.org/abs/2411.18440</link>
<guid>https://arxiv.org/abs/2411.18440</guid>
<content:encoded><![CDATA[
arXiv:2411.18440v2 Announce Type: replace-cross 
Abstract: Redshift measures the distance to galaxies and underlies our understanding of the origin of the Universe and galaxy evolution. Spectroscopic redshift is the gold-standard method for measuring redshift, but it requires about $1000$ times more telescope time than broad-band imaging. That extra cost limits sky coverage and sample size and puts large spectroscopic surveys out of reach. Photometric redshift methods rely on imaging in multiple color filters and template fitting, yet they ignore the wealth of information carried by galaxy shape and structure. We demonstrate that a diffusion model conditioned on continuous redshift learns this missing joint structure, reproduces known morphology-$z$ correlations. We verify on the HyperSuprime-Cam survey, that the model captures redshift-dependent trends in ellipticity, semi-major axis, S\'ersic index, and isophotal area that these generated images correlate closely with true redshifts on test data. To our knowledge this is the first study to establish a direct link between galaxy morphology and redshift. Our approach offers a simple and effective path to redshift estimation from imaging data and will help unlock the full potential of upcoming wide-field surveys.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols</title>
<link>https://arxiv.org/abs/2503.06991</link>
<guid>https://arxiv.org/abs/2503.06991</guid>
<content:encoded><![CDATA[
arXiv:2503.06991v2 Announce Type: replace-cross 
Abstract: Machine unlearning is a process to remove specific data points from a trained model while maintaining the performance on retain data, addressing privacy or legal requirements. Despite its importance, existing unlearning evaluations tend to focus on logit-based metrics (i.e., accuracy) under small-scale scenarios. We observe that this could lead to a false sense of security in unlearning approaches under real-world scenarios. In this paper, we conduct a new comprehensive evaluation that employs representation-based evaluations of the unlearned model under large-scale scenarios to verify whether the unlearning approaches genuinely eliminate the targeted forget data from the model's representation perspective. Our analysis reveals that current state-of-the-art unlearning approaches either completely degrade the representational quality of the unlearned model or merely modify the classifier (i.e., the last layer), thereby achieving superior logit-based evaluation metrics while maintaining significant representational similarity to the original model. Furthermore, we introduce a rigorous unlearning evaluation setup, in which the forgetting classes exhibit semantic similarity to downstream task classes, necessitating that feature representations diverge significantly from those of the original model, thus enabling a more rigorous evaluation from a representation perspective. We hope our benchmark serves as a standardized protocol for evaluating unlearning algorithms under realistic conditions.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinTURBO: Semi-Supervised Fine-Tuning of Foundation Models via Mutual Information Decompositions for Downstream Task and Latent Spaces</title>
<link>https://arxiv.org/abs/2503.07851</link>
<guid>https://arxiv.org/abs/2503.07851</guid>
<content:encoded><![CDATA[
arXiv:2503.07851v2 Announce Type: replace-cross 
Abstract: We present a semi-supervised fine-tuning framework for foundation models that utilises mutual information decomposition to address the challenges of training for a limited amount of labelled data. Our approach derives two distinct lower bounds: i) for the downstream task space, such as classification, optimised using conditional and marginal cross-entropy alongside Kullback-Leibler divergence, and ii) for the latent space representation, regularised and aligned using a contrastive-like decomposition. This fine-tuning strategy retains the pre-trained structure of the foundation model, modifying only a specialised projector module comprising a small transformer and a token aggregation technique. Experiments on several datasets demonstrate significant improvements in classification tasks under extremely low-labelled conditions by effectively leveraging unlabelled data.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Robust Localization for Humanoid Soccer Robot via Iterative Landmark Matching</title>
<link>https://arxiv.org/abs/2503.11020</link>
<guid>https://arxiv.org/abs/2503.11020</guid>
<content:encoded><![CDATA[
arXiv:2503.11020v2 Announce Type: replace-cross 
Abstract: Accurate robot localization is essential for effective operation. Monte Carlo Localization (MCL) is commonly used with known maps but is computationally expensive due to landmark matching for each particle. Humanoid robots face additional challenges, including sensor noise from locomotion vibrations and a limited field of view (FOV) due to camera placement. This paper proposes a fast and robust localization method via iterative landmark matching (ILM) for humanoid robots. The iterative matching process improves the accuracy of the landmark association so that it does not need MCL to match landmarks to particles. Pose estimation with the outlier removal process enhances its robustness to measurement noise and faulty detections. Furthermore, an additional filter can be utilized to fuse inertial data from the inertial measurement unit (IMU) and pose data from localization. We compared ILM with Iterative Closest Point (ICP), which shows that ILM method is more robust towards the error in the initial guess and easier to get a correct matching. We also compared ILM with the Augmented Monte Carlo Localization (aMCL), which shows that ILM method is much faster than aMCL and even more accurate. The proposed method's effectiveness is thoroughly evaluated through experiments and validated on the humanoid robot ARTEMIS during RoboCup 2024 adult-sized soccer competition.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGB-Event Fusion with Self-Attention for Collision Prediction</title>
<link>https://arxiv.org/abs/2505.04258</link>
<guid>https://arxiv.org/abs/2505.04258</guid>
<content:encoded><![CDATA[
arXiv:2505.04258v2 Announce Type: replace-cross 
Abstract: Ensuring robust and real-time obstacle avoidance is critical for the safe operation of autonomous robots in dynamic, real-world environments. This paper proposes a neural network framework for predicting the time and collision position of an unmanned aerial vehicle with a dynamic object, using RGB and event-based vision sensors. The proposed architecture consists of two separate encoder branches, one for each modality, followed by fusion by self-attention to improve prediction accuracy. To facilitate benchmarking, we leverage the ABCD [8] dataset collected that enables detailed comparisons of single-modality and fusion-based approaches. At the same prediction throughput of 50Hz, the experimental results show that the fusion-based model offers an improvement in prediction accuracy over single-modality approaches of 1% on average and 10% for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105% in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for position and 26% for time error at a similar computational cost, making it a competitive alternative. Additionally, we evaluate quantized versions of the event-based models, applying 1- to 8-bit quantization to assess the trade-offs between predictive performance and computational efficiency. These findings highlight the trade-offs of multi-modal perception using RGB and event-based cameras in robotic applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera</title>
<link>https://arxiv.org/abs/2505.03093</link>
<guid>https://arxiv.org/abs/2505.03093</guid>
<content:encoded><![CDATA[
<div> LiDAR, forest inventories, DBH measurement, 360 video camera, photogrammetry <br />
Summary: <br />
The study presents a low-cost alternative to LiDAR-based techniques for accurate DBH measurement in forest inventories using a consumer-grade 360 video camera. Their pipeline includes dense point cloud reconstruction via photogrammetry software, trunk segmentation using SAM masks, and DBH estimation using a RANSAC-based technique. Their method achieved a median absolute relative error of 5-9% compared to manual measurements, only slightly higher than LiDAR-based estimates. The use of a single 360 camera significantly reduces costs and operational complexity, making it a feasible option for ecological monitoring and resource management in forest environments. They also introduced an interactive visualization tool for inspecting segmented trees and their estimated DBH. <div>
arXiv:2505.03093v2 Announce Type: replace 
Abstract: Forest inventories rely on accurate measurements of the diameter at breast height (DBH) for ecological monitoring, resource management, and carbon accounting. While LiDAR-based techniques can achieve centimeter-level precision, they are cost-prohibitive and operationally complex. We present a low-cost alternative that only needs a consumer-grade 360 video camera. Our semi-automated pipeline comprises of (i) a dense point cloud reconstruction using Structure from Motion (SfM) photogrammetry software called Agisoft Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based technique to estimate cross section shape and DBH. We introduce an interactive visualization tool for inspecting segmented trees and their estimated DBH. On 61 acquisitions of 43 trees under a variety of conditions, our method attains median absolute relative errors of 5-9% with respect to "ground-truth" manual measurements. This is only 2-4% higher than LiDAR-based estimates, while employing a single 360 camera that costs orders of magnitude less, requires minimal setup, and is widely available.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium</title>
<link>https://arxiv.org/abs/2505.09746</link>
<guid>https://arxiv.org/abs/2505.09746</guid>
<content:encoded><![CDATA[
<div> 4D Flow MRI, left atrium, hemodynamics, computational framework, prognostic biomarkers
Summary:
The study introduces an open-source computational framework for analyzing 4D Flow MRI in the left atrium. The framework allows qualitative and quantitative analysis of hemodynamic parameters with high accuracy, even with data from different centers. It demonstrates robustness in automated segmentations and can analyze a spectrum of disorders. The research aims to investigate the potential of energy, vorticity, and pressure parameters in the left atrium as prognostic biomarkers. <div>
arXiv:2505.09746v1 Announce Type: new 
Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular filling, but our comprehension of its hemodynamics is significantly limited by the constraints of conventional ultrasound analysis. 4D flow magnetic resonance imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial hemodynamics. However, the low velocities within the LA and the limited spatial resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore, the absence of dedicated computational frameworks, combined with diverse acquisition protocols and vendors, complicates gathering large cohorts for studying the prognostic value of hemodynamic parameters provided by 4D Flow MRI. In this study, we introduce the first open-source computational framework tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive qualitative and quantitative analysis of advanced hemodynamic parameters. Our framework proves robust to data from different centers of varying quality, producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95 $<$ 3 mm), even with limited training data. Additionally, we conducted the first comprehensive assessment of energy, vorticity, and pressure parameters in the LA across a spectrum of disorders to investigate their potential as prognostic biomarkers.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyadic Mamba: Long-term Dyadic Human Motion Synthesis</title>
<link>https://arxiv.org/abs/2505.09827</link>
<guid>https://arxiv.org/abs/2505.09827</guid>
<content:encoded><![CDATA[
<div> Keywords: dyadic human motion, State-Space Models, transformer-based approaches, long-term motion synthesis, benchmark 

Summary:
Dyadic Mamba introduces a novel approach utilizing State-Space Models (SSMs) for generating realistic dyadic human motion from text descriptions. It addresses the limitations of transformer-based methods by facilitating information flow between individual motion sequences through concatenation, eliminating the need for complex cross-attention mechanisms. The method achieves competitive performance on short-term benchmarks and outperforms transformer-based approaches on longer sequences. It also proposes a new benchmark for evaluating long-term motion synthesis quality, providing a standardized framework for future research. Overall, Dyadic Mamba showcases the effectiveness of SSM-based architectures in tackling the challenging task of long-term dyadic human motion synthesis, showcasing promising results for generating high-quality motion sequences of arbitrary lengths. 

<br /><br />Summary: <div>
arXiv:2505.09827v1 Announce Type: new 
Abstract: Generating realistic dyadic human motion from text descriptions presents significant challenges, particularly for extended interactions that exceed typical training sequence lengths. While recent transformer-based approaches have shown promising results for short-term dyadic motion synthesis, they struggle with longer sequences due to inherent limitations in positional encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach that leverages State-Space Models (SSMs) to generate high-quality dyadic human motion of arbitrary length. Our method employs a simple yet effective architecture that facilitates information flow between individual motion sequences through concatenation, eliminating the need for complex cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves competitive performance on standard short-term benchmarks while significantly outperforming transformer-based approaches on longer sequences. Additionally, we propose a new benchmark for evaluating long-term motion synthesis quality, providing a standardized framework for future research. Our results demonstrate that SSM-based architectures offer a promising direction for addressing the challenging task of long-term dyadic human motion synthesis from text descriptions.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes</title>
<link>https://arxiv.org/abs/2505.09829</link>
<guid>https://arxiv.org/abs/2505.09829</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, semi-supervised methods, organ boundary prediction, annotation costs, computational efficiency

Summary:
BoundarySeg is a novel approach for medical image segmentation that improves accuracy without the need for unannotated data. This multi-task framework incorporates organ boundary prediction as an auxiliary task to full organ segmentation, utilizing the consistency between the two task predictions for additional supervision. Unlike traditional semi-supervised methods, BoundarySeg does not rely on scarce unannotated data, making it a cost-effective solution. The proposed approach achieves performance comparable to or exceeding state-of-the-art methods while maintaining computational efficiency. By leveraging existing annotations, BoundarySeg overcomes the limitations of low data regimes, offering a practical solution for large-scale medical data analysis. The code for BoundarySeg will be made available upon acceptance for further research and implementation. 

<br /><br />Summary: <div>
arXiv:2505.09829v1 Announce Type: new 
Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging due to stringent privacy regulations and data protection policies. In addition, annotating medical images requires that domain experts manually delineate anatomical structures, making the process both time-consuming and costly. As a result, semi-supervised methods have gained popularity for reducing annotation costs. However, the performance of semi-supervised methods is heavily dependent on the availability of unannotated data, and their effectiveness declines when such data are scarce or absent. To overcome this limitation, we propose a simple, yet effective and computationally efficient approach for medical image segmentation that leverages only existing annotations. We propose BoundarySeg , a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between the two task predictions to provide additional supervision. This strategy improves segmentation accuracy, especially in low data regimes, allowing our method to achieve performance comparable to or exceeding state-of-the-art semi supervised approaches all without relying on unannotated data or increasing computational demands. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.09858</link>
<guid>https://arxiv.org/abs/2505.09858</guid>
<content:encoded><![CDATA[
<div> Keywords: computer-assisted interventions, deep learning, data imbalance, surgical videos, synthetic generation

Summary: 
Computer-assisted interventions benefit from deep learning methods leveraging surgical video data. However, data imbalance within these datasets presents challenges for model development. To address this, a novel two-stage text-conditioned diffusion-based method is proposed to generate high-quality surgical videos for under-represented classes. The approach utilizes a 2D latent diffusion model for spatial content and integrates temporal attention layers for consistency. A rejection sampling strategy selects suitable synthetic samples, effectively augmenting datasets to mitigate class imbalance issues. Evaluation on surgical action recognition and intra-operative event prediction tasks demonstrates significant performance improvements with the incorporation of synthetic videos generated by the proposed method. The implementation is open-sourced for further research and application. 

Summary: <br /><br />Computer-assisted interventions can benefit from deep learning methods using surgical videos. However, data imbalance in these datasets poses challenges for model development. To address this issue, a novel two-stage text-conditioned diffusion-based method is proposed to generate high-quality surgical videos for under-represented classes. The approach uses a 2D latent diffusion model for spatial content and integrates temporal attention layers for consistency. A rejection sampling strategy selects suitable synthetic samples, effectively augmenting datasets to mitigate class imbalance issues. Evaluation on surgical action recognition and intra-operative event prediction tasks demonstrates significant performance improvements with the incorporation of synthetic videos generated by the proposed method. The implementation is open-sourced for further research and application. <div>
arXiv:2505.09858v1 Announce Type: new 
Abstract: Computer-assisted interventions can improve intra-operative guidance, particularly through deep learning methods that harness the spatiotemporal information in surgical videos. However, the severe data imbalance often found in surgical video datasets hinders the development of high-performing models. In this work, we aim to overcome the data imbalance by synthesizing surgical videos. We propose a unique two-stage, text-conditioned diffusion-based method to generate high-fidelity surgical videos for under-represented classes. Our approach conditions the generation process on text prompts and decouples spatial and temporal modeling by utilizing a 2D latent diffusion model to capture spatial content and then integrating temporal attention layers to ensure temporal consistency. Furthermore, we introduce a rejection sampling strategy to select the most suitable synthetic samples, effectively augmenting existing datasets to address class imbalance. We evaluate our method on two downstream tasks-surgical action recognition and intra-operative event prediction-demonstrating that incorporating synthetic videos from our approach substantially enhances model performance. We open-source our implementation at https://gitlab.com/nct_tso_public/surgvgen.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction</title>
<link>https://arxiv.org/abs/2505.09859</link>
<guid>https://arxiv.org/abs/2505.09859</guid>
<content:encoded><![CDATA[
<div> Keywords: compositional concept learning, structured representations, analogical mapping, schema, deep learning 

Summary: 
Probabilistic Schema Induction (PSI) is a new model that uses deep learning to learn compositional visual concepts from limited examples. It operates on structured representations of examples, employing analogical mapping to identify shared relational structures across examples and create schemas. By considering both object-level and relational similarity, PSI outperforms models using unstructured feature vectors. Its success lies in amplifying relations relevant to classification, similar to selective attention mechanisms in traditional models. The model's adaptive strategy prioritizes relational similarity over object-level similarity and gives weight to distinguishing relations. This demonstrates the importance of structured representations and analogical mapping in quickly learning complex concepts. Overall, PSI's human-like performance showcases the potential of leveraging deep learning to create psychologically plausible models. 

<br /><br />Summary: <div>
arXiv:2505.09859v1 Announce Type: new 
Abstract: The ability to learn new visual concepts from limited examples is a hallmark of human cognition. While traditional category learning models represent each example as an unstructured feature vector, compositional concept learning is thought to depend on (1) structured representations of examples (e.g., directed graphs consisting of objects and their relations) and (2) the identification of shared relational structure across examples through analogical mapping. Here, we introduce Probabilistic Schema Induction (PSI), a prototype model that employs deep learning to perform analogical mapping over structured representations of only a handful of examples, forming a compositional concept called a schema. In doing so, PSI relies on a novel conception of similarity that weighs object-level similarity and relational similarity, as well as a mechanism for amplifying relations relevant to classification, analogous to selective attention parameters in traditional models. We show that PSI produces human-like learning performance and outperforms two controls: a prototype model that uses unstructured feature vectors extracted from a deep learning model, and a variant of PSI with weaker structured representations. Notably, we find that PSI's human-like performance is driven by an adaptive strategy that increases relational similarity over object-level similarity and upweights the contribution of relations that distinguish classes. These findings suggest that structured representations and analogical mapping are critical to modeling rapid human-like learning of compositional visual concepts, and demonstrate how deep learning can be leveraged to create psychological models.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Gaussian Splatting SLAM</title>
<link>https://arxiv.org/abs/2505.09915</link>
<guid>https://arxiv.org/abs/2505.09915</guid>
<content:encoded><![CDATA[
<div> NeRF, 3DGS, visual SLAM, LSG-SLAM, large-scale<br />
Summary:<br />
The paper introduces LSG-SLAM, a large-scale 3DGS-based visual SLAM system using stereo cameras. It utilizes a multi-modality strategy for estimating poses under large view changes and feature-alignment warping constraints for tracking. LSG-SLAM addresses scalability with continuous Gaussian Splatting submaps for unbounded scenes. Loop closure is performed through place recognition and pose optimization. Global optimization and structure refinement enhance reconstruction quality. Evaluations on EuRoc and KITTI datasets show LSG-SLAM outperforms existing methods. The project page is available at https://lsg-slam.github.io.<br /> <div>
arXiv:2505.09915v1 Announce Type: new 
Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown encouraging and impressive results for visual SLAM. However, most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored. This paper introduces a large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. In tracking, we introduce feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. For the scalability of large-scale scenarios, we introduce continuous Gaussian Splatting submaps to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. After the global optimization of camera poses and Gaussian points, a structure refinement module enhances the reconstruction quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches. Project page: https://lsg-slam.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.09926</link>
<guid>https://arxiv.org/abs/2505.09926</guid>
<content:encoded><![CDATA[
<div> Keywords: Universal visual anomaly detection, AdaptCLIP, zero/few-shot generalization, comparative learning, industrial and medical domains

Summary: 
The article introduces AdaptCLIP, a novel method for universal visual anomaly detection that does not require additional fine-tuning in open scenarios. AdaptCLIP learns adaptive visual and textual representations alternately, incorporating both contextual and aligned residual features in comparative learning between query and normal image prompt. The method adds visual, textual, and prompt-query adapters to CLIP models, enabling zero/few-shot generalization across domains. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks in industrial and medical domains, surpassing existing methods. The approach is simple yet effective, supporting a training-free manner on target domains once trained on a base dataset. The code and model for AdaptCLIP will be made available on GitHub at https://github.com/gaobb/AdaptCLIP. 

Summary: <br /><br />Keywords: Universal visual anomaly detection, AdaptCLIP, zero/few-shot generalization, comparative learning, industrial and medical domains <div>
arXiv:2505.09926v1 Announce Type: new 
Abstract: Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.09927</link>
<guid>https://arxiv.org/abs/2505.09927</guid>
<content:encoded><![CDATA[
<div> domain adaptation, source-free domain adaptation, medical datasets, style translation, pseudo-labels

Summary:
- The paper introduces a novel framework for source-free domain adaptation (SFDA) in medical datasets to improve model performance in the presence of domain gaps.
- The proposed approach includes preadaptation to generate a preadapted model, providing an initialization for the target model and producing high-quality enhanced pseudo-labels without adding extra parameters.
- A data-dependent frequency prompt is used to translate target domain images into a source-like style more effectively.
- The framework employs a style-related layer fine-tuning strategy designed for SFDA to train the target model using prompted target domain images and pseudo-labels.
- Experimental results on cross-modality abdominal and cardiac SFDA segmentation tasks demonstrate that the proposed method surpasses existing state-of-the-art approaches. 

<br /><br />Summary: <div>
arXiv:2505.09927v1 Announce Type: new 
Abstract: Domain adaptation addresses the challenge of model performance degradation caused by domain gaps. In the typical setup for unsupervised domain adaptation, labeled data from a source domain and unlabeled data from a target domain are used to train a target model. However, access to labeled source domain data, particularly in medical datasets, can be restricted due to privacy policies. As a result, research has increasingly shifted to source-free domain adaptation (SFDA), which requires only a pretrained model from the source domain and unlabeled data from the target domain data for adaptation. Existing SFDA methods often rely on domain-specific image style translation and self-supervision techniques to bridge the domain gap and train the target domain model. However, the quality of domain-specific style-translated images and pseudo-labels produced by these methods still leaves room for improvement. Moreover, training the entire model during adaptation can be inefficient under limited supervision. In this paper, we propose a novel SFDA framework to address these challenges. Specifically, to effectively mitigate the impact of domain gap in the initial training phase, we introduce preadaptation to generate a preadapted model, which serves as an initialization of target model and allows for the generation of high-quality enhanced pseudo-labels without introducing extra parameters. Additionally, we propose a data-dependent frequency prompt to more effectively translate target domain images into a source-like style. To further enhance adaptation, we employ a style-related layer fine-tuning strategy, specifically designed for SFDA, to train the target model using the prompted target domain images and pseudo-labels. Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks demonstrate that our proposed method outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety</title>
<link>https://arxiv.org/abs/2505.09935</link>
<guid>https://arxiv.org/abs/2505.09935</guid>
<content:encoded><![CDATA[
<div> Intersection Safety, Vulnerable Road Users, Crossing Intentions, Predictive Model, Real-time Inference Speed
Summary: 
The VRU-CIPI framework utilizes a sequential attention-based model to predict Vulnerable Road Users' (VRUs) crossing intentions at urban intersections. By incorporating a Gated Recurrent Unit (GRU) and a multi-head Transformer self-attention mechanism, the model captures temporal dynamics and spatial dependencies crucial for accurate predictions. Tested on the UCF-VRU dataset, VRU-CIPI achieved a high accuracy of 96.45% and demonstrated real-time inference speeds of up to 33 frames per second. Additionally, by integrating with Infrastructure-to-Vehicles (I2V) communication, the framework can enhance intersection safety by activating crossing signals and providing early warnings to connected vehicles. This proactive approach ensures smoother and safer interactions for all road users. 
<br /><br />Summary: <div>
arXiv:2505.09935v1 Announce Type: new 
Abstract: Understanding and predicting human behavior in-thewild, particularly at urban intersections, remains crucial for enhancing interaction safety between road users. Among the most critical behaviors are crossing intentions of Vulnerable Road Users (VRUs), where misinterpretation may result in dangerous conflicts with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a sequential attention-based model designed to predict VRU crossing intentions at intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal dynamics in VRU movements, combined with a multi-head Transformer self-attention mechanism to encode contextual and spatial dependencies critical for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed achieves state-of-the-art performance with an accuracy of 96.45% and achieving real-time inference speed reaching 33 frames per second. Furthermore, by integrating with Infrastructure-to-Vehicles (I2V) communication, our approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles, ensuring smoother and safer interactions for all road users.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.09939</link>
<guid>https://arxiv.org/abs/2505.09939</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, change detection, non-registration, disasters, dataset

Summary: 
In this study, the authors introduce a new remote sensing change detection task called non-registration change detection, aimed at addressing the challenges posed by emergencies such as natural disasters, human-made accidents, and military actions. The study identifies eight scenarios that could lead to non-registration issues, highlighting the need for specialized approaches. The authors propose unique image transformation methods tailored to these scenarios to convert registration change detection datasets into non-registration versions. Through experiments, they show that non-registration change detection can significantly impact the performance of state-of-the-art methods. The code and dataset for the study are publicly available, allowing for further exploration and validation of the proposed techniques.

<br /><br />Summary: <div>
arXiv:2505.09939v1 Announce Type: new 
Abstract: In this study, we propose a novel remote sensing change detection task, non-registration change detection, to address the increasing number of emergencies such as natural disasters, anthropogenic accidents, and military strikes. First, in light of the limited discourse on the issue of non-registration change detection, we systematically propose eight scenarios that could arise in the real world and potentially contribute to the occurrence of non-registration problems. Second, we develop distinct image transformation schemes tailored to various scenarios to convert the available registration change detection dataset into a non-registration version. Finally, we demonstrate that non-registration change detection can cause catastrophic damage to the state-of-the-art methods. Our code and dataset are available at https://github.com/ShanZard/NRCD.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2505.09943</link>
<guid>https://arxiv.org/abs/2505.09943</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared small target detection, contour-aware, saliency priors, surround-convergent prior extraction, feature fusion 

Summary: 
The paper introduces a contour-aware and saliency priors embedding network (CSPENet) for improving infrared small target detection (ISTD) performance. The proposed network includes a surround-convergent prior extraction module (SCPEM) to capture target contour information effectively. This module extracts boosted saliency and multi-scale structural priors for accurate localization and detailed representation. A dual-branch priors embedding architecture (DBPEA) is employed to fuse these priors at optimal network positions. An attention-guided feature enhancement module (AGFEM) refines feature representations and enhances saliency estimation accuracy. Experimental results show that CSPENet outperforms existing methods in ISTD on public datasets. The code for CSPENet is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.09943v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range of civilian and military applications. Existing methods suffer from deficiencies in the localization of dim targets and the perception of contour information under dense clutter environments, severely limiting their detection performance. To tackle these issues, we propose a contour-aware and saliency priors embedding network (CSPENet) for ISTD. We first design a surround-convergent prior extraction module (SCPEM) that effectively captures the intrinsic characteristic of target contour pixel gradients converging toward their center. This module concurrently extracts two collaborative priors: a boosted saliency prior for accurate target localization and multi-scale structural priors for comprehensively enriching contour detail representation. Building upon this, we propose a dual-branch priors embedding architecture (DBPEA) that establishes differentiated feature fusion pathways, embedding these two priors at optimal network positions to achieve performance enhancement. Finally, we develop an attention-guided feature enhancement module (AGFEM) to refine feature representations and improve saliency estimation accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art methods in detection performance. The code is available at https://github.com/IDIP2025/CSPENet.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction</title>
<link>https://arxiv.org/abs/2505.09965</link>
<guid>https://arxiv.org/abs/2505.09965</guid>
<content:encoded><![CDATA[
<div> Keywords: precision medicine, disease progression, spatio-temporal dynamics, MambaControl, Alzheimer's disease prediction

Summary:
MambaControl is a new framework designed for modelling disease progression in precision medicine. It addresses the challenge of capturing complex spatio-temporal dynamics while maintaining anatomical integrity. The framework integrates selective state-space modelling with diffusion processes to predict medical image trajectories accurately. MambaControl combines Mamba-based long-range modelling with graph-guided anatomical control to represent anatomical correlations effectively. It also utilizes Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail. The framework achieves state-of-the-art performance in predicting Alzheimer's disease progression, demonstrating improved prediction quality and anatomical fidelity. MambaControl has the potential to provide personalised prognosis and support clinical decision-making in the context of progressive disorders. <br /><br />Summary: MambaControl is a novel framework that integrates selective state-space modelling and diffusion processes for accurate prediction of medical image trajectories. It effectively captures subtle structural changes over time while maintaining anatomical consistency. The framework combines long-range modelling with graph-guided anatomical control and Fourier-enhanced spectral graph representations to achieve superior performance in Alzheimer's disease prediction. Its advancements in progression prediction quality and anatomical fidelity highlight its potential for personalised prognosis and clinical decision support in precision medicine. <div>
arXiv:2505.09965v1 Announce Type: new 
Abstract: Modelling disease progression in precision medicine requires capturing complex spatio-temporal dynamics while preserving anatomical integrity. Existing methods often struggle with longitudinal dependencies and structural consistency in progressive disorders. To address these limitations, we introduce MambaControl, a novel framework that integrates selective state-space modelling with diffusion processes for high-fidelity prediction of medical image trajectories. To better capture subtle structural changes over time while maintaining anatomical consistency, MambaControl combines Mamba-based long-range modelling with graph-guided anatomical control to more effectively represent anatomical correlations. Furthermore, we introduce Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail, enabling MambaControl to achieve state-of-the-art performance in Alzheimer's disease prediction. Quantitative and regional evaluations demonstrate improved progression prediction quality and anatomical fidelity, highlighting its potential for personalised prognosis and clinical decision support.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2505.09967</link>
<guid>https://arxiv.org/abs/2505.09967</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial expression recognition, Texture Key Driver Factors, Texture-Aware Feature Extractor, Dual Contextual Information Filtering, State-of-the-art performance

Summary: 
Texture Key Driver Factors (TKDF) play a crucial role in facial expression recognition (FER) by capturing localized texture regions that exhibit strong discriminative power for different emotional categories. The proposed framework introduces a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF) to effectively extract and refine texture cues in facial images. TAFE uses a ResNet-based backbone with multi-branch attention to extract fine-grained texture representations, while DCIF refines these features with adaptive pooling and attention mechanisms. Experimental results on RAF-DB and KDEF datasets show that the method achieves state-of-the-art performance in FER. By incorporating TKDFs into FER pipelines, the framework demonstrates effectiveness and robustness in capturing subtle facial expression cues for accurate emotion recognition. 

Summary: <div>
arXiv:2505.09967v1 Announce Type: new 
Abstract: Facial expression recognition (FER) in the wild remains a challenging task due to the subtle and localized nature of expression-related features, as well as the complex variations in facial appearance. In this paper, we introduce a novel framework that explicitly focuses on Texture Key Driver Factors (TKDF), localized texture regions that exhibit strong discriminative power across emotional categories. By carefully observing facial image patterns, we identify that certain texture cues, such as micro-changes in skin around the brows, eyes, and mouth, serve as primary indicators of emotional dynamics. To effectively capture and leverage these cues, we propose a FER architecture comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced with multi-branch attention to extract fine-grained texture representations, while DCIF refines these features by filtering context through adaptive pooling and attention mechanisms. Experimental results on RAF-DB and KDEF datasets demonstrate that our method achieves state-of-the-art performance, verifying the effectiveness and robustness of incorporating TKDFs into FER pipelines.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2505.09971</link>
<guid>https://arxiv.org/abs/2505.09971</guid>
<content:encoded><![CDATA[
<div> Keywords: Airborne laser scanning, Point cloud segmentation, Continuous Test-Time Adaptation, Entropy-based consistency loss, Benchmarks 

Summary: 
The paper introduces APCoTTA, a novel method for Continuous Test-Time Adaptation tailored for ALS point cloud segmentation. It addresses challenges such as domain shifts and error accumulation during adaptation by incorporating a dynamic trainable layer selection module, an entropy-based consistency loss, and a random parameter interpolation mechanism. These techniques help improve model stability and balance target adaptation with source knowledge retention. The authors also introduce two benchmarks, ISPRSC and H3DC, to provide standardized evaluation metrics for CTTA in the context of ALS point cloud segmentation. Experimental results demonstrate that APCoTTA outperforms direct inference, achieving significant improvements in mean Intersection over Union (mIoU) on the benchmarks. The code and benchmarks are made available for further research and development. 

<br /><br />Summary: <div>
arXiv:2505.09971v1 Announce Type: new 
Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task for large-scale 3D scene understanding. In real-world applications, models are typically fixed after training. However, domain shifts caused by changes in the environment, sensor types, or sensor degradation often lead to a decline in model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by adapting a source-pretrained model to evolving, unlabeled target domains. Despite its potential, research on ALS point clouds remains limited, facing challenges such as the absence of standardized datasets and the risk of catastrophic forgetting and error accumulation during prolonged adaptation. To tackle these challenges, we propose APCoTTA, the first CTTA method tailored for ALS point cloud semantic segmentation. We propose a dynamic trainable layer selection module. This module utilizes gradient information to select low-confidence layers for training, and the remaining layers are kept frozen, mitigating catastrophic forgetting. To further reduce error accumulation, we propose an entropy-based consistency loss. By losing such samples based on entropy, we apply consistency loss only to the reliable samples, enhancing model stability. In addition, we propose a random parameter interpolation mechanism, which randomly blends parameters from the selected trainable layers with those of the source model. This approach helps balance target adaptation and source knowledge retention, further alleviating forgetting. Finally, we construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA benchmarks for ALS point cloud segmentation. Experimental results demonstrate that APCoTTA achieves the best performance on two benchmarks, with mIoU improvements of approximately 9% and 14% over direct inference. The new benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation</title>
<link>https://arxiv.org/abs/2505.09986</link>
<guid>https://arxiv.org/abs/2505.09986</guid>
<content:encoded><![CDATA[
<div> algorithm, underwater images, compression, HQUIC, evaluation

Summary:<br />
The article introduces HQUIC, a new underwater image compression algorithm that leverages the unique characteristics of underwater scenes for enhanced efficiency. HQUIC utilizes an ALTC module to adaptively predict attenuation coefficients and global light information, addressing issues caused by lighting and tone variations. It also incorporates a codebook to extract common objects in underwater images, enhancing performance. Additionally, HQUIC dynamically weights multi-scale frequency components to prioritize critical information for distortion quality. Evaluations on various underwater datasets show that HQUIC outperforms existing compression methods in terms of compression efficiency. <div>
arXiv:2505.09986v1 Announce Type: new 
Abstract: With the increasing exploration and exploitation of the underwater world, underwater images have become a critical medium for human interaction with marine environments, driving extensive research into their efficient transmission and storage. However, contemporary underwater image compression algorithms fail to fully leverage the unique characteristics distinguishing underwater scenes from terrestrial images, resulting in suboptimal performance. To address this limitation, we introduce HQUIC, designed to exploit underwater-image-specific features for enhanced compression efficiency. HQUIC employs an ALTC module to adaptively predict the attenuation coefficients and global light information of the images, which effectively mitigates the issues caused by the differences in lighting and tone existing in underwater images. Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the common objects within underwater images and enhances the performance of the main branch. Furthermore, HQUIC dynamically weights multi-scale frequency components, prioritizing information critical for distortion quality while discarding redundant details. Extensive evaluations on diverse underwater datasets demonstrate that HQUIC outperforms state-of-the-art compression methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointArena: Probing Multimodal Grounding Through Language-Guided Pointing</title>
<link>https://arxiv.org/abs/2505.09990</link>
<guid>https://arxiv.org/abs/2505.09990</guid>
<content:encoded><![CDATA[
<div> benchmarking, pointing, multimodal models, robotic manipulation, reasoning tasks 

Summary: 
The article introduces PointArena, a platform for evaluating multimodal pointing abilities in various reasoning scenarios. PointArena consists of three components: Point-Bench, a dataset with 1,000 pointing tasks across five reasoning categories; Point-Battle, an interactive arena for comparing models; and Point-Act, a robotic manipulation system for real-world evaluations. State-of-the-art models were tested, with Molmo-72B performing best. Supervised training for pointing tasks improved model performance significantly. The study also highlights the importance of precise pointing in enabling multimodal models to connect abstract reasoning with real-world actions. The results show that proprietary models are catching up to open-source models in performance. Overall, the evaluations emphasize the vital role of accurate pointing abilities in multimodal models across various applications. <br /><br /> <div>
arXiv:2505.09990v1 Announce Type: new 
Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. Project page: https://pointarena.github.io/
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Descriptive Image-Text Matching with Graded Contextual Similarity</title>
<link>https://arxiv.org/abs/2505.09997</link>
<guid>https://arxiv.org/abs/2505.09997</guid>
<content:encoded><![CDATA[
<div> Keywords: Image-text matching, Descriptive flexibility, TF-IDF, Graded contextual similarity, Hierarchical reasoning <br />
Summary: <br />
This article introduces a new approach called Descriptive Image-Text Matching (DITM) to improve the process of building correspondences between images and text by considering their many-to-many relationships. DITM leverages the descriptive flexibility of language to learn graded contextual similarity and balance pairwise similarity using cumulative TF-IDF scores. By refining false negative labeling and aligning relevant sentences in a generic-to-specific order, DITM enhances the discovery of optimal matches and potential positive pairs. The method outperforms existing approaches in representing complex image-text relationships and improves hierarchical reasoning ability, as demonstrated through experiments on multiple datasets. <div>
arXiv:2505.09997v1 Announce Type: new 
Abstract: Image-text matching aims to build correspondences between visual and textual data by learning their pairwise similarities. Most existing approaches have adopted sparse binary supervision, indicating whether a pair of images and sentences matches or not. However, such sparse supervision covers a limited subset of image-text relationships, neglecting their inherent many-to-many correspondences; an image can be described in numerous texts at different descriptive levels. Moreover, existing approaches overlook the implicit connections from general to specific descriptions, which form the underlying rationale for the many-to-many relationships between vision and language. In this work, we propose descriptive image-text matching, called DITM, to learn the graded contextual similarity between image and text by exploring the descriptive flexibility of language. We formulate the descriptiveness score of each sentence with cumulative term frequency-inverse document frequency (TF-IDF) to balance the pairwise similarity according to the keywords in the sentence. Our method leverages sentence descriptiveness to learn robust image-text matching in two key ways: (1) to refine the false negative labeling, dynamically relaxing the connectivity between positive and negative pairs, and (2) to build more precise matching, aligning a set of relevant sentences in a generic-to-specific order. By moving beyond rigid binary supervision, DITM enhances the discovery of both optimal matches and potential positive pairs. Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the effectiveness of our method in representing complex image-text relationships compared to state-of-the-art approaches. In addition, DITM enhances the hierarchical reasoning ability of the model, supported by the extensive analysis on HierarCaps benchmark.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching</title>
<link>https://arxiv.org/abs/2505.09998</link>
<guid>https://arxiv.org/abs/2505.09998</guid>
<content:encoded><![CDATA[
<div> sketch-driven, 3D garment generation, virtual fashion, AR/VR environments, democratized design  
<br />  
Summary:  
A new framework is introduced for 3D garment generation through 3D sketches in AR/VR, allowing ordinary users to create personalized digital clothing. The system combines a conditional diffusion model, sketch encoder, and adaptive learning strategy to interpret free-hand input and generate realistic garments. The KO3DClothes dataset is also introduced to address the lack of training data. Extensive experiments and user studies show superior performance in fidelity and usability compared to existing methods, indicating the potential for democratized fashion design on consumer platforms.  
<br /> <div>
arXiv:2505.09998v1 Announce Type: new 
Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and smart devices, people increasingly seek ways to express their identity through virtual fashion. However, existing 3D garment design tools remain inaccessible to everyday users due to steep technical barriers and limited data. In this work, we introduce a 3D sketch-driven 3D garment generation framework that empowers ordinary users - even those without design experience - to create high-quality digital clothing through simple 3D sketches in AR/VR environments. By combining a conditional diffusion model, a sketch encoder trained in a shared latent space, and an adaptive curriculum learning strategy, our system interprets imprecise, free-hand input and produces realistic, personalized garments. To address the scarcity of training data, we also introduce KO3DClothes, a new dataset of paired 3D garments and user-created sketches. Extensive experiments and user studies confirm that our method significantly outperforms existing baselines in both fidelity and usability, demonstrating its promise for democratized fashion design on next-generation consumer platforms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of YOLOv8 in monocular downward multiple Car Target detection</title>
<link>https://arxiv.org/abs/2505.10016</link>
<guid>https://arxiv.org/abs/2505.10016</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous driving, Object detection, YOLOv8, Bidirectional pyramid structure, Detection accuracy

Summary:
The article introduces an improved autonomous target detection network based on YOLOv8, addressing limitations in current object detection technologies. By incorporating structural reparameterization and a bidirectional pyramid structure network model, the proposed approach achieves precise detection of multi-scale, small, and remote objects. A novel detection pipeline enhances efficiency and accuracy, with experimental results showing a 65% detection accuracy for large and small objects. The model is highlighted for its potential in real-world applications and autonomous driving competitions, particularly excelling in single-target and small-object detection scenarios. This advancement showcases significant progress in autonomous driving technology, paving the way for enhanced safety, efficiency, and emergency responses in modern transportation systems.<br /><br />Summary: <div>
arXiv:2505.10016v1 Announce Type: new 
Abstract: Autonomous driving technology is progressively transforming traditional car driving methods, marking a significant milestone in modern transportation. Object detection serves as a cornerstone of autonomous systems, playing a vital role in enhancing driving safety, enabling autonomous functionality, improving traffic efficiency, and facilitating effective emergency responses. However, current technologies such as radar for environmental perception, cameras for road perception, and vehicle sensor networks face notable challenges, including high costs, vulnerability to weather and lighting conditions, and limited resolution.To address these limitations, this paper presents an improved autonomous target detection network based on YOLOv8. By integrating structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework, the proposed approach achieves highly efficient and precise detection of multi-scale, small, and remote objects. Experimental results demonstrate that the enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showcasing significant advancements over traditional methods.This improved model holds substantial potential for real-world applications and is well-suited for autonomous driving competitions, such as the Formula Student Autonomous China (FSAC), particularly excelling in scenarios involving single-target and small-object detection.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction</title>
<link>https://arxiv.org/abs/2505.10027</link>
<guid>https://arxiv.org/abs/2505.10027</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, latent diffusion model, remote sensing, super-resolution, image reconstruction
Summary:
This paper introduces a reinforcement learning-based latent diffusion model (LDM) fine-tuning approach for remote sensing image super-resolution. By optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model, the method significantly enhances super-resolution quality in structured and complex natural scenes. Experiments on the RESISC45 dataset demonstrate improvements over the baseline model, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10. The proposed method effectively preserves image details, handles complex scenes, and enhances adaptability across a variety of scenes.<br /><br />Summary: <div>
arXiv:2505.10027v1 Announce Type: new 
Abstract: With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the method's effectiveness in enhancing super-resolution quality and adaptability across scenes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera</title>
<link>https://arxiv.org/abs/2505.10030</link>
<guid>https://arxiv.org/abs/2505.10030</guid>
<content:encoded><![CDATA[
<div> deep learning, disease identification, coconut tree, precision agriculture, AI-based

Summary:
DeepSeqCoco is a deep learning model designed for automatic disease identification in coconut trees. It outperforms existing models with up to 99.5% accuracy and reduces training and prediction times significantly. The model was tested with different optimizers, showing that the hybrid SGD-Adam configuration achieved the lowest validation loss of 2.81%. This suggests a promising future for AI-based disease monitoring systems in agriculture, particularly in developing countries where early diagnosis is crucial. By providing accurate and scalable disease identification, DeepSeqCoco has the potential to improve agricultural yield and efficiency. <div>
arXiv:2505.10030v1 Announce Type: new 
Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly in developing countries where conventional farming practices restrict early diagnosis and intervention. Current disease identification methods are manual, labor-intensive, and non-scalable. In response to these limitations, we come up with DeepSeqCoco, a deep learning based model for accurate and automatic disease identification from coconut tree images. The model was tested under various optimizer settings, such as SGD, Adam, and hybrid configurations, to identify the optimal balance between accuracy, minimization of loss, and computational cost. Results from experiments indicate that DeepSeqCoco can achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than existing models) with the hybrid SGD-Adam showing the lowest validation loss of 2.81%. It also shows a drop of up to 18% in training time and up to 85% in prediction time for input images. The results point out the promise of the model to improve precision agriculture through an AI-based, scalable, and efficient disease monitoring system.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis</title>
<link>https://arxiv.org/abs/2505.10046</link>
<guid>https://arxiv.org/abs/2505.10046</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image synthesis, large language models, diffusion transformers, multi-modal generation, empirical study

Summary: 
This paper delves into the design space of text-to-image synthesis by examining the fusion of large language models (LLMs) and diffusion transformers (DiTs). Unlike previous studies that focused on overall system performance without detailed comparisons or disclosed training methods, this work provides empirical insights through controlled comparisons with established baselines. It analyzes crucial design choices and offers a reproducible training recipe for achieving large-scale results. By addressing gaps in existing research and offering practical guidelines, this study aims to provide valuable data points and insights for future research in multi-modal generation.<br /><br />Summary: <div>
arXiv:2505.10046v1 Announce Type: new 
Abstract: This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field</title>
<link>https://arxiv.org/abs/2505.10049</link>
<guid>https://arxiv.org/abs/2505.10049</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic scene representation, neural radiance fields, 3D Gaussian splatting, differentiable volumetric rendering, motion representation paradigms

Summary: 
Dynamic scene representation and reconstruction have made significant advancements in recent years, driven by developments in neural radiance fields and 3D Gaussian splatting techniques. These methods have been adapted to address the complexities of 4D dynamic scenes, improving motion representation quality and scene reconstruction. The survey reviews over 200 papers on dynamic scene representation using radiance fields, categorizing works based on motion representation paradigms, reconstruction techniques, auxiliary information integration, and regularization approaches. By analyzing different methodologies, the survey aims to provide a comprehensive overview for researchers entering the field and offer experienced practitioners a systematic understanding of the principles and frontiers in dynamic scene reconstruction. Persistent challenges and future research directions are also highlighted in the survey. <br /><br />Summary: <div>
arXiv:2505.10049v1 Announce Type: new 
Abstract: Dynamic scene representation and reconstruction have undergone transformative advances in recent years, catalyzed by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques. While initially developed for static environments, these methodologies have rapidly evolved to address the complexities inherent in 4D dynamic scenes through an expansive body of research. Coupled with innovations in differentiable volumetric rendering, these approaches have significantly enhanced the quality of motion representation and dynamic scene reconstruction, thereby garnering substantial attention from the computer vision and graphics communities. This survey presents a systematic analysis of over 200 papers focused on dynamic scene representation using radiance field, spanning the spectrum from implicit neural representations to explicit Gaussian primitives. We categorize and evaluate these works through multiple critical lenses: motion representation paradigms, reconstruction techniques for varied scene dynamics, auxiliary information integration strategies, and regularization approaches that ensure temporal consistency and physical plausibility. We organize diverse methodological approaches under a unified representational framework, concluding with a critical examination of persistent challenges and promising research directions. By providing this comprehensive overview, we aim to establish a definitive reference for researchers entering this rapidly evolving field while offering experienced practitioners a systematic understanding of both conceptual principles and practical frontiers in dynamic scene reconstruction.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language</title>
<link>https://arxiv.org/abs/2505.10055</link>
<guid>https://arxiv.org/abs/2505.10055</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large Multimodal Models, Optical Character Recognition, Pashto language, PsOCR <br />
<br />
Summary: This paper evaluates the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. The study addresses challenges in Pashto NLP, such as the cursive script and limited datasets, by introducing the PsOCR synthetic dataset. With annotated images across various font families, colors, and layouts, PsOCR facilitates training and evaluating models like CNNs and Transformers. Testing seven open-source and four closed-source LMMs on a benchmark subset showed Gemini and Qwen-7B as top performers. The research emphasizes the assessment of LMM capabilities and limitations for Pashto OCR, laying groundwork for future studies in related scripts like Arabic and Urdu. PsOCR is accessible for further exploration. <br /> <div>
arXiv:2505.10055v1 Announce Type: new 
Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. Natural Language Processing (NLP) in Pashto faces several challenges due to the cursive nature of its script and a scarcity of structured datasets. To address this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one million images annotated with bounding boxes at word, line, and document levels, suitable for training and evaluating models based on different architectures, including Convolutional Neural Networks (CNNs) and Transformers. PsOCR covers variations across 1,000 unique font families, colors, image sizes, and layouts. A benchmark subset of 10K images was selected to evaluate the performance of several LMMs, including seven open-source models: DeepSeek's Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results demonstrate that Gemini achieves the best performance among all models, whereas among open-source models, Qwen-7B stands out. This work provides an insightful assessment of the capabilities and limitations of current LMMs for OCR tasks in Pashto and establishes a foundation for further research not only in Pashto OCR but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is available at https://github.com/zirak-ai/PashtoOCR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars</title>
<link>https://arxiv.org/abs/2505.10072</link>
<guid>https://arxiv.org/abs/2505.10072</guid>
<content:encoded><![CDATA[
<div> StyleGAN, 3D Gaussian blendshapes, facial image stylization, Toonify, animatable head avatars <br />
<br />Summary:
The article introduces a new framework called ToonifyGB that extends the use of Toonify for creating stylized 3D head avatars using Gaussian blendshapes. The framework consists of two stages: Stage 1 focuses on generating stylized videos from input frames using an improved StyleGAN, allowing for a more stable video output that captures high-frequency details. In Stage 2, a stylized neutral head model and expression blendshapes are learned from the generated video to efficiently render stylized avatars with various expressions. The effectiveness of ToonifyGB is validated on a benchmark dataset using two distinct styles  Arcane and Pixar. Overall, ToonifyGB enhances the capabilities of facial image stylization and animatable head avatar reconstruction using 3D Gaussian blendshapes. <div>
arXiv:2505.10072v1 Announce Type: new 
Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based framework, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1 (stylized video generation), we employ an improved StyleGAN to generate the stylized video from the input video frames, which addresses the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN. This process provides a more stable video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, and efficiently generate high-quality animation in the next stage. In Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head model and a set of expression blendshapes from the generated video. By combining the neutral head model with expression blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary expressions. We validate the effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane and Pixar.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10088</link>
<guid>https://arxiv.org/abs/2505.10088</guid>
<content:encoded><![CDATA[
<div> propose, Multi-Modal Representation Learning, adaptation, generalization, transfer learning
Summary:
Multi-Modal Representation Learning (MMRL) addresses the overfitting of large-scale pre-trained Vision-Language Models (VLMs) when adapting to limited few-shot data by introducing a shared representation space for text and image modalities. MMRL optimizes representation tokens in higher encoder layers for task-specific features, while preserving general knowledge in lower layers. Training involves joint optimization of class and representation features with a regularized alignment to the frozen VLM's zero-shot features. At inference, a decoupling strategy is used for base and novel tasks, leveraging class and representation features accordingly. MMRL++ builds on this by reducing trainable parameters and enhancing intra-modal interactions, leading to improved performance across datasets. Extensive experiments demonstrate the effectiveness of both MMRL and MMRL++ in achieving a balance between task-specific adaptation and generalization.<br /><br />Summary: <div>
arXiv:2505.10088v1 Announce Type: new 
Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this, we propose Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, modality-agnostic representation space. MMRL generates space tokens projected into both text and image encoders as representation tokens, enabling more effective cross-modal interactions. Unlike prior methods that mainly optimize class token features, MMRL inserts representation tokens into higher encoder layers--where task-specific features are more prominent--while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term aligning class and text features with the frozen VLM's zero-shot features. At inference, a decoupling strategy uses both class and representation features for base tasks, but only class features for novel tasks due to their stronger generalization. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces trainable parameters and enhances intra-modal interactions--particularly across the layers of representation tokens--allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering</title>
<link>https://arxiv.org/abs/2505.10118</link>
<guid>https://arxiv.org/abs/2505.10118</guid>
<content:encoded><![CDATA[
<div> visual token pruning, Hausdorff distance, multi-objective optimization, covering problem, scalability

Summary: 
This study introduces a novel approach, Multi-Objective Balanced Covering (MoB), to address the varying importance of prompt alignment and visual preservation in visual token pruning. By deriving a closed-form error bound based on the Hausdorff distance and leveraging epsilon-covering theory, the trade-off between these objectives is quantified and optimal levels are identified under a fixed budget. MoB reformulates token pruning as a bi-objective covering problem, allowing for a flexible budget allocation through greedy radius trading. The approach offers a provable performance bound and linear scalability, enabling efficient adaptation to challenging pruning scenarios. Experimental results demonstrate that MoB achieves a high preservation rate of performance for various tasks while significantly reducing the number of visual tokens. Integration with advanced models and diverse vision-language tasks further confirms the effectiveness and versatility of the proposed approach in enhancing model efficiency and performance. 

<br /><br />Summary: <div>
arXiv:2505.10118v1 Announce Type: new 
Abstract: Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance. To address this, we derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging $\epsilon$-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading. MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMITATE: Image Registration with Context for unknown time frame recovery</title>
<link>https://arxiv.org/abs/2505.10124</link>
<guid>https://arxiv.org/abs/2505.10124</guid>
<content:encoded><![CDATA[
<div> conditional U-Net architecture, image registration, 4D-CT scans, radiotherapy treatment, movement artefacts <br />
Summary: <br />
This paper presents a novel image registration formalism for estimating unknown condition-related images using known images and associated conditions. The formalism is implemented through a conditional U-Net architecture that incorporates conditional information without requiring fixed images. The formalism is applied to the challenging task of registering 4D-CT scans of moving tumors during radiotherapy treatment, particularly in thoracoabdominal regions. The approach minimizes reconstruction artefacts arising from irregular patient breathing and poor correlation of breathing signal to internal motion. Experimental results on clinical data demonstrate artifact-free volume reconstructions with real-time latencies. The publicly available code repository enables easy replication and further exploration of the proposed method. <div>
arXiv:2505.10124v1 Announce Type: new 
Abstract: In this paper, we formulate a novel image registration formalism dedicated to the estimation of unknown condition-related images, based on two or more known images and their associated conditions. We show how to practically model this formalism by using a new conditional U-Net architecture, which fully takes into account the conditional information and does not need any fixed image. Our formalism is then applied to image moving tumors for radiotherapy treatment at different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal regions. This driving application is particularly complex as it requires to stitch a collection of sequential 2D slices into several 3D volumes at different organ positions. Movement interpolation with standard methods then generates well known reconstruction artefacts in the assembled volumes due to irregular patient breathing, hysteresis and poor correlation of breathing signal to internal motion. Results obtained on 4D-CT clinical data showcase artefact-free volumes achieved through real-time latencies. The code is publicly available at https://github.com/Kheil-Z/IMITATE .
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2505.10152</link>
<guid>https://arxiv.org/abs/2505.10152</guid>
<content:encoded><![CDATA[
<div> style augmentation, federated domain generalization, multi-source collaborative, domain-invariant learning, data decentralization<br />
<br />
Summary:<br />
The article introduces a novel approach called Multi-source Collaborative Style Augmentation and Domain-invariant learning (MCSAD) for federated domain generalization. This method addresses the limitations of existing style augmentation techniques by incorporating a multi-source collaborative style augmentation module to explore a broader style space. Additionally, domain-invariant learning is conducted through cross-domain feature alignment within the same class and classes relation ensemble distillation between different classes. By iteratively enhancing data styles and ensuring domain invariance, the model can effectively generalize to unseen target domains. Experimental results demonstrate that MCSAD outperforms state-of-the-art federated domain generalization methods, showcasing its efficacy in improving generalization performance across multiple datasets. <br /> <div>
arXiv:2505.10152v1 Announce Type: new 
Abstract: Federated domain generalization aims to learn a generalizable model from multiple decentralized source domains for deploying on the unseen target domain. The style augmentation methods have achieved great progress on domain generalization. However, the existing style augmentation methods either explore the data styles within isolated source domain or interpolate the style information across existing source domains under the data decentralization scenario, which leads to limited style space. To address this issue, we propose a Multi-source Collaborative Style Augmentation and Domain-invariant learning method (MCSAD) for federated domain generalization. Specifically, we propose a multi-source collaborative style augmentation module to generate data in the broader style space. Furthermore, we conduct domain-invariant learning between the original data and augmented data by cross-domain feature alignment within the same class and classes relation ensemble distillation between different classes to learn a domain-invariant model. By alternatively conducting collaborative style augmentation and domain-invariant learning, the model can generalize well on unseen target domain. Extensive experiments on multiple domain generalization datasets indicate that our method significantly outperforms the state-of-the-art federated domain generalization methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Saliency Dataset Bias</title>
<link>https://arxiv.org/abs/2505.10169</link>
<guid>https://arxiv.org/abs/2505.10169</guid>
<content:encoded><![CDATA[
<div> Architecture, Saliency prediction, Dataset bias, Generalization gap, Multi-scale structure <br />
Summary:
Recent advances in image-based saliency prediction have achieved high performance levels on various benchmarks. However, predicting fixations across multiple saliency datasets remains challenging due to dataset bias, leading to a significant drop in performance when models are applied to different datasets. Increasing dataset diversity does not fully address this gap, with a majority of the difference attributed to dataset-specific biases. A novel architecture with a limited number of dataset-specific parameters has been proposed to address the generalization gap, showing significant improvement in performance with as few as 50 samples. The model sets a new state-of-the-art on multiple datasets, providing insights into spatial saliency properties and revealing complex multi-scale effects that combine absolute and relative sizes.<br /><br /> <div>
arXiv:2505.10169v1 Announce Type: new 
Abstract: Recent advances in image-based saliency prediction are approaching gold standard performance levels on existing benchmarks. Despite this success, we show that predicting fixations across multiple saliency datasets remains challenging due to dataset bias. We find a significant performance drop (around 40%) when models trained on one dataset are applied to another. Surprisingly, increasing dataset diversity does not resolve this inter-dataset gap, with close to 60% attributed to dataset-specific biases. To address this remaining generalization gap, we propose a novel architecture extending a mostly dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters that govern interpretable mechanisms such as multi-scale structure, center bias, and fixation spread. Adapting only these parameters to new data accounts for more than 75% of the generalization gap, with a large fraction of the improvement achieved with as few as 50 samples. Our model sets a new state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark (MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from unrelated datasets, but with a substantial boost when adapting to the respective training datasets. The model also provides valuable insights into spatial saliency properties, revealing complex multi-scale effects that combine both absolute and relative sizes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</title>
<link>https://arxiv.org/abs/2505.10205</link>
<guid>https://arxiv.org/abs/2505.10205</guid>
<content:encoded><![CDATA[
<div> framework, food volume estimation, mobile device-driven 3D reconstruction, AR-capable mobile devices, food video segmentation<br />
Summary:<br />
The article introduces VolE, a framework for accurate food volume estimation using mobile device-driven 3D reconstruction. Unlike existing methods that rely on single-purpose hardware or sensor-oriented information, VolE captures images and camera locations in free motion to generate precise 3D models. It is a reference- and depth-free framework that leverages food video segmentation to generate food masks for real-world measurement. Additionally, a new food dataset with challenging scenarios is introduced. Experimental results show that VolE outperforms existing techniques with a 2.22% Mean Absolute Percentage Error (MAPE), demonstrating its superior performance in food volume estimation. <div>
arXiv:2505.10205v1 Announce Type: new 
Abstract: Accurate food volume estimation is crucial for medical nutrition management and health monitoring applications, but current food volume estimation methods are often limited by mononuclear data, leveraging single-purpose hardware such as 3D scanners, gathering sensor-oriented information such as depth information, or relying on camera calibration using a reference object. In this paper, we present VolE, a novel framework that leverages mobile device-driven 3D reconstruction to estimate food volume. VolE captures images and camera locations in free motion to generate precise 3D models, thanks to AR-capable mobile devices. To achieve real-world measurement, VolE is a reference- and depth-free framework that leverages food video segmentation for food mask generation. We also introduce a new food dataset encompassing the challenging scenarios absent in the previous benchmarks. Our experiments demonstrate that VolE outperforms the existing volume estimation techniques across multiple datasets by achieving 2.22 % MAPE, highlighting its superior performance in food volume estimation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation</title>
<link>https://arxiv.org/abs/2505.10223</link>
<guid>https://arxiv.org/abs/2505.10223</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, data augmentation, MixUp, Auxiliary Fourier Augmentation, nnU-Net<br />
Summary:<br />
- Medical image segmentation models trained on curated datasets often perform poorly in real clinical settings due to distribution mismatches.
- Traditional data augmentation techniques may not be robust enough for diverse real-world scenarios.
- MixUp and Auxiliary Fourier Augmentation are evaluated as alternative augmentation strategies to improve generalization and robustness in medical image segmentation.
- These methods effectively enhance feature representations by promoting separability and compactness.
- Integration of these techniques into nnU-Net training pipelines offers an easy-to-implement solution for improving the reliability of medical segmentation models in real-world applications.<br /> 

Summary: <div>
arXiv:2505.10223v1 Announce Type: new 
Abstract: Medical image segmentation models are often trained on curated datasets, leading to performance degradation when deployed in real-world clinical settings due to mismatches between training and test distributions. While data augmentation techniques are widely used to address these challenges, traditional visually consistent augmentation strategies lack the robustness needed for diverse real-world scenarios. In this work, we systematically evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary Fourier Augmentation. These methods mitigate the effects of multiple variations without explicitly targeting specific sources of distribution shifts. We demonstrate how these techniques significantly improve out-of-distribution generalization and robustness to imaging variations across a wide range of transformations in cardiac cine MRI and prostate MRI segmentation. We quantitatively find that these augmentation methods enhance learned feature representations by promoting separability and compactness. Additionally, we highlight how their integration into nnU-Net training pipelines provides an easy-to-implement, effective solution for enhancing the reliability of medical segmentation models in real-world applications.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging</title>
<link>https://arxiv.org/abs/2505.10231</link>
<guid>https://arxiv.org/abs/2505.10231</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, medical imaging, fairness, human-AI alignment, generalization 

Summary:
Deep neural networks are highly successful in medical imaging, but they can exhibit biases that lead to fairness disparities among different demographic groups. This study delves into the concept of Human-AI alignment in the medical imaging domain, uncovering that integrating human insights can mitigate fairness gaps and improve generalization outside the training dataset. However, over-alignment can potentially result in performance trade-offs, underlining the importance of implementing balanced strategies. These findings emphasize the significance of Human-AI alignment in developing equitable, resilient, and versatile medical AI systems that strike a harmonious balance between human expertise and machine efficiency. The code for this study is openly accessible on the GitHub platform at https://github.com/Roypic/Aligner. 

<br /><br />Summary: 
- Deep neural networks are effective in medical imaging but can exhibit biases leading to fairness disparities.
- Incorporating human insights reduces fairness gaps and improves generalization in medical AI systems.
- Excessive alignment may result in performance trade-offs, emphasizing the need for calibrated strategies.
- Human-AI alignment is crucial for developing fair, robust, and generalizable medical AI systems.
- The study code is available for public access on GitHub. <div>
arXiv:2505.10231v1 Announce Type: new 
Abstract: Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</title>
<link>https://arxiv.org/abs/2505.10238</link>
<guid>https://arxiv.org/abs/2505.10238</guid>
<content:encoded><![CDATA[
<div> 3D motion sequences, human image animation, 4DMoT, MV-DiT, MTVCrafter <br />
<br />
Summary: <br />
The article presents MTVCrafter, a novel framework for human image animation that directly models raw 3D motion sequences. By using 4D motion tokens generated by 4DMoT, MTVCrafter offers robust spatio-temporal cues and flexible control, surpassing existing methods that rely on 2D-rendered pose images. The framework also introduces MV-DiT, which leverages motion tokens as context for human image animation in a 3D world. MTVCrafter achieves state-of-the-art results in pose-guided human video generation, with an FID-VID of 6.98, showing superior performance and generalization across diverse open-world characters and scenarios. This advancement marks a significant step forward in the field, opening up new possibilities for human image animation research and development. The code and video demos are available for further exploration and experimentation. <br /> <div>
arXiv:2505.10238v1 Announce Type: new 
Abstract: Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are provided in the supplementary material and at this anonymous GitHub link: https://anonymous.4open.science/r/MTVCrafter-1B13.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2505.10250</link>
<guid>https://arxiv.org/abs/2505.10250</guid>
<content:encoded><![CDATA[
<div> Keywords: Human mesh recovery, ADHMR, Preference optimization, HMR-Scorer, Probabilistic methods

Summary: 
ADHMR proposes a novel framework for human mesh recovery (HMR) from a single image. The framework utilizes a preference optimization approach to align a Diffusion-based HMR model, addressing depth ambiguity and occlusion issues. By training a human mesh prediction assessment model, HMR-Scorer, images can be evaluated without 3D annotations, enabling data cleaning and improving existing HMR models. The preference dataset created using HMR-Scorer facilitates the finetuning of the base model through direct preference optimization. ADHMR surpasses current state-of-the-art methods and demonstrates superior performance, particularly in handling in-the-wild images. The framework's code is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2505.10250v1 Announce Type: new 
Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to depth ambiguity and occlusions. Probabilistic methods have tried to solve this by generating numerous plausible 3D human mesh predictions, but they often exhibit misalignment with 2D image observations and weak robustness to in-the-wild images. To address these issues, we propose ADHMR, a framework that Aligns a Diffusion-based HMR model in a preference optimization manner. First, we train a human mesh prediction assessment model, HMR-Scorer, capable of evaluating predictions even for in-the-wild images without 3D annotations. We then use HMR-Scorer to create a preference dataset, where each input image has a pair of winner and loser mesh predictions. This dataset is used to finetune the base model using direct preference optimization. Moreover, HMR-Scorer also helps improve existing HMR models by data cleaning, even with fewer training samples. Extensive experiments show that ADHMR outperforms current state-of-the-art methods. Code is available at: https://github.com/shenwenhao01/ADHMR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot</title>
<link>https://arxiv.org/abs/2505.10257</link>
<guid>https://arxiv.org/abs/2505.10257</guid>
<content:encoded><![CDATA[
<div> alignment, generalist, driving agent, SAGE Deer, benchmark 
Summary:
Sage Deer is a new intelligent driving agent designed to cater to different users' comfort, interaction, and safety needs. It stands out in three key areas: super alignment, catering to different preferences and biases; being a generalist, understanding various inputs to reason out user behavior; and self-eliciting, being able to elicit implicit thought chains. The agent can analyze physiological indicators, facial emotions, hand and body movements, driving scenarios, and behavioral decisions. A large-scale benchmark was created to measure the agent's perceptual decision-making ability and accuracy in super alignment. Sage Deer aims to enhance the driving experience by providing personalized and safe interactions for users. 
<br /><br />Summary: <div>
arXiv:2505.10257v1 Announce Type: new 
Abstract: The intelligent driving cockpit, an important part of intelligent driving, needs to match different users' comfort, interaction, and safety needs. This paper aims to build a  Super-Aligned and GEneralist DRiving agent, SAGE DeeR. Sage Deer achieves three highlights: (1) Super alignment: It achieves different reactions according to different people's preferences and biases. (2) Generalist: It can understand the multi-view and multi-mode inputs to reason the user's physiological indicators, facial emotions, hand movements, body movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It can elicit implicit thought chains in the language space to further increase generalist and super-aligned abilities. Besides, we collected multiple data sets and built a large-scale benchmark. This benchmark measures the deer's perceptual decision-making ability and the super alignment's accuracy.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Driving Maps by Deep Learning-based Trail Map Extraction</title>
<link>https://arxiv.org/abs/2505.10258</link>
<guid>https://arxiv.org/abs/2505.10258</guid>
<content:encoded><![CDATA[
<div> Mapping, HD maps, autonomous driving systems, offline mapping, deep learning

Summary: 
- High-definition (HD) maps are essential for autonomous driving systems for accurate planning.
- Automated map creation methods have been developed to reduce manual labeling efforts.
- Online mapping has become popular for up-to-date map availability.
- Challenges such as temporal consistency and sensor occlusion persist in online mapping.
- A novel offline mapping approach integrating trail data has been proposed.
- The method uses transformer-based deep learning models for global map construction.
- Continuous updates are enabled while remaining sensor-agnostic, improving efficiency.
- Superior performance compared to online mapping approaches is demonstrated.
- Improved generalization to new environments and sensor configurations is achieved.
- Validation on benchmark datasets highlights the robustness and applicability of the approach. 

<br /><br />Summary: <div>
arXiv:2505.10258v1 Announce Type: new 
Abstract: High-definition (HD) maps offer extensive and accurate environmental information about the driving scene, making them a crucial and essential element for planning within autonomous driving systems. To avoid extensive efforts from manual labeling, methods for automating the map creation have emerged. Recent trends have moved from offline mapping to online mapping, ensuring availability and actuality of the utilized maps. While the performance has increased in recent years, online mapping still faces challenges regarding temporal consistency, sensor occlusion, runtime, and generalization. We propose a novel offline mapping approach that integrates trails - informal routes used by drivers - into the map creation process. Our method aggregates trail data from the ego vehicle and other traffic participants to construct a comprehensive global map using transformer-based deep learning models. Unlike traditional offline mapping, our approach enables continuous updates while remaining sensor-agnostic, facilitating efficient data transfer. Our method demonstrates superior performance compared to state-of-the-art online mapping approaches, achieving improved generalization to previously unseen environments and sensor configurations. We validate our approach on two benchmark datasets, highlighting its robustness and applicability in autonomous driving systems.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HandReader: Advanced Techniques for Efficient Fingerspelling Recognition</title>
<link>https://arxiv.org/abs/2505.10267</link>
<guid>https://arxiv.org/abs/2505.10267</guid>
<content:encoded><![CDATA[
<div> Keywords: fingerspelling recognition, sign language, HandReader, Temporal Shift-Adaptive Module, Temporal Pose Encoder

Summary:
HandReader introduces three architectures, including HandReader_RGB, HandReader_KP, and HandReader_RGB+KP, to improve fingerspelling recognition in sign language. HandReader_RGB utilizes a Temporal Shift-Adaptive Module to process RGB features effectively. HandReader_KP incorporates a Temporal Pose Encoder to operate on keypoints as tensors, capturing temporal and spatial information. HandReader_RGB+KP combines RGB and keypoint modalities for enhanced performance. The models achieve state-of-the-art results on the ChicagoFSWild and ChicagoFSWild+ datasets and demonstrate high accuracy on the Znaki dataset, the first open dataset for Russian fingerspelling. The Znaki dataset and pre-trained HandReader models are publicly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.10267v1 Announce Type: new 
Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the interpretation of proper names, characterized by fast hand movements during signing. Although previous works on fingerspelling recognition have focused on processing the temporal dimension of videos, there remains room for improving the accuracy of these approaches. This paper introduces HandReader, a group of three architectures designed to address the fingerspelling recognition task. HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to process RGB features from videos of varying lengths while preserving important sequential information. HandReader$_{KP}$ is built on the proposed Temporal Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition in a batch allows the encoder to pass them through 2D and 3D convolution layers, utilizing temporal and spatial information and accumulating keypoints coordinates. We also introduce HandReader_RGB+KP - architecture with a joint encoder to benefit from RGB and keypoint modalities. Each HandReader model possesses distinct advantages and achieves state-of-the-art results on the ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate high performance on the first open dataset for Russian fingerspelling, Znaki, presented in this paper. The Znaki dataset and HandReader pre-trained models are publicly available.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting</title>
<link>https://arxiv.org/abs/2505.10281</link>
<guid>https://arxiv.org/abs/2505.10281</guid>
<content:encoded><![CDATA[
<div> dataset, marine fog, deep learning, multi-regional, geostationary satellites

Summary:
The article introduces MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting. With annotations from 15 coastal fog-prone regions and six geostationary satellites, MFogHub includes over 68,000 high-resolution samples. By incorporating diverse regions and satellite perspectives, the dataset enables evaluation of detection and forecasting methods under varying conditions. Experiments with 16 baseline models illustrate MFogHub's ability to reveal generalization fluctuations due to regional and satellite differences, while also supporting the development of targeted fog prediction techniques. The dataset aims to advance practical monitoring and scientific understanding of marine fog dynamics globally. The dataset and code can be accessed at https://github.com/kaka0910/MFogHub. 

<br /><br />Summary: <div>
arXiv:2505.10281v1 Announce Type: new 
Abstract: Deep learning approaches for marine fog detection and forecasting have outperformed traditional methods, demonstrating significant scientific and practical importance. However, the limited availability of open-source datasets remains a major challenge. Existing datasets, often focused on a single region or satellite, restrict the ability to evaluate model performance across diverse conditions and hinder the exploration of intrinsic marine fog characteristics. To address these limitations, we introduce \textbf{MFogHub}, the first multi-regional and multi-satellite dataset to integrate annotated marine fog observations from 15 coastal fog-prone regions and six geostationary satellites, comprising over 68,000 high-resolution samples. By encompassing diverse regions and satellite perspectives, MFogHub facilitates rigorous evaluation of both detection and forecasting methods under varying conditions. Extensive experiments with 16 baseline models demonstrate that MFogHub can reveal generalization fluctuations due to regional and satellite discrepancy, while also serving as a valuable resource for the development of targeted and scalable fog prediction techniques. Through MFogHub, we aim to advance both the practical monitoring and scientific understanding of marine fog dynamics on a global scale. The dataset and code are at \href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2505.10289</link>
<guid>https://arxiv.org/abs/2505.10289</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositional Zero-Shot Learning, Multi-Stage Cross-modal Interaction, CLIP, fine-grained local features, attention weights 

Summary: 
The study introduces a Multi-Stage Cross-modal Interaction (MSCI) model to enhance Compositional Zero-Shot Learning (CZSL) by addressing the limitations of CLIP in capturing fine-grained local features. The MSCI model utilizes two self-adaptive aggregators to extract and integrate local and global visual information from CLIP's visual encoder. This allows for the progressive incorporation of key information into textual representations, improving the model's perception of fine-grained details. The MSCI model also dynamically adjusts attention weights between global and local visual information based on different combinations and elements, providing flexibility in adapting to diverse scenarios. Experimental results on three datasets demonstrate the effectiveness and superiority of the proposed model. The data and code for the MSCI model are available at the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2505.10289v1 Announce Type: new 
Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object combinations by leveraging known combinations. Existing studies basically rely on the cross-modal alignment capabilities of CLIP but tend to overlook its limitations in capturing fine-grained local features, which arise from its architectural and training paradigm. To address this issue, we propose a Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and utilizes intermediate-layer information from CLIP's visual encoder. Specifically, we design two self-adaptive aggregators to extract local information from low-level visual features and integrate global information from high-level visual features, respectively. These key information are progressively incorporated into textual representations through a stage-by-stage interaction mechanism, significantly enhancing the model's perception capability for fine-grained local visual information. Additionally, MSCI dynamically adjusts the attention weights between global and local visual information based on different combinations, as well as different elements within the same combination, allowing it to flexibly adapt to diverse scenarios. Experiments on three widely used datasets fully validate the effectiveness and superiority of the proposed model. Data and code are available at https://github.com/ltpwy/MSCI.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation</title>
<link>https://arxiv.org/abs/2505.10292</link>
<guid>https://arxiv.org/abs/2505.10292</guid>
<content:encoded><![CDATA[
<div> Dataset, visual storytelling, character identity, grounded stories, object consistency <br />
Summary: 
The article introduces StoryReasoning, a dataset containing structured scene analyses and grounded stories to address issues related to maintaining character identity and linking actions in visual storytelling systems. The dataset consists of 4,178 stories derived from 52,016 movie images and features explicit modeling of multi-frame relationships through structured tabular representations. The proposed approach includes cross-frame object re-identification, chain-of-thought reasoning for narrative modeling, and a grounding scheme linking textual elements to visual entities. By fine-tuning the Qwen2.5-VL 7B model to create Qwen Storyteller, end-to-end object detection, re-identification, and landmark detection are achieved while ensuring consistent object references throughout the story. Evaluation results show a significant reduction in hallucinations per story when compared to a non-fine-tuned model. <br /><br />Summary: <div>
arXiv:2505.10292v1 Announce Type: new 
Abstract: Visual storytelling systems struggle to maintain character identity across frames and link actions to appropriate subjects, frequently leading to referential hallucinations. These issues can be addressed through grounding of characters, objects, and other entities on the visual elements. We propose StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie images, with both structured scene analyses and grounded stories. Each story maintains character and object consistency across frames while explicitly modeling multi-frame relationships through structured tabular representations. Our approach features cross-frame object re-identification using visual similarity and face recognition, chain-of-thought reasoning for explicit narrative modeling, and a grounding scheme that links textual elements to visual entities across multiple frames. We establish baseline performance by fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end object detection, re-identification, and landmark detection while maintaining consistent object references throughout the story. Evaluation demonstrates a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when compared to a non-fine-tuned model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&amp;E Images using ViT Foundation Models</title>
<link>https://arxiv.org/abs/2505.10294</link>
<guid>https://arxiv.org/abs/2505.10294</guid>
<content:encoded><![CDATA[
<div> H&amp;E staining, multiplex immunofluorescence, cancer diagnosis, MIPHEI, ViT foundation models <br />
Summary: <br />
Histopathological analysis plays a crucial role in cancer diagnosis, with H&amp;E staining providing valuable information on cell morphology. However, multiplex immunofluorescence (mIF) offers more precise cell type identification but faces challenges in clinical adoption. To address this, MIPHEI integrates ViT foundation models to predict mIF signals from H&amp;E images, achieving accurate cell-type classification for various markers. The model outperforms baseline and random classifiers, demonstrating its ability to capture complex relationships between nuclear morphologies and molecular markers in H&amp;E images. MIPHEI shows promise in enabling cell-type-aware analysis of large-scale H&amp;E datasets, potentially leading to insights into spatial cellular organization and patient outcomes. <div>
arXiv:2505.10294v1 Announce Type: new 
Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with Hematoxylin and Eosin (H&amp;E) staining routinely acquired for every patient to visualize cell morphology and tissue architecture. On the other hand, multiplex immunofluorescence (mIF) enables more precise cell type identification via proteomic markers, but has yet to achieve widespread clinical adoption due to cost and logistical constraints. To bridge this gap, we introduce MIPHEI (Multiplex Immunofluorescence Prediction from H&amp;E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&amp;E images. MIPHEI targets a comprehensive panel of markers spanning nuclear content, immune lineages (T cells, B cells, myeloid), epithelium, stroma, vasculature, and proliferation. We train our model using the publicly available ORION dataset of restained H&amp;E and mIF images from colorectal cancer tissue, and validate it on two independent datasets. MIPHEI achieves accurate cell-type classification from H&amp;E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers. Our results indicate that our model effectively captures the complex relationships between nuclear morphologies in their tissue context, as visible in H&amp;E images and molecular markers defining specific cell types. MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&amp;E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability</title>
<link>https://arxiv.org/abs/2505.10351</link>
<guid>https://arxiv.org/abs/2505.10351</guid>
<content:encoded><![CDATA[
<div> membership inference, self-supervised learning, privacy concerns, PartCrop, defense methods <br />
Summary: <br />
This paper introduces a new method called PartCrop for performing membership inference on visual self-supervised models in a black-box setting. The method leverages the shared part-aware capability among models and stronger part response on the training data. Extensive attacks on self-supervised models with different training protocols and structures validate the effectiveness of PartCrop. Defense methods such as early stop, differential privacy, and shrinking crop scale range are evaluated and found to be effective against PartCrop attacks. Additionally, the impacts of scaling from both data and model aspects are quantitatively studied, leading to the proposal of a scalable PartCrop-v2 with structural improvements. Overall, the study addresses privacy concerns in self-supervised learning and provides practical solutions for defending against membership inference attacks. <div>
arXiv:2505.10351v1 Announce Type: new 
Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled data, but it also confronts significant privacy concerns, especially in vision. In this paper, we perform membership inference on visual self-supervised models in a more realistic setting: self-supervised training method and details are unknown for an adversary when attacking as he usually faces a black-box system in practice. In this setting, considering that self-supervised model could be trained by completely different self-supervised paradigms, e.g., masked image modeling and contrastive learning, with complex training details, we propose a unified membership inference method called PartCrop. It is motivated by the shared part-aware capability among models and stronger part response on the training data. Specifically, PartCrop crops parts of objects in an image to query responses within the image in representation space. We conduct extensive attacks on self-supervised models with different training protocols and structures using three widely used image datasets. The results verify the effectiveness and generalization of PartCrop. Moreover, to defend against PartCrop, we evaluate two common approaches, i.e., early stop and differential privacy, and propose a tailored method called shrinking crop scale range. The defense experiments indicate that all of them are effective. Finally, besides prototype testing on toy visual encoders and small-scale image datasets, we quantitatively study the impacts of scaling from both data and model aspects in a realistic scenario and propose a scalable PartCrop-v2 by introducing two structural improvements to PartCrop. Our code is at https://github.com/JiePKU/PartCrop.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity</title>
<link>https://arxiv.org/abs/2505.10352</link>
<guid>https://arxiv.org/abs/2505.10352</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, SpikeVideoFormer, spike-driven attention, video tasks, efficiency gains

Summary:
SpikeVideoFormer is introduced as an efficient spike-driven video Transformer with linear temporal complexity. It features a spike-driven Hamming attention (SDHA) that adapts traditional real-valued attention to spike-driven attention. Various spike-driven space-time attention designs are analyzed, identifying an optimal scheme for video tasks with linear temporal complexity. The model demonstrates strong generalization ability and efficiency in diverse video tasks such as classification, human pose tracking, and semantic segmentation. Empirical results show that SpikeVideoFormer achieves state-of-the-art performance compared to existing SNN approaches, with significant improvements in human pose tracking and semantic segmentation tasks. It matches the performance of recent ANN-based methods while offering significant efficiency gains, with up to 16x, 10x, and 5x improvements in the three tasks.

<br /><br />Summary: 
SpikeVideoFormer is an efficient spike-driven video Transformer with linear temporal complexity, featuring spike-driven Hamming attention for video tasks. It outperforms existing SNN approaches, achieving state-of-the-art results in various tasks like human pose tracking and semantic segmentation. The model matches recent ANN-based methods while significantly enhancing efficiency, with substantial improvements in performance across different video tasks. <div>
arXiv:2505.10352v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to Artificial Neural Networks (ANNs) in various vision tasks, while offering superior energy efficiency. However, existing SNN-based Transformers primarily focus on single-image tasks, emphasizing spatial features while not effectively leveraging SNNs' efficiency in video-based vision tasks. In this paper, we introduce SpikeVideoFormer, an efficient spike-driven video Transformer, featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design a spike-driven Hamming attention (SDHA) which provides a theoretically guided adaptation from traditional real-valued attention to spike-driven attention. Building on SDHA, we further analyze various spike-driven space-time attention designs and identify an optimal scheme that delivers appealing performance for video tasks, while maintaining only linear temporal complexity. The generalization ability and efficiency of our model are demonstrated across diverse downstream video tasks, including classification, human pose tracking, and semantic segmentation. Empirical results show our method achieves state-of-the-art (SOTA) performance compared to existing SNN approaches, with over 15\% improvement on the latter two tasks. Additionally, it matches the performance of recent ANN-based methods while offering significant efficiency gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the three tasks. https://github.com/JimmyZou/SpikeVideoFormer
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Lightweight Smartphone ISP with Unpaired Data</title>
<link>https://arxiv.org/abs/2505.10420</link>
<guid>https://arxiv.org/abs/2505.10420</guid>
<content:encoded><![CDATA[
<div> Keywords: Image Signal Processor, deep learning, smartphone camera, unpaired data, lightweight neural network

Summary:
This article introduces a novel training method for a learnable Image Signal Processor (ISP) in smartphone cameras, utilizing deep learning techniques. The traditional method of acquiring paired data for training is costly and challenging. The proposed approach eliminates the need for direct correspondences between raw images and ground-truth data by using unpaired data. The method utilizes a multi-term loss function and adversarial training with multiple discriminators to maintain content structure while learning color and texture characteristics. Lightweight neural network architectures suitable for mobile devices are used as backbones in the training process. The method is evaluated on Zurich RAW to RGB and Fujifilm UltraISP datasets and shows strong potential, achieving high fidelity across multiple evaluation metrics. The code and pre-trained models are made available for further research and development. <br /><br />Summary: <div>
arXiv:2505.10420v1 Announce Type: new 
Abstract: The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision language models have difficulty recognizing virtual objects</title>
<link>https://arxiv.org/abs/2505.10453</link>
<guid>https://arxiv.org/abs/2505.10453</guid>
<content:encoded><![CDATA[
<div> virtual objects, scene comprehension, vision language models, AI systems, multimodal input
Summary:<br /><br />Vision language models (VLMs) combine language and vision encoders to process multimodal input. While capable of tasks like automatic captioning, their comprehension of scene visuospatial properties remains uncertain. The study proposes using virtual objects in descriptions to test scene comprehension. By introducing virtual objects not visually depicted, such as a kite stuck in a tree in an image showing a person under a tree, VLMs should update their representations and reason spatial relations. The research systematically evaluates leading VLMs and finds their processing of virtual objects lacking. This highlights a gap in VLMs' scene comprehension abilities, indicating a need for further development to enhance their understanding of complex semantic tasks. <br /><br />Summary: <div>
arXiv:2505.10453v1 Announce Type: new 
Abstract: Vision language models (VLMs) are AI systems paired with both language and vision encoders to process multimodal input. They are capable of performing complex semantic tasks such as automatic captioning, but it remains an open question about how well they comprehend the visuospatial properties of scenes depicted in the images they process. We argue that descriptions of virtual objects -- objects that are not visually represented in an image -- can help test scene comprehension in these AI systems. For example, an image that depicts a person standing under a tree can be paired with the following prompt: imagine that a kite is stuck in the tree. VLMs that comprehend the scene should update their representations and reason sensibly about the spatial relations between all three objects. We describe systematic evaluations of state-of-the-art VLMs and show that their ability to process virtual objects is inadequate.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.10473</link>
<guid>https://arxiv.org/abs/2505.10473</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian splatting, rendering quality, quantity-quality trade-off, optimization, ControlGS

Summary: ControlGS is a new method for 3D Gaussian splatting that allows users to adjust the trade-off between the number of Gaussians used and rendering quality. It achieves consistent quantity-quality control across different scenes, from compact objects to large outdoor scenes, by automatically finding desirable trade-off points based on user-specified preferences. ControlGS outperforms existing methods by producing higher rendering quality with fewer Gaussians, making it more efficient in terms of storage and computational costs. It also offers stepless control over the trade-off, allowing users to fine-tune the balance according to their specific needs. Overall, ControlGS provides a flexible and intuitive way to optimize 3D Gaussian splatting for diverse practical requirements. 

<br /><br />Summary: ControlGS is a new method for 3D Gaussian splatting that enables users to adjust the trade-off between the number of Gaussians used and rendering quality. It achieves consistent quantity-quality control across different scenes, outperforming existing methods by producing higher rendering quality with fewer Gaussians. ControlGS offers stepless control over the trade-off and provides a flexible and intuitive way to optimize 3D Gaussian splatting for diverse practical requirements. <div>
arXiv:2505.10473v1 Announce Type: new 
Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks to minimize the number of Gaussians used while preserving high rendering quality, introducing an inherent trade-off between Gaussian quantity and rendering quality. Existing methods strive for better quantity-quality performance, but lack the ability for users to intuitively adjust this trade-off to suit practical needs such as model deployment under diverse hardware and communication constraints. Here, we present ControlGS, a 3DGS optimization method that achieves semantically meaningful and cross-scene consistent quantity-quality control while maintaining strong quantity-quality performance. Through a single training run using a fixed setup and a user-specified hyperparameter reflecting quantity-quality preference, ControlGS can automatically find desirable quantity-quality trade-off points across diverse scenes, from compact objects to large outdoor scenes. It also outperforms baselines by achieving higher rendering quality with fewer Gaussians, and supports a broad adjustment range with stepless control over the trade-off.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logos as a Well-Tempered Pre-train for Sign Language Recognition</title>
<link>https://arxiv.org/abs/2505.10481</link>
<guid>https://arxiv.org/abs/2505.10481</guid>
<content:encoded><![CDATA[
<div> Keywords: isolated sign language recognition, cross-language transfer learning, Logos dataset, visually similar sign groups, pre-trained models

Summary:
The paper addresses challenges in isolated sign language recognition, particularly the lack of data for individual sign languages and ambiguity in labeling similar signs with different meanings. To tackle these issues, the authors introduce the Logos dataset for Russian Sign Language, which is extensive and includes visually similar sign groups. They demonstrate that a model pretrained on Logos can serve as a universal encoder for various sign language recognition tasks, including few-shot learning. By exploring cross-language transfer learning, they show that joint training with multiple classification heads enhances accuracy on low-resource target datasets. Additionally, explicitly labeling visually similar signs improves model quality as a visual encoder for downstream tasks. The study surpasses current state-of-the-art results on the WLASL dataset and achieves competitive performance on the AUTSL dataset using a single-stream model processing RGB video. The source code, dataset, and pretrained models are publicly available. 

<br /><br />Summary: <div>
arXiv:2505.10481v1 Announce Type: new 
Abstract: This paper examines two aspects of the isolated sign language recognition (ISLR) task. First, despite the availability of a number of datasets, the amount of data for most individual sign languages is limited. It poses the challenge of cross-language ISLR model training, including transfer learning. Second, similar signs can have different semantic meanings. It leads to ambiguity in dataset labeling and raises the question of the best policy for annotating such signs. To address these issues, this study presents Logos, a novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by the number of signers and one of the largest available datasets while also the largest RSL dataset in size and vocabulary. It is shown that a model, pre-trained on the Logos dataset can be used as a universal encoder for other language SLR tasks, including few-shot learning. We explore cross-language transfer learning approaches and find that joint training using multiple classification heads benefits accuracy for the target lowresource datasets the most. The key feature of the Logos dataset is explicitly annotated visually similar sign groups. We show that explicitly labeling visually similar signs improves trained model quality as a visual encoder for downstream tasks. Based on the proposed contributions, we outperform current state-of-the-art results for the WLASL dataset and get competitive results for the AUTSL dataset, with a single stream model processing solely RGB video. The source code, dataset, and pre-trained models are publicly available.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2505.10483</link>
<guid>https://arxiv.org/abs/2505.10483</guid>
<content:encoded><![CDATA[
<div> evaluation framework, unified multimodal models, instruction-following capabilities, benchmark, UniBench

Summary:
The article introduces UniEval, an evaluation framework for unified multimodal models that aims to streamline the evaluation process by eliminating the need for extra models, images, or annotations. UniEval includes UniBench, a comprehensive benchmark with 81 diverse tags to assess model performance. The framework also introduces UniScore, a new metric that aligns closely with human evaluations and surpasses current metrics in evaluating instruction-following capabilities. Experimental results demonstrate that UniBench is more challenging than existing benchmarks and provides valuable insights into the strengths of state-of-the-art unified and visual generation models. Overall, UniEval offers a simplified and unified approach to evaluating multimodal models, enhancing their capabilities in instruction-following tasks. 

<br /><br />Summary: <div>
arXiv:2505.10483v1 Announce Type: new 
Abstract: The emergence of unified multimodal understanding and generation models is rapidly attracting attention because of their ability to enhance instruction-following capabilities while minimizing model redundancy. However, there is a lack of a unified evaluation framework for these models, which would enable an elegant, simplified, and overall evaluation. Current models conduct evaluations on multiple task-specific benchmarks, but there are significant limitations, such as the lack of overall results, errors from extra evaluation models, reliance on extensive labeled images, benchmarks that lack diversity, and metrics with limited capacity for instruction-following evaluation. To tackle these challenges, we introduce UniEval, the first evaluation framework designed for unified multimodal models without extra models, images, or annotations. This facilitates a simplified and unified evaluation process. The UniEval framework contains a holistic benchmark, UniBench (supports both unified and visual generation models), along with the corresponding UniScore metric. UniBench includes 81 fine-grained tags contributing to high diversity. Experimental results indicate that UniBench is more challenging than existing benchmarks, and UniScore aligns closely with human evaluations, surpassing current metrics. Moreover, we extensively evaluated SoTA unified and visual generation models, uncovering new insights into Univeral's unique values.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs</title>
<link>https://arxiv.org/abs/2505.10496</link>
<guid>https://arxiv.org/abs/2505.10496</guid>
<content:encoded><![CDATA[
<div> evaluation framework, synthetic chest radiograph generation, generative AI, medical domain, text-to-image architectures

Summary: 
The article introduces CheXGenBench, a comprehensive evaluation framework for synthetic chest radiograph generation that assesses fidelity, privacy risks, and clinical utility of leading text-to-image generative models in the medical domain. The framework addresses methodological inconsistencies, outdated comparisons, and disconnected assessment criteria that hinder evaluations in the medical AI field. It includes standardized data partitioning and a unified protocol with over 20 quantitative metrics to analyze generation quality, privacy vulnerabilities, and clinical applicability. The results highlight inefficiencies in current evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent comparisons. CheXGenBench establishes a benchmark for the medical AI community, enabling objective comparisons and facilitating integration of existing and future generative models. Additionally, a synthetic dataset, SynthCheX-75K, generated by the top-performing model (Sana 0.6B) is released to support further research in this area. The framework, models, and dataset are available at https://raman1121.github.io/CheXGenBench/.<br /><br />Summary: <div>
arXiv:2505.10496v1 Announce Type: new 
Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks</title>
<link>https://arxiv.org/abs/2505.10497</link>
<guid>https://arxiv.org/abs/2505.10497</guid>
<content:encoded><![CDATA[
arXiv:2505.10497v1 Announce Type: new 
Abstract: Face recognition has evolved significantly with the advancement of deep learning techniques, enabling its widespread adoption in various applications requiring secure authentication. However, this progress has also increased its exposure to presentation attacks, including face morphing, which poses a serious security threat by allowing one identity to impersonate another. Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face recognition with enhanced robustness to face morphing attacks. Our method modifies the classification task by introducing a dual-branch classification strategy that effectively handles the ambiguity in the labeling of face morphs. This adaptation allows the model to incorporate morph images into the training process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its effectiveness in enhancing robustness against face morphing attacks. Furthermore, our approach is universally applicable and can be integrated into existing face recognition training pipelines to improve classification-based recognition methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multi-Image Question Answering via Submodular Subset Selection</title>
<link>https://arxiv.org/abs/2505.10533</link>
<guid>https://arxiv.org/abs/2505.10533</guid>
<content:encoded><![CDATA[
arXiv:2505.10533v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have achieved high performance in vision-language tasks involving single image but they struggle when presented with a collection of multiple images (Multiple Image Question Answering scenario). These tasks, which involve reasoning over large number of images, present issues in scalability (with increasing number of images) and retrieval performance. In this work, we propose an enhancement for retriever framework introduced in MIRAGE model using submodular subset selection techniques. Our method leverages query-aware submodular functions, such as GraphCut, to pre-select a subset of semantically relevant images before main retrieval component. We demonstrate that using anchor-based queries and augmenting the data improves submodular-retriever pipeline effectiveness, particularly in large haystack sizes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis</title>
<link>https://arxiv.org/abs/2505.10541</link>
<guid>https://arxiv.org/abs/2505.10541</guid>
<content:encoded><![CDATA[
arXiv:2505.10541v1 Announce Type: new 
Abstract: Recent advancements have enhanced the capability of Multimodal Large Language Models (MLLMs) to comprehend multi-image information. However, existing benchmarks primarily evaluate answer correctness, overlooking whether models genuinely comprehend the visual input. To address this, we define implicit visual misunderstanding (IVM), where MLLMs provide correct answers without fully comprehending the visual input. Through our analysis, we decouple the visual and textual modalities within the causal attention module, revealing that attention distribution increasingly converges on the image associated with the correct answer as the network layers deepen. This insight leads to the introduction of a scale-agnostic metric, \textit{attention accuracy}, and a novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the model's visual understanding via internal mechanisms, remaining robust to positional biases for more reliable assessments. Furthermore, we extend our approach to finer granularities and demonstrate its effectiveness in unimodal scenarios, underscoring its versatility and generalizability.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data</title>
<link>https://arxiv.org/abs/2505.10551</link>
<guid>https://arxiv.org/abs/2505.10551</guid>
<content:encoded><![CDATA[
arXiv:2505.10551v1 Announce Type: new 
Abstract: With the development of photorealistic diffusion models, models trained in part or fully on synthetic data achieve progressively better results. However, diffusion models still routinely generate images that would not exist in reality, such as a dog floating above the ground or with unrealistic texture artifacts. We define the concept of feasibility as whether attributes in a synthetic image could realistically exist in the real-world domain; synthetic images containing attributes that violate this criterion are considered infeasible. Intuitively, infeasible images are typically considered out-of-distribution; thus, training on such images is expected to hinder a model's ability to generalize to real-world data, and they should therefore be excluded from the training set whenever possible. However, does feasibility really matter? In this paper, we investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture. We introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model. Our experiments show that feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Also, the attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Finally, mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.10557</link>
<guid>https://arxiv.org/abs/2505.10557</guid>
<content:encoded><![CDATA[
arXiv:2505.10557v1 Announce Type: new 
Abstract: Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at https://github.com/mathllm/MathCoder.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Vision Tokenizer Tuning</title>
<link>https://arxiv.org/abs/2505.10562</link>
<guid>https://arxiv.org/abs/2505.10562</guid>
<content:encoded><![CDATA[
arXiv:2505.10562v1 Announce Type: new 
Abstract: Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is agnostic to downstream tasks requiring varied representations and semantics. This decoupled paradigm introduces a critical misalignment: The loss of the vision tokenization can be the representation bottleneck for target tasks. For example, errors in tokenizing text in a given image lead to poor results when recognizing or generating them. To address this, we propose ETT, an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth Anything with Any Prior</title>
<link>https://arxiv.org/abs/2505.10565</link>
<guid>https://arxiv.org/abs/2505.10565</guid>
<content:encoded><![CDATA[
arXiv:2505.10565v1 Announce Type: new 
Abstract: This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-Fixup: Advancing Photo Editing with 3D Priors</title>
<link>https://arxiv.org/abs/2505.10566</link>
<guid>https://arxiv.org/abs/2505.10566</guid>
<content:encoded><![CDATA[
arXiv:2505.10566v1 Announce Type: new 
Abstract: Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative diffusion model surrogates for mechanistic agent-based biological models</title>
<link>https://arxiv.org/abs/2505.09630</link>
<guid>https://arxiv.org/abs/2505.09630</guid>
<content:encoded><![CDATA[
arXiv:2505.09630v1 Announce Type: cross 
Abstract: Mechanistic, multicellular, agent-based models are commonly used to investigate tissue, organ, and organism-scale biology at single-cell resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework for developing and interrogating these models. CPMs become computationally expensive at large space- and time- scales making application and investigation of developed models difficult. Surrogate models may allow for the accelerated evaluation of CPMs of complex biological systems. However, the stochastic nature of these models means each set of parameters may give rise to different model configurations, complicating surrogate model development. In this work, we leverage denoising diffusion probabilistic models to train a generative AI surrogate of a CPM used to investigate \textit{in vitro} vasculogenesis. We describe the use of an image classifier to learn the characteristics that define unique areas of a 2-dimensional parameter space. We then apply this classifier to aid in surrogate model selection and verification. Our CPM model surrogate generates model configurations 20,000 timesteps ahead of a reference configuration and demonstrates approximately a 22x reduction in computational time as compared to native code execution. Our work represents a step towards the implementation of DDPMs to develop digital twins of stochastic biological systems.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses</title>
<link>https://arxiv.org/abs/2505.09819</link>
<guid>https://arxiv.org/abs/2505.09819</guid>
<content:encoded><![CDATA[
arXiv:2505.09819v1 Announce Type: cross 
Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern recognition (PR) control systems that translate electromyography (EMG) signals into desired movements. As prosthesis movement complexity increases, users often struggle to produce sufficiently distinct EMG patterns for reliable classification. Existing training typically involves heuristic, trial-and-error user adjustments to static decoder boundaries. Goal: We introduce the Reviewer, a 3D visual interface projecting EMG signals directly into the decoder's classification space, providing intuitive, real-time insight into PR algorithm behavior. This structured feedback reduces cognitive load and fosters mutual, data-driven adaptation between user-generated EMG patterns and decoder boundaries. Methods: A 10-session study with 12 able-bodied participants compared PR performance after motor-based training and updating using the Reviewer versus conventional virtual arm visualization. Performance was assessed using a Fitts law task that involved the aperture of the cursor and the control of orientation. Results: Participants trained with the Reviewer achieved higher completion rates, reduced overshoot, and improved path efficiency and throughput compared to the standard visualization group. Significance: The Reviewer introduces decoder-informed motor training, facilitating immediate and consistent PR-based myoelectric control improvements. By iteratively refining control through real-time feedback, this approach reduces reliance on trial-and-error recalibration, enabling a more adaptive, self-correcting training framework. Conclusion: The 3D visual feedback significantly improves PR control in novice operators through structured training, enabling feedback-driven adaptation and reducing reliance on extensive heuristic adjustments.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions</title>
<link>https://arxiv.org/abs/2505.09831</link>
<guid>https://arxiv.org/abs/2505.09831</guid>
<content:encoded><![CDATA[
arXiv:2505.09831v1 Announce Type: cross 
Abstract: Hematoxylin and eosin (H&amp;E) staining is a gold standard for microscopic diagnosis in pathology. However, H&amp;E staining does not capture all the diagnostic information that may be needed. To obtain additional molecular information, immunohistochemical (IHC) stains highlight proteins that mark specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells. While IHC stains are vital for prognosis and treatment guidance, they are typically only available at specialized centers and time consuming to acquire, leading to treatment delays for patients. Virtual staining, enabled by deep learning-based image translation models, provides a promising alternative by computationally generating IHC stains from H&amp;E stained images. Although many GAN and diffusion based image to image (I2I) translation methods have been used for virtual staining, these models treat image patches as independent data points, which results in increased and more diverse data requirements for effective generation. We present ImplicitStainer, a novel approach that leverages local implicit functions to improve image translation, specifically virtual staining performance, by focusing on pixel-level predictions. This method enhances robustness to variations in dataset sizes, delivering high-quality results even with limited data. We validate our approach on two datasets using a comprehensive set of metrics and benchmark it against over fifteen state-of-the-art GAN- and diffusion based models. Full Code and models trained will be released publicly via Github upon acceptance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction</title>
<link>https://arxiv.org/abs/2505.09985</link>
<guid>https://arxiv.org/abs/2505.09985</guid>
<content:encoded><![CDATA[
arXiv:2505.09985v1 Announce Type: cross 
Abstract: Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT projection data into equal subsets and employs multi-subsets diffusion model (MSDM) to learn from each subset independently. This targeted learning approach reduces complexity and enhances the reconstruction of fine details. Furthermore, the integration of one-whole diffusion model (OWDM) with complete sinogram data acts as a global information constraint, which can reduce the possibility of generating erroneous or inconsistent sinogram information. Moreover, the OSMM's unsupervised learning framework provides strong robustness and generalizability, adapting seamlessly to varying sparsity levels of CT sinograms. This ensures consistent and reliable performance across different clinical scenarios. Experimental results demonstrate that OSMM outperforms traditional diffusion models in terms of image quality and noise resilience, offering a powerful and versatile solution for advanced CT imaging in sparse-view scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation</title>
<link>https://arxiv.org/abs/2505.10075</link>
<guid>https://arxiv.org/abs/2505.10075</guid>
<content:encoded><![CDATA[
arXiv:2505.10075v1 Announce Type: cross 
Abstract: This paper investigates training better visual world models for robot manipulation, i.e., models that can predict future visual observations by conditioning on past frames and robot actions. Specifically, we consider world models that operate on RGB-D frames (RGB-D world models). As opposed to canonical approaches that handle dynamics prediction mostly implicitly and reconcile it with visual rendering in a single model, we introduce FlowDreamer, which adopts 3D scene flow as explicit motion representations. FlowDreamer first predicts 3D scene flow from past frame and action conditions with a U-Net, and then a diffusion model will predict the future frame utilizing the scene flow. FlowDreamer is trained end-to-end despite its modularized nature. We conduct experiments on 4 different benchmarks, covering both video prediction and visual planning tasks. The results demonstrate that FlowDreamer achieves better performance compared to other baseline RGB-D world models by 7% on semantic similarity, 11% on pixel quality, and 6% on success rate in various robot manipulation domains.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality</title>
<link>https://arxiv.org/abs/2505.10144</link>
<guid>https://arxiv.org/abs/2505.10144</guid>
<content:encoded><![CDATA[
arXiv:2505.10144v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours</title>
<link>https://arxiv.org/abs/2505.10271</link>
<guid>https://arxiv.org/abs/2505.10271</guid>
<content:encoded><![CDATA[
arXiv:2505.10271v1 Announce Type: cross 
Abstract: We present a deep learning model for high-resolution probabilistic precipitation forecasting over an 8-hour horizon in Europe, overcoming the limitations of radar-only deep learning models with short forecast lead times. Our model efficiently integrates multiple data sources - including radar, satellite, and physics-based numerical weather prediction (NWP) - while capturing long-range interactions, resulting in accurate forecasts with robust uncertainty quantification through consistent probabilistic maps. Featuring a compact architecture, it enables more efficient training and faster inference than existing models. Extensive experiments demonstrate that our model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models, setting a new standard for high-resolution precipitation forecasting in Europe, ensuring a balance between accuracy, interpretability, and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.10312</link>
<guid>https://arxiv.org/abs/2505.10312</guid>
<content:encoded><![CDATA[
arXiv:2505.10312v1 Announce Type: cross 
Abstract: In the realm of Human Activity Recognition (HAR), obtaining high quality and variance data is still a persistent challenge due to high costs and the inherent variability of real-world activities. This study introduces a generation dataset by deep learning approaches (Attention Autoencoder and conditional Generative Adversarial Networks). Another problem that data heterogeneity is a critical challenge, one of the solutions is to shuffle the data to homogenize the distribution. Experimental results demonstrate that the random sequence strategy significantly improves classification performance, achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64 $\pm$ 0.01. For that, disrupting temporal dependencies through random sequence reordering compels the model to focus on instantaneous recognition, thereby improving robustness against activity transitions. This approach not only broadens the effective training dataset but also offers promising avenues for enhancing HAR systems in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding</title>
<link>https://arxiv.org/abs/2505.10405</link>
<guid>https://arxiv.org/abs/2505.10405</guid>
<content:encoded><![CDATA[
arXiv:2505.10405v1 Announce Type: cross 
Abstract: Generative semantic communication (Gen-SemCom) with large artificial intelligence (AI) model promises a transformative paradigm for 6G networks, which reduces communication costs by transmitting low-dimensional prompts rather than raw data. However, purely prompt-driven generation loses fine-grained visual details. Additionally, there is a lack of systematic metrics to evaluate the performance of Gen-SemCom systems. To address these issues, we develop a hybrid Gen-SemCom system with a critical information embedding (CIE) framework, where both text prompts and semantically critical features are extracted for transmissions. First, a novel approach of semantic filtering is proposed to select and transmit the semantically critical features of images relevant to semantic label. By integrating the text prompt and critical features, the receiver reconstructs high-fidelity images using a diffusion-based generative model. Next, we propose the generative visual information fidelity (GVIF) metric to evaluate the visual quality of the generated image. By characterizing the statistical models of image features, the GVIF metric quantifies the mutual information between the distorted features and their original counterparts. By maximizing the GVIF metric, we design a channel-adaptive Gen-SemCom system that adaptively control the volume of features and compression rate according to the channel state. Experimental results validate the GVIF metric's sensitivity to visual fidelity, correlating with both the PSNR and critical information volume. In addition, the optimized system achieves superior performance over benchmarking schemes in terms of higher PSNR and lower FID scores.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIF: Anomaly detection via preference embedding</title>
<link>https://arxiv.org/abs/2505.10441</link>
<guid>https://arxiv.org/abs/2505.10441</guid>
<content:encoded><![CDATA[
arXiv:2505.10441v1 Announce Type: cross 
Abstract: We address the problem of detecting anomalies with respect to structured patterns. To this end, we conceive a novel anomaly detection method called PIF, that combines the advantages of adaptive isolation methods with the flexibility of preference embedding. Specifically, we propose to embed the data in a high dimensional space where an efficient tree-based method, PI-Forest, is employed to compute an anomaly score. Experiments on synthetic and real datasets demonstrate that PIF favorably compares with state-of-the-art anomaly detection techniques, and confirm that PI-Forest is better at measuring arbitrary distances and isolate points in the preference space.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAL: Searching Expandable Architectures for Incremental Learning</title>
<link>https://arxiv.org/abs/2505.10457</link>
<guid>https://arxiv.org/abs/2505.10457</guid>
<content:encoded><![CDATA[
arXiv:2505.10457v1 Announce Type: cross 
Abstract: Incremental learning is a machine learning paradigm where a model learns from a sequential stream of tasks. This setting poses a key challenge: balancing plasticity (learning new tasks) and stability (preserving past knowledge). Neural Architecture Search (NAS), a branch of AutoML, automates the design of the architecture of Deep Neural Networks and has shown success in static settings. However, existing NAS-based approaches to incremental learning often rely on expanding the model at every task, making them impractical in resource-constrained environments. In this work, we introduce SEAL, a NAS-based framework tailored for data-incremental learning, a scenario where disjoint data samples arrive sequentially and are not stored for future access. SEAL adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric. Stability is preserved through cross-distillation training after each expansion step. The NAS component jointly searches for both the architecture and the optimal expansion policy. Experiments across multiple benchmarks demonstrate that SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods. These results highlight the promise of combining NAS and selective expansion for efficient, adaptive learning in incremental scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation</title>
<link>https://arxiv.org/abs/2505.10464</link>
<guid>https://arxiv.org/abs/2505.10464</guid>
<content:encoded><![CDATA[
arXiv:2505.10464v1 Announce Type: cross 
Abstract: Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities' anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68\% in the Dice score while maintaining solid robustness. The dataset and code are public via https://github.com/JeMing-creater/HWA-UNETR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-contrast laser endoscopy for in vivo gastrointestinal imaging</title>
<link>https://arxiv.org/abs/2505.10492</link>
<guid>https://arxiv.org/abs/2505.10492</guid>
<content:encoded><![CDATA[
arXiv:2505.10492v1 Announce Type: cross 
Abstract: White light endoscopy is the clinical gold standard for detecting diseases in the gastrointestinal tract. Most applications involve identifying visual abnormalities in tissue color, texture, and shape. Unfortunately, the contrast of these features is often subtle, causing many clinically relevant cases to go undetected. To overcome this challenge, we introduce Multi-contrast Laser Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable spectral, coherent, and directional illumination. We demonstrate three capabilities of MLE: enhancing tissue chromophore contrast with multispectral diffuse reflectance, quantifying blood flow using laser speckle contrast imaging, and characterizing mucosal topography using photometric stereo. We validate MLE with benchtop models, then demonstrate MLE in vivo during clinical colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold improvement in contrast and a five-fold improvement in color difference compared to white light and narrow band imaging. With the ability to reveal multiple complementary types of tissue contrast while seamlessly integrating into the clinical environment, MLE shows promise as an investigative tool to improve gastrointestinal imaging.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Token Prediction Needs Registers</title>
<link>https://arxiv.org/abs/2505.10518</link>
<guid>https://arxiv.org/abs/2505.10518</guid>
<content:encoded><![CDATA[
arXiv:2505.10518v1 Announce Type: cross 
Abstract: Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10526</link>
<guid>https://arxiv.org/abs/2505.10526</guid>
<content:encoded><![CDATA[
arXiv:2505.10526v1 Announce Type: cross 
Abstract: Speculative decoding significantly accelerates language model inference by enabling a lightweight draft model to propose multiple tokens that a larger target model verifies simultaneously. However, applying this technique to vision-language models (VLMs) presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context. We introduce Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models (MASSV), which transforms existing small language models into effective multimodal drafters through a two-phase approach. MASSV first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions. Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV provides a scalable, architecture-compatible method for accelerating both current and future VLMs.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Customization of Text-to-Vector Generation with Image Diffusion Priors</title>
<link>https://arxiv.org/abs/2505.10558</link>
<guid>https://arxiv.org/abs/2505.10558</guid>
<content:encoded><![CDATA[
arXiv:2505.10558v1 Announce Type: cross 
Abstract: Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Efficient 3D Human Pose Tracking from Events with Spiking Spatiotemporal Transformer</title>
<link>https://arxiv.org/abs/2303.09681</link>
<guid>https://arxiv.org/abs/2303.09681</guid>
<content:encoded><![CDATA[
arXiv:2303.09681v5 Announce Type: replace 
Abstract: Event camera, as an asynchronous vision sensor capturing scene dynamics, presents new opportunities for highly efficient 3D human pose tracking. Existing approaches typically adopt modern-day Artificial Neural Networks (ANNs), such as CNNs or Transformer, where sparse events are converted into dense images or paired with additional gray-scale images as input. Such practices, however, ignore the inherent sparsity of events, resulting in redundant computations, increased energy consumption, and potentially degraded performance. Motivated by these observations, we introduce the first sparse Spiking Neural Networks (SNNs) framework for 3D human pose tracking based solely on events. Our approach eliminates the need to convert sparse data to dense formats or incorporate additional images, thereby fully exploiting the innate sparsity of input events. Central to our framework is a novel Spiking Spatiotemporal Transformer, which enables bi-directional spatiotemporal fusion of spike pose features and provides a guaranteed similarity measurement between binary spike features in spiking attention. Moreover, we have constructed a large-scale synthetic dataset, SynEventHPD, that features a broad and diverse set of 3D human motions, as well as much longer hours of event streams. Empirical experiments demonstrate the superiority of our approach over existing state-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6% energy cost. Furthermore, our approach outperforms existing SNN-based benchmarks in this task, highlighting the effectiveness of our proposed SNN framework. The dataset will be released upon acceptance, and code can be found at https://github.com/JimmyZou/HumanPoseTracking_SNN.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Student Behavioral Engagement using Histogram of Actions</title>
<link>https://arxiv.org/abs/2307.09420</link>
<guid>https://arxiv.org/abs/2307.09420</guid>
<content:encoded><![CDATA[
arXiv:2307.09420v2 Announce Type: replace 
Abstract: In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition. The proposed approach recognizes student actions then predicts the student behavioral engagement level. For student action recognition, we use human skeletons to model student postures and upper body movements. To learn the dynamics of student upper body, a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies. This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged. To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with Multimodal Diffusion</title>
<link>https://arxiv.org/abs/2401.14066</link>
<guid>https://arxiv.org/abs/2401.14066</guid>
<content:encoded><![CDATA[
arXiv:2401.14066v3 Announce Type: replace 
Abstract: Although remarkable progress has been made in image style transfer, style is just one of the components of artistic paintings. Directly transferring extracted style features to natural images often results in outputs with obvious synthetic traces. This is because key painting attributes including layout, perspective, shape, and semantics often cannot be conveyed and expressed through style transfer. Large-scale pretrained text-to-image generation models have demonstrated their capability to synthesize a vast amount of high-quality images. However, even with extensive textual descriptions, it is challenging to fully express the unique visual properties and details of paintings. Moreover, generic models often disrupt the overall artistic effect when modifying specific areas, making it more complicated to achieve a unified aesthetic in artworks. Our main novel idea is to integrate multimodal semantic information as a synthesis guide into artworks, rather than transferring style to the real world. We also aim to reduce the disruption to the harmony of artworks while simplifying the guidance conditions. Specifically, we propose an innovative multi-task unified framework called CreativeSynth, based on the diffusion model with the ability to coordinate multimodal inputs. CreativeSynth combines multimodal features with customized attention mechanisms to seamlessly integrate real-world semantic content into the art domain through Cross-Art-Attention for aesthetic maintenance and semantic fusion. We demonstrate the results of our method across a wide range of different art categories, proving that CreativeSynth bridges the gap between generative models and artistic expression. Code and results are available at https://github.com/haha-lisa/CreativeSynth.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</title>
<link>https://arxiv.org/abs/2403.07547</link>
<guid>https://arxiv.org/abs/2403.07547</guid>
<content:encoded><![CDATA[
arXiv:2403.07547v2 Announce Type: replace 
Abstract: Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. To effectively handle this issue, we propose sequential motion understanding radiance fields (SMURF), a novel approach that models continuous camera motion and leverages the explicit volumetric representation method for robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a module designed to model a continuous camera movements for processing blurry inputs. Our model is evaluated against benchmark datasets and demonstrates state-of-the-art performance both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>https://arxiv.org/abs/2405.03689</link>
<guid>https://arxiv.org/abs/2405.03689</guid>
<content:encoded><![CDATA[
arXiv:2405.03689v2 Announce Type: replace 
Abstract: Language is often used to describe physical interaction, yet most 3D human pose estimation methods overlook this rich source of information. We bridge this gap by leveraging large multimodal models (LMMs) as priors for reconstructing contact poses, offering a scalable alternative to traditional methods that rely on human annotations or motion capture data. Our approach extracts contact-relevant descriptors from an LMM and translates them into tractable losses to constrain 3D human pose optimization. Despite its simplicity, our method produces compelling reconstructions for both two-person interactions and self-contact scenarios, accurately capturing the semantics of physical and social interactions. Our results demonstrate that LMMs can serve as powerful tools for contact prediction and pose estimation, offering an alternative to costly manual human annotations or motion capture data. Our code is publicly available at https://prosepose.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2406.02147</link>
<guid>https://arxiv.org/abs/2406.02147</guid>
<content:encoded><![CDATA[
arXiv:2406.02147v2 Announce Type: replace 
Abstract: 3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods are still in the early stages of development and lack systematic improvements, failing to track objects in certain complex scenarios, like occlusions and the small size of target object's situations. In this paper, we first summarize the current end-to-end 3D MOT framework by decomposing it into three constituent parts: query initialization, query propagation, and query matching. Then we propose corresponding improvements, which lead to a strong yet simple tracker: S2-Track. Specifically, for query initialization, we present 2D-Prompted Query Initialization, which leverages predicted 2D object and depth information to prompt an initial estimate of the object's 3D location. For query propagation, we introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty of complex environment in object prediction with probabilistic attention. For query matching, we propose a Hierarchical Query Denoising strategy to enhance training robustness and convergence. As a result, our S2-Track achieves state-of-the-art performance on nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st place on the nuScenes tracking task leaderboard.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Video Highlight Detection by Learning from Audio and Visual Recurrence</title>
<link>https://arxiv.org/abs/2407.13933</link>
<guid>https://arxiv.org/abs/2407.13933</guid>
<content:encoded><![CDATA[
arXiv:2407.13933v2 Announce Type: replace 
Abstract: With the exponential growth of video content, the need for automated video highlight detection to extract key moments or highlights from lengthy videos has become increasingly pressing. This technology has the potential to enhance user experiences by allowing quick access to relevant content across diverse domains. Existing methods typically rely either on expensive manually labeled frame-level annotations, or on a large external dataset of videos for weak supervision through category information. To overcome this, we focus on unsupervised video highlight detection, eliminating the need for manual annotations. We propose a novel unsupervised approach which capitalizes on the premise that significant moments tend to recur across multiple videos of the similar category in both audio and visual modalities. Surprisingly, audio remains under-explored, especially in unsupervised algorithms, despite its potential to detect key moments. Through a clustering technique, we identify pseudo-categories of videos and compute audio pseudo-highlight scores for each video by measuring the similarities of audio features among audio clips of all the videos within each pseudo-category. Similarly, we also compute visual pseudo-highlight scores for each video using visual features. Then, we combine audio and visual pseudo-highlights to create the audio-visual pseudo ground-truth highlight of each video for training an audio-visual highlight detection network. Extensive experiments and ablation studies on three benchmarks showcase the superior performance of our method over prior work.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models</title>
<link>https://arxiv.org/abs/2410.01262</link>
<guid>https://arxiv.org/abs/2410.01262</guid>
<content:encoded><![CDATA[
arXiv:2410.01262v3 Announce Type: replace 
Abstract: While many diffusion models perform well when controlling for particular aspect among style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper first introduces a novel training-free algorithm in fine-grained generation, Aggregation of Multiple Diffusion Models (AMDM), which integrates features from multiple diffusion models into a specified model to activate specific features and enable fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional control generation in diffusion models: We can fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: https://github.com/Hammour-steak/AMDM.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality</title>
<link>https://arxiv.org/abs/2410.16430</link>
<guid>https://arxiv.org/abs/2410.16430</guid>
<content:encoded><![CDATA[
arXiv:2410.16430v2 Announce Type: replace 
Abstract: Human hand and head movements are the most pervasive input modalities in extended reality (XR) and are significant for a wide range of applications. However, prior works on hand and head modelling in XR only explored a single modality or focused on specific applications. We present HaHeAE - a novel self-supervised method for learning generalisable joint representations of hand and head movements in XR. At the core of our method is an autoencoder (AE) that uses a graph convolutional network-based semantic encoder and a diffusion-based stochastic encoder to learn the joint semantic and stochastic representations of hand-head movements. It also features a diffusion-based decoder to reconstruct the original signals. Through extensive evaluations on three public XR datasets, we show that our method 1) significantly outperforms commonly used self-supervised methods by up to 74.0% in terms of reconstruction quality and is generalisable across users, activities, and XR environments, 2) enables new applications, including interpretable hand-head cluster identification and variable hand-head movement generation, and 3) can serve as an effective feature extractor for downstream tasks. Together, these results demonstrate the effectiveness of our method and underline the potential of self-supervised methods for jointly modelling hand-head behaviours in extended reality.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for View-Adaptive Rendering</title>
<link>https://arxiv.org/abs/2411.05731</link>
<guid>https://arxiv.org/abs/2411.05731</guid>
<content:encoded><![CDATA[
arXiv:2411.05731v3 Announce Type: replace 
Abstract: Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in real-time, high-quality 3D scene rendering. However, it faces several challenges, including Gaussian redundancy, limited ability to capture view-dependent effects, and difficulties in handling complex lighting and specular reflections. Additionally, methods that use spherical harmonics for color representation often struggle to effectively capture anisotropic components, especially when modeling view-dependent colors under complex lighting conditions, leading to insufficient contrast and unnatural color saturation. To address these limitations, we introduce PEP-GS, a perceptually-enhanced framework that dynamically predicts Gaussian attributes, including opacity, color, and covariance. We replace traditional spherical harmonics with a Hierarchical Granular-Structural Attention mechanism, which enables more accurate modeling of complex view-dependent color effects. By employing a stable and interpretable framework for opacity and covariance estimation, PEP-GS avoids the removal of essential Gaussians prematurely, ensuring a more accurate scene representation. Furthermore, perceptual optimization is applied to the final rendered images, enhancing perceptual consistency across different views and ensuring high-quality renderings with improved texture fidelity and fine-scale detail preservation. Experimental results demonstrate that PEP-GS outperforms state-of-the-art methods, particularly in challenging scenarios involving view-dependent effects and fine-scale details.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with Fused Geometric and Semantic Guidance</title>
<link>https://arxiv.org/abs/2411.08665</link>
<guid>https://arxiv.org/abs/2411.08665</guid>
<content:encoded><![CDATA[
arXiv:2411.08665v2 Announce Type: replace 
Abstract: OpenStreetMap (OSM), a rich and versatile source of volunteered geographic information (VGI), facilitates human self-localization and scene understanding by integrating nearby visual observations with vectorized map data. However, the disparity in modalities and perspectives poses a major challenge for effectively matching camera imagery with compact map representations, thereby limiting the full potential of VGI data in real-world localization applications.
  Inspired by the fact that the human brain relies on the fusion of geometric and semantic understanding for spatial localization tasks, we propose the OSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach based on first-person-view images against the OSM maps. It integrates semantic and geometric guidance to significantly improve accuracy, robustness, and generalization capability. First, we equip the OSMLoc with the visual foundational model to extract powerful image features. Second, a geometry-guided depth distribution adapter is proposed to bridge the monocular depth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings from the OSM data are utilized as auxiliary guidance for image-to-OSM feature matching. To validate the proposed OSMLoc, we collect a worldwide cross-area and cross-condition (CC) benchmark for extensive evaluation. Experiments on the MGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the superiority of our method. Code, pre-trained models, CC validation benchmark, and additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</title>
<link>https://arxiv.org/abs/2411.14347</link>
<guid>https://arxiv.org/abs/2411.14347</guid>
<content:encoded><![CDATA[
arXiv:2411.14347v3 Announce Type: replace 
Abstract: In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images</title>
<link>https://arxiv.org/abs/2412.16028</link>
<guid>https://arxiv.org/abs/2412.16028</guid>
<content:encoded><![CDATA[
arXiv:2412.16028v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has attracted significant attention for its high-quality novel view rendering, inspiring research to address real-world challenges. While conventional methods depend on sharp images for accurate scene reconstruction, real-world scenarios are often affected by defocus blur due to finite depth of field, making it essential to account for realistic 3D scene representation. In this study, we propose CoCoGaussian, a Circle of Confusion-aware Gaussian Splatting that enables precise 3D scene representation using only defocused images. CoCoGaussian addresses the challenge of defocus blur by modeling the Circle of Confusion (CoC) through a physically grounded approach based on the principles of photographic defocus. Exploiting 3D Gaussians, we compute the CoC diameter from depth and learnable aperture information, generating multiple Gaussians to precisely capture the CoC shape. Furthermore, we introduce a learnable scaling factor to enhance robustness and provide more flexibility in handling unreliable depth in scenes with reflective or refractive surfaces. Experiments on both synthetic and real-world datasets demonstrate that CoCoGaussian achieves state-of-the-art performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild</title>
<link>https://arxiv.org/abs/2412.16147</link>
<guid>https://arxiv.org/abs/2412.16147</guid>
<content:encoded><![CDATA[
arXiv:2412.16147v2 Announce Type: replace 
Abstract: Seagrass meadows play a crucial role in marine ecosystems, providing benefits such as carbon sequestration, water quality improvement, and habitat provision. Monitoring the distribution and abundance of seagrass is essential for environmental impact assessments and conservation efforts. However, the current manual methods of analyzing underwater video data to assess seagrass coverage are time-consuming and subjective. This work explores the use of deep learning models to automate the process of seagrass detection and coverage estimation from underwater video data. We create a new dataset of over 8,300 annotated underwater images, and subsequently evaluate several deep learning architectures, including ResNet, InceptionNetV3, DenseNet, and Vision Transformer for the task of binary classification on the presence and absence of seagrass by transfer learning. The results demonstrate that deep learning models, particularly Vision Transformers, can achieve high performance in predicting eelgrass presence, with AUROC scores exceeding 0.95 on the final test dataset. The application of underwater image enhancement further improved the models' prediction capabilities. Furthermore, we introduce a novel approach for estimating seagrass coverage from video data, showing promising preliminary results that align with expert manual labels, and indicating potential for consistent and scalable monitoring. The proposed methodology allows for the efficient processing of large volumes of video data, enabling the acquisition of much more detailed information on seagrass distributions in comparison to current manual methods. This information is crucial for environmental impact assessments and monitoring programs, as seagrasses are important indicators of coastal ecosystem health. This project demonstrates the value that deep learning can bring to the field of marine ecology and environmental monitoring.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illegal Waste Detection in Remote Sensing Images: A Case Study</title>
<link>https://arxiv.org/abs/2502.06607</link>
<guid>https://arxiv.org/abs/2502.06607</guid>
<content:encoded><![CDATA[
arXiv:2502.06607v3 Announce Type: replace 
Abstract: Environmental crime is the third largest criminal activity worldwide, with significant revenues coming from illegal management of solid waste. Thanks to the increasing availability and the decreasing cost of Very High Resolution Remote Sensing (VHR RS) images, the fight against environmental crime can nowadays rely on modern image-analysis tools to support photo-interpretation for scanning vast territories in search of illegal waste disposal sites. This paper illustrates a semi-automatic waste detection pipeline, developed in collaboration with a regional environmental protection agency, for detecting candidate illegal dumping sites in VHR RS images. To optimize the effectiveness of the waste detector, extensive experiments evaluate such design choices as the network architecture, the ground resolution and geographic span of the input images, as well as the pretraining procedures. The best model attains remarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A generalization study assesses the performance variation when the detector processes images from a territory substantially different from the one used during training, incurring only a moderate performance loss, i.e., 6.5% decrease in the F1-Score. Finally, an exercise in which photo interpreters compare the territory scanning effort with and without the support of the waste detector assesses the concrete benefit of using a computer-aided image analysis tool in a professional environment protection agency. Results show that a reduction up to 30% of the time spent for waste site detection can be attained.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoMamba: An Efficient Foundation Model for Endoscopic Videos via Hierarchical Pre-training</title>
<link>https://arxiv.org/abs/2502.19090</link>
<guid>https://arxiv.org/abs/2502.19090</guid>
<content:encoded><![CDATA[
arXiv:2502.19090v2 Announce Type: replace 
Abstract: Endoscopic video-based tasks, such as visual navigation and surgical phase recognition, play a crucial role in minimally invasive surgeries by providing real-time assistance. While recent video foundation models have shown promise, their applications are hindered by (1) computational inefficiencies and (2) suboptimal performance caused by limited data for pre-training in endoscopy. To address these issues, we present EndoMamba, a foundation model designed for real-time inference while learning generalized spatiotemporal representations. First, to mitigate computational inefficiencies, we propose the EndoMamba backbone, optimized for real-time inference. Inspired by recent advancements in state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial modeling within individual frames and vanilla Mamba blocks for past-to-present reasoning across the temporal domain. This design enables both strong spatiotemporal modeling and efficient inference in online video streams. Second, we propose a self-supervised hierarchical pre-training diagram to enhance EndoMamba's representation learning using endoscopic videos and incorporating general video domain knowledge. Specifically, our approach combines masked reconstruction with auxiliary supervision, leveraging low-level reconstruction to capture spatial-temporal structures and high-level alignment to transfer broader knowledge from a pretrained general-video domain foundation model. Extensive experiments on four downstream tasks--classification, segmentation, surgical phase recognition, and localization--demonstrate that EndoMamba outperforms existing foundation models and task-specific methods while maintaining real-time inference speed. The source code is available at https://github.com/TianCuteQY/EndoMamba.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model</title>
<link>https://arxiv.org/abs/2503.11792</link>
<guid>https://arxiv.org/abs/2503.11792</guid>
<content:encoded><![CDATA[
arXiv:2503.11792v2 Announce Type: replace 
Abstract: For 3D face modeling, the recently developed 3D-aware neural rendering methods are able to render photorealistic face images with arbitrary viewing directions. The training of the parametric controllable 3D-aware face models, however, still relies on a large-scale dataset that is lab-collected. To address this issue, this paper introduces "StyleMorpheus", the first style-based neural 3D Morphable Face Model (3DMM) that is trained on in-the-wild images. It inherits 3DMM's disentangled controllability (over face identity, expression, and appearance) but without the need for accurately reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder structure. The encoder aims at learning a representative disentangled parametric code space and the decoder improves the disentanglement using shape and appearance-related style codes in the different sub-modules of the network. Furthermore, we fine-tune the decoder through style-based generative adversarial learning to achieve photorealistic 3D rendering quality. The proposed style-based design enables StyleMorpheus to achieve state-of-the-art 3D-aware face reconstruction results, while also allowing disentangled control of the reconstructed face. Our model achieves real-time rendering speed, allowing its use in virtual reality applications. We also demonstrate the capability of the proposed style-based design in face editing applications such as style mixing and color editing. Project homepage: https://github.com/ubc-3d-vision-lab/StyleMorpheus.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets</title>
<link>https://arxiv.org/abs/2503.20291</link>
<guid>https://arxiv.org/abs/2503.20291</guid>
<content:encoded><![CDATA[
arXiv:2503.20291v2 Announce Type: replace 
Abstract: Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at intermediate resolution (4-8 {\AA}) is crucial in protein structure determination. Recent advances in deep learning have led to the development of automated approaches for enhancing experimental cryo-EM density maps. Yet, these methods are not optimized for intermediate-resolution maps and rely on map density features alone. To address this, we propose CryoSAMU, a novel method designed to enhance 3D cryo-EM density maps of protein structures using structure-aware multimodal U-Nets and trained on curated intermediate-resolution density maps. We comprehensively evaluate CryoSAMU across various metrics and demonstrate its competitive performance compared to state-of-the-art methods. Notably, CryoSAMU achieves significantly faster processing speed, showing promise for future practical applications. Our code is available at https://github.com/chenwei-zhang/CryoSAMU.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Image Compression with Dictionary-based Entropy Model</title>
<link>https://arxiv.org/abs/2504.00496</link>
<guid>https://arxiv.org/abs/2504.00496</guid>
<content:encoded><![CDATA[
arXiv:2504.00496v2 Announce Type: replace 
Abstract: Learned image compression methods have attracted great research interest and exhibited superior rate-distortion performance to the best classical image compression standards of the present. The entropy model plays a key role in learned image compression, which estimates the probability distribution of the latent representation for further entropy coding. Most existing methods employed hyper-prior and auto-regressive architectures to form their entropy models. However, they only aimed to explore the internal dependencies of latent representation while neglecting the importance of extracting prior from training data. In this work, we propose a novel entropy model named Dictionary-based Cross Attention Entropy model, which introduces a learnable dictionary to summarize the typical structures occurring in the training dataset to enhance the entropy model. Extensive experimental results have demonstrated that the proposed model strikes a better balance between performance and latency, achieving state-of-the-art results on various benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment</title>
<link>https://arxiv.org/abs/2504.02522</link>
<guid>https://arxiv.org/abs/2504.02522</guid>
<content:encoded><![CDATA[
arXiv:2504.02522v2 Announce Type: replace 
Abstract: The capacity of Vision transformers (ViTs) to handle variable-sized inputs is often constrained by computational complexity and batch processing limitations. Consequently, ViTs are typically trained on small, fixed-size images obtained through downscaling or cropping. While reducing computational burden, these methods result in significant information loss, negatively affecting tasks like image aesthetic assessment. We introduce Charm, a novel tokenization approach that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale information simultaneously. Charm prioritizes high-resolution details in specific regions while downscaling others, enabling shorter fixed-size input sequences for ViTs while incorporating essential information. Charm is designed to be compatible with pre-trained ViTs and their learned positional embeddings. By providing multiscale input and introducing variety to input tokens, Charm improves ViT performance and generalizability for image aesthetic assessment. We avoid cropping or changing the aspect ratio to further preserve information. Extensive experiments demonstrate significant performance improvements on various image aesthetic and quality assessment datasets (up to 8.1 %) using a lightweight ViT backbone. Code and pre-trained models are available at https://github.com/FBehrad/Charm.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment</title>
<link>https://arxiv.org/abs/2504.04722</link>
<guid>https://arxiv.org/abs/2504.04722</guid>
<content:encoded><![CDATA[
arXiv:2504.04722v2 Announce Type: replace 
Abstract: Tactile graphics are essential for providing access to visual information for the 43 million people globally living with vision loss. Traditional methods for creating these graphics are labor-intensive and cannot meet growing demand. We introduce TactileNet, the first comprehensive dataset and AI-driven framework for generating embossing-ready 2D tactile templates using text-to-image Stable Diffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes SD models to produce high-fidelity, guideline-compliant graphics while reducing computational costs. Quantitative evaluations with tactile experts show 92.86% adherence to accessibility standards. Structural fidelity analysis revealed near-human design similarity, with an SSIM of 0.538 between generated graphics and expert-designed tactile images. Notably, our method preserves object silhouettes better than human designs (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation of manual tactile abstraction. The framework scales to 32,000 images (7,050 high-quality) across 66 classes, with prompt editing enabling customizable outputs (e.g., adding or removing details). By automating the 2D template generation step-compatible with standard embossing workflows-TactileNet accelerates production while preserving design flexibility. This work demonstrates how AI can augment (not replace) human expertise to bridge the accessibility gap in education and beyond. Code, data, and models will be publicly released to foster further research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation</title>
<link>https://arxiv.org/abs/2504.05904</link>
<guid>https://arxiv.org/abs/2504.05904</guid>
<content:encoded><![CDATA[
arXiv:2504.05904v2 Announce Type: replace 
Abstract: Recent mainstream unsupervised video object segmentation (UVOS) motion-appearance approaches use either the bi-encoder structure to separately encode motion and appearance features, or the uni-encoder structure for joint encoding. However, these methods fail to properly balance the motion-appearance relationship. Consequently, even with complex fusion modules for motion-appearance integration, the extracted suboptimal features degrade the models' overall performance. Moreover, the quality of optical flow varies across scenarios, making it insufficient to rely solely on optical flow to achieve high-quality segmentation results. To address these challenges, we propose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which better balances the motion-appearance relationship and incorporates model's intrinsic saliency information to enhance segmentation performance. Specifically, considering that optical flow maps are derived from RGB images, they share both commonalities and differences. Accordingly, we propose a novel Trunk-Collateral structure for motion-appearance UVOS. The shared trunk backbone captures the motion-appearance commonality, while the collateral branch learns the uniqueness of motion features. Furthermore, an Intrinsic Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the model's intrinsic saliency information to refine high-level features, and provide pixel-level guidance for motion-appearance fusion, thereby enhancing performance without additional input. Experimental results show that SMTC-Net achieved state-of-the-art performance on three UVOS datasets ( 89.2% J&amp;F on DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video salient object detection (VSOD) benchmarks with the notable increase, demonstrating its effectiveness and superiority over previous methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Humans Subtle Differences with DIFFusion</title>
<link>https://arxiv.org/abs/2504.08046</link>
<guid>https://arxiv.org/abs/2504.08046</guid>
<content:encoded><![CDATA[
arXiv:2504.08046v2 Announce Type: replace 
Abstract: Scientific expertise often requires recognizing subtle visual differences that remain challenging to articulate even for domain experts. We present a system that leverages generative models to automatically discover and visualize minimal discriminative features between categories while preserving instance identity. Our method generates counterfactual visualizations with subtle, targeted transformations between classes, performing well even in domains where data is sparse, examples are unpaired, and category boundaries resist verbal description. Experiments across six domains, including black hole simulations, butterfly taxonomy, and medical imaging, demonstrate accurate transitions with limited training data, highlighting both established discriminative features and novel subtle distinctions that measurably improved category differentiation. User studies confirm our generated counterfactuals significantly outperform traditional approaches in teaching humans to correctly differentiate between fine-grained classes, showing the potential of generative models to advance visual learning and scientific research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada</title>
<link>https://arxiv.org/abs/2504.13231</link>
<guid>https://arxiv.org/abs/2504.13231</guid>
<content:encoded><![CDATA[
arXiv:2504.13231v2 Announce Type: replace 
Abstract: Rapid information access is vital during wildfires, yet traditional data sources are slow and costly. Social media offers real-time updates, but extracting relevant insights remains a challenge. We present WildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian wildfires, annotated across twelve key themes. Evaluating both vision-language models and custom-trained classifiers, we show that while zero-shot prompting offers quick deployment, even simple trained models outperform them when labelled data is available. Our best-performing transformer-based fine-tuned model reaches 83% f-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this model can be used to uncover trends during wildfires. Our findings highlight the enduring importance of tailored datasets and task-specific training. Importantly, such datasets should be localized, as disaster response requirements vary across regions and contexts.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</title>
<link>https://arxiv.org/abs/2405.18418</link>
<guid>https://arxiv.org/abs/2405.18418</guid>
<content:encoded><![CDATA[
arXiv:2405.18418v3 Announce Type: replace-cross 
Abstract: Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis: T1w MRI to Tau PET</title>
<link>https://arxiv.org/abs/2406.12632</link>
<guid>https://arxiv.org/abs/2406.12632</guid>
<content:encoded><![CDATA[
arXiv:2406.12632v2 Announce Type: replace-cross 
Abstract: There is a demand for medical image synthesis or translation to generate synthetic images of missing modalities from available data. This need stems from challenges such as restricted access to high-cost imaging devices, government regulations, or failure to follow up with patients or study participants. In medical imaging, preserving high-level semantic features is often more critical than achieving pixel-level accuracy. Perceptual loss functions are widely employed to train medical image synthesis or translation models, as they quantify differences in high-level image features using a pre-trained feature extraction network. While 3D and 2.5D perceptual losses are used in 3D medical image synthesis, they face challenges, such as the lack of pre-trained 3D models or difficulties in balancing loss reduction across different planes. In this work, we focus on synthesizing 3D tau PET images from 3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that sequentially computes the 2D average perceptual loss for each of the axial, coronal, and sagittal planes over epochs, with the cycle duration gradually decreasing. Additionally, we process tau PET images using by-manufacturer standardization to enhance the preservation of high-SUVR regions indicative of tau pathology and mitigate SUVR variability caused by inter-manufacturer differences. We combine the proposed loss with SSIM and MSE losses and demonstrate its effectiveness in improving both quantitative and qualitative performance across various generative models, including U-Net, UNETR, SwinUNETR, CycleGAN, and Pix2Pix.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Action Pretraining from Videos</title>
<link>https://arxiv.org/abs/2410.11758</link>
<guid>https://arxiv.org/abs/2410.11758</guid>
<content:encoded><![CDATA[
arXiv:2410.11758v2 Announce Type: replace-cross 
Abstract: We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI for ECG to CMR Translation Study</title>
<link>https://arxiv.org/abs/2411.13602</link>
<guid>https://arxiv.org/abs/2411.13602</guid>
<content:encoded><![CDATA[
arXiv:2411.13602v2 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of global mortality, necessitating accessible and accurate diagnostic tools. While cardiac magnetic resonance imaging (CMR) provides gold-standard insights into cardiac structure and function, its clinical utility is limited by high cost and complexity. In contrast, electrocardiography (ECG) is inexpensive and widely available but lacks the granularity of CMR. We propose CardioNets, a deep learning framework that translates 12-lead ECG signals into CMR-level functional parameters and synthetic images, enabling scalable cardiac assessment. CardioNets integrates cross-modal contrastive learning and generative pretraining, aligning ECG with CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via a masked autoregressive model. Trained on 159,819 samples from five cohorts, including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and externally validated on independent clinical datasets (n=3,767), CardioNets achieved strong performance across disease screening and phenotype estimation tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8% and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human physicians using both ECG and real CMR. These results suggest that CardioNets offers a promising, low-cost alternative to CMR for large-scale CVD screening, particularly in resource-limited settings. Future efforts will focus on clinical deployment and regulatory validation of ECG-based synthetic imaging.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Black box Adversarial Attacks</title>
<link>https://arxiv.org/abs/2411.16782</link>
<guid>https://arxiv.org/abs/2411.16782</guid>
<content:encoded><![CDATA[
arXiv:2411.16782v2 Announce Type: replace-cross 
Abstract: Adversarial examples usually exhibit good cross-model transferability, enabling attacks on black-box models with limited information about their architectures and parameters, which are highly threatening in commercial black-box scenarios. Model ensembling is an effective strategy to improve the transferability of adversarial examples by attacking multiple surrogate models. However, since prior studies usually adopt few models in the ensemble, there remains an open question of whether scaling the number of models can further improve black-box attacks. Inspired by the scaling law of large foundation models, we investigate the scaling laws of black-box adversarial attacks in this work. Through theoretical analysis and empirical evaluations, we conclude with clear scaling laws that using more surrogate models enhances adversarial transferability. Comprehensive experiments verify the claims on standard image classifiers, diverse defended models and multimodal large language models using various adversarial attack methods. Specifically, by scaling law, we achieve 90%+ transfer attack success rate on even proprietary models like GPT-4o. Further visualization indicates that there is also a scaling law on the interpretability and semantics of adversarial perturbations.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An unsupervised method for MRI recovery: Deep image prior with structured sparsity</title>
<link>https://arxiv.org/abs/2501.01482</link>
<guid>https://arxiv.org/abs/2501.01482</guid>
<content:encoded><![CDATA[
arXiv:2501.01482v2 Announce Type: replace-cross 
Abstract: Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Trust-Guided Approach to MR Image Reconstruction with Side Information</title>
<link>https://arxiv.org/abs/2501.03021</link>
<guid>https://arxiv.org/abs/2501.03021</guid>
<content:encoded><![CDATA[
arXiv:2501.03021v2 Announce Type: replace-cross 
Abstract: Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust- Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates</title>
<link>https://arxiv.org/abs/2504.08353</link>
<guid>https://arxiv.org/abs/2504.08353</guid>
<content:encoded><![CDATA[
arXiv:2504.08353v2 Announce Type: replace-cross 
Abstract: Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization</title>
<link>https://arxiv.org/abs/2505.03186</link>
<guid>https://arxiv.org/abs/2505.03186</guid>
<content:encoded><![CDATA[
arXiv:2505.03186v2 Announce Type: replace-cross 
Abstract: The inherent synchronization between a speaker's lip movements, voice, and the underlying linguistic content offers a rich source of information for improving speech processing tasks, especially in challenging conditions where traditional audio-only systems falter. We introduce CoGenAV, a powerful and data-efficient model designed to learn versatile audio-visual representations applicable across a wide range of speech and audio-visual tasks. CoGenAV is trained by optimizing a dual objective derived from natural audio-visual synchrony, contrastive feature alignment and generative text prediction, using only 223 hours of labeled data from the LRS2 dataset. This contrastive-generative synchronization strategy effectively captures fundamental cross-modal correlations. We showcase the effectiveness and versatility of the learned CoGenAV representations on multiple benchmarks. When utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these representations contribute to achieving a state-of-the-art Word Error Rate (WER) of 1.27. They also enable strong performance in Visual Speech Recognition (VSR) with a WER of 20.5 on LRS2, and significantly improve performance in noisy environments by over 70%. Furthermore, CoGenAV representations benefit speech reconstruction tasks, boosting performance in Speech Enhancement and Separation, and achieve competitive results in audio-visual synchronization tasks like Active Speaker Detection (ASD). Our model will be open-sourced to facilitate further development and collaboration within both academia and industry.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features</title>
<link>https://arxiv.org/abs/2505.08800</link>
<guid>https://arxiv.org/abs/2505.08800</guid>
<content:encoded><![CDATA[
<div> Keywords: railway safety, driver fatigue, behavior-based monitoring system, Directed-Graph Neural Network, vision-based technologies

Summary: 
A new study introduces an online behavior-based monitoring system using a custom Directed-Graph Neural Network (DGNN) to classify train driver states as alert, not alert, or pathological. Input representations were optimized through an ablation study comparing skeletal-only, facial-only, and combined features, with the combined features achieving the highest accuracy of 80.88% in the three-class model. The combination of facial and skeletal features also resulted in over 99% accuracy in binary alertness classification. A novel dataset was introduced that includes simulated pathological conditions, expanding the assessment of risks related to fatigue and health in train driver monitoring. This research represents a significant advancement in enhancing railway safety through the utilization of vision-based technologies for advanced online monitoring.

<br /><br />Summary: <div>
arXiv:2505.08800v1 Announce Type: new 
Abstract: Driver fatigue poses a significant challenge to railway safety, with traditional systems like the dead-man switch offering limited and basic alertness checks. This study presents an online behavior-based monitoring system utilizing a customised Directed-Graph Neural Network (DGNN) to classify train driver's states into three categories: alert, not alert, and pathological. To optimize input representations for the model, an ablation study was performed, comparing three feature configurations: skeletal-only, facial-only, and a combination of both. Experimental results show that combining facial and skeletal features yields the highest accuracy (80.88%) in the three-class model, outperforming models using only facial or skeletal features. Furthermore, this combination achieves over 99% accuracy in the binary alertness classification. Additionally, we introduced a novel dataset that, for the first time, incorporates simulated pathological conditions into train driver monitoring, broadening the scope for assessing risks related to fatigue and health. This work represents a step forward in enhancing railway safety through advanced online monitoring using vision-based technologies.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions</title>
<link>https://arxiv.org/abs/2505.08801</link>
<guid>https://arxiv.org/abs/2505.08801</guid>
<content:encoded><![CDATA[
<div> Skeletal model, OptiGait-LGBM model, RUET-GAIT dataset, gait recognition, real-world scenarios 
<br />
Summary: 
The paper introduces the OptiGait-LGBM model for gait recognition in uncontrolled outdoor environments using a skeletal model approach. It addresses challenges such as varying illumination and non-overlapping camera views by utilizing landmark positions and generating numerical datasets. The RUET-GAIT dataset is introduced to represent complex gait sequences in real-world scenarios. The model outperforms ensemble techniques like Random Forest and CatBoost in terms of accuracy, memory usage, and training time, providing a low-cost and memory-efficient solution. The approach aims to recognize person re-identification with minimal computational cost compared to existing methods, making it suitable for practical applications requiring gait authentication from a distance. 
<br /> <div>
arXiv:2505.08801v1 Announce Type: new 
Abstract: Gait recognition, known for its ability to identify individuals from a distance, has gained significant attention in recent times due to its non-intrusive verification. While video-based gait identification systems perform well on large public datasets, their performance drops when applied to real-world, unconstrained gait data due to various factors. Among these, uncontrolled outdoor environments, non-overlapping camera views, varying illumination, and computational efficiency are core challenges in gait-based authentication. Currently, no dataset addresses all these challenges simultaneously. In this paper, we propose an OptiGait-LGBM model capable of recognizing person re-identification under these constraints using a skeletal model approach, which helps mitigate inconsistencies in a person's appearance. The model constructs a dataset from landmark positions, minimizing memory usage by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to represent uncontrolled gait sequences in complex outdoor environments. The process involves extracting skeletal joint landmarks, generating numerical datasets, and developing an OptiGait-LGBM gait classification model. Our aim is to address the aforementioned challenges with minimal computational cost compared to existing methods. A comparative analysis with ensemble techniques such as Random Forest and CatBoost demonstrates that the proposed approach outperforms them in terms of accuracy, memory usage, and training time. This method provides a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction</title>
<link>https://arxiv.org/abs/2505.08808</link>
<guid>https://arxiv.org/abs/2505.08808</guid>
<content:encoded><![CDATA[
<div> sparse representation, HD map construction, geometric cues, semantic cues, centerline detection

Summary:
In this paper, the authors propose a novel approach to sparse HD map construction that rivals dense representations in efficiency and performance. They enhance sparse representation techniques with a dedicated network architecture for feature extraction, a sparse-dense segmentation auxiliary task to leverage geometric and semantic cues, and a denoising module guided by physical priors. Their method, SparseMeXt, achieves state-of-the-art results on the nuScenes dataset, with SparseMeXt-Large reaching an mAP of 68.9% at over 20 fps. By challenging the traditional reliance on dense representations, this work demonstrates the untapped potential of sparse methods in HD map construction and sets a new benchmark for efficiency-performance trade-offs in the field. <div>
arXiv:2505.08808v1 Announce Type: new 
Abstract: Recent advancements in high-definition \emph{HD} map construction have demonstrated the effectiveness of dense representations, which heavily rely on computationally intensive bird's-eye view \emph{BEV} features. While sparse representations offer a more efficient alternative by avoiding dense BEV processing, existing methods often lag behind due to the lack of tailored designs. These limitations have hindered the competitiveness of sparse representations in online HD map construction. In this work, we systematically revisit and enhance sparse representation techniques, identifying key architectural and algorithmic improvements that bridge the gap with--and ultimately surpass--dense approaches. We introduce a dedicated network architecture optimized for sparse map feature extraction, a sparse-dense segmentation auxiliary task to better leverage geometric and semantic cues, and a denoising module guided by physical priors to refine predictions. Through these enhancements, our method achieves state-of-the-art performance on the nuScenes dataset, significantly advancing HD map construction and centerline detection. Specifically, SparseMeXt-Tiny reaches a mean average precision \emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for sparse representations in HD map construction. These results underscore the untapped potential of sparse methods, challenging the conventional reliance on dense representations and redefining efficiency-performance trade-offs in the field.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian</title>
<link>https://arxiv.org/abs/2505.08811</link>
<guid>https://arxiv.org/abs/2505.08811</guid>
<content:encoded><![CDATA[
<div> Keywords: Underwater 3D scene reconstruction, Tensorized Underwater Gaussian Splatting (TUGS), Adaptive Medium Estimation (AME), light attenuation, backscatter effects

Summary:
TUGS addresses the challenges in underwater 3D scene reconstruction by effectively modeling complex interactions between object geometries and water media. It utilizes tensorized higher-order Gaussians and an adaptive medium estimation module to accurately simulate light attenuation and backscatter effects underwater. TUGS outperforms existing methods in rendering high-quality underwater images with faster speeds and less memory usage. The lightweight design of TUGS allows for efficient reconstruction using limited parameters, making it suitable for memory-constrained underwater UAV applications. Extensive experiments on real-world datasets validate the superior reconstruction quality achieved by TUGS. <br /><br />Summary: <div>
arXiv:2505.08811v1 Announce Type: new 
Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems. Therefore, we propose Tensorized Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction. TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments. Compared to other NeRF-based and GS-based methods designed for underwater, TUGS is able to render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Deep Learning Model in Image Recognition via Coverage Test</title>
<link>https://arxiv.org/abs/2505.08814</link>
<guid>https://arxiv.org/abs/2505.08814</guid>
<content:encoded><![CDATA[
<div> coverage metrics, neural networks, deep neural networks, security testing, empirical research

Summary:
This paper investigates the relationships and patterns of four coverage metrics - primary functionality, boundary, hierarchy, and structural coverage - in deep neural networks (DNNs). Empirical experiments were conducted using different DNN architectures (LeNet, VGG, ResNet) and models of varying depths (5 to 54 layers) to analyze the relationships between model depth, configuration information, and neural network coverage. Additionally, the relationships between modified decision/condition coverage and dataset size were explored. The study aims to contribute to the understanding of how different depths and configurations impact the coverage of neural networks and provide insights into improving the security testing of DNN models. Three potential future directions for research in this area are also proposed. 

<br /><br />Summary: <div>
arXiv:2505.08814v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards SFW sampling for diffusion models via external conditioning</title>
<link>https://arxiv.org/abs/2505.08817</link>
<guid>https://arxiv.org/abs/2505.08817</guid>
<content:encoded><![CDATA[
<div> Conditional Trajectory Correction, Multimodal Models, Contrastive Language Image Pre-training, NSFW Detection, Image Quality <br />
Summary: <br />
Score-based generative models (SBMs), known as diffusion models, are widely used for image generation but have faced challenges generating NSFW content. This study proposes a safe-for-work (SFW) sampler that utilizes external sources, such as multimodal models and CLIP, to guide SBMs away from undesired regions and prevent unsafe outputs. The SFW sampler incorporates a Conditional Trajectory Correction step to improve safety without compromising image quality. Results on the Stable Diffusion model demonstrate that the SFW sampler effectively reduces explicit content generation and aligns well with existing approaches for NSFW detection. The method allows for user-defined NSFW classes and minimally impacts images that do not require correction. Overall, the study highlights the potential of model-agnostic conditioning in enhancing SBM performance and ensuring the responsible use of image synthesis technology. <br /> <div>
arXiv:2505.08817v1 Announce Type: new 
Abstract: Score-based generative models (SBM), also known as diffusion models, are the de facto state of the art for image synthesis. Despite their unparalleled performance, SBMs have recently been in the spotlight for being tricked into creating not-safe-for-work (NSFW) content, such as violent images and non-consensual nudity. Current approaches that prevent unsafe generation are based on the models' own knowledge, and the majority of them require fine-tuning. This article explores the use of external sources for ensuring safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional Trajectory Correction step that guides the samples away from undesired regions in the ambient space using multimodal models as the source of conditioning. Furthermore, using Contrastive Language Image Pre-training (CLIP), our method admits user-defined NSFW classes, which can vary in different settings. Our experiments on the text-to-image SBM Stable Diffusion validate that the proposed SFW sampler effectively reduces the generation of explicit content while being competitive with other fine-tuning-based approaches, as assessed via independent NSFW detectors. Moreover, we evaluate the impact of the SFW sampler on image quality and show that the proposed correction scheme comes at a minor cost with negligible effect on samples not needing correction. Our study confirms the suitability of the SFW sampler towards aligned SBM models and the potential of using model-agnostic conditioning for the prevention of unwanted images.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.08833</link>
<guid>https://arxiv.org/abs/2505.08833</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, urban planning, satellite imagery, OpenStreetMap, controlled urban imagery generation

Summary: 
Generative AI, specifically a Stable Diffusion model with ControlNet, is utilized to automate urban planning by generating high-fidelity satellite imagery based on land use descriptions, infrastructure, and natural environments. Data integration from OpenStreetMap enhances the model's accuracy and diversity in creating realistic urban landscapes across different cities. The model's performance is evaluated using FID and KID scores, demonstrating robustness and quality in producing varied urban designs. By incorporating language prompts and control imagery, the model effectively captures design constraints and preferences, generating images preferred by urban planners and the general public. This research sets a benchmark for controlled urban imagery generation, showcasing the potential of generative AI in improving planning workflows and engaging the public.<br /><br />Summary: <div>
arXiv:2505.08833v1 Announce Type: new 
Abstract: Generative AI offers new opportunities for automating urban planning by creating site-specific urban layouts and enabling flexible design exploration. However, existing approaches often struggle to produce realistic and practical designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion model, extended with ControlNet, to generate high-fidelity satellite imagery conditioned on land use descriptions, infrastructure, and natural environments. To overcome data availability limitations, we spatially link satellite imagery with structured land use and constraint information from OpenStreetMap. Using data from three major U.S. cities, we demonstrate that the proposed diffusion model generates realistic and diverse urban landscapes by varying land-use configurations, road networks, and water bodies, facilitating cross-city learning and design diversity. We also systematically evaluate the impacts of varying language prompts and control imagery on the quality of satellite imagery generation. Our model achieves high FID and KID scores and demonstrates robustness across diverse urban contexts. Qualitative assessments from urban planners and the general public show that generated images align closely with design descriptions and constraints, and are often preferred over real images. This work establishes a benchmark for controlled urban imagery generation and highlights the potential of generative AI as a tool for enhancing planning workflows and public engagement.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd Scene Analysis using Deep Learning Techniques</title>
<link>https://arxiv.org/abs/2505.08834</link>
<guid>https://arxiv.org/abs/2505.08834</guid>
<content:encoded><![CDATA[
<div> crowd scene analysis, crowd counting, anomaly detection, self-supervised training, MultiColumn CNN

Summary: 
The research focuses on crowd scene analysis, specifically crowd counting and anomaly detection. To address the challenges of data annotation and model effectiveness, a novel approach combining self-supervised training and MultiColumn CNN is proposed for crowd counting. This enables the model to learn features at different levels, improving its performance on various crowd scene challenges. For anomaly detection, a spatiotemporal model based on VGG19 is developed, extracting spatial and temporal features using CNN and LSTM blocks. The model is designed for binary classification to detect normal or abnormal behaviors in crowds, with improved performance achieved by utilizing dense residual blocks instead of fully connected layers. Experimental evaluations on datasets such as the Hockey Fight dataset and SCVD dataset demonstrate the superiority of the proposed models over existing state-of-the-art approaches.<br /><br />Summary: <div>
arXiv:2505.08834v1 Announce Type: new 
Abstract: Our research is focused on two main applications of crowd scene analysis crowd counting and anomaly detection In recent years a large number of researches have been presented in the domain of crowd counting We addressed two main challenges in this domain 1 Deep learning models are datahungry paradigms and always need a large amount of annotated data for the training of algorithm It is timeconsuming and costly task to annotate such large amount of data Selfsupervised training is proposed to deal with this challenge 2 MCNN consists of multicolumns of CNN with different sizes of filters by presenting a novel approach based on a combination of selfsupervised training and MultiColumn CNN This enables the model to learn features at different levels and makes it effective in dealing with challenges of occluded scenes nonuniform density complex backgrounds and scale invariation The proposed model was evaluated on publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly detection addressing challenges like lighting environmental conditions unexpected objects and scalability The model extracts spatial and temporal features allowing it to be generalized to realworld scenes Spatial features are learned using CNN while temporal features are learned using LSTM blocks The model works on binary classification and can detect normal or abnormal behavior The models performance is improved by replacing fully connected layers with dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset show our models outperform other stateoftheart approaches
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Autonomous Driving: Frontiers and Opportunities</title>
<link>https://arxiv.org/abs/2505.08854</link>
<guid>https://arxiv.org/abs/2505.08854</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, autonomous driving, Level 5 autonomy, image generation, LiDAR

Summary: 
Generative Artificial Intelligence (GenAI) is a transformative technology revolutionizing industries with its content creation, reasoning, and planning capabilities. This survey explores the role of GenAI in achieving fully autonomous driving, particularly at Level 5 autonomy. It delves into the principles of generative modeling, including VAEs, GANs, and Large Language Models (LLMs), and their applications in image and LiDAR generation, decision making, and smart transportation networks. The survey also addresses challenges such as generalization, safety checks, regulatory compliance, and ethical concerns. Research plans are proposed to address theoretical assurances, trust metrics, transport integration, and socio-technical influence. This comprehensive review serves as a valuable resource for researchers, engineers, and policymakers navigating the intersection of generative AI and advanced autonomous mobility. An online repository of cited works is also available for reference. 

<br /><br />Summary: <div>
arXiv:2505.08854v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety</title>
<link>https://arxiv.org/abs/2505.08882</link>
<guid>https://arxiv.org/abs/2505.08882</guid>
<content:encoded><![CDATA[
<div> Keywords: transportation safety, road damage detection, potholes, cracks, deep learning

Summary: 
A new study introduces a system designed to enhance transportation safety by detecting road anomalies such as potholes and cracks. The system utilizes deep learning technology and Raspberry Pi to detect and classify various types of road damage, providing real-time data to authorities. It also warns nearby vehicles of potential hazards on the road, aiming to prevent accidents. The innovative solution can count road anomalies and transmit data to the cloud for further action. By proactively notifying authorities and drivers of road defects, the system seeks to mitigate accidents caused by road hazards. This comprehensive approach aims to improve overall road safety and create a safer environment for the community.<br /><br />Summary: <div>
arXiv:2505.08882v1 Announce Type: new 
Abstract: This study aims to improve transportation safety, especially traffic safety. Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. To tackle this problem and improve road safety, a comprehensive system has been developed to detect potholes, cracks (e.g. alligator, transverse, longitudinal), classify their sizes, and transmit this data to the cloud for appropriate action by authorities. The system also broadcasts warning signals to nearby vehicles warning them if a severe anomaly is detected on the road. Moreover, the system can count road anomalies in real-time. It is emulated through the utilization of Raspberry Pi, a camera module, deep learning model, laptop, and cloud service. Deploying this innovative solution aims to proactively enhance road safety by notifying relevant authorities and drivers about the presence of potholes and cracks to take actions, thereby mitigating potential accidents arising from this prevalent road hazard leading to safer road conditions for the whole community.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images</title>
<link>https://arxiv.org/abs/2505.08886</link>
<guid>https://arxiv.org/abs/2505.08886</guid>
<content:encoded><![CDATA[
<div> Keywords: skin cancer, artificial intelligence, diagnostic aids, image processing, machine learning

Summary: 
- The study addresses the growing concern over skin cancer rates and the need for advanced diagnostic tools due to limited awareness and expertise. 
- Artificial Intelligence (AI) shows promise in differentiating between malignant and benign skin lesions, but its integration into clinical practice is still developing. 
- The research utilizes a combination of image processing techniques and machine learning algorithms, specifically neuro-fuzzy and colonial competition methods. 
- By applying this approach to dermoscopic images from the ISIC database, the method achieved a high accuracy of 94% on a dataset of 560 images. 
- The results suggest that this methodology can significantly aid clinicians in the early detection of melanoma, thereby improving skin cancer diagnostics. 

<br /><br />Summary: <div>
arXiv:2505.08886v1 Announce Type: new 
Abstract: The rising incidence of skin cancer, coupled with limited public awareness and a shortfall in clinical expertise, underscores an urgent need for advanced diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool in this domain, particularly for distinguishing malignant from benign skin lesions. Leveraging publicly available datasets of skin lesions, researchers have been developing AI-based diagnostic solutions. However, the integration of such computer systems in clinical settings is still nascent. This study aims to bridge this gap by employing a fusion of image processing techniques and machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches. Applied to dermoscopic images from the ISIC database, our method achieved a notable accuracy of 94% on a dataset of 560 images. These results underscore the potential of our approach in aiding clinicians in the early detection of melanoma, thereby contributing significantly to skin cancer diagnostics.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems</title>
<link>https://arxiv.org/abs/2505.08909</link>
<guid>https://arxiv.org/abs/2505.08909</guid>
<content:encoded><![CDATA[
<div> PnP methods, deep denoisers, Poisson inverse problems, CoCo denoiser, Hamiltonian regularization, spectral regularization<br />
<br />
Summary:<br />
The article introduces a new approach for Plug-and-Play (PnP) methods in imaging problems using deep denoisers. Traditional PnP methods require strong convexity or smoothness in the fidelity term and a non-expansive denoiser, which may not be suitable for Poisson inverse problems. The proposed CoCo denoiser is conservative and may be expansive, leading to improved denoising performance. A novel training strategy incorporating Hamiltonian and spectral regularization is used to train the CoCo denoiser. The CoCo denoiser is proven to be the proximal operator of a weakly convex function, allowing for a restoration model with an implicit weakly convex prior. The global convergence of PnP methods to a stationary point of the restoration model is established. Experimental results demonstrate the superiority of the proposed approach in terms of visual quality and quantitative metrics. <div>
arXiv:2505.08909v1 Announce Type: new 
Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results in imaging problems. They typically require strong convexity or smoothness of the fidelity term and a (residual) non-expansive denoiser for convergence. These assumptions, however, are violated in Poisson inverse problems, and non-expansiveness can hinder denoising performance. To address these challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be (residual) expansive, leading to improved denoising. By leveraging the generalized Helmholtz decomposition, we introduce a novel training strategy that combines Hamiltonian regularization to promote conservativeness and spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser is a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior. The global convergence of PnP methods to a stationary point of this restoration model is established. Extensive experimental results demonstrate that our approach outperforms closely related methods in both visual quality and quantitative metrics.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind Maya: Building a Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2505.08910</link>
<guid>https://arxiv.org/abs/2505.08910</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Multilingual, Low-resource languages, Cultural contexts, Open-source 

Summary: 
Maya is introduced as an open-source Multilingual Vision-Language Model (VLM) to address the limitations faced by existing models. It focuses on improving performance on low-resource languages and varied cultural contexts, which have been overlooked in current VLMs. The key contributions of Maya include a multilingual image-text pretraining dataset in eight languages, as well as a multilingual image-text model that supports these languages. By leveraging the LLaVA pretraining dataset, Maya enhances cultural and linguistic comprehension in vision-language tasks. The code for Maya is available on GitHub, providing a valuable resource for researchers and developers working in the field of VLMs. This initiative aims to bridge the gap in performance between widely spoken languages and underrepresented linguistic groups, paving the way for more inclusive and diverse applications of VLM technology. 

<br /><br />Summary: <div>
arXiv:2505.08910v1 Announce Type: new 
Abstract: In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Channel Selection in Self-Attention For Person Re-Identification</title>
<link>https://arxiv.org/abs/2505.08961</link>
<guid>https://arxiv.org/abs/2505.08961</guid>
<content:encoded><![CDATA[
<div> Differentiable Channel Selection Attention module, DCS-Attention, self-attention, Information Bottleneck, Re-ID task 

Summary:<br />
The paper introduces the Differentiable Channel Selection Attention module, DCS-Attention, which selectively chooses informative channels during attention weight computation. It can be integrated with fixed or learnable backbones and is motivated by the Information Bottleneck principle. A novel variational upper bound for the IB loss is derived for training the networks with DCS-Attention. Experimental results on person Re-ID benchmarks using DCS-FB and DCS-DNAS demonstrate improved prediction accuracy for identifying person identities. The code for the work is available on GitHub. <div>
arXiv:2505.08961v1 Announce Type: new 
Abstract: In this paper, we propose a novel attention module termed the Differentiable Channel Selection Attention module, or the DCS-Attention module. In contrast with conventional self-attention, the DCS-Attention module features selection of informative channels in the computation of the attention weights. The selection of the feature channels is performed in a differentiable manner, enabling seamless integration with DNN training. Our DCS-Attention is compatible with either fixed neural network backbones or learnable backbones with Differentiable Neural Architecture Search (DNAS), leading to DCS with Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our DCS-Attention is motivated by the principle of Information Bottleneck (IB), and a novel variational upper bound for the IB loss, which can be optimized by SGD, is derived and incorporated into the training loss of the networks with the DCS-Attention modules. In this manner, a neural network with DCS-Attention modules is capable of selecting the most informative channels for feature extraction so that it enjoys state-of-the-art performance for the Re-ID task. Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention in learning discriminative features critical to identifying person identities. The code of our work is available at https://github.com/Statistical-Deep-Learning/DCS-Attention.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training</title>
<link>https://arxiv.org/abs/2505.08971</link>
<guid>https://arxiv.org/abs/2505.08971</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, pre-training, next-token prediction, importance sampling, scaling properties

Summary: 
PRIOR is a new approach for vision-language model pre-training that addresses the issue of fitting to noise and hallucination by prioritizing image-related tokens. This is achieved through the use of a reference model, a text-only language model trained on captions without images, to weight each token based on its probability for training the vision-language model. Tokens directly related to visual inputs are given lower probabilities by the text-only model, leading to a token-specific re-weighting in the training loss. PRIOR outperforms the standard next-token prediction approach by 19% and 8% on various benchmarks for LVLMs with and without visual encoders, respectively. Additionally, PRIOR demonstrates superior scaling properties, offering higher scaling coefficients that indicate greater potential for performance improvements with increasing compute and data. <br /><br />Summary: <div>
arXiv:2505.08971v1 Announce Type: new 
Abstract: In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking</title>
<link>https://arxiv.org/abs/2505.08999</link>
<guid>https://arxiv.org/abs/2505.08999</guid>
<content:encoded><![CDATA[
<div> Keywords: visual tracking, adversarial attacks, deep learning, meta-learning, transferability<br />
<br />
Summary: <br />
Visual tracking methods based on convolutional neural networks and Transformers have shown excellent performance but are vulnerable to security issues. Addressing these vulnerabilities is essential for real-world applications. The proposed adaptive meta-gradient adversarial attack (AMGA) method integrates multi-model ensembles and meta-learning strategies to enhance attack effectiveness. By combining momentum mechanisms and Gaussian smoothing, AMGA improves the transferability of adversarial examples across different models. The method randomly selects models and optimizes gradient directions to minimize the gap between white- and black-box attacks, achieving superior performance in black-box scenarios. Experimental results on datasets like OTB2015 and LaSOT demonstrate the efficacy of AMGA in improving attack performance, transferability, and deception of adversarial examples. The code and data for AMGA are available for further research and development. <br /> <div>
arXiv:2505.08999v1 Announce Type: new 
Abstract: In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction</title>
<link>https://arxiv.org/abs/2505.09018</link>
<guid>https://arxiv.org/abs/2505.09018</guid>
<content:encoded><![CDATA[
<div> CGM, deep learning, dietary monitoring, caloric estimation, multimodal sensing  
Summary:  
- The study addresses the challenge of accurately estimating caloric intake for Type 2 diabetes management using a new multimodal deep learning framework.  
- By combining CGM time-series data, Demographic/Microbiome data, and pre-meal food images, the model enhances caloric estimation accuracy.  
- The model incorporates attention-based encoding, convolutional feature extraction for meal imagery, and multi-layer perceptrons for CGM and microbiome data.  
- Evaluation on a dataset of 40 participants showed that the model achieved a Root Mean Squared Relative Error (RMSRE) of 0.2544, surpassing baseline models by over 50%.  
- The results demonstrate the potential of multimodal sensing in improving automated dietary assessment tools for chronic disease management.  

<br /><br />Summary: <div>
arXiv:2505.09018v1 Announce Type: new 
Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet accurately estimating caloric intake remains a major challenge. While continuous glucose monitors (CGMs) offer valuable physiological data, they often fall short in capturing the full nutritional profile of meals due to inter-individual and meal-specific variability. In this work, we introduce a multimodal deep learning framework that jointly leverages CGM time-series data, Demographic/Microbiome, and pre-meal food images to enhance caloric estimation. Our model utilizes attention based encoding and a convolutional feature extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome data followed by a late fusion strategy for joint reasoning. We evaluate our approach on a curated dataset of over 40 participants, incorporating synchronized CGM, Demographic and Microbiome data and meal photographs with standardized caloric labels. Our model achieves a Root Mean Squared Relative Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These findings demonstrate the potential of multimodal sensing to improve automated dietary assessment tools for chronic disease management.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition</title>
<link>https://arxiv.org/abs/2505.09073</link>
<guid>https://arxiv.org/abs/2505.09073</guid>
<content:encoded><![CDATA[
<div> Keywords: facial recognition, pose differences, domain adaptation, attention mapping, joint entropy loss

Summary:
This article addresses the challenge of pose differences affecting facial recognition performance by introducing a domain adaptive framework. The framework leverages shared attention mapping to highlight common patterns between 2D facial images and 3D facial data, improving pose invariance. Additionally, a joint entropy regularizing loss is used to enhance consistency between the intersecting 2D and 3D representations. The proposed framework outperforms existing methods on FaceScape and ARL-VTF datasets, showing significant improvements in profile (90) TAR @ 1% FAR. By emphasizing correlations and promoting consistency, this novel approach demonstrates the potential to enhance facial recognition performance across large pose differences. <div>
arXiv:2505.09073v1 Announce Type: new 
Abstract: Despite recent advances in facial recognition, there remains a fundamental issue concerning degradations in performance due to substantial perspective (pose) differences between enrollment and query (probe) imagery. Therefore, we propose a novel domain adaptive framework to facilitate improved performances across large discrepancies in pose by enabling image-based (2D) representations to infer properties of inherently pose invariant point cloud (3D) representations. Specifically, our proposed framework achieves better pose invariance by using (1) a shared (joint) attention mapping to emphasize common patterns that are most correlated between 2D facial images and 3D facial data and (2) a joint entropy regularizing loss to promote better consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D and 3D representations$\unicode{x2014}$by leveraging both attention maps. This framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$) TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and 1.57$\unicode{x0025}$, respectively.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions</title>
<link>https://arxiv.org/abs/2505.09092</link>
<guid>https://arxiv.org/abs/2505.09092</guid>
<content:encoded><![CDATA[
<div> dataset, Lane Keeping Assist, OpenLKA, driving data, vehicle models <br />
Summary: <br />
OpenLKA introduces a new dataset for evaluating Lane Keeping Assist (LKA) systems in real-world conditions. The dataset includes 400 hours of driving data from over 50 vehicle models, collected in various scenarios such as challenging road geometries, adverse weather, and traffic. It provides a multimodal approach, combining CAN bus streams, high-resolution dash-cam video, Openpilot outputs, and scene annotations for comprehensive analysis. By integrating vehicle-internal signals with perception and semantic context, OpenLKA aims to benchmark production LKA systems, identify critical operational scenarios, and evaluate road infrastructure for autonomous driving readiness. The dataset is open-access and available on GitHub for researchers and developers to use for LKA improvement and evaluation purposes. <br /> <div>
arXiv:2505.09092v1 Announce Type: new 
Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access. This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement. It includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing in Tampa, Florida and global contributions from the Comma.ai driving community. The dataset spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions and surrounding traffic. The dataset is multimodal, comprising: i) full CAN bus streams, decoded using custom reverse-engineered DBC files to extract key LKA events (e.g., system disengagements, lane detection failures); ii) synchronized high-resolution dash-cam video; iii) real-time outputs from Openpilot, providing accurate estimates of road curvature and lane positioning; iv) enhanced scene annotations generated by Vision Language Models, describing lane visibility, pavement quality, weather, lighting, and traffic conditions. By integrating vehicle-internal signals with high-fidelity perception and rich semantic context, OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving. The dataset is publicly available at: https://github.com/OpenLKA/OpenLKA.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning</title>
<link>https://arxiv.org/abs/2505.09118</link>
<guid>https://arxiv.org/abs/2505.09118</guid>
<content:encoded><![CDATA[
<div> interaction, scene graphs, vision-language models, reasoning, spatial relationships

Summary: 
The paper introduces Interaction-augmented Scene Graph Reasoning (ISGR) framework to enhance vision-language models' (VLMs) interactional reasoning capabilities. The framework addresses challenges in conventional scene graphs by improving relationship sets through a dual-stream graph constructor combining spatial relation extraction and interaction-aware captioning. It activates VLMs' knowledge of object functionalities through targeted interaction queries, enabling active reasoning about object interactions. Additionally, a long-term memory reinforcement learning strategy with specialized rewards enhances VLMs' ability to generalize interaction reasoning to new scenes. Experimental results show significant performance improvements, particularly on tasks requiring complex scene understanding. The ISGR framework provides a novel approach to enhancing interaction reasoning in VLMs, improving their ability to reason about complex interactions in visual scenes. <div>
arXiv:2505.09118v1 Announce Type: new 
Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting vision-language models' (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMs' interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMs' latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a lone-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at https://github.com/open_upon_acceptance.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance</title>
<link>https://arxiv.org/abs/2505.09123</link>
<guid>https://arxiv.org/abs/2505.09123</guid>
<content:encoded><![CDATA[
<div> Keywords: Big model, Camouflaged Object Detection, Segment Anything Model, Promotion Point Targeting Network, Key point selection

Summary:
This study introduces a new framework for Camouflaged Object Detection (COD) using the Segment Anything Model (SAM), which previous studies deemed unsuitable for this task. The researchers develop the Promotion Point Targeting Network (PPT-net) to predict camouflaged objects' presence at candidate points in the image using multi-scale features. They also implement a key point selection (KPS) algorithm to guide segmentation by promoting positive and negative points contrastively. This approach marks the first application of big models in COD and outperforms existing methods on three datasets across six metrics. By leveraging SAM as an off-the-shelf solution, the study demonstrates the advantages of using pre-existing models over designing new ones from scratch, making COD a less challenging task that only requires finding informative promotions rather than precise ones.

<br /><br />Summary: <div>
arXiv:2505.09123v1 Announce Type: new 
Abstract: Big model has emerged as a new research paradigm that can be applied to various down-stream tasks with only minor effort for domain adaption. Correspondingly, this study tackles Camouflaged Object Detection (COD) leveraging the Segment Anything Model (SAM). The previous studies declared that SAM is not workable for COD but this study reveals that SAM works if promoted properly, for which we devise a new framework to render point promotions: First, we develop the Promotion Point Targeting Network (PPT-net) to leverage multi-scale features in predicting the probabilities of camouflaged objects' presences at given candidate points over the image. Then, we develop a key point selection (KPS) algorithm to deploy both positive and negative point promotions contrastively to SAM to guide the segmentation. It is the first work to facilitate big model for COD and achieves plausible results experimentally over the existing methods on 3 data sets under 6 metrics. This study demonstrates an off-the-shelf methodology for COD by leveraging SAM, which gains advantage over designing professional models from scratch, not only in performance, but also in turning the problem to a less challenging task, that is, seeking informative but not exactly precise promotions.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</title>
<link>https://arxiv.org/abs/2505.09129</link>
<guid>https://arxiv.org/abs/2505.09129</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, surveillance video, color features, tactical mission, deployability <br />
<br />
Summary: 
This paper presents a lightweight anomaly detection framework for surveillance videos in high sensitivity tactical missions. The framework combines unsupervised KMeans clustering with RGB channel histogram modeling to detect structural anomalies and color mutation signals in key frames. The method was tested on operation surveillance video footage from an African country and successfully identified highly anomalous frames related to various threats. The approach demonstrated effectiveness in tactical assassination warning, object screening, and environmental monitoring without access to original data. The study highlights the significance of color features as carriers of low semantic battlefield signals and suggests future enhancements through graph neural networks and temporal modeling. The proposed framework offers deployability and tactical interpretation value in high-risk security tasks. <br /> <div>
arXiv:2505.09129v1 Announce Type: new 
Abstract: The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.09139</link>
<guid>https://arxiv.org/abs/2505.09139</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, object detection, prompt refinement, Contrastive Class Alignment Score, automatic prompt selection

Summary:
In this paper, the authors propose a method for improving object detection accuracy in vision-language models (VLMs) by automating prompt refinement. They introduce a novel metric called the Contrastive Class Alignment Score (CCAS), which evaluates the semantic alignment of prompts with target object classes while avoiding similarity to confounding classes. Using a large language model, diverse prompt candidates are generated and filtered through CCAS, computed with prompt embeddings from a sentence transformer. The evaluation on challenging object categories shows that the automated selection of high-precision prompts enhances object detection accuracy without requiring additional model training or labeled data. This scalable and model-agnostic pipeline offers a systematic approach to prompt engineering for VLM-based detection systems. 

<br /><br />Summary: <div>
arXiv:2505.09139v1 Announce Type: new 
Abstract: Vision-language models (VLMs) offer flexible object detection through natural language prompts but suffer from performance variability depending on prompt phrasing. In this paper, we introduce a method for automated prompt refinement using a novel metric called the Contrastive Class Alignment Score (CCAS), which ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes. Our method generates diverse prompt candidates via a large language model and filters them through CCAS, computed using prompt embeddings from a sentence transformer. We evaluate our approach on challenging object categories, demonstrating that our automatic selection of high-precision prompts improves object detection accuracy without the need for additional model training or labeled data. This scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation</title>
<link>https://arxiv.org/abs/2505.09140</link>
<guid>https://arxiv.org/abs/2505.09140</guid>
<content:encoded><![CDATA[
<div> Transformer, 3D point cloud generation, topology-aware, bottleneck structure, feature extraction

Summary:
Topology-Aware Diffusion Transformer (TopoDiT-3D) is proposed to enhance 3D point cloud generation by incorporating global topological information often overlooked in existing models. The bottleneck structure, utilizing Perceiver Resampler, integrates topological data extracted through persistent homology and filters redundant local features to improve training efficiency. TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency, highlighting the importance of rich topological information in 3D point cloud generation. The synergistic approach of incorporating topology-aware mechanisms with conventional local feature learning demonstrates improved performance in capturing complex geometries and maintaining shape consistency. The availability of videos and code on GitHub allows for further exploration and implementation of TopoDiT-3D in various applications. <div>
arXiv:2505.09140v1 Announce Type: new 
Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly improved 3D point cloud generation. However, existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries. To address this limitation, we propose TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure for 3D point cloud generation. Specifically, we design the bottleneck structure utilizing Perceiver Resampler, which not only offers a mode to integrate topological information extracted through persistent homology into feature learning, but also adaptively filters out redundant local features to improve training efficiency. Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning. Videos and code are available at https://github.com/Zechao-Guan/TopoDiT-3D.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection</title>
<link>https://arxiv.org/abs/2505.09155</link>
<guid>https://arxiv.org/abs/2505.09155</guid>
<content:encoded><![CDATA[
<div> segmentation, circuit schematics, net detection, AMSnet 2.0, positional information <br />
<br />
The paper introduces a novel net detection mechanism based on segmentation for understanding circuit schematics. This method improves upon existing techniques by providing high robustness and recovering positional information for digital reconstruction of schematics. The proposed approach addresses the limitations of current multimodal large language models (MLLMs) in recognizing circuit schematics, particularly in dealing with complex or noisy schematics. The authors expand the AMSnet dataset to create AMSnet 2.0, which includes 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and detailed positional information for circuit components and nets. This dataset enhancement enables the development of more accurate and reliable schematic parsing algorithms, facilitating improved understanding and analysis of circuit schematics for various applications. <br /><br />Summary: <div>
arXiv:2505.09155v1 Announce Type: new 
Abstract: Current multimodal large language models (MLLMs) struggle to understand circuit schematics due to their limited recognition capabilities. This could be attributed to the lack of high-quality schematic-netlist training data. Existing work such as AMSnet applies schematic parsing to generate netlists. However, these methods rely on hard-coded heuristics and are difficult to apply to complex or noisy schematics in this paper. We therefore propose a novel net detection mechanism based on segmentation with high robustness. The proposed method also recovers positional information, allowing digital reconstruction of schematics. We then expand AMSnet dataset with schematic images from various sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information for circuit components and nets, whereas AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2505.09168</link>
<guid>https://arxiv.org/abs/2505.09168</guid>
<content:encoded><![CDATA[
<div> Keywords: Camouflage Object Detection, DRRNet, Global Context, Local Details, Reverse Refinement

Summary:
DRRNet addresses the challenges in Camouflage Object Detection by introducing a four-stage architecture that focuses on capturing both global context and local details. The model includes an Omni-Context Feature Extraction Module to capture camouflage patterns and a Local Detail Extraction Module to enhance microstructural information. DRRNet integrates dual representations of scene understanding and structural awareness and utilizes a reverse refinement module to suppress background interference and enhance object boundaries. Through two rounds of inverse refinement, the model effectively improves the continuity of object boundaries. Experimental results show that DRRNet outperforms existing methods on benchmark datasets. The code for DRRNet is publicly available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2505.09168v1 Announce Type: new 
Abstract: The core challenge in Camouflage Object Detection (COD) lies in the indistinguishable similarity between targets and backgrounds in terms of color, texture, and shape. This causes existing methods to either lose edge details (such as hair-like fine structures) due to over-reliance on global semantic information or be disturbed by similar backgrounds (such as vegetation patterns) when relying solely on local features. We propose DRRNet, a four-stage architecture characterized by a "context-detail-fusion-refinement" pipeline to address these issues. Specifically, we introduce an Omni-Context Feature Extraction Module to capture global camouflage patterns and a Local Detail Extraction Module to supplement microstructural information for the full-scene context module. We then design a module for forming dual representations of scene understanding and structural awareness, which fuses panoramic features and local features across various scales. In the decoder, we also introduce a reverse refinement module that leverages spatial edge priors and frequency-domain noise suppression to perform a two-stage inverse refinement of the output. By applying two successive rounds of inverse refinement, the model effectively suppresses background interference and enhances the continuity of object boundaries. Experimental results demonstrate that DRRNet significantly outperforms state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/jerrySunning/DRRNet.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System</title>
<link>https://arxiv.org/abs/2505.09178</link>
<guid>https://arxiv.org/abs/2505.09178</guid>
<content:encoded><![CDATA[
<div> Efficiency, Adaptation Strategy, Plug-and-Play, Modular Architecture, Open-Source Platform
Summary:<br /><br />UniCAD is a unified architecture developed to simplify the creation and deployment of multi-task computer-aided diagnosis (CAD) systems for medical imaging. It leverages pre-trained vision models to handle both 2D and 3D medical images efficiently with minimal task-specific parameters. The key innovations of UniCAD are its low-rank adaptation strategy that achieves high performance with minimal trainable parameters and its modular architecture that allows for easy integration of different tasks and experts. The platform also provides an open-source environment for researchers to share and access lightweight CAD experts, fostering a more collaborative research ecosystem. Comprehensive experiments across 12 medical datasets show that UniCAD outperforms existing methods in terms of accuracy and deployment efficiency. The source code and project page for UniCAD are available at https://mii-laboratory.github.io/UniCAD/. <div>
arXiv:2505.09178v1 Announce Type: new 
Abstract: The growing complexity and scale of visual model pre-training have made developing and deploying multi-task computer-aided diagnosis (CAD) systems increasingly challenging and resource-intensive. Furthermore, the medical imaging community lacks an open-source CAD platform to enable the rapid creation of efficient and extendable diagnostic models. To address these issues, we propose UniCAD, a unified architecture that leverages the robust capabilities of pre-trained vision foundation models to seamlessly handle both 2D and 3D medical images while requiring only minimal task-specific parameters. UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation strategy is employed to adapt a pre-trained visual model to the medical image domain, achieving performance on par with fully fine-tuned counterparts while introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular architecture that combines a frozen foundation model with multiple plug-and-play experts, enabling diverse tasks and seamless functionality expansion. Building on this unified CAD architecture, we establish an open-source platform where researchers can share and access lightweight CAD experts, fostering a more equitable and efficient research ecosystem. Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency. The source code and project page are available at https://mii-laboratory.github.io/UniCAD/.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Quantization: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.09188</link>
<guid>https://arxiv.org/abs/2505.09188</guid>
<content:encoded><![CDATA[
<div> Zero-shot Quantization, deep learning models, network quantization, data generation strategies, privacy constraints

Summary:
Zero-shot Quantization (ZSQ) is a method for reducing memory and computational demands of deep learning models without requiring access to training data. This approach is beneficial for deployment on resource-constrained devices in scenarios where data privacy, security, or regulatory constraints limit data access. The paper provides a formal definition of the ZSQ problem and categorizes existing ZSQ methods based on data generation strategies. It highlights the key challenges, motivations, core ideas, and key takeaways of these methods, offering a comprehensive overview of the field. The paper also suggests future research directions to address remaining limitations and enhance ZSQ techniques. This survey is the first to delve into ZSQ methods, presenting a valuable resource for researchers and practitioners in the field. 

<br /><br />Summary: <div>
arXiv:2505.09188v1 Announce Type: new 
Abstract: Network quantization has proven to be a powerful approach to reduce the memory and computational demands of deep learning models for deployment on resource-constrained devices. However, traditional quantization methods often rely on access to training data, which is impractical in many real-world scenarios due to privacy, security, or regulatory constraints. Zero-shot Quantization (ZSQ) emerges as a promising solution, achieving quantization without requiring any real data. In this paper, we provide a comprehensive overview of ZSQ methods and their recent advancements. First, we provide a formal definition of the ZSQ problem and highlight the key challenges. Then, we categorize the existing ZSQ methods into classes based on data generation strategies, and analyze their motivations, core ideas, and key takeaways. Lastly, we suggest future research directions to address the remaining limitations and advance the field of ZSQ. To the best of our knowledge, this paper is the first in-depth survey on ZSQ.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2505.09196</link>
<guid>https://arxiv.org/abs/2505.09196</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-light image enhancement, gene effect, parameter dynamic evolution, neural network models, computational photography 

Summary: 
The paper introduces the concept of the gene effect in low-light image enhancement, where randomly resetting certain parameters can unexpectedly improve performance. This phenomenon limits the effectiveness of complex neural network models as random parameters sometimes outperform learned ones. The authors attribute the gene effect to static parameters, which hinder adaptability to different image environments. Inspired by biological evolution, they propose parameter dynamic evolution (PDE) as a solution. PDE simulates gene mutation and recombination through parameter orthogonal generation, allowing models to adapt and mitigate the gene effect. Experimental results confirm the effectiveness of PDE in enhancing image quality. The code for the proposed techniques will be made available to the public. 

<br /><br />Summary: <div>
arXiv:2505.09196v1 Announce Type: new 
Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational photography, aiming to improve illumination, reduce noise, and enhance image quality. While recent advancements focus on designing increasingly complex neural network models, we observe a peculiar phenomenon: resetting certain parameters to random values unexpectedly improves enhancement performance for some images. Drawing inspiration from biological genes, we term this phenomenon the gene effect. The gene effect limits enhancement performance, as even random parameters can sometimes outperform learned ones, preventing models from fully utilizing their capacity. In this paper, we investigate the reason and propose a solution. Based on our observations, we attribute the gene effect to static parameters, analogous to how fixed genetic configurations become maladaptive when environments change. Inspired by biological evolution, where adaptation to new environments relies on gene mutation and recombination, we propose parameter dynamic evolution (PDE) to adapt to different images and mitigate the gene effect. PDE employs a parameter orthogonal generation technique and the corresponding generated parameters to simulate gene recombination and gene mutation, separately. Experiments validate the effectiveness of our techniques. The code will be released to the public.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures</title>
<link>https://arxiv.org/abs/2505.09251</link>
<guid>https://arxiv.org/abs/2505.09251</guid>
<content:encoded><![CDATA[
<div> Keywords: Metasurface, radar absorbing structures, convolutional neural network, electromagnetic response, computational efficiency

Summary: 
Metasurface-based radar absorbing structures (RAS) are essential for stealth technology and electromagnetic shielding due to their ability to selectively absorb specific frequencies with minimal thickness. Conventional design methods for these structures are time-consuming and computationally intensive. To address this, a surrogate model using a convolutional neural network (CNN) with Huber loss function was developed to accelerate electromagnetic response prediction for multi-layered RAS. The model achieved high accuracy with cosine similarity of 99.9% and mean square error of 0.001 after 1000 epochs of training. Validated through full-wave simulations and experiments, the model significantly reduced computational time while maintaining accuracy. <div>
arXiv:2505.09251v1 Announce Type: new 
Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for applications like stealth technology, electromagnetic (EM) shielding, etc. due to their capability to achieve frequency selective absorption characteristics with minimal thickness and reduced weight penalty. However, the conventional approach for the EM design and optimization of these structures relies on forward simulations, using full wave simulation tools, to predict the electromagnetic (EM) response of candidate meta atoms. This process is computationally intensive, extremely time consuming and requires exploration of large design spaces. To overcome this challenge, we propose a surrogate model that significantly accelerates the prediction of EM responses of multi-layered metasurface-based RAS. A convolutional neural network (CNN) based architecture with Huber loss function has been employed to estimate the reflection characteristics of the RAS model. The proposed model achieved a cosine similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of training. The efficiency of the model has been established via full wave simulations as well as experiment where it demonstrated significant reduction in computational time while maintaining high predictive accuracy.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping</title>
<link>https://arxiv.org/abs/2505.09252</link>
<guid>https://arxiv.org/abs/2505.09252</guid>
<content:encoded><![CDATA[
<div> Keywords: intracranial hemorrhage, non-contrast computed tomography, deep learning models, multi-modal large language models, medical imaging analysis

Summary: 
This study evaluated the performance of zero-shot multi-modal large language models (MLLMs) in comparison to traditional deep learning methods for intracranial hemorrhage (ICH) classification and subtyping on non-contrast computed tomography. Traditional deep learning models outperformed MLLMs in both binary classification of ICH presence and subtype classification. The MLLMs, including GPT-4o and Gemini 2.0 Flash, exhibited inferior performance in subtype classification tasks. While MLLMs have enhanced interpretability through language interactions, their accuracy in ICH subtyping remains lower than that of deep networks. The study emphasizes the potential of MLLMs in medical imaging analysis but suggests the need for further refinement and development of more precise models for improving performance in three-dimensional medical image processing.<br /><br />Summary: <div>
arXiv:2505.09252v1 Announce Type: new 
Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes on non-contrast computed tomography is critical for prognosis prediction and therapeutic decision-making, yet remains challenging due to low contrast and blurring boundaries. This study evaluates the performance of zero-shot multi-modal large language models (MLLMs) compared to traditional deep learning methods in ICH binary classification and subtyping. Methods: We utilized a dataset provided by RSNA, comprising 192 NCCT volumes. The study compares various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2, with conventional deep learning models, including ResNet50 and Vision Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such as ICH presence, subtype classification, localization, and volume estimation. Results: The results indicate that in the ICH binary classification task, traditional deep learning models outperform MLLMs comprehensively. For subtype classification, MLLMs also exhibit inferior performance compared to traditional deep learning models, with Gemini 2.0 Flash achieving an macro-averaged precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While MLLMs excel in interactive capabilities, their overall accuracy in ICH subtyping is inferior to deep networks. However, MLLMs enhance interpretability through language interactions, indicating potential in medical imaging analysis. Future efforts will focus on model refinement and developing more precise MLLMs to improve performance in three-dimensional medical image processing.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Augmentation for Pose-invariant Face Recognition</title>
<link>https://arxiv.org/abs/2505.09256</link>
<guid>https://arxiv.org/abs/2505.09256</guid>
<content:encoded><![CDATA[
<div> novel approach, face recognition, head poses, synthetic data, feature aggregation <br />
Summary: 
This paper introduces a novel approach called Pose-TTA to enhance face recognition performance by augmenting head poses during testing without the need for additional training. By using a portrait animator, the method aligns faces at inference time, generating matching side-profile images for comparison instead of frontalising them. A weighted feature aggregation strategy is implemented to address any distortions or biases in the synthetic data, improving the reliability of the augmented images. Extensive experiments on various datasets show that Pose-TTA consistently enhances inference performance across different pre-trained face recognition models. The method is easy to integrate into existing face recognition pipelines as it does not require retraining or fine-tuning of the underlying models. <br />Summary: <div>
arXiv:2505.09256v1 Announce Type: new 
Abstract: The goal of this paper is to enhance face recognition performance by augmenting head poses during the testing phase. Existing methods often rely on training on frontalised images or learning pose-invariant representations, yet both approaches typically require re-training and testing for each dataset, involving a substantial amount of effort. In contrast, this study proposes Pose-TTA, a novel approach that aligns faces at inference time without additional training. To achieve this, we employ a portrait animator that transfers the source image identity into the pose of a driving image. Instead of frontalising a side-profile face -- which can introduce distortion -- Pose-TTA generates matching side-profile images for comparison, thereby reducing identity information loss. Furthermore, we propose a weighted feature aggregation strategy to address any distortions or biases arising from the synthetic data, thus enhancing the reliability of the augmented images. Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance. Moreover, our method is straightforward to integrate into existing face recognition pipelines, as it requires no retraining or fine-tuning of the underlying recognition models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</title>
<link>https://arxiv.org/abs/2505.09263</link>
<guid>https://arxiv.org/abs/2505.09263</guid>
<content:encoded><![CDATA[
<div> few-shot Anomaly-driven Generation, realistic anomalies, diverse anomalies, weakly-supervised anomaly detection, industrial inspection
Summary:<br />
- The paper addresses the challenge of anomaly detection in industrial inspection with limited anomaly samples by proposing a few-shot Anomaly-driven Generation (AnoGen) method.
- AnoGen guides a diffusion model to generate realistic and diverse anomalies using only a few real anomalies, improving anomaly detection model training.
- The method comprises three stages: learning anomaly distribution, guiding the diffusion model to generate anomalies on specific objects, and leveraging weakly-supervised anomaly detection for model training.
- Experimental results on the MVTec dataset show that the generated anomalies enhance anomaly classification and segmentation performance, with DRAEM and DesTSeg achieving significant improvements.
- The code and generated anomalous data are available on the project's GitHub repository for further research and implementation. 
Summary: <div>
arXiv:2505.09263v1 Announce Type: new 
Abstract: Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt</title>
<link>https://arxiv.org/abs/2505.09264</link>
<guid>https://arxiv.org/abs/2505.09264</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised, anomaly detection, self-attention transformers, reconstruction networks, One Normal Image Prompt

Summary: 
OneNIP introduces a novel method for unified anomaly detection using unsupervised reconstruction networks with self-attention transformers. By reconstructing normal features and restoring anomaly features with just one normal image prompt, OneNIP improves anomaly detection performance significantly. This approach addresses the issue of perfect reconstruction for both normal and anomaly features, enhancing detection accuracy. Additionally, a supervised refiner is proposed to improve pixel-level anomaly segmentation by regressing reconstruction errors using real normal and synthesized anomalous images. The effectiveness of OneNIP is demonstrated on industry benchmarks such as MVTec, BTAD, and VisA, outperforming previous methods. The code and pre-trained models for OneNIP are publicly available for further research and implementation. <br /><br />Summary: <div>
arXiv:2505.09264v1 Announce Type: new 
Abstract: Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning</title>
<link>https://arxiv.org/abs/2505.09265</link>
<guid>https://arxiv.org/abs/2505.09265</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot visual anomaly segmentation, vision-language models, MetaUAS, change segmentation, universal anomaly segmentation 

Summary: 
Zero- and few-shot visual anomaly segmentation typically rely on vision-language models using textual prompts, limiting their flexibility. This paper introduces a novel approach using a pure visual foundation model for universal anomaly segmentation. By unifying anomaly segmentation into change segmentation, the model can leverage synthetic image pairs for training, independent of specific anomaly datasets. The proposed MetaUAS framework utilizes a one-prompt meta-learning approach and a soft feature alignment module to handle geometric variations for improved segmentation accuracy. This method does not require pre-trained visual-language models or specialized anomaly detection datasets, making it versatile for segmenting unseen anomalies without language guidance. MetaUAS outperforms existing zero-shot, few-shot, and full-shot anomaly segmentation methods in terms of efficiency and effectiveness. The code and pre-trained models are publicly available for further research and application. 

Summary: <br /><br />Zero- and few-shot visual anomaly segmentation typically rely on vision-language models using textual prompts, limiting their flexibility. This paper introduces a novel approach using a pure visual foundation model for universal anomaly segmentation. By unifying anomaly segmentation into change segmentation, the model can leverage synthetic image pairs for training, independent of specific anomaly datasets. The proposed MetaUAS framework utilizes a one-prompt meta-learning approach and a soft feature alignment module to handle geometric variations for improved segmentation accuracy. This method does not require pre-trained visual-language models or specialized anomaly detection datasets, making it versatile for segmenting unseen anomalies without language guidance. MetaUAS outperforms existing zero-shot, few-shot, and full-shot anomaly segmentation methods in terms of efficiency and effectiveness. The code and pre-trained models are publicly available for further research and application. <div>
arXiv:2505.09265v1 Announce Type: new 
Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Medical Imaging Segmentation: A Survey</title>
<link>https://arxiv.org/abs/2505.09274</link>
<guid>https://arxiv.org/abs/2505.09274</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, segmentation, Generative AI, Few-Shot Learning, Foundation Models

Summary:<br /><br />Medical imaging plays a crucial role in healthcare, with segmentation being a challenging task due to various factors such as data complexity and privacy constraints. Recent advancements in methodologies like Generative AI, Few-Shot Learning, Foundation Models, and Universal Models show promise in addressing these challenges. These approaches aim to improve generalization and domain adaptation, offering potential solutions to longstanding issues in medical image segmentation. The survey provides an in-depth exploration of theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Despite advancements, limitations and unresolved issues remain, highlighting the need for further research to enhance the practicality and accessibility of segmentation models in medical imaging. A GitHub Repository is being maintained to track and update innovations in this evolving field. <div>
arXiv:2505.09274v1 Announce Type: new 
Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care. Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints. Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise. This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models. These approaches offer promising solutions to longstanding challenges. We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging. We are maintaining a \href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting butterfly species presence from satellite imagery using soft contrastive regularisation</title>
<link>https://arxiv.org/abs/2505.09306</link>
<guid>https://arxiv.org/abs/2505.09306</guid>
<content:encoded><![CDATA[
<div> biodiversity monitoring, remote sensing data, butterfly species presence, Resnet-based model, contrastive regularisation loss <br />
Summary:<br />
The article introduces a new dataset for predicting butterfly species presence in the UK using satellite data. It discusses the shift towards using remote sensing data for biodiversity monitoring and the challenges and opportunities it presents. The study focuses on optimizing a Resnet-based model for predicting multi-species presence from satellite images, with a particular emphasis on locations with high biodiversity. A new, soft supervised contrastive regularization loss is proposed to improve prediction accuracy, especially for probabilistic labels like species presence data. The research aims to address the growing need for scalable biodiversity monitoring methods and highlights the importance of accurately predicting species biodiversity from remote sensing data for efficient monitoring and conservation efforts. <div>
arXiv:2505.09306v1 Announce Type: new 
Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage. Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images. This paper presents a new data set for predicting butterfly species presence from satellite data in the United Kingdom. We experimentally optimise a Resnet-based model to predict multi-species presence from 4-band satellite images, and find that this model especially outperforms the mean rate baseline for locations with high species biodiversity. To improve performance, we develop a soft, supervised contrastive regularisation loss that is tailored to probabilistic labels (such as species-presence data), and demonstrate that this improves prediction accuracy. In summary, our new data set and contrastive regularisation method contribute to the open challenge of accurately predicting species biodiversity from remote sensing data, which is key for efficient biodiversity monitoring.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Video Compression using 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.09324</link>
<guid>https://arxiv.org/abs/2505.09324</guid>
<content:encoded><![CDATA[
<div> Neural Video Codecs, Deep Learning, Video Compression, ROI, Gaussian Splatting <br />
Summary:<br />
The research community is moving towards neural video codecs (NVC) that utilize deep learning techniques instead of traditional codec pipelines. These NVCs offer content-aware compression strategies and higher efficiency. However, their computational demands limit real-time applications like video conferencing. To address this issue, a region-of-interest (ROI) based neural video compression model using 2D Gaussian Splatting is proposed. This approach allows for real-time decoding and requires fewer data points compared to traditional methods. By leveraging a content-aware initialization strategy and a novel redundancy-reduction mechanism, the encoding time of the Gaussian splatting-based image codec is accelerated by 88%. This advancement paves the way for NVCs to be used in video conferencing applications, offering improved adaptability and compression efficiency. <br />Summary: <div>
arXiv:2505.09324v1 Announce Type: new 
Abstract: The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis</title>
<link>https://arxiv.org/abs/2505.09329</link>
<guid>https://arxiv.org/abs/2505.09329</guid>
<content:encoded><![CDATA[
<div> scaling, medical vision, foundation models, self-supervised learning, BioVFM-21M

Summary:<br />
- The study explores scaling behaviors in developing scalable medical vision foundation models through self-supervised learning.
- A large-scale biomedical image dataset, BioVFM-21M, is introduced to support scalable pretraining.
- Scaling up provides benefits but varies across tasks in the medical domain.
- Factors correlated with scaling benefits include task characteristics, data diversity, pretraining methods, and computational efficiency.
- The proposed BioVFM, pretrained on 21 million biomedical images, outperforms previous state-of-the-art foundation models across 12 medical benchmarks. <br /> <div>
arXiv:2505.09329v1 Announce Type: new 
Abstract: Scaling up model and data size have demonstrated impressive performance improvement over a wide range of tasks. Despite extensive studies on scaling behaviors for general-purpose tasks, medical images exhibit substantial differences from natural data. It remains unclear the key factors in developing medical vision foundation models at scale due to the absence of an extensive understanding of scaling behavior in the medical domain. In this paper, we explored the scaling behavior across model sizes, training algorithms, data sizes, and imaging modalities in developing scalable medical vision foundation models by self-supervised learning. To support scalable pretraining, we introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a wide range of biomedical image modalities and anatomies. We observed that scaling up does provide benefits but varies across tasks. Additional analysis reveals several factors correlated with scaling benefits. Finally, we propose BioVFM, a large-scale medical vision foundation model pretrained on 21 million biomedical images, which outperforms the previous state-of-the-art foundation models across 12 medical benchmarks. Our results highlight that while scaling up is beneficial for pursuing better performance, task characteristics, data diversity, pretraining methods, and computational efficiency remain critical considerations for developing scalable medical foundation models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2505.09336</link>
<guid>https://arxiv.org/abs/2505.09336</guid>
<content:encoded><![CDATA[
<div> MultiviewVLM, vision-language model, unsupervised contrastive multiview representation learning, facial emotions, 3D/4D data<br />
<br />
Summary:<br />
MultiviewVLM is a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. The architecture integrates pseudo-labels from generated textual prompts for aligning emotional semantics. It proposes a joint embedding space to align multiview representations without explicit supervision and enhances model discriminability through a novel multiview contrastive learning strategy using stable positive-negative pair sampling. A gradient-friendly loss function ensures smoother convergence, and the model is optimized for distributed training for scalability. Extensive experiments show that MultiviewVLM surpasses existing methods and can be easily adapted to various real-world applications with minimal modifications.<br /> <div>
arXiv:2505.09336v1 Announce Type: new 
Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. Our architecture integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics. To capture shared information across multi-views, we propose a joint embedding space that aligns multiview representations without requiring explicit supervision. We further enhance the discriminability of our model through a novel multiview contrastive learning strategy that leverages stable positive-negative pair sampling. A gradient-friendly loss function is introduced to promote smoother and more stable convergence, and the model is optimized for distributed training to ensure scalability. Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis</title>
<link>https://arxiv.org/abs/2505.09358</link>
<guid>https://arxiv.org/abs/2505.09358</guid>
<content:encoded><![CDATA[
<div> conditional generative models, pretrained latent diffusion models, dense image analysis tasks, zero-shot generalization, Marigold

Summary:
Marigold introduces a new approach to fine-tuning pretrained latent diffusion models like Stable Diffusion for dense image analysis tasks. By leveraging the knowledge extracted from these models, Marigold achieves state-of-the-art zero-shot generalization on tasks such as monocular depth estimation, surface normals prediction, and intrinsic decomposition. The approach requires minimal modification of the original model's architecture and can be trained on small synthetic datasets using just a single GPU over a few days. This method capitalizes on the success of text-to-image generative models, particularly those using denoising diffusion in a latent space, to enhance the performance of deep learning models in data-scarce settings. The Marigold project showcases the potential of leveraging pretrained models for efficient transfer learning and demonstrates the depth of understanding these models possess of the visual world. <div>
arXiv:2505.09358v1 Announce Type: new 
Abstract: The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo</title>
<link>https://arxiv.org/abs/2505.09368</link>
<guid>https://arxiv.org/abs/2505.09368</guid>
<content:encoded><![CDATA[
<div> Dataset, Robustness, Optical Flow, Scene Flow, Stereo Vision <br />
Summary:
RobustSpring is introduced as a dataset and benchmark focusing on evaluating the robustness of models to image corruptions in optical flow, scene flow, and stereo vision tasks. By applying 20 different image corruptions in a consistent manner to the high-resolution Spring dataset, RobustSpring creates challenging conditions for model evaluation. A new corruption robustness metric allows for comparisons of model robustness, with integration into the Spring benchmark enabling dual evaluations of accuracy and robustness. Initial model benchmarking reveals that accuracy does not necessarily correlate with robustness, and that robustness can vary widely across different types of corruptions. RobustSpring aims to promote the development of computer vision models that excel in both accuracy and resilience, with the dataset and benchmark being made publicly available for future research and applications. <br /><br />Summary: <div>
arXiv:2505.09368v1 Announce Type: new 
Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision algorithms generally focus on model accuracy rather than robustness to image corruptions like noise or rain. Hence, the resilience of models to such real-world perturbations is largely unquantified. To address this, we present RobustSpring, a comprehensive dataset and benchmark for evaluating robustness to image corruptions for optical flow, scene flow, and stereo models. RobustSpring applies 20 different image corruptions, including noise, blur, color changes, quality degradations, and weather distortions, in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, creating a suite of 20,000 corrupted images that reflect challenging conditions. RobustSpring enables comparisons of model robustness via a new corruption robustness metric. Integration with the Spring benchmark enables public two-axis evaluations of both accuracy and robustness. We benchmark a curated selection of initial models, observing that accurate models are not necessarily robust and that robustness varies widely by corruption type. RobustSpring is a new computer vision benchmark that treats robustness as a first-class citizen to foster models that combine accuracy with resilience. It will be available at https://spring-benchmark.org.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment</title>
<link>https://arxiv.org/abs/2505.09372</link>
<guid>https://arxiv.org/abs/2505.09372</guid>
<content:encoded><![CDATA[
<div> Keywords: dermatology, vision-language pretraining, multimodal challenge, knowledge-enhanced, MAKE framework

Summary:
MAKE is a novel framework designed for enhancing vision-language pretraining in dermatology. It addresses the limitations of traditional text-based approaches by incorporating multiple knowledge aspects, utilizing a multi-aspect contrastive learning strategy. The framework breaks down clinical narratives into knowledge-enhanced sub-texts and aligns them with image features for more accurate diagnosis. Additionally, a diagnosis-guided weighting scheme prioritizes clinically significant sub-captions. By pretraining on a large dataset of dermatological image-text pairs, MAKE surpasses existing VLP models in skin disease classification, concept annotation, and cross-modal retrieval tasks. The code for MAKE will be publicly available, offering a valuable tool for advancing AI applications in dermatology.

<br /><br />Summary: <div>
arXiv:2505.09372v1 Announce Type: new 
Abstract: Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision-language pretraining (VLP) has advanced medical AI, its effectiveness in dermatology is limited by text length constraints and the lack of structured texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced vision-language pretraining framework for zero-shot dermatological tasks. Recognizing that comprehensive dermatological descriptions require multiple knowledge aspects that exceed standard text constraints, our framework introduces: (1) a multi-aspect contrastive learning strategy that decomposes clinical narratives into knowledge-enhanced sub-texts through large language models, (2) a fine-grained alignment mechanism that connects subcaptions with diagnostically relevant image features, and (3) a diagnosis-guided weighting scheme that adaptively prioritizes different sub-captions based on clinical significance prior. Through pretraining on 403,563 dermatological image-text pairs collected from education resources, MAKE significantly outperforms state-of-the-art VLP models on eight datasets across zero-shot skin disease classification, concept annotation, and cross-modal retrieval tasks. Our code will be made publicly available at https: //github.com/SiyuanYan1/MAKE.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-driven Motion Generation: Overview, Challenges and Directions</title>
<link>https://arxiv.org/abs/2505.09379</link>
<guid>https://arxiv.org/abs/2505.09379</guid>
<content:encoded><![CDATA[
<div> Keywords: text-driven motion generation, human movements, natural language, motion synthesis, language-driven human motion synthesis <br />
Summary: 
Text-driven motion generation is a valuable approach for creating human movements directly from natural language, offering flexibility and accessibility in controlling animated characters. Traditional motion synthesis models focus on predicting future poses from initial sequences, often conditioned on action labels. Modern text-to-motion generation approaches are categorized based on architecture (VAE-based, diffusion-based, hybrid models) and motion representation (discrete and continuous strategies). Popular datasets, evaluation methods, and benchmarks have shaped progress in this area. The review aims to highlight the current state of the field, key challenges, limitations, and future directions for research. Researchers and practitioners in virtual reality, gaming, human-computer interaction, and robotics can benefit from this comprehensive survey. <br /> <br />Summary: <div>
arXiv:2505.09379v1 Announce Type: new 
Abstract: Text-driven motion generation offers a powerful and intuitive way to create human movements directly from natural language. By removing the need for predefined motion inputs, it provides a flexible and accessible approach to controlling animated characters. This makes it especially useful in areas like virtual reality, gaming, human-computer interaction, and robotics. In this review, we first revisit the traditional perspective on motion synthesis, where models focused on predicting future poses from observed initial sequences, often conditioned on action labels. We then provide a comprehensive and structured survey of modern text-to-motion generation approaches, categorizing them from two complementary perspectives: (i) architectural, dividing methods into VAE-based, diffusion-based, and hybrid models; and (ii) motion representation, distinguishing between discrete and continuous motion generation strategies. In addition, we explore the most widely used datasets, evaluation methods, and recent benchmarks that have shaped progress in this area. With this survey, we aim to capture where the field currently stands, bring attention to its key challenges and limitations, and highlight promising directions for future exploration. We hope this work offers a valuable starting point for researchers and practitioners working to push the boundaries of language-driven human motion synthesis.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform</title>
<link>https://arxiv.org/abs/2505.09380</link>
<guid>https://arxiv.org/abs/2505.09380</guid>
<content:encoded><![CDATA[
<div> AI, radiology, intracranial hemorrhage, NeoMedSys, model refinement <br />
<br />
Summary: The study introduced NeoMedSys, a radiology software platform, for deploying and refining AI models in clinical settings. The platform was tested in two large hospitals in Norway, focusing on improving the VIOLA-AI model for intracranial hemorrhage detection. By iteratively retraining the model based on real-time radiologist feedback, significant improvements were observed in sensitivity (90.3% from 79.2%), specificity (89.3% from 80.7%), and AUC (0.949 from 0.873). The study demonstrated the effectiveness of NeoMedSys in facilitating the enhancement of AI model performance through automated bleed detection and segmentation. This approach highlights the importance of real-time radiologist involvement in the refinement process, showcasing the potential for improving diagnostic accuracy in radiology with AI tools. <div>
arXiv:2505.09380v1 Announce Type: new 
Abstract: Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2). We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining. Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873). Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization</title>
<link>https://arxiv.org/abs/2505.09385</link>
<guid>https://arxiv.org/abs/2505.09385</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, Semantic segmentation, Class consistency, Domain shift, Adversarial mechanism<br />
<br />
Summary: 
FedSaaS is a novel federated segmentation framework that addresses the issue of class consistency in semantic segmentation tasks, particularly in the presence of domain shift. By utilizing class exemplars at both local and global levels, FedSaaS ensures that class representations align effectively. The server-side modeling of class prototypes supervises the global branch of clients, while an adversarial mechanism on the client side harmonizes global and local contributions for consistent outputs. Multilevel contrastive losses further enforce consistency between the two-level representations within the semantic space. Experimental results on various driving scene segmentation datasets show that FedSaaS outperforms existing methods, significantly improving segmentation accuracy and effectively tackling the class-consistency representation problem.<br /> 
Summary: <div>
arXiv:2505.09385v1 Announce Type: new 
Abstract: Federated semantic segmentation enables pixel-level classification in images through collaborative learning while maintaining data privacy. However, existing research commonly overlooks the fine-grained class relationships within the semantic space when addressing heterogeneous problems, particularly domain shift. This oversight results in ambiguities between class representation. To overcome this challenge, we propose a novel federated segmentation framework that strikes class consistency, termed FedSaaS. Specifically, we introduce class exemplars as a criterion for both local- and global-level class representations. On the server side, the uploaded class exemplars are leveraged to model class prototypes, which supervise global branch of clients, ensuring alignment with global-level representation. On the client side, we incorporate an adversarial mechanism to harmonize contributions of global and local branches, leading to consistent output. Moreover, multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space. Extensive experiments on several driving scene segmentation datasets demonstrate that our framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy and effectively addressing the class-consistency representation problem.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling</title>
<link>https://arxiv.org/abs/2505.09406</link>
<guid>https://arxiv.org/abs/2505.09406</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic scene reconstruction, Autonomous driving, Neural Radiance Fields, Monocular camera, Optical flow

Summary: 
FreeDriveRF is a novel method for dynamic scene reconstruction in autonomous driving using only sequential RGB images, eliminating the need for poses inputs. By decoupling dynamic and static parts early in the sampling process with semantic supervision, image blurring and artifacts are reduced. A warped ray-guided dynamic object rendering consistency loss is introduced to address challenges posed by object motion and occlusion in monocular cameras. This loss utilizes optical flow to improve the dynamic modeling process. The method also incorporates estimated dynamic flow to enhance the stability and accuracy of unbounded scene reconstruction through pose optimization. Experimental results on KITTI and Waymo datasets demonstrate the superior performance of FreeDriveRF in dynamic scene modeling for autonomous driving. 

<br /><br />Summary: <div>
arXiv:2505.09406v1 Announce Type: new 
Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to perceive and interpret complex scene changes more precisely. Dynamic Neural Radiance Fields (NeRFs) have recently shown promising capability in scene modeling. However, many existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity. To address this, we propose FreeDriveRF, which reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs. We innovatively decouple dynamic and static parts at the early sampling level using semantic supervision, mitigating image blurring and artifacts. To overcome the challenges posed by object motion and occlusion in monocular camera, we introduce a warped ray-guided dynamic object rendering consistency loss, utilizing optical flow to better constrain the dynamic modeling process. Additionally, we incorporate estimated dynamic flow to constrain the pose optimization process, improving the stability and accuracy of unbounded scene reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of our method in dynamic scene modeling for autonomous driving.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians</title>
<link>https://arxiv.org/abs/2505.09413</link>
<guid>https://arxiv.org/abs/2505.09413</guid>
<content:encoded><![CDATA[
<div> prediction, point clouds, 2D Gaussians, rendering, generalization

Summary:<br />
The article introduces a novel point cloud rendering method that predicts 2D Gaussians from point clouds. The method consists of two identical modules with an entire-patch architecture that can be applied to multiple datasets. It normalizes and initializes the Gaussians using point cloud information and employs splitting decoders to refine the initial predictions. This allows the method to effectively handle sparse point clouds and generalize to different categories of point clouds. The predicted Gaussians are directly used for rendering without the need for additional refinement, leading to high-quality results. Extensive experiments on various datasets demonstrate that the proposed method achieves state-of-the-art performance and superior generalization. The code for the method is available on GitHub for reference. <br />Summary: <div>
arXiv:2505.09413v1 Announce Type: new 
Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds to achieve photo-realistic rendering but still depend on categorical priors, dense point clouds, or additional refinements. Hence, we introduce a novel point cloud rendering method by predicting 2D Gaussians from point clouds. Our method incorporates two identical modules with an entire-patch architecture enabling the network to be generalized to multiple datasets. The module normalizes and initializes the Gaussians utilizing the point cloud information including normals, colors and distances. Then, splitting decoders are employed to refine the initial Gaussians by duplicating them and predicting more accurate results, making our methodology effectively accommodate sparse point clouds as well. Once trained, our approach exhibits direct generalization to point clouds across different categories. The predicted Gaussians are employed directly for rendering without additional refinement on the rendered images, retaining the benefits of 2D Gaussians. We conduct extensive experiments on various datasets, and the results demonstrate the superiority and generalization of our method, which achieves SOTA performance. The code is available at https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.09415</link>
<guid>https://arxiv.org/abs/2505.09415</guid>
<content:encoded><![CDATA[
<div> Keywords: Face anti-spoofing, multimodal large language models, FaceShield, spoof-aware vision perception, prompt-guided vision token masking

Summary:
FaceShield is a multimodal large language model designed for face anti-spoofing (FAS) tasks, with pre-training and fine-tuning datasets. It can authenticate faces, detect spoofing attacks, provide reasoning, and localize attack areas. The model uses spoof-aware vision perception and prompt-guided vision token masking to enhance performance. Extensive experiments show that FaceShield outperforms existing deep learning models and general MLLMs in FAS tasks, including classification, reasoning, and attack localization. The proposed datasets, protocols, and codes will be made available soon. The model's interpretability and reasoning capabilities make it a valuable tool for protecting facial recognition systems from presentation attacks.
<br /><br />Summary: <div>
arXiv:2505.09415v1 Announce Type: new 
Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems from presentation attacks. Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. Recently, multimodal large language models (MLLMs) have shown strong capabilities in perception, reasoning, and decision-making in visual tasks. However, there is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task. To address this gap, we propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the model's generalization ability. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization. Our instruction datasets, protocols, and codes will be released soon.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.09422</link>
<guid>https://arxiv.org/abs/2505.09422</guid>
<content:encoded><![CDATA[
<div> fusion, 4D radar, LiDAR, MoRAL, object detection  
Summary:  
- MoRAL is a motion-aware fusion framework for 3D object detection, addressing inter-frame misalignment and leveraging dynamic information from 4D radar.
- The Motion-aware Radar Encoder (MRE) compensates for radar misalignment caused by object movement during accumulation.
- The Motion Attention Gated Fusion (MAGF) module integrates radar motion features to guide LiDAR features towards dynamic foreground objects.
- Extensive evaluations on the VoD dataset show MoRAL outperforming existing methods, with the highest mAP of 73.30% in the entire area and 88.68% in the driving corridor.
- MoRAL achieves the best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in the driving corridor.  
<br /><br />Summary: <div>
arXiv:2505.09422v1 Announce Type: new 
Abstract: Reliable autonomous driving systems require accurate detection of traffic participants. To this end, multi-modal fusion has emerged as an effective strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame radar point clouds have demonstrated the effectiveness in bridging the point density gap. However, they often neglect radar point clouds' inter-frame misalignment caused by object movement during accumulation and do not fully exploit the object dynamic information from 4D radar. In this paper, we propose MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is designed to compensate for inter-frame radar misalignment from moving objects. Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion features to guide LiDAR features to focus on dynamic foreground objects. Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL outperforms existing methods, achieving the highest mAP of 73.30% in the entire area and 88.68% in the driving corridor. Notably, our method also achieves the best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in the driving corridor.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient LiDAR Reflectance Compression via Scanning Serialization</title>
<link>https://arxiv.org/abs/2505.09433</link>
<guid>https://arxiv.org/abs/2505.09433</guid>
<content:encoded><![CDATA[
arXiv:2505.09433v1 Announce Type: new 
Abstract: Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods. To address this, we introduce SerLiC, a serialization-based neural compression framework to fully exploit the intrinsic characteristics of LiDAR reflectance. SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization, offering a device-centric perspective for reflectance analysis. Each point is then tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance, for effective dependencies exploration. For efficient sequential modeling, Mamba is incorporated with a dual parallelization scheme, enabling simultaneous autoregressive dependency capture and fast processing. Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. Moreover, a lightweight version of SerLiC achieves > 10 fps (frames per second) with just 111K parameters, which is attractive for real-world applications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records</title>
<link>https://arxiv.org/abs/2505.09435</link>
<guid>https://arxiv.org/abs/2505.09435</guid>
<content:encoded><![CDATA[
arXiv:2505.09435v1 Announce Type: new 
Abstract: Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIP's three-stage framework--cleansing, attunement, and unification--addresses these challenges by (1) removing background frames, (2) leveraging large language models to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level cross-attention to resolve multi-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy</title>
<link>https://arxiv.org/abs/2505.09450</link>
<guid>https://arxiv.org/abs/2505.09450</guid>
<content:encoded><![CDATA[
arXiv:2505.09450v1 Announce Type: new 
Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally invasive diagnostic procedure. However, an aspiration needle tracker addressing rapid reciprocating motion is still missing. MrTrack, an aspiration needle tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a Mamba-based register extractor to sequentially distill global context from each historical search map, storing these temporal cues in a register bank. The Mamba-based register retriever then retrieves temporal prompts from the register bank to provide external cues when current vision features are temporarily unusable due to rapid reciprocating motion and imaging degradation. A self-supervised register diversify loss is proposed to encourage feature diversity and dimension independence within the learned register, mitigating feature collapse. Comprehensive experiments conducted on both motorized and manual aspiration datasets demonstrate that MrTrack not only outperforms state-of-the-art trackers in accuracy and robustness but also achieves superior inference efficiency.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos</title>
<link>https://arxiv.org/abs/2505.09455</link>
<guid>https://arxiv.org/abs/2505.09455</guid>
<content:encoded><![CDATA[
arXiv:2505.09455v1 Announce Type: new 
Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show promising results for extracting soccer events from broadcast videos. However, when operated in the high-recall, low-precision regime required for exhaustive event coverage in soccer analytics, their lack of contextual understanding becomes apparent: many false positives could be resolved by considering a broader sequence of actions and game-state information. In this work, we address this limitation by reasoning at the game level and improving STAD through the addition of a denoising sequence transduction task. Sequences of noisy, context-free player-centric predictions are processed alongside clean game state information using a Transformer-based encoder-decoder model. By modeling extended temporal context and reasoning jointly over team-level dynamics, our method leverages the "language of soccer" - its tactical regularities and inter-player dependencies - to generate "denoised" sequences of actions. This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video and complementing existing pixel-based methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A 2D Semantic-Aware Position Encoding for Vision Transformers</title>
<link>https://arxiv.org/abs/2505.09466</link>
<guid>https://arxiv.org/abs/2505.09466</guid>
<content:encoded><![CDATA[
arXiv:2505.09466v1 Announce Type: new 
Abstract: Vision transformers have demonstrated significant advantages in computer vision tasks due to their ability to capture long-range dependencies and contextual relationships through self-attention. However, existing position encoding techniques, which are largely borrowed from natural language processing, fail to effectively capture semantic-aware positional relationships between image patches. Traditional approaches like absolute position encoding and relative position encoding primarily focus on 1D linear position relationship, often neglecting the semantic similarity between distant yet contextually related patches. These limitations hinder model generalization, translation equivariance, and the ability to effectively handle repetitive or structured patterns in images. In this paper, we propose 2-Dimensional Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding method with semantic awareness that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates. Our method enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the gap between position encoding and perceptual similarity, thereby improving performance on computer vision tasks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2505.09484</link>
<guid>https://arxiv.org/abs/2505.09484</guid>
<content:encoded><![CDATA[
arXiv:2505.09484v1 Announce Type: new 
Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance. Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts. To address these challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential \textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features. Furthermore, the \textbf{R}epresentation \textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions. We also design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation (\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance. These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations. Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy. The code will be released soon.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput</title>
<link>https://arxiv.org/abs/2505.09498</link>
<guid>https://arxiv.org/abs/2505.09498</guid>
<content:encoded><![CDATA[
arXiv:2505.09498v1 Announce Type: new 
Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks. Our approach includes tailored architectural choices, token compression mechanisms, data curation, training schemes, and a novel image processing technique called implicit semantic stitching that effectively balances computational load and model performance. Through extensive evaluations on 11 standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves state-of-the-art results in both speed and accuracy, making it a promising solution for deployment in resource-constrained environments and large-scale real-time applications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2505.09528</link>
<guid>https://arxiv.org/abs/2505.09528</guid>
<content:encoded><![CDATA[
arXiv:2505.09528v1 Announce Type: new 
Abstract: In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems. Code is available at https://github.com/jwen307/quality_uq.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contactless Cardiac Pulse Monitoring Using Event Cameras</title>
<link>https://arxiv.org/abs/2505.09529</link>
<guid>https://arxiv.org/abs/2505.09529</guid>
<content:encoded><![CDATA[
arXiv:2505.09529v1 Announce Type: new 
Abstract: Time event cameras are a novel technology for recording scene information at extremely low latency and with low power consumption. Event cameras output a stream of events that encapsulate pixel-level light intensity changes within the scene, capturing information with a higher dynamic range and temporal resolution than traditional cameras. This study investigates the contact-free reconstruction of an individual's cardiac pulse signal from time event recording of their face using a supervised convolutional neural network (CNN) model. An end-to-end model is trained to extract the cardiac signal from a two-dimensional representation of the event stream, with model performance evaluated based on the accuracy of the calculated heart rate. The experimental results confirm that physiological cardiac information in the facial region is effectively preserved within the event stream, showcasing the potential of this novel sensor for remote heart rate monitoring. The model trained on event frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm) compared to the RMSE of 2.92 bpm achieved by the baseline model trained on standard camera frames. Furthermore, models trained on event frames generated at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an RMSE of 2.54 and 2.13 bpm, respectively.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes</title>
<link>https://arxiv.org/abs/2505.09562</link>
<guid>https://arxiv.org/abs/2505.09562</guid>
<content:encoded><![CDATA[
arXiv:2505.09562v1 Announce Type: new 
Abstract: Autonomous vehicles need a complete map of their surroundings to plan and act. This has sparked research into the tasks of 3D occupancy prediction, 3D scene completion, and 3D panoptic scene completion, which predict a dense map of the ego vehicle's surroundings as a voxel grid. Scene completion extends occupancy prediction by predicting occluded regions of the voxel grid, and panoptic scene completion further extends this task by also distinguishing object instances within the same class; both aspects are crucial for path planning and decision-making. However, 3D panoptic scene completion is currently underexplored. This work introduces a novel framework for 3D panoptic scene completion that extends existing 3D semantic scene completion models. We propose an Object Module and Panoptic Module that can easily be integrated with 3D occupancy and scene completion methods presented in the literature. Our approach leverages the available annotations in occupancy benchmarks, allowing individual object shapes to be learned as a differentiable problem. The code is available at https://github.com/nicolamarinello/OffsetOcc .
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation</title>
<link>https://arxiv.org/abs/2505.09564</link>
<guid>https://arxiv.org/abs/2505.09564</guid>
<content:encoded><![CDATA[
arXiv:2505.09564v1 Announce Type: new 
Abstract: Cardiac image segmentation is an important step in many cardiac image analysis and modeling tasks such as motion tracking or simulations of cardiac mechanics. While deep learning has greatly advanced segmentation in clinical settings, there is limited work on pre-clinical imaging, notably in porcine models, which are often used due to their anatomical and physiological similarity to humans. However, differences between species create a domain shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown promise for robust medical image segmentation; yet their applicability to porcine data remains largely unexplored. In this work, we investigate whether foundation models can generate sufficiently accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels. Our method requires no manually annotated pig data, relying instead on iterative updates to improve segmentation quality. We demonstrate that this self-training process not only enhances segmentation accuracy but also smooths out temporal inconsistencies across consecutive frames. Although our results are encouraging, there remains room for improvement, for example by incorporating more sophisticated self-training strategies and by exploring additional foundation models and other cardiac imaging technologies.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</title>
<link>https://arxiv.org/abs/2505.09568</link>
<guid>https://arxiv.org/abs/2505.09568</guid>
<content:encoded><![CDATA[
arXiv:2505.09568v1 Announce Type: new 
Abstract: Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Forget your Inverse DDIM for Image Editing</title>
<link>https://arxiv.org/abs/2505.09571</link>
<guid>https://arxiv.org/abs/2505.09571</guid>
<content:encoded><![CDATA[
arXiv:2505.09571v1 Announce Type: new 
Abstract: The field of text-to-image generation has undergone significant advancements with the introduction of diffusion models. Nevertheless, the challenge of editing real images persists, as most methods are either computationally intensive or produce poor reconstructions. This paper introduces SAGE (Self-Attention Guidance for image Editing) - a novel technique leveraging pre-trained diffusion models for image editing. SAGE builds upon the DDIM algorithm and incorporates a novel guidance mechanism utilizing the self-attention layers of the diffusion U-Net. This mechanism computes a reconstruction objective based on attention maps generated during the inverse DDIM process, enabling efficient reconstruction of unedited regions without the need to precisely reconstruct the entire input image. Thus, SAGE directly addresses the key challenges in image editing. The superiority of SAGE over other methods is demonstrated through quantitative and qualitative evaluations and confirmed by a statistically validated comprehensive user study, in which all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE ranks as the top-performing method in seven out of 10 quantitative analyses and secures second and third places in the remaining three.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.09591</link>
<guid>https://arxiv.org/abs/2505.09591</guid>
<content:encoded><![CDATA[
arXiv:2505.09591v1 Announce Type: new 
Abstract: Despite remarkable progress in multimodal models for Visual Question Answering (VQA), there remain major reliability concerns because the models can often be overconfident and miscalibrated, especially in out-of-distribution (OOD) settings. Plenty has been done to address such issues for unimodal models, but little work exists for multimodal cases. Here, we address unreliability in multimodal models by proposing a Variational VQA approach. Specifically, instead of fine-tuning vision-language models by using AdamW, we employ a recently proposed variational algorithm called IVON, which yields a posterior distribution over model parameters. Through extensive experiments, we show that our approach improves calibration and abstentions without sacrificing the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce Expected Calibration Error by more than 50% compared to the AdamW baseline and raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of distribution shifts, the performance gain is even higher, achieving 8% Coverage (@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we present variational learning as a viable option to enhance the reliability of multimodal models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightLab: Controlling Light Sources in Images with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.09608</link>
<guid>https://arxiv.org/abs/2505.09608</guid>
<content:encoded><![CDATA[
arXiv:2505.09608v1 Announce Type: new 
Abstract: We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing</title>
<link>https://arxiv.org/abs/2505.09615</link>
<guid>https://arxiv.org/abs/2505.09615</guid>
<content:encoded><![CDATA[
arXiv:2505.09615v1 Announce Type: new 
Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Learning for Label-Efficient Cancer Image Classification in Oncology</title>
<link>https://arxiv.org/abs/2505.08798</link>
<guid>https://arxiv.org/abs/2505.08798</guid>
<content:encoded><![CDATA[
arXiv:2505.08798v1 Announce Type: cross 
Abstract: The application of AI in oncology has been limited by its reliance on large, annotated datasets and the need for retraining models for domain-specific diagnostic tasks. Taking heed of these limitations, we investigated in-context learning as a pragmatic alternative to model retraining by allowing models to adapt to new diagnostic tasks using only a few labeled examples at inference, without the need for retraining. Using four vision-language models (VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our knowledge, this is the first study to compare the performance of multiple VLMs on different oncology classification tasks. Without any parameter updates, all models showed significant gains with few-shot prompting, with GPT-4o reaching an F1 score of 0.81 in binary classification and 0.60 in multi-class classification settings. While these results remain below the ceiling of fully fine-tuned systems, they highlight the potential of ICL to approximate task-specific behavior using only a handful of examples, reflecting how clinicians often reason from prior cases. Notably, open-source models like Paligemma and CLIP demonstrated competitive gains despite their smaller size, suggesting feasibility for deployment in computing constrained clinical environments. Overall, these findings highlight the potential of ICL as a practical solution in oncology, particularly for rare cancers and resource-limited contexts where fine-tuning is infeasible and annotated data is difficult to obtain.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thoughts on Objectives of Sparse and Hierarchical Masked Image Model</title>
<link>https://arxiv.org/abs/2505.08819</link>
<guid>https://arxiv.org/abs/2505.08819</guid>
<content:encoded><![CDATA[
arXiv:2505.08819v1 Announce Type: cross 
Abstract: Masked image modeling is one of the most poplular objectives of training. Recently, the SparK model has been proposed with superior performance among self-supervised learning models. This paper proposes a new mask pattern for this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the effect of the mask pattern used for image masking in pre-training on performance.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores</title>
<link>https://arxiv.org/abs/2505.08835</link>
<guid>https://arxiv.org/abs/2505.08835</guid>
<content:encoded><![CDATA[
arXiv:2505.08835v1 Announce Type: cross 
Abstract: The advent of convenient and efficient fully unmanned stores equipped with artificial intelligence-based automated checkout systems marks a new era in retail. However, these systems have inherent artificial intelligence security vulnerabilities, which are exploited via adversarial patch attacks, particularly in physical environments. This study demonstrated that adversarial patches can severely disrupt object detection models used in unmanned stores, leading to issues such as theft, inventory discrepancies, and interference. We investigated three types of adversarial patch attacks -- Hiding, Creating, and Altering attacks -- and highlighted their effectiveness. We also introduce the novel color histogram similarity loss function by leveraging attacker knowledge of the color information of a target class object. Besides the traditional confusion-matrix-based attack success rate, we introduce a new bounding-boxes-based metric to analyze the practical impact of these attacks. Starting with attacks on object detection models trained on snack and fruit datasets in a digital environment, we evaluated the effectiveness of adversarial patches in a physical testbed that mimicked a real unmanned store with RGB cameras and realistic conditions. Furthermore, we assessed the robustness of these attacks in black-box scenarios, demonstrating that shadow attacks can enhance success rates of attacks even without direct access to model parameters. Our study underscores the necessity for robust defense strategies to protect unmanned stores from adversarial threats. Highlighting the limitations of the current defense mechanisms in real-time detection systems and discussing various proactive measures, we provide insights into improving the robustness of object detection models and fortifying unmanned retail environments against these attacks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08837</link>
<guid>https://arxiv.org/abs/2505.08837</guid>
<content:encoded><![CDATA[
arXiv:2505.08837v1 Announce Type: cross 
Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is complex and dynamic. Static security policies have become inadequate as threats evolve and cloud resources exhibit elasticity [1]. This paper addresses the limitations of static policies by proposing a security policy management framework that uses reinforcement learning (RL) to adapt dynamically. Specifically, we employ deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, enabling the learning and continuous adjustment of controls such as firewall rules and Identity and Access Management (IAM) policies. The proposed RL based solution leverages cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies, maximizing threat mitigation, and compliance while minimizing resource impact. Experimental results demonstrate that our adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and substantially reducing incident detection and response times by 58%. In addition, it maintains high conformity with security requirements and efficient resource usage. These findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts</title>
<link>https://arxiv.org/abs/2505.08838</link>
<guid>https://arxiv.org/abs/2505.08838</guid>
<content:encoded><![CDATA[
arXiv:2505.08838v1 Announce Type: cross 
Abstract: Ultrasound (US) report generation is a challenging task due to the variability of US images, operator dependence, and the need for standardized text. Unlike X-ray and CT, US imaging lacks consistent datasets, making automation difficult. In this study, we propose a unified framework for multi-organ and multilingual US report generation, integrating fragment-based multilingual training and leveraging the standardized nature of US reports. By aligning modular text fragments with diverse imaging data and curating a bilingual English-Chinese dataset, the method achieves consistent and clinically accurate text generation across organ sites and languages. Fine-tuning with selective unfreezing of the vision transformer (ViT) further improves text-image alignment. Compared to the previous state-of-the-art KMVE method, our approach achieves relative gains of about 2\% in BLEU scores, approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly reducing errors such as missing or incorrect content. By unifying multi-organ and multi-language report generation into a single, scalable framework, this work demonstrates strong potential for real-world clinical workflows.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Total Variation-Based Image Decomposition and Denoising for Microscopy Images</title>
<link>https://arxiv.org/abs/2505.08843</link>
<guid>https://arxiv.org/abs/2505.08843</guid>
<content:encoded><![CDATA[
arXiv:2505.08843v1 Announce Type: cross 
Abstract: Experimentally acquired microscopy images are unavoidably affected by the presence of noise and other unwanted signals, which degrade their quality and might hide relevant features. With the recent increase in image acquisition rate, modern denoising and restoration solutions become necessary. This study focuses on image decomposition and denoising of microscopy images through a workflow based on total variation (TV), addressing images obtained from various microscopy techniques, including atomic force microscopy (AFM), scanning tunneling microscopy (STM), and scanning electron microscopy (SEM). Our approach consists in restoring an image by extracting its unwanted signal components and subtracting them from the raw one, or by denoising it. We evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving this goal in distinct study cases. Huber-ROF proved to be the most flexible one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a wider applicability of this method in microscopy, restricted not only to STM, AFM, and SEM images. The Python code used for this study is publicly available as part of AiSurf. It is designed to be integrated into experimental workflows for image acquisition or can be used to denoise previously acquired images.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of Conformal Prediction in Cervical Atypia Classification</title>
<link>https://arxiv.org/abs/2505.08845</link>
<guid>https://arxiv.org/abs/2505.08845</guid>
<content:encoded><![CDATA[
arXiv:2505.08845v1 Announce Type: cross 
Abstract: Deep learning based cervical cancer classification can potentially increase access to screening in low-resource regions. However, deep learning models are often overconfident and do not reliably reflect diagnostic uncertainty. Moreover, they are typically optimized to generate maximum-likelihood predictions, which fail to convey uncertainty or ambiguity in their results. Such challenges can be addressed using conformal prediction, a model-agnostic framework for generating prediction sets that contain likely classes for trained deep-learning models. The size of these prediction sets indicates model uncertainty, contracting as model confidence increases. However, existing conformal prediction evaluation primarily focuses on whether the prediction set includes or covers the true class, often overlooking the presence of extraneous classes. We argue that prediction sets should be truthful and valuable to end users, ensuring that the listed likely classes align with human expectations rather than being overly relaxed and including false positives or unlikely classes. In this study, we comprehensively validate conformal prediction sets using expert annotation sets collected from multiple annotators. We evaluate three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification. Our expert annotation-based analysis reveals that conventional coverage-based evaluations overestimate performance and that current conformal prediction methods often produce prediction sets that are not well aligned with human labels. Additionally, we explore the capabilities of the conformal prediction methods in identifying ambiguous and out-of-distribution data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntrinsicEdit: Precise generative image manipulation in intrinsic space</title>
<link>https://arxiv.org/abs/2505.08889</link>
<guid>https://arxiv.org/abs/2505.08889</guid>
<content:encoded><![CDATA[
arXiv:2505.08889v1 Announce Type: cross 
Abstract: Generative diffusion models have advanced image editing with high-quality results and intuitive interfaces such as prompts and semantic drawing. However, these interfaces lack precise control, and the associated methods typically specialize on a single editing task. We introduce a versatile, generative workflow that operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision for a range of editing operations. Building atop the RGB-X diffusion framework, we address key challenges of identity preservation and intrinsic-channel entanglement. By incorporating exact diffusion inversion and disentangled channel manipulation, we enable precise, efficient editing with automatic resolution of global illumination effects -- all without additional data collection or model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including color and texture adjustments, object insertion and removal, global relighting, and their combinations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</title>
<link>https://arxiv.org/abs/2505.08919</link>
<guid>https://arxiv.org/abs/2505.08919</guid>
<content:encoded><![CDATA[
arXiv:2505.08919v1 Announce Type: cross 
Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical treatment planning for lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to assess the reconstruction comprehensively. Further, due to the absence of publicly available shape datasets to benchmark reconstruction algorithms, we developed a shape dataset named Lung3D, including the 3D models of 800 labeled pulmonary segments and the corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/M3DV/ImPulSe.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery</title>
<link>https://arxiv.org/abs/2505.08932</link>
<guid>https://arxiv.org/abs/2505.08932</guid>
<content:encoded><![CDATA[
arXiv:2505.08932v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and forest monitoring, including seed dispersal in hard-to-reach terrains. However, a detailed understanding of the forest floor remains a challenge due to high natural variability, quickly changing environmental parameters, and ambiguous annotations due to unclear definitions. To address this issue, we adapt the Segment Anything Model (SAM), a vision foundation model with strong generalization capabilities, to segment forest floor objects such as tree stumps, vegetation, and woody debris. To this end, we employ parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of additional model parameters while keeping the original weights fixed. We adjust SAM's mask decoder to generate masks corresponding to our dataset categories, allowing for automatic segmentation without manual prompting. Our results show that the adapter-based PEFT method achieves the highest mean intersection over union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a lightweight alternative for resource-constrained UAV platforms.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-step manipulation task and motion planning guided by video demonstration</title>
<link>https://arxiv.org/abs/2505.08949</link>
<guid>https://arxiv.org/abs/2505.08949</guid>
<content:encoded><![CDATA[
arXiv:2505.08949v1 Announce Type: cross 
Abstract: This work aims to leverage instructional video to solve complex multi-step task-and-motion planning tasks in robotics. Towards this goal, we propose an extension of the well-established Rapidly-Exploring Random Tree (RRT) planner, which simultaneously grows multiple trees around grasp and release states extracted from the guiding video. Our key novelty lies in combining contact states and 3D object poses extracted from the guiding video with a traditional planning algorithm that allows us to solve tasks with sequential dependencies, for example, if an object needs to be placed at a specific location to be grasped later. We also investigate the generalization capabilities of our approach to go beyond the scene depicted in the instructional video. To demonstrate the benefits of the proposed video-guided planning approach, we design a new benchmark with three challenging tasks: (I) 3D re-arrangement of multiple objects between a table and a shelf, (ii) multi-step transfer of an object through a tunnel, and (iii) transferring objects using a tray similar to a waiter transfers dishes. We demonstrate the effectiveness of our planning algorithm on several robots, including the Franka Emika Panda and the KUKA KMR iiwa. For a seamless transfer of the obtained plans to the real robot, we develop a trajectory refinement approach formulated as an optimal control problem (OCP).
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ</title>
<link>https://arxiv.org/abs/2505.08990</link>
<guid>https://arxiv.org/abs/2505.08990</guid>
<content:encoded><![CDATA[
arXiv:2505.08990v1 Announce Type: cross 
Abstract: Live video streaming is increasingly popular on social media platforms. With the growth of live streaming comes an increased need for robust content moderation to remove dangerous, illegal, or otherwise objectionable content. Whereas video on demand distribution enables offline content analysis, live streaming imposes restrictions on latency for both analysis and distribution. In this paper, we present extensions to the in-progress Media Over QUIC Transport protocol that enable real-time content moderation in one-to-many video live streams. Importantly, our solution removes only the video segments that contain objectionable content, allowing playback resumption as soon as the stream conforms to content policies again. Content analysis tasks may be transparently distributed to arbitrary client devices. We implement and evaluate our system in the context of light strobe removal for photosensitive viewers, finding that streaming clients experience an increased latency of only one group-of-pictures duration.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural BRDF Importance Sampling by Reparameterization</title>
<link>https://arxiv.org/abs/2505.08998</link>
<guid>https://arxiv.org/abs/2505.08998</guid>
<content:encoded><![CDATA[
arXiv:2505.08998v1 Announce Type: cross 
Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged as popular material representations for enhancing realism in physically-based rendering. Yet their importance sampling remains a significant challenge. In this paper, we introduce a reparameterization-based formulation of neural BRDF importance sampling that seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples. The reparameterization-based formulation transfers the distribution learning task to a problem of identifying BRDF integral substitutions. In contrast to previous methods that rely on invertible networks and multi-step inference to reconstruct BRDF distributions, our model removes these constraints, which offers greater flexibility and efficiency. Our variance and performance analysis demonstrates that our reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-cache: Efficient Robot Trajectory Retrieval System</title>
<link>https://arxiv.org/abs/2505.09040</link>
<guid>https://arxiv.org/abs/2505.09040</guid>
<content:encoded><![CDATA[
arXiv:2505.09040v1 Announce Type: cross 
Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that accelerates real-world robot inference by leveraging big-data retrieval and learning from experience. While modern Vision-Language-Action (VLA) models can handle diverse robotic tasks, they often incur high per-step inference costs, resulting in significant latency, sometimes minutes per task. In contrast, RT-cache stores a large-scale Memory of previously successful robot trajectories and retrieves relevant multistep motion snippets, drastically reducing inference overhead. By integrating a Memory Builder with a Trajectory Retrieval, we develop an efficient retrieval process that remains tractable even for extremely large datasets. RT-cache flexibly accumulates real-world experiences and replays them whenever the current scene matches past states, adapting quickly to new or unseen environments with only a few additional samples. Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks both faster and more successfully than a baseline lacking retrieval, suggesting a practical, data-driven solution for real-time manipulation.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis</title>
<link>https://arxiv.org/abs/2505.09091</link>
<guid>https://arxiv.org/abs/2505.09091</guid>
<content:encoded><![CDATA[
arXiv:2505.09091v1 Announce Type: cross 
Abstract: In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis</title>
<link>https://arxiv.org/abs/2505.09109</link>
<guid>https://arxiv.org/abs/2505.09109</guid>
<content:encoded><![CDATA[
arXiv:2505.09109v1 Announce Type: cross 
Abstract: Due to the deformability of garments, generating a large amount of high-quality data for robotic garment manipulation tasks is highly challenging. In this paper, we present a synthetic garment dataset that can be used for robotic garment folding. We begin by constructing geometric garment templates based on keypoints and applying generative models to generate realistic texture patterns. Leveraging these keypoint annotations, we generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. To improve robustness, we propose KG-DAgger, which uses a keypoint-based strategy to generate demonstration data for recovering from failures. KG-DAgger significantly improves the model performance, boosting the real-world success rate by 25\%. After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75\% success rate in the real world. Experiments in both simulation and real-world settings validate the effectiveness of our proposed framework.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Urban Critical Green Space Development Using Machine Learning</title>
<link>https://arxiv.org/abs/2505.09175</link>
<guid>https://arxiv.org/abs/2505.09175</guid>
<content:encoded><![CDATA[
arXiv:2505.09175v1 Announce Type: cross 
Abstract: This paper presents a novel framework for prioritizing urban green space development in Tehran using diverse socio-economic, environmental, and sensitivity indices. The indices were derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the Weather Research & Forecasting (WRF) model. The WRF model was used to estimate the air temperature at a 1 km resolution due to insufficient meteorological stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C, respectively. After data preparation, several machine learning models were used for binary vegetation cover classification including XGBoost, LightGBM, Random Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94% in Overall Accuracy, Recall, and F1-score. Then, the probability of areas lacking vegetation cover was assessed using socio-economic, environmental and sensitivity indices. This resulted in the RF generating an urban green space development prioritization map. Feature Importance Analysis revealed that the most significant indices were nightly land surface temperature (LST) and sensitive population. Finally, the framework performance was validated through microclimate simulation to assess the critical areas after and before the green space development by green roofs. The simulation demonstrated reducing air temperature by up to 0.67{\deg}C after utilizing the green roof technology in critical areas. As a result, this framework provides a valuable tool for urban planners to develop green spaces.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression</title>
<link>https://arxiv.org/abs/2505.09193</link>
<guid>https://arxiv.org/abs/2505.09193</guid>
<content:encoded><![CDATA[
arXiv:2505.09193v1 Announce Type: cross 
Abstract: Recent forward prediction-based learned video compression (LVC) methods have achieved impressive results, even surpassing VVC reference software VTM under the Low Delay B (LDB) configuration. In contrast, learned bidirectional video compression (BVC) remains underexplored and still lags behind its forward-only counterparts. This performance gap is mainly due to the limited ability to extract diverse and accurate contexts: most existing BVCs primarily exploit temporal motion while neglecting non-local correlations across frames. Moreover, they lack the adaptability to dynamically suppress harmful contexts arising from fast motion or occlusion. To tackle these challenges, we propose BiECVC, a BVC framework that incorporates diversified local and non-local context modeling along with adaptive context gating. For local context enhancement, BiECVC reuses high-quality features from lower layers and aligns them using decoded motion vectors without introducing extra motion overhead.To model non-local dependencies efficiently, we adopt a linear attention mechanism that balances performance and complexity. To further mitigate the impact of inaccurate context prediction, we introduce Bidirectional Context Gating, inspired by data-dependent decay in recent autoregressive language models, to dynamically filter contextual information based on conditional coding results. Extensive experiments demonstrate that BiECVC achieves state-of-the-art performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2 under the Random Access (RA) configuration with intra periods of 32 and 64, respectively. To our knowledge, BiECVC is the first learned video codec to surpass VTM 13.2 RA across all standard test datasets. Code will be available at https://github.com/JiangWeibeta/ECVC.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDBench: Large-Scale Electron Density Data for Molecular Modeling</title>
<link>https://arxiv.org/abs/2505.09262</link>
<guid>https://arxiv.org/abs/2505.09262</guid>
<content:encoded><![CDATA[
arXiv:2505.09262v1 Announce Type: cross 
Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on the learning of atoms, molecules, and simple quantum chemical properties (such as energy and force), but ignore the importance of electron density (ED) $\rho(r)$ in accurately understanding molecular force fields (MFFs). ED describes the probability of finding electrons at specific locations around atoms or molecules, which uniquely determines all ground state properties (such as energy, molecular structure, etc.) of interactive multi-particle systems according to the Hohenberg-Kohn theorem. However, the calculation of ED relies on the time-consuming first-principles density functional theory (DFT) which leads to the lack of large-scale ED data and limits its application in MLFFs. In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED designed to advance learning-based research at the electronic scale. Built upon the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million molecules. To comprehensively evaluate the ability of models to understand and utilize electronic information, we design a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation. Our evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, we show that learning-based method can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.09315</link>
<guid>https://arxiv.org/abs/2505.09315</guid>
<content:encoded><![CDATA[
arXiv:2505.09315v1 Announce Type: cross 
Abstract: In recent years, diffusion model has shown its potential across diverse domains from vision generation to language modeling. Transferring its capabilities to modern autonomous driving systems has also emerged as a promising direction.In this work, we propose TransDiffuser, an encoder-decoder based generative trajectory planning model for end-to-end autonomous driving. The encoded scene information serves as the multi-modal conditional input of the denoising decoder. To tackle the mode collapse dilemma in generating high-quality diverse trajectories, we introduce a simple yet effective multi-modal representation decorrelation optimization mechanism during the training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis</title>
<link>https://arxiv.org/abs/2505.09323</link>
<guid>https://arxiv.org/abs/2505.09323</guid>
<content:encoded><![CDATA[
arXiv:2505.09323v1 Announce Type: cross 
Abstract: This study, we propose a novel Q-space Guided Collaborative Attention Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI (MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly acquired structural MRI data. Q-CATN employs a collaborative attention mechanism to effectively extract complementary information from multiple modalities and dynamically adjust its internal representations based on flexible q-space information, eliminating the need for fixed sampling schemes. Additionally, we introduce a range of task-specific constraints to preserve anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic relationships between directional DWI signal distributions and q-space. Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD, and QGAN, in estimating parameter maps and fiber tracts both quantitatively and qualitatively, while preserving fine-grained details. Notably, its ability to accommodate flexible q-space sampling highlights its potential as a promising toolkit for clinical and research applications. Our code is available at https://github.com/Idea89560041/Q-CATN.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images</title>
<link>https://arxiv.org/abs/2505.09334</link>
<guid>https://arxiv.org/abs/2505.09334</guid>
<content:encoded><![CDATA[
arXiv:2505.09334v1 Announce Type: cross 
Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early detection and accurate diagnosis are critical for improving survival rates. While deep learning, particularly convolutional neural networks (CNNs), has revolutionized medical image analysis by detecting subtle patterns indicative of early-stage lung cancer, its adoption faces challenges. These models are often computationally expensive and require significant resources, making them unsuitable for resource constrained environments. Additionally, their lack of transparency hinders trust and broader adoption in sensitive fields like healthcare. Knowledge distillation addresses these challenges by transferring knowledge from large, complex models (teachers) to smaller, lightweight models (students). We propose a knowledge distillation-based approach for lung cancer detection, incorporating explainable AI (XAI) techniques to enhance model transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3, and VGG16, are evaluated as teacher models. We developed and trained a lightweight student model, Distilled Custom Student Network (DCSNet) using ResNet50 as the teacher. This approach not only ensures high diagnostic performance in resource-constrained settings but also addresses transparency concerns, facilitating the adoption of AI-driven diagnostic tools in healthcare.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks</title>
<link>https://arxiv.org/abs/2505.09344</link>
<guid>https://arxiv.org/abs/2505.09344</guid>
<content:encoded><![CDATA[
arXiv:2505.09344v1 Announce Type: cross 
Abstract: Determining the performance of a Deep Neural Network during Neural Architecture Search processes is essential for identifying optimal architectures and hyperparameters. Traditionally, this process requires training and evaluation of each network, which is time-consuming and resource-intensive. Zero-cost proxies estimate performance without training, serving as an alternative to traditional training. However, recent proxies often lack generalization across diverse scenarios and provide only relative rankings rather than predicted accuracies. To address these limitations, we propose GreenFactory, an ensemble of zero-cost proxies that leverages a random forest regressor to combine multiple predictors' strengths and directly predict model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust results across multiple datasets. Specifically, GreenFactory achieves high Kendall correlations on NATS-Bench-SSS, indicating substantial agreement between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120, showcasing its reliability in both search spaces.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression</title>
<link>https://arxiv.org/abs/2505.09356</link>
<guid>https://arxiv.org/abs/2505.09356</guid>
<content:encoded><![CDATA[
arXiv:2505.09356v1 Announce Type: cross 
Abstract: Precise initialization plays a critical role in the performance of localization algorithms, especially in the context of robotics, autonomous driving, and computer vision. Poor localization accuracy is often a consequence of inaccurate initial poses, particularly noticeable in GNSS-denied environments where GPS signals are primarily relied upon for initialization. Recent advances in leveraging deep neural networks for pose regression have led to significant improvements in both accuracy and robustness, especially in estimating complex spatial relationships and orientations. In this paper, we introduce APR-Transformer, a model architecture inspired by state-of-the-art methods, which predicts absolute pose (3D position and 3D orientation) using either image or LiDAR data. We demonstrate that our proposed method achieves state-of-the-art performance on established benchmark datasets such as the Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our experiments to include our custom complex APR-BeIntelli dataset. Additionally, we validate the reliability of our approach in GNSS-denied environments by deploying the model in real-time on an autonomous test vehicle. This showcases the practical feasibility and effectiveness of our approach. The source code is available at:https://github.com/GT-ARC/APR-Transformer.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units</title>
<link>https://arxiv.org/abs/2505.09393</link>
<guid>https://arxiv.org/abs/2505.09393</guid>
<content:encoded><![CDATA[
arXiv:2505.09393v1 Announce Type: cross 
Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net</title>
<link>https://arxiv.org/abs/2505.09521</link>
<guid>https://arxiv.org/abs/2505.09521</guid>
<content:encoded><![CDATA[
arXiv:2505.09521v1 Announce Type: cross 
Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain CNNs that fail to capture cross-channel time-frequency cues or on heavy transformer/GAN decoders that strain memory and stability. We propose Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts these issues via a Multi-directional Time-Frequency Convolutional Attention Encoder, stacking temporal, spectral and joint convolutions with self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space blocks enable efficient long-range spatial modelling. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2505.09565</link>
<guid>https://arxiv.org/abs/2505.09565</guid>
<content:encoded><![CDATA[
arXiv:2505.09565v1 Announce Type: cross 
Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple motion-corrupted low-resolution 2D slices constitutes a critical step in image-based diagnostics of moving subjects, such as fetal brain Magnetic Resonance Imaging (MRI). Existing solutions struggle with image artifacts and severe subject motion or require slice pre-alignment to achieve satisfying reconstruction performance. We propose a novel SVR method to enable fast and accurate MRI reconstruction even in cases of severe image and motion corruption. Our approach performs motion correction, outlier handling, and super-resolution reconstruction with all operations being entirely based on implicit neural representations. The model can be initialized with task-specific priors through fully self-supervised meta-learning on either simulated or real-world data. In extensive experiments including over 480 reconstructions of simulated and clinical MRI brain data from different centers, we prove the utility of our method in cases of severe subject motion and image artifacts. Our results demonstrate improvements in reconstruction quality, especially in the presence of severe motion, compared to state-of-the-art methods, and up to 50% reduction in reconstruction time.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image</title>
<link>https://arxiv.org/abs/2207.14425</link>
<guid>https://arxiv.org/abs/2207.14425</guid>
<content:encoded><![CDATA[
arXiv:2207.14425v2 Announce Type: replace 
Abstract: In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting conditions by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting conditions, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EiHi Net: Out-of-Distribution Generalization Paradigm</title>
<link>https://arxiv.org/abs/2209.14946</link>
<guid>https://arxiv.org/abs/2209.14946</guid>
<content:encoded><![CDATA[
arXiv:2209.14946v3 Announce Type: replace 
Abstract: This paper develops a new EiHi net to solve the out-of-distribution (OoD) generalization problem in deep learning. EiHi net is a model learning paradigm that can be blessed on any visual backbone. This paradigm can change the previous learning method of the deep model, namely find out correlations between inductive sample features and corresponding categories, which suffers from pseudo correlations between indecisive features and labels. We fuse SimCLR and VIC-Reg via explicitly and dynamically establishing the original - positive - negative sample pair as a minimal learning element, the deep model iteratively establishes a relationship close to the causal one between features and labels, while suppressing pseudo correlations. To further validate the proposed model, and strengthen the established causal relationships, we develop a human-in-the-loop strategy, with few guidance samples, to prune the representation space directly. Finally, it is shown that the developed EiHi net makes significant improvements in the most difficult and typical OoD dataset Nico, compared with the current SOTA results, without any domain ($e.g.$ background, irrelevant features) information.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2401.07378</link>
<guid>https://arxiv.org/abs/2401.07378</guid>
<content:encoded><![CDATA[
arXiv:2401.07378v3 Announce Type: replace 
Abstract: Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F$^3$Loc: Fusion and Filtering for Floorplan Localization</title>
<link>https://arxiv.org/abs/2403.03370</link>
<guid>https://arxiv.org/abs/2403.03370</guid>
<content:encoded><![CDATA[
arXiv:2403.03370v2 Announce Type: replace 
Abstract: In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2407.19708</link>
<guid>https://arxiv.org/abs/2407.19708</guid>
<content:encoded><![CDATA[
arXiv:2407.19708v5 Announce Type: replace 
Abstract: Low-light image enhancement is an important task in computer vision, essential for improving the visibility and quality of images captured in non-optimal lighting conditions. Inadequate illumination can lead to significant information loss and poor image quality, impacting various applications such as surveillance. photography, or even autonomous driving. In this regard, automated methods have been developed to automatically adjust illumination in the image for a better visual perception. Current enhancement techniques often use specific datasets to enhance low-light images, but still present challenges when adapting to diverse real-world conditions, where illumination degradation may be localized to specific regions. To address this challenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose main approach is the use of a classification mechanism to determine whether local or global illumination enhancement is required. Subsequently, estimator networks adjust illumination based on this classification and simultaneously enhance color fidelity. ALEN integrates the Light Classification Network (LCNet) for illuminance categorization, complemented by the Single-Channel Network (SCNet), and Multi-Channel Network (MCNet) for precise estimation of illumination and color, respectively. Extensive experiments on publicly available datasets for low-light conditions were carried out to underscore ALEN's robust generalization capabilities, demonstrating superior performance in both quantitative metrics and qualitative assessments when compared to recent state-of-the-art methods. The ALEN not only enhances image quality in terms of visual perception but also represents an advancement in high-level vision tasks, such as semantic segmentation, as presented in this work. The code of this method is available at https://github.com/xingyumex/ALEN
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOP-Distrib: Revisiting 6D Pose Estimation Benchmarks for Better Evaluation under Visual Ambiguities</title>
<link>https://arxiv.org/abs/2408.17297</link>
<guid>https://arxiv.org/abs/2408.17297</guid>
<content:encoded><![CDATA[
arXiv:2408.17297v3 Announce Type: replace 
Abstract: 6D pose estimation aims at determining the object pose that best explains the camera observation. The unique solution for non-ambiguous objects can turn into a multi-modal pose distribution for symmetrical objects or when occlusions of symmetry-breaking elements happen, depending on the viewpoint. Currently, 6D pose estimation methods are benchmarked on datasets that consider, for their ground truth annotations, visual ambiguities as only related to global object symmetries, whereas they should be defined per-image to account for the camera viewpoint. We thus first propose an automatic method to re-annotate those datasets with a 6D pose distribution specific to each image, taking into account the object surface visibility in the image to correctly determine the visual ambiguities. Second, given this improved ground truth, we re-evaluate the state-of-the-art single pose methods and show that this greatly modifies the ranking of these methods. Third, as some recent works focus on estimating the complete set of solutions, we derive a precision/recall formulation to evaluate them against our image-wise distribution ground truth, making it the first benchmark for pose distribution methods on real images.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Homography is All You Need: IMM-based Joint Homography and Multiple Object State Estimation</title>
<link>https://arxiv.org/abs/2409.02562</link>
<guid>https://arxiv.org/abs/2409.02562</guid>
<content:encoded><![CDATA[
arXiv:2409.02562v3 Announce Type: replace 
Abstract: A novel online MOT algorithm, IMM Joint Homography State Estimation (IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the only additional 3D information, whereas other 3D MOT methods use regular 3D measurements. By jointly modelling the homography matrix and its dynamics as part of track state vectors, IMM-JHSE removes the explicit influence of camera motion compensation techniques on predicted track position states, which was prevalent in previous approaches. Expanding upon this, static and dynamic camera motion models are combined using an IMM filter. A simple bounding box motion model is used to predict bounding box positions to incorporate image plane information. In addition to applying an IMM to camera motion, a non-standard IMM approach is applied where bounding-box-based BIoU scores are mixed with ground-plane-based Mahalanobis distances in an IMM-like fashion to perform association only, making IMM-JHSE robust to motion away from the ground plane. Finally, IMM-JHSE makes use of dynamic process and measurement noise estimation techniques. IMM-JHSE improves upon related techniques, including UCMCTrack, OC-SORT, C-BIoU and ByteTrack on the DanceTrack and KITTI-car datasets, increasing HOTA by 2.64 and 2.11, respectively, while offering competitive performance on the MOT17, MOT20 and KITTI-pedestrian datasets. Using publicly available detections, IMM-JHSE outperforms almost all other 2D MOT methods and is outperformed only by 3D MOT methods -- some of which are offline -- on the KITTI-car dataset. Compared to tracking-by-attention methods, IMM-JHSE shows remarkably similar performance on the DanceTrack dataset and outperforms them on the MOT17 dataset. The code is publicly available: https://github.com/Paulkie99/imm-jhse.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features</title>
<link>https://arxiv.org/abs/2409.18769</link>
<guid>https://arxiv.org/abs/2409.18769</guid>
<content:encoded><![CDATA[
arXiv:2409.18769v5 Announce Type: replace 
Abstract: Periorbital distances are critical markers for diagnosing and monitoring a range of oculoplastic and craniofacial conditions. Manual measurement, however, is subjective and prone to intergrader variability. Automated methods have been developed but remain limited by standardized imaging requirements, small datasets, and a narrow focus on individual measurements. We developed a segmentation pipeline trained on a domain-specific dataset of healthy eyes and compared its performance against the Segment Anything Model (SAM) and the prior benchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple disease classes and imaging conditions. We further investigated the use of predicted periorbital distances as features for disease classification under in-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow classifiers, CNNs, and fusion models. Our segmentation model achieved state-of-the-art accuracy across all datasets, with error rates within intergrader variability and superior performance relative to SAM and PeriorbitAI. In classification tasks, models trained on periorbital distances matched CNN performance on ID data (77--78\% accuracy) and substantially outperformed CNNs under OOD conditions (63--68\% accuracy vs. 14\%). Fusion models achieved the highest ID accuracy (80\%) but were sensitive to degraded CNN features under OOD shifts. Segmentation-derived periorbital distances provide robust, explainable features for disease classification and generalize better under domain shift than CNN image classifiers. These results establish a new benchmark for periorbital distance prediction and highlight the potential of anatomy-based AI pipelines for real-world deployment in oculoplastic and craniofacial care.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos</title>
<link>https://arxiv.org/abs/2410.07795</link>
<guid>https://arxiv.org/abs/2410.07795</guid>
<content:encoded><![CDATA[
arXiv:2410.07795v4 Announce Type: replace 
Abstract: Human motion capture from monocular videos has made significant progress in recent years. However, modern approaches often produce temporal artifacts, e.g. in form of jittery motion and struggle to achieve smooth and physically plausible motions. Explicitly integrating physics, in form of internal forces and exterior torques, helps alleviating these artifacts. Current state-of-the-art approaches make use of an automatic PD controller to predict torques and reaction forces in order to re-simulate the input kinematics, i.e. the joint angles of a predefined skeleton. However, due to imperfect physical models, these methods often require simplifying assumptions and extensive preprocessing of the input kinematics to achieve good performance. To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach. We develop a control loop as a meta-PD controller to predict internal joint torques and external reaction forces, followed by a physics-based motion simulation. A recurrent neural network is introduced to realize a Kalman filter that attentively balances the kinematics input and simulated motion, resulting in an optimal-state dynamics prediction. We show that this filtering step is crucial to provide an online supervision that helps balancing the shortcoming of the respective input motions, thus being important for not only capturing accurate global motion trajectories but also producing physically plausible human poses. The proposed approach excels in the physics-based human pose estimation task and demonstrates the physical plausibility of the predictive dynamics, compared to state of the art. The code is available on https://github.com/cuongle1206/OSDCap
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflecting Topology Consistency and Abnormality via Learnable Attentions for Airway Labeling</title>
<link>https://arxiv.org/abs/2410.23854</link>
<guid>https://arxiv.org/abs/2410.23854</guid>
<content:encoded><![CDATA[
arXiv:2410.23854v2 Announce Type: replace 
Abstract: Accurate airway anatomical labeling is crucial for clinicians to identify and navigate complex bronchial structures during bronchoscopy. Automatic airway anatomical labeling is challenging due to significant individual variability and anatomical variations. Previous methods are prone to generate inconsistent predictions, which is harmful for preoperative planning and intraoperative navigation. This paper aims to address these challenges by proposing a novel method that enhances topological consistency and improves the detection of abnormal airway branches. We propose a novel approach incorporating two modules: the Soft Subtree Consistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module constructs a soft subtree to capture clinically relevant topological relationships, allowing for flexible feature aggregation within and across subtrees. The ABS module facilitates the interaction between node features and prototypes to distinguish abnormal branches, preventing the erroneous aggregation of features between normal and abnormal nodes. Evaluated on a challenging dataset characterized by severe airway distortion and atrophy, our method achieves superior performance compared to state-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the segmental level and an 83.7% accuracy at the subsegmental level, representing a 1.4% increase in subsegmental accuracy and a 3.1% increase in topological consistency. Notably, the method demonstrates reliable performance in cases with disease-induced airway deformities, ensuring consistent and accurate labeling.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment</title>
<link>https://arxiv.org/abs/2412.01986</link>
<guid>https://arxiv.org/abs/2412.01986</guid>
<content:encoded><![CDATA[
arXiv:2412.01986v2 Announce Type: replace 
Abstract: Mesh quality assessment (MQA) models play a critical role in the design, optimization, and evaluation of mesh operation systems in a wide variety of applications. Current MQA models, whether model-based methods using topology-aware features or projection-based approaches working on rendered 2D projections, often fail to capture the intricate interactions between texture and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid full-reference colored MQA framework that integrates model-based and projection-based approaches, capturing complex interactions between textural information and 3D structures for enriched quality representations. Our method employs graph learning to extract detailed 3D representations, which are then projected to 2D using a novel feature rendering process that precisely aligns them with colored projections. This enables the exploration of geometry-texture interactions via cross-attention, producing comprehensive mesh quality representations. Extensive experiments demonstrate HybridMQA's superior performance across diverse datasets, highlighting its ability to effectively leverage geometry-texture interactions for a thorough understanding of mesh quality. Our implementation will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model Trained with a Single GPU in Just One Day</title>
<link>https://arxiv.org/abs/2412.05888</link>
<guid>https://arxiv.org/abs/2412.05888</guid>
<content:encoded><![CDATA[
arXiv:2412.05888v2 Announce Type: replace 
Abstract: Medical image segmentation involves partitioning medical images into meaningful regions, with a focus on identifying anatomical structures and lesions. It has broad applications in healthcare, and deep learning methods have enabled significant advancements in automating this process. Recently, the introduction of the Segmentation Anything Model (SAM), the first foundation model for segmentation task, has prompted researchers to adapt it for the medical domain to improve performance across various tasks. However, SAM's large model size and high GPU requirements hinder its scalability and development in the medical domain. In this work, we propose MCP-MedSAM, a powerful and lightweight medical SAM model designed to be trainable on a single A100 GPU with 40GB of memory within one day while delivering superior segmentation performance. Recognizing the significant internal differences between modalities and the need for direct segmentation target information within bounding boxes, we introduce two kinds of prompts: the modality prompt and the content prompt. After passing through the prompt encoder, their embedding representations can further improve the segmentation performance by incorporating more relevant information without adding significant training overhead. Additionally, we adopt an effective modality-based data sampling strategy to address data imbalance between modalities, ensuring more balanced performance across all modalities. Our method was trained and evaluated using a large-scale challenge dataset, compared to top-ranking methods on the challenge leaderboard, MCP-MedSAM achieved superior performance while requiring only one day of training on a single GPU. The code is publicly available at \textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An ocean front detection and tracking algorithm</title>
<link>https://arxiv.org/abs/2502.15250</link>
<guid>https://arxiv.org/abs/2502.15250</guid>
<content:encoded><![CDATA[
arXiv:2502.15250v4 Announce Type: replace 
Abstract: Existing ocean front detection methods--including histogram-based variance analysis, Lyapunov exponent, gradient thresholding, and machine learning--suffer from critical limitations: discontinuous outputs, over-detection, reliance on single-threshold decisions, and lack of open-source implementations. To address these challenges, this paper proposes the Bayesian Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA). The framework introduces three innovations: (1) a Bayesian decision mechanism that integrates gradient priors and field operators to eliminate manual threshold sensitivity; (2) morphological refinement algorithms for merging fragmented fronts, deleting spurious rings, and thinning frontal zones to pixel-level accuracy; and (3) a novel metric space definition for temporal front tracking, enabling systematic analysis of front evolution. Validated on global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\%$ compared to histogram-based methods while achieving superior intensity ($0.16^\circ$C/km), continuity, and spatiotemporal coherence. The open-source release bridges a critical gap in reproducible oceanographic research.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-R1: Reinforcing Video Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2503.21776</link>
<guid>https://arxiv.org/abs/2503.21776</guid>
<content:encoded><![CDATA[
arXiv:2503.21776v2 Announce Type: replace 
Abstract: Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data are released in: https://github.com/tulerfeng/Video-R1.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation</title>
<link>https://arxiv.org/abs/2504.14988</link>
<guid>https://arxiv.org/abs/2504.14988</guid>
<content:encoded><![CDATA[
arXiv:2504.14988v2 Announce Type: replace 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal perception capabilities, garnering significant attention. While numerous evaluation studies have emerged, assessing LVLMs both holistically and on specialized tasks, fine-grained image tasks-fundamental to computer vision-remain largely unexplored. To fill this gap, we introduce a comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 1.01 million questions and 0.33 million images. Our evaluation systematically examines LVLMs from both human-oriented and machine-oriented perspectives, focusing on their semantic recognition and fine-grained feature representation capabilities. Through extensive experiments on twelve representative LVLMs/VLMs, we uncover key findings regarding the influence of training paradigms, modality alignment, perturbation susceptibility, and fine-grained category reasoning on task performance. This work provides critical insights into the limitations of current LVLMs and offers guidance for future data construction and model design in the development of more advanced LVLMs. Our code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction</title>
<link>https://arxiv.org/abs/2505.02126</link>
<guid>https://arxiv.org/abs/2505.02126</guid>
<content:encoded><![CDATA[
arXiv:2505.02126v2 Announce Type: replace 
Abstract: Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects</title>
<link>https://arxiv.org/abs/2204.03139</link>
<guid>https://arxiv.org/abs/2204.03139</guid>
<content:encoded><![CDATA[
arXiv:2204.03139v2 Announce Type: replace-cross 
Abstract: Research in manipulation of deformable objects is typically conducted on a limited range of scenarios, because handling each scenario on hardware takes significant effort. Realistic simulators with support for various types of deformations and interactions have the potential to speed up experimentation with novel tasks and algorithms. However, for highly deformable objects it is challenging to align the output of a simulator with the behavior of real objects. Manual tuning is not intuitive, hence automated methods are needed. We view this alignment problem as a joint perception-inference challenge and demonstrate how to use recent neural network architectures to successfully perform simulation parameter inference from real point clouds. We analyze the performance of various architectures, comparing their data and training requirements. Furthermore, we propose to leverage differentiable point cloud sampling and differentiable simulation to significantly reduce the time to achieve the alignment. We employ an efficient way to propagate gradients from point clouds to simulated meshes and further through to the physical simulation parameters, such as mass and stiffness. Experiments with highly deformable objects show that our method can achieve comparable or better alignment with real object behavior, while reducing the time needed to achieve this by more than an order of magnitude. Videos and supplementary material are available at https://diffcloud.github.io.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error correcting 2D-3D cascaded network for myocardial infarct scar segmentation on late gadolinium enhancement cardiac magnetic resonance images</title>
<link>https://arxiv.org/abs/2306.14725</link>
<guid>https://arxiv.org/abs/2306.14725</guid>
<content:encoded><![CDATA[
arXiv:2306.14725v2 Announce Type: replace-cross 
Abstract: Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is considered the in vivo reference standard for assessing infarct size (IS) and microvascular obstruction (MVO) in ST-elevation myocardial infarction (STEMI) patients. However, the exact quantification of those markers of myocardial infarct severity remains challenging and very time-consuming. As LGE distribution patterns can be quite complex and hard to delineate from the blood pool or epicardial fat, automatic segmentation of LGE CMR images is challenging. In this work, we propose a cascaded framework of two-dimensional and three-dimensional convolutional neural networks (CNNs) which enables to calculate the extent of myocardial infarction in a fully automated way. By artificially generating segmentation errors which are characteristic for 2D CNNs during training of the cascaded framework we are enforcing the detection and correction of 2D segmentation errors and hence improve the segmentation accuracy of the entire method. The proposed method was trained and evaluated on two publicly available datasets. We perform comparative experiments where we show that our framework outperforms state-of-the-art reference methods in segmentation of myocardial infarction. Furthermore, in extensive ablation studies we show the advantages that come with the proposed error correcting cascaded method. The code of this project is publicly available at https://github.com/matthi99/EcorC.git
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Call to Arms: AI Should be Critical for Social Media Analysis of Conflict Zones</title>
<link>https://arxiv.org/abs/2311.00810</link>
<guid>https://arxiv.org/abs/2311.00810</guid>
<content:encoded><![CDATA[
arXiv:2311.00810v3 Announce Type: replace-cross 
Abstract: The massive proliferation of social media data represents a transformative opportunity for conflict studies and for tracking the proliferation and use of weaponry, as conflicts are increasingly documented in these online spaces. At the same time, the scale and types of data available are problematic for traditional open-source intelligence. This paper focuses on identifying specific weapon systems and the insignias of the armed groups using them as documented in the Ukraine war, as these tasks are critical to operational intelligence and tracking weapon proliferation, especially given the scale of international military aid given to Ukraine. The large scale of social media makes manual assessment difficult, however, so this paper presents early work that uses computer vision models to support this task. We demonstrate that these models can both identify weapons embedded in images shared in social media and how the resulting collection of military-relevant images and their post times interact with the offline, real-world conflict. Not only can we then track changes in the prevalence of images of tanks, land mines, military trucks, etc., we find correlations among time series data associated with these images and the daily fatalities in this conflict. This work shows substantial opportunity for examining similar online documentation of conflict contexts, and we also point to future avenues where computer vision can be further improved for these open-source intelligence tasks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues</title>
<link>https://arxiv.org/abs/2403.02043</link>
<guid>https://arxiv.org/abs/2403.02043</guid>
<content:encoded><![CDATA[
arXiv:2403.02043v2 Announce Type: replace-cross 
Abstract: Light field cameras and multi-camera arrays have emerged as promising solutions for accurately estimating depth by passively capturing light information. This is possible because the 3D information of a scene is embedded in the 4D light field geometry. Commonly, depth estimation methods extract this information relying on gradient information, heuristic-based optimisation models, or learning-based approaches. This paper focuses mainly on explicitly understanding and exploiting 4D geometrical cues for light field depth estimation. Thus, a novel method is proposed, based on a non-learning-based optimisation approach for depth estimation that explicitly considers surface normal accuracy and occlusion regions by utilising a fully explainable 4D geometric model of the light field. The 4D model performs depth/disparity estimation by determining the orientations and analysing the intersections of key 2D planes in 4D space, which are the images of 3D-space points in the 4D light field. Experimental results show that the proposed method outperforms both learning-based and non-learning-based state-of-the-art methods in terms of surface normal angle accuracy, achieving a Median Angle Error on planar surfaces, on average, 26.3$\%$ lower than the state-of-the-art, and still being competitive with state-of-the-art methods in terms of MSE ${\times}$ 100 and Badpix 0.07.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Quantum Annealing and Spiking Neuromorphic Computing for Sampling Binary Sparse Coding QUBO Problems</title>
<link>https://arxiv.org/abs/2405.20525</link>
<guid>https://arxiv.org/abs/2405.20525</guid>
<content:encoded><![CDATA[
arXiv:2405.20525v2 Announce Type: replace-cross 
Abstract: We consider the problem of computing a sparse binary representation of an image. To be precise, given an image and an overcomplete, non-orthonormal basis, we aim to find a sparse binary vector indicating the minimal set of basis vectors that when added together best reconstruct the given input. We formulate this problem with an $L_2$ loss on the reconstruction error, and an $L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing sparsity. This yields a quadratic unconstrained binary optimization problem (QUBO), whose optimal solution(s) in general is NP-hard to find. The contribution of this work is twofold. First, we solve the sparse representation QUBOs by solving them both on a D-Wave quantum annealer with Pegasus chip connectivity via minor embedding, as well as on the Intel Loihi 2 spiking neuromorphic processor using a stochastic Non-equilibrium Boltzmann Machine (NEBM). Second, we deploy Quantum Evolution Monte Carlo with Reverse Annealing and iterated warm starting on Loihi 2 to evolve the solution quality from the respective machines. The solutions are benchmarked against simulated annealing, a classical heuristic, and the optimal solutions are computed using CPLEX. Iterated reverse quantum annealing performs similarly to simulated annealing, although simulated annealing is always able to sample the optimal solution whereas quantum annealing was not always able to. The Loihi 2 solutions that are sampled are on average more sparse than the solutions from any of the other methods. We demonstrate that both quantum annealing and neuromorphic computing are suitable for binary sparse coding QUBOs, and that Loihi 2 outperforms a D-Wave quantum annealer standard linear-schedule anneal, while iterated reverse quantum annealing performs much better than both unmodified linear-schedule quantum annealing and iterated warm starting on Loihi 2.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METDrive: Multi-modal End-to-end Autonomous Driving with Temporal Guidance</title>
<link>https://arxiv.org/abs/2409.12667</link>
<guid>https://arxiv.org/abs/2409.12667</guid>
<content:encoded><![CDATA[
arXiv:2409.12667v3 Announce Type: replace-cross 
Abstract: Multi-modal end-to-end autonomous driving has shown promising advancements in recent work. By embedding more modalities into end-to-end networks, the system's understanding of both static and dynamic aspects of the driving environment is enhanced, thereby improving the safety of autonomous driving. In this paper, we introduce METDrive, an end-to-end system that leverages temporal guidance from the embedded time series features of ego states, including rotation angles, steering, throttle signals, and waypoint vectors. The geometric features derived from perception sensor data and the time series features of ego state data jointly guide the waypoint prediction with the proposed temporal guidance loss function. We evaluated METDrive on the CARLA leaderboard benchmarks, achieving a driving score of 70%, a route completion score of 94%, and an infraction score of 0.78.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2409.13498</link>
<guid>https://arxiv.org/abs/2409.13498</guid>
<content:encoded><![CDATA[
arXiv:2409.13498v2 Announce Type: replace-cross 
Abstract: Recent advancements in computer vision, particularly in detection, segmentation, and classification, have significantly impacted various domains. However, these advancements are tied to RGB-based systems, which are insufficient for applications in industries like waste sorting, pharmaceuticals, and defense, where advanced object characterization beyond shape or color is necessary. Hyperspectral (HS) imaging, capturing both spectral and spatial information, addresses these limitations and offers advantages over conventional technologies such as X-ray fluorescence and Raman spectroscopy, particularly in terms of speed, cost, and safety.
  This study evaluates the potential of combining HS imaging with deep learning for material characterization. The research involves: i) designing an experimental setup with HS camera, conveyor, and controlled lighting; ii) generating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with semi-automated mask generation and Raman spectroscopy-based labeling; and iii) developing a deep learning model trained on HS images for pixel-level material classification. The model achieved 99.94\% classification accuracy, demonstrating robustness in color, size, and shape invariance, and effectively handling material overlap. Limitations, such as challenges with black objects, are also discussed. Extending computer vision beyond RGB to HS imaging proves feasible, overcoming major limitations of traditional methods and showing strong potential for future applications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2409.18872</link>
<guid>https://arxiv.org/abs/2409.18872</guid>
<content:encoded><![CDATA[
arXiv:2409.18872v2 Announce Type: replace-cross 
Abstract: This paper presents a method for virtual contrast enhancement in breast MRI, offering a promising non-invasive alternative to traditional contrast agent-based DCE-MRI acquisition. Using a conditional generative adversarial network, we predict DCE-MRI images, including jointly-generated sequences of multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs, enabling tumor localization and characterization without the associated health risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe), assessing their utility in a tumor segmentation downstream task, and conclude with an analysis of the temporal patterns in multi-sequence DCE-MRI generation. Our approach demonstrates promising results in generating realistic and useful DCE-MRI sequences, highlighting the potential of virtual contrast enhancement for improving breast cancer diagnosis and treatment, particularly for patients where contrast agent administration is contraindicated.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Traffic Anomalies from Generative Models on Real-Time Observations</title>
<link>https://arxiv.org/abs/2502.01391</link>
<guid>https://arxiv.org/abs/2502.01391</guid>
<content:encoded><![CDATA[
arXiv:2502.01391v2 Announce Type: replace-cross 
Abstract: Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
arXiv:2503.21696v2 Announce Type: replace-cross 
Abstract: Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling</title>
<link>https://arxiv.org/abs/2504.14219</link>
<guid>https://arxiv.org/abs/2504.14219</guid>
<content:encoded><![CDATA[
arXiv:2504.14219v2 Announce Type: replace-cross 
Abstract: We present PRISM, a unified framework that enables multiple image generation and editing tasks in a single foundational model. Starting from a pre-trained text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy to produce RGB images along with intrinsic maps (referred to as X layers) simultaneously. Unlike previous approaches, which infer intrinsic properties individually or require separate models for decomposition and conditional generation, PRISM maintains consistency across modalities by generating all intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX generation, RGB-to-X decomposition, and X-to-RGBX conditional generation. Additionally, PRISM enables both global and local image editing through conditioning on selected intrinsic layers and text prompts. Extensive experiments demonstrate the competitive performance of PRISM both for intrinsic image decomposition and conditional image generation while preserving the base model's text-to-image generation capability.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets</title>
<link>https://arxiv.org/abs/2504.21227</link>
<guid>https://arxiv.org/abs/2504.21227</guid>
<content:encoded><![CDATA[
arXiv:2504.21227v2 Announce Type: replace-cross 
Abstract: Deep learning models have great potential in medical imaging, including orthodontics and skeletal maturity assessment. However, applying a model to data different from its training set can lead to unreliable predictions that may impact patient care. To address this, we propose a comprehensive verification framework that evaluates model suitability through multiple complementary strategies. First, we introduce a Gradient Attention Map (GAM)-based approach that analyzes attention patterns using Grad-CAM and compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance. Second, we extend verification to early convolutional feature maps, capturing structural mis-alignments missed by attention alone. Finally, we incorporate an additional garbage class into the classification model to explicitly reject out-of-distribution inputs. Experimental results demonstrate that these combined methods effectively identify unsuitable models and inputs, promoting safer and more reliable deployment of deep learning in medical imaging.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
<link>https://arxiv.org/abs/2505.02831</link>
<guid>https://arxiv.org/abs/2505.02831</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion transformers, representation guidance, Self-Representation Alignment, generative training, self-distillation  

<br /><br />Summary: This study addresses the challenges in improving generative training by proposing a novel method called Self-Representation Alignment (SRA). Recent works have shown that meaningful internal representations can enhance both the training speed and output quality in diffusion transformers. Traditional methods either require complex external representation frameworks or rely on large pre-trained foundation models for guidance. In contrast, SRA leverages the inherent discriminative process of diffusion transformers, allowing them to generate representation guidance internally during generative training. The method works by aligning the output latent representations from earlier layers (which have higher noise) with those from later layers (with lower noise). This alignment process enhances overall representation learning without the need for external components. Experimental results demonstrate that applying SRA to Diffusion Transformers (DiTs) and Simple Transformers (SiTs) results in consistent performance improvements. Notably, SRA surpasses existing approaches that depend on auxiliary representation frameworks and achieves performance levels comparable to methods reliant on sophisticated external representation priors, highlighting its efficacy and simplicity in optimizing generative processes. <div>
arXiv:2505.02831v3 Announce Type: replace 
Abstract: Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance the generation quality of diffusion transformers. However, existing approaches necessitate to either introduce an external and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation Alignment (SRA), a simple yet straightforward method that obtains representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in the earlier layer with higher noise to that in the later layer with lower noise to progressively enhance the overall representation learning during only the generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that are heavily dependent on powerful external representation priors.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing</title>
<link>https://arxiv.org/abs/2505.07984</link>
<guid>https://arxiv.org/abs/2505.07984</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal language model, remote sensing imagery, fine-tuning, Group Relative Policy Optimization, military installations

Summary: 
MilChat is a lightweight multimodal language model designed for analyzing remote sensing imagery in secluded areas, particularly focusing on challenging missile launch sites. A new dataset, MilData, was compiled to train the model with expert-verified aerial images and detailed captions highlighting military installations. The model underwent supervised fine-tuning using CoT reasoning annotations and utilized Group Relative Policy Optimization to improve domain-specific cue detection while minimizing false positives. Empirical evaluations showed that MilChat outperformed larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification tasks. Achieving over 80% recall and 98% precision on the MilData benchmark, the study highlights the effectiveness of targeted fine-tuning and reinforcement learning in specialized domains. 

<br /><br />Summary: <div>
arXiv:2505.07984v1 Announce Type: new 
Abstract: Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed MilChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, MilData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that MilChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed MilData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Foundation Model Embedding-Based Semantic Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.07998</link>
<guid>https://arxiv.org/abs/2505.07998</guid>
<content:encoded><![CDATA[
<div> semantic anomaly detection, visual embeddings, autonomous systems, instance segmentation, filtering

Summary: 
This paper investigates semantic anomaly detection in autonomous systems using state-of-the-art vision foundation models. The framework proposed compares local vision embeddings from runtime images to a database of safe scenarios to identify anomalies. Two variants of the framework are considered, one using raw grid-based embeddings and the other leveraging instance segmentation for object-centric representations. A filtering mechanism is introduced to reduce false positives and enhance robustness. Evaluation on CARLA-simulated anomalies shows that the instance-based method with filtering achieves performance comparable to GPT-4o, with precise anomaly localization. These results demonstrate the potential of vision embeddings from foundation models for real-time anomaly detection in autonomous systems. 

Summary: <div>
arXiv:2505.07998v1 Announce Type: new 
Abstract: Semantic anomalies are contextually invalid or unusual combinations of familiar visual elements that can cause undefined behavior and failures in system-level reasoning for autonomous systems. This work explores semantic anomaly detection by leveraging the semantic priors of state-of-the-art vision foundation models, operating directly on the image. We propose a framework that compares local vision embeddings from runtime images to a database of nominal scenarios in which the autonomous system is deemed safe and performant. In this work, we consider two variants of the proposed framework: one using raw grid-based embeddings, and another leveraging instance segmentation for object-centric representations. To further improve robustness, we introduce a simple filtering mechanism to suppress false positives. Our evaluations on CARLA-simulated anomalies show that the instance-based method with filtering achieves performance comparable to GPT-4o, while providing precise anomaly localization. These results highlight the potential utility of vision embeddings from foundation models for real-time anomaly detection in autonomous systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</title>
<link>https://arxiv.org/abs/2505.08013</link>
<guid>https://arxiv.org/abs/2505.08013</guid>
<content:encoded><![CDATA[
<div> Keywords: feature detection, structure-from-motion, SLAM, deformable transformer, keypoint detection

Summary:
Robust feature detection and description in structure-from-motion and SLAM applications remain challenging, particularly in cases of significant viewpoint changes. Existing methods focus on local features but fail to capture long-range relationships. The Robust Deformable Detector (RDD) introduced in this study leverages deformable transformers to incorporate global context and geometric invariance through deformable self-attention mechanisms. By focusing on key locations, the deformable attention effectively reduces search space complexity and models geometric invariance. The RDD outperforms state-of-the-art methods in keypoint detection/description tasks and can handle semi-dense matching. The study also introduces two new benchmarks, emphasizing large viewpoint and scale variations, as well as an Air-to-Ground benchmark, showcasing the method's effectiveness for 3D reconstruction across different altitudes.

<br /><br />Summary: <div>
arXiv:2505.08013v1 Announce Type: new 
Abstract: As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Interpretable Subtask Reasoning for Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.08084</link>
<guid>https://arxiv.org/abs/2505.08084</guid>
<content:encoded><![CDATA[
<div> Keywords: visual question answering, multimodal large language models, reasoning, interpretability, subtask-driven training

Summary:<br />
The article introduces VISTAR, a framework for enhancing interpretability and reasoning in visual question answering tasks. VISTAR utilizes subtask-driven training to generate structured reasoning sequences within multimodal large language models. By fine-tuning these models to produce step-by-step rationales, VISTAR improves reasoning accuracy while maintaining interpretability. The framework addresses the limitations of current methods by integrating both textual and visual explanations directly into the model, eliminating the need for external models. Experimental results on two benchmarks demonstrate the effectiveness of VISTAR in improving reasoning performance. The code and dataset for VISTAR will be made available for further research and development. 

Summary: <div>
arXiv:2505.08084v1 Announce Type: new 
Abstract: Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)</title>
<link>https://arxiv.org/abs/2505.08086</link>
<guid>https://arxiv.org/abs/2505.08086</guid>
<content:encoded><![CDATA[
<div> Keywords: wound classification, Artificial Intelligence, transfer learning, multi-modal network, medical image analysis <br />
Summary: <br />
- Effective diagnosis of acute and hard-to-heal wounds is crucial for providing proper patient care, especially in cases of infection, peripheral vascular disease, and increasing wound depth.
- Utilizing Artificial Intelligence (AI) in medical image interpretation can significantly improve early disease detection.
- A multi-modal AI model based on transfer learning (TL) was proposed, combining Xception and GMRNN architectures for wound classification with high accuracy.
- The model incorporated features extracted by TL algorithm and location features for classifying diabetic, pressure, surgical, and venous ulcers.
- The proposed methodology showcased exceptional accuracy in accurately classifying the most commonly occurring wound types using images and their locations. <div>
arXiv:2505.08086v1 Announce Type: new 
Abstract: The effective diagnosis of acute and hard-to-heal wounds is crucial for wound care practitioners to provide effective patient care. Poor clinical outcomes are often linked to infection, peripheral vascular disease, and increasing wound depth, which collectively exacerbate these comorbidities. However, diagnostic tools based on Artificial Intelligence (AI) speed up the interpretation of medical images and improve early detection of disease. In this article, we propose a multi-modal AI model based on transfer learning (TL), which combines two state-of-the-art architectures, Xception and GMRNN, for wound classification. The multi-modal network is developed by concatenating the features extracted by a transfer learning algorithm and location features to classify the wound types of diabetic, pressure, surgical, and venous ulcers. The proposed method is comprehensively compared with deep neural networks (DNN) for medical image analysis. The experimental results demonstrate a notable wound-class classifications (containing only diabetic, pressure, surgical, and venous) vary from 78.77 to 100\% in various experiments. The results presented in this study showcase the exceptional accuracy of the proposed methodology in accurately classifying the most commonly occurring wound types using wound images and their corresponding locations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing</title>
<link>https://arxiv.org/abs/2505.08101</link>
<guid>https://arxiv.org/abs/2505.08101</guid>
<content:encoded><![CDATA[
<div> Point cloud processing, autonomous driving, 3D object recognition, Point Transformer V3, distillation framework<br />
<br />
Summary: 
This paper introduces a novel distillation framework for transferring knowledge from a high-capacity teacher model to a lightweight student model in point cloud processing applications. The framework leverages topology-aware representations and gradient-guided knowledge distillation to capture geometric structures and guide the learning process effectively. Experimental results on Nuscenes, SemanticKITTI, and Waymo datasets show competitive performance with a 16x reduction in model size and nearly 1.9x decrease in inference time compared to the teacher model. The proposed method achieves state-of-the-art performance in segmentation on NuScenes, surpassing prior knowledge distillation baselines in LiDAR data training. Available publicly on GitHub at: https://github.com/HySonLab/PointDistill<br /><br /> <div>
arXiv:2505.08101v1 Announce Type: new 
Abstract: Point cloud processing has gained significant attention due to its critical role in applications such as autonomous driving and 3D object recognition. However, deploying high-performance models like Point Transformer V3 in resource-constrained environments remains challenging due to their high computational and memory demands. This work introduces a novel distillation framework that leverages topology-aware representations and gradient-guided knowledge distillation to effectively transfer knowledge from a high-capacity teacher to a lightweight student model. Our approach captures the underlying geometric structures of point clouds while selectively guiding the student model's learning process through gradient-based feature alignment. Experimental results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the proposed method achieves competitive performance, with an approximately 16x reduction in model size and a nearly 1.9x decrease in inference time compared to its teacher model. Notably, on NuScenes, our method achieves state-of-the-art performance among knowledge distillation techniques trained solely on LiDAR data, surpassing prior knowledge distillation baselines in segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors</title>
<link>https://arxiv.org/abs/2505.08111</link>
<guid>https://arxiv.org/abs/2505.08111</guid>
<content:encoded><![CDATA[
<div> sleep position classification, pressure-sensitive mats, deep learning, transfer learning, polysomnography

Summary:
The study focuses on classifying four-way sleep positions using data from pressure-sensitive mats (PSMs) placed under mattresses in a sleep clinic. Sleep positions can impact sleep quality and the prevalence of sleep disorders like apnea. The researchers used transfer learning techniques to adapt pre-trained deep learning models for accurate sleep position estimation from a low-resolution PSM dataset collected in a polysomnography sleep lab. They leveraged Vision Transformer models pre-trained on ImageNet and a model for human pose estimation. Their approach surpassed previous methods using deep learning and traditional machine learning models. The performance was evaluated on 112 nights of patient recordings and validated on a higher-resolution dataset from 13 patients. Despite challenges with low-resolution data, the approach shows promise for real-world application in clinical settings.<br /><br />Summary: <div>
arXiv:2505.08111v1 Announce Type: new 
Abstract: Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of monitoring patients during sleep. We focus on four-way sleep position classification using data collected from a PSM placed under a mattress in a sleep clinic. Sleep positions can affect sleep quality and the prevalence of sleep disorders, such as apnea. Measurements were performed on patients with suspected sleep disorders referred for assessments at a sleep clinic. Training deep learning models can be challenging in clinical settings due to the need for large amounts of labeled data. To overcome the shortage of labeled training data, we utilize transfer learning to adapt pre-trained deep learning models to accurately estimate sleep positions from a low-resolution PSM dataset collected in a polysomnography sleep lab. Our approach leverages Vision Transformer models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a pre-trained model for human pose estimation (ViTPose). These approaches outperform previous work from PSM-based sleep pose classification using deep learning (TCN) as well as traditional machine learning models (SVM, XGBoost, Random Forest) that use engineered features. We evaluate the performance of sleep position classification from 112 nights of patient recordings and validate it on a higher resolution 13-patient dataset. Despite the challenges of differentiating between sleep positions from low-resolution PSM data, our approach shows promise for real-world deployment in clinical settings
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Now you see it, Now you don't: Damage Label Agreement in Drone &amp; Satellite Post-Disaster Imagery</title>
<link>https://arxiv.org/abs/2505.08117</link>
<guid>https://arxiv.org/abs/2505.08117</guid>
<content:encoded><![CDATA[
<div> Keywords: damage labels, satellite imagery, drone imagery, machine learning, building damage assessment

Summary: 
This paper examines discrepancies in damage labels from satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey. The analysis reveals a significant 29.02% label disagreement between the sources, highlighting potential risks in deploying machine learning damage assessment systems. Satellite-derived labels were found to under-report damage by at least 20.43% compared to drone-derived labels, indicating a misrepresentation of actual conditions. The differing distributions of satellite and drone-derived labels suggest that computer vision and machine learning models trained on either dataset may not accurately represent real-world scenarios. To address these issues and prevent potential societal harm, the paper offers four recommendations for enhancing reliability and transparency in deploying CV/ML damage assessment systems. 

<br /><br />Summary: <div>
arXiv:2505.08117v1 Announce Type: new 
Abstract: This paper audits damage labels derived from coincident satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey, finding 29.02% label disagreement and significantly different distributions between the two sources, which presents risks and potential harms during the deployment of machine learning damage assessment systems. Currently, there is no known study of label agreement between drone and satellite imagery for building damage assessment. The only prior work that could be used to infer if such imagery-derived labels agree is limited by differing damage label schemas, misaligned building locations, and low data quantities. This work overcomes these limitations by comparing damage labels using the same damage label schemas and building locations from three hurricanes, with the 15,814 buildings representing 19.05 times more buildings considered than the most relevant prior work. The analysis finds satellite-derived labels significantly under-report damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and satellite- and drone-derived labels represent significantly different distributions (p<5.1x10^-175). This indicates that computer vision and machine learning (CV/ML) models trained on at least one of these distributions will misrepresent actual conditions, as the differing satellite and drone-derived distributions cannot simultaneously represent the distribution of actual conditions in a scene. This potential misrepresentation poses ethical risks and potential societal harm if not managed. To reduce the risk of future societal harms, this paper offers four recommendations to improve reliability and transparency to decisio-makers when deploying CV/ML damage assessment systems in practice
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections</title>
<link>https://arxiv.org/abs/2505.08123</link>
<guid>https://arxiv.org/abs/2505.08123</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-material decomposition, SEMMD, JSover, spectral CT, implicit neural representation

Summary: 
JSover is a new one-step framework for multi-material decomposition (MMD) in CT imaging that improves the accuracy and efficiency of traditional methods. Unlike previous two-step approaches, JSover simultaneously reconstructs tissue compositions and estimates the energy spectrum directly from single-energy CT projections. By incorporating physics-informed spectral priors, it creates a virtual spectral CT system from SE acquisitions, reducing beam hardening artifacts. Additionally, JSover leverages implicit neural representation (INR) to enhance material map estimation using unsupervised deep learning. Experimental results demonstrate that JSover surpasses existing SEMMD techniques in accuracy and computational performance on both simulated and real CT datasets. This advancement in MMD technology holds potential for a wide range of clinical applications by enabling more reliable quantitative tissue composition reconstructions from single-energy CT scans.<br /><br />Summary: <div>
arXiv:2505.08123v1 Announce Type: new 
Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLAG: Scalable Language-Augmented Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08124</link>
<guid>https://arxiv.org/abs/2505.08124</guid>
<content:encoded><![CDATA[
<div> framework, language-augmented, scene representations, Gaussian splatting, scalability
Summary:
SLAG is a multi-GPU framework that accelerates the encoding of language-augmented scene representations for robotics applications. By integrating 2D visual-language model features into 3D scenes using SAM and CLIP, SLAG eliminates the need for loss functions and enables highly parallelized scene encoding. The method achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while maintaining embedding quality on datasets like ScanNet and LERF. Additionally, SLAG introduces a vector database for efficient embedding storage and retrieval, making it suitable for time-sensitive and data-intensive scenarios. The framework holds promise for use in search-and-rescue operations, smart cities, mining, and other large-scale robotics applications. Visit the project website for more information: https://slag-project.github.io/. 

<br /><br />Summary: <div>
arXiv:2505.08124v1 Announce Type: new 
Abstract: Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Multi-Object Tracking with an Event Camera</title>
<link>https://arxiv.org/abs/2505.08126</link>
<guid>https://arxiv.org/abs/2505.08126</guid>
<content:encoded><![CDATA[
<div> Event cameras, Asynchronous Event Multi-Object Tracking, AEMOT, object detection, object tracking<br />
Summary:<br />
Events cameras with their low latency output, high temporal resolution, and dynamic range, are ideal for detecting and tracking objects in dynamic environments. The Asynchronous Event Multi-Object Tracking (AEMOT) algorithm processes individual raw events asynchronously to detect and track multiple objects. By identifying salient event blob features through optical flow analysis, AEMOT tracks candidate objects using the Asynchronous Event Blob (AEB) tracker. A novel validation stage distinguishes and estimates object characteristics such as position, velocity, size, and orientation in real time. Tested on the Bee Swarm Dataset, AEMOT outperforms other event-based detection and tracking algorithms with precision and recall rates exceeding 37%. The algorithm and the labeled event dataset will be made open source. <br />Summary: <div>
arXiv:2505.08126v1 Announce Type: new 
Abstract: Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoKD: Multi-Task Optimization for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.08170</link>
<guid>https://arxiv.org/abs/2505.08170</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Multi-Task Optimization, Gradient Conflicts, Gradient Dominance, Image Classification, Object Detection <br />
<br />Summary: <br />Compact models can be effectively trained through Knowledge Distillation (KD), with two main challenges being balancing guidance from the teacher model and the task objective, and handling knowledge representation disparities between teacher and student models. To address these challenges, Multi-Task Optimization for Knowledge Distillation (MoKD) is proposed. MoKD reformulates KD as a multi-objective optimization problem to balance objectives and introduces a subspace learning framework to improve knowledge transfer by projecting feature representations into a high-dimensional space. Results from experiments on ImageNet-1K dataset and COCO dataset show that MoKD outperforms existing methods in image classification and object detection, achieving state-of-the-art performance with greater efficiency. Additionally, MoKD models achieve state-of-the-art performance compared to models trained from scratch. <div>
arXiv:2505.08170v1 Announce Type: new 
Abstract: Compact models can be effectively trained through Knowledge Distillation (KD), a technique that transfers knowledge from larger, high-performing teacher models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing learning from the teacher's guidance and the task objective, and 2) handling the disparity in knowledge representation between teacher and student models. To address these, we propose Multi-Task Optimization for Knowledge Distillation (MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where task-specific and distillation gradients are misaligned, and b) Gradient Dominance, where one objective's gradient dominates, causing imbalance. MoKD reformulates KD as a multi-objective optimization problem, enabling better balance between objectives. Additionally, it introduces a subspace learning framework to project feature representations into a high-dimensional space, improving knowledge transfer. Our MoKD is demonstrated to outperform existing methods through extensive experiments on image classification using the ImageNet-1K dataset and object detection using the COCO dataset, achieving state-of-the-art performance with greater efficiency. To the best of our knowledge, MoKD models also achieve state-of-the-art performance compared to models trained from scratch.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification</title>
<link>https://arxiv.org/abs/2505.08173</link>
<guid>https://arxiv.org/abs/2505.08173</guid>
<content:encoded><![CDATA[
<div> causal inference, long-tail classification, Visual Transformers, TSCNet, multi-scale causal interventions <br />
<br />
Summary: 
This paper addresses the challenge of handling biases introduced by class imbalance in long-tail classification by investigating the influence of existing causal models on CNNs and Visual Transformers. The study shows that Visual Transformers' global feature representation makes it difficult for causal methods to model associations between fine-grained features and predictions, especially for tail classes with similar visual appearance. To tackle this issue, the paper proposes TSCNet, a two-stage causal modeling method that employs multi-scale causal interventions. The first stage focuses on hierarchical causal representation learning by decoupling background and objects and applying backdoor interventions. The second stage, counterfactual logits bias calibration, refines the model's decision boundary by constructing a counterfactual balanced data distribution. Experimental results demonstrate that TSCNet outperforms existing methods in eliminating biases caused by data imbalance. <br /><br /> <div>
arXiv:2505.08173v1 Announce Type: new 
Abstract: Causal inference has emerged as a promising approach to mitigate long-tail classification by handling the biases introduced by class imbalance. However, along with the change of advanced backbone models from Convolutional Neural Networks (CNNs) to Visual Transformers (ViT), existing causal models may not achieve an expected performance gain. This paper investigates the influence of existing causal models on CNNs and ViT variants, highlighting that ViT's global feature representation makes it hard for causal methods to model associations between fine-grained features and predictions, which leads to difficulties in classifying tail classes with similar visual appearance. To address these issues, this paper proposes TSCNet, a two-stage causal modeling method to discover fine-grained causal associations through multi-scale causal interventions. Specifically, in the hierarchical causal representation learning stage (HCRL), it decouples the background and objects, applying backdoor interventions at both the patch and feature level to prevent model from using class-irrelevant areas to infer labels which enhances fine-grained causal representation. In the counterfactual logits bias calibration stage (CLBC), it refines the optimization of model's decision boundary by adaptive constructing counterfactual balanced data distribution to remove the spurious associations in the logits caused by data distribution. Extensive experiments conducted on various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate multiple biases introduced by data imbalance, which outperforms existing methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images</title>
<link>https://arxiv.org/abs/2505.08178</link>
<guid>https://arxiv.org/abs/2505.08178</guid>
<content:encoded><![CDATA[
<div> DGORNet, Disparity Estimation, Surgical Images, Occlusion, Monocular Depth<br />
<br />
Summary: <br />
The study introduces DGORNet, a novel Depth Guided Occlusion-Aware Disparity Refinement Network for improving stereo laparoscopic image disparity estimation. It addresses challenges such as occlusion and limited labeled data by leveraging monocular depth information and introducing a Position Embedding module for enhanced spatial context. The network also incorporates an Optical Flow Difference Loss for unlabeled data to enhance robustness in dynamic surgical scenes. Experimental results on the SCARED dataset demonstrate that DGORNet outperforms existing methods in terms of End-Point Error and Root Mean Squared Error, particularly in occlusion and texture-less regions. Ablation studies validate the significant contributions of the Position Embedding and Optical Flow Difference Loss in improving spatial and temporal consistency. DGORNet proves to be an effective solution for enhancing disparity estimation in laparoscopic surgery, providing practical insights for addressing challenges in disparity estimation and data scarcity. <div>
arXiv:2505.08178v1 Announce Type: new 
Abstract: Occlusion and the scarcity of labeled surgical data are significant challenges in disparity estimation for stereo laparoscopic images. To address these issues, this study proposes a Depth Guided Occlusion-Aware Disparity Refinement Network (DGORNet), which refines disparity maps by leveraging monocular depth information unaffected by occlusion. A Position Embedding (PE) module is introduced to provide explicit spatial context, enhancing the network's ability to localize and refine features. Furthermore, we introduce an Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal continuity across video frames to improve robustness in dynamic surgical scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean Squared Error (RMSE), particularly in occlusion and texture-less regions. Ablation studies confirm the contributions of the Position Embedding and Optical Flow Difference Loss, highlighting their roles in improving spatial and temporal consistency. These results underscore DGORNet's effectiveness in enhancing disparity estimation for laparoscopic surgery, offering a practical solution to challenges in disparity estimation and data limitations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2505.08190</link>
<guid>https://arxiv.org/abs/2505.08190</guid>
<content:encoded><![CDATA[
<div> diffusion models, image inpainting, raindrop removal, single image, Generative Adversarial Network <br />
<br />
Summary:
Raindrop removal from images is a challenging task, especially when relying on a single image. Common approaches involve detecting raindrop regions and then restoring the background based on these detections. The use of Generative Adversarial Networks (GANs) is prevalent for background restoration. However, recent advancements in diffusion models have improved image inpainting techniques significantly. This paper introduces a novel method for raindrop removal using diffusion-based image inpainting from a single image. By leveraging the capabilities of diffusion models, this technique offers a promising approach for effectively removing raindrops from images in a single-step process. <div>
arXiv:2505.08190v1 Announce Type: new 
Abstract: Raindrop removal is a challenging task in image processing. Removing raindrops while relying solely on a single image further increases the difficulty of the task. Common approaches include the detection of raindrop regions in the image, followed by performing a background restoration process conditioned on those regions. While various methods can be applied for the detection step, the most common architecture used for background restoration is the Generative Adversarial Network (GAN). Recent advances in the use of diffusion models have led to state-of-the-art image inpainting techniques. In this paper, we introduce a novel technique for raindrop removal from a single image using diffusion-based image inpainting.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2505.08196</link>
<guid>https://arxiv.org/abs/2505.08196</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, Dynamic Scene Reconstruction, Anchor-Based Structure, Rate-Distortion Optimization, Hierarchical Pipeline

Summary: 
ADC-GS introduces a novel approach for dynamic scene reconstruction by organizing Gaussian primitives into an anchor-based structure within a canonical space, improving performance by reducing redundancy and optimizing representation. It utilizes a hierarchical pipeline to capture motions at varying granularities and adopts rate-distortion optimization to achieve an optimal balance between bitrate consumption and representation fidelity. The method outperforms existing approaches in rendering speed by 300%-800% while maintaining state-of-the-art storage efficiency and rendering quality. The code for ADC-GS is available on GitHub at https://github.com/H-Huang774/ADC-GS.git. <div>
arXiv:2505.08196v1 Announce Type: new 
Abstract: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Watermarking in the Era of Diffusion Models: Advances and Challenges</title>
<link>https://arxiv.org/abs/2505.08197</link>
<guid>https://arxiv.org/abs/2505.08197</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, stable diffusion, visual watermarks, deepfake detection, diffusion models

Summary: 
This paper discusses the implications of advanced generative artificial intelligence technologies like Stable Diffusion on copyright infringement and the need for robust protection mechanisms such as visual watermarks. Traditional deepfake detection methods are ineffective against sophisticated manipulations, but diffusion models offer enhanced detection accuracy by learning features and embedding imperceptible watermarks. The integration of diffusion models and watermarking security is crucial in preserving ownership rights in the face of evolving forgery threats. The analysis focuses on the strengths and challenges of watermark techniques in the context of diffusion models, highlighting the importance of innovative solutions to protect digital content. The research aims to advance the discourse on safeguarding visual content and ensuring the integrity of ownership in the age of generative AI. 

<br /><br />Summary: <div>
arXiv:2505.08197v1 Announce Type: new 
Abstract: As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix</title>
<link>https://arxiv.org/abs/2505.08228</link>
<guid>https://arxiv.org/abs/2505.08228</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, adverse weather conditions, data augmentation, autonomous driving, robustness

Summary:<br /><br />Enhancing the robustness of object detection systems in adverse weather conditions is crucial for autonomous driving technology. The study introduces a novel approach using the diffusion model Instruct Pix2Pix to develop prompting methodologies for generating realistic datasets with weather-based augmentations. This aims to mitigate the impact of challenging weather on state-of-the-art object detection models like Faster R-CNN and YOLOv10. Experiments conducted in the CARLA simulator and real-world datasets BDD100K and ACDC demonstrate the effectiveness of the approach. The key contributions include quantifying the performance gap in object detection models in difficult weather conditions and showcasing how tailored data augmentation strategies can enhance model robustness. This research lays a solid foundation for enhancing perception systems in demanding environmental scenarios, offering pathways for future advancements in autonomous driving technology. 

Summary: <div>
arXiv:2505.08228v1 Announce Type: new 
Abstract: Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.
  The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective</title>
<link>https://arxiv.org/abs/2505.08231</link>
<guid>https://arxiv.org/abs/2505.08231</guid>
<content:encoded><![CDATA[
<div> Object detection, maritime navigation, dataset, HMPNet, shipborne object detection <br />
Summary: 
- The article introduces Navigation12, a dataset for object detection in maritime environments, addressing the lack of maritime-specific data.
- HMPNet, a lightweight architecture, is proposed for shipborne object detection, with improved accuracy and computational efficiency compared to existing methods.
- HMPNet incorporates a hierarchical dynamic modulation backbone for feature aggregation, a matrix cascading poly-scale neck, and a polymerization weight sharing detector for efficient multi-scale feature aggregation.
- Empirical evaluations show HMPNet outperforms the current state-of-the-art model YOLOv11n, with a 3.3% improvement in mean Average Precision and 23% reduction in parameters.
- The proposed dataset and architecture aim to enhance visual perception techniques in autonomous maritime navigation systems. 
<br /><br />Summary: <div>
arXiv:2505.08231v1 Announce Type: new 
Abstract: In the realm of intelligent maritime navigation, object detection from a shipborne perspective is paramount. Despite the criticality, the paucity of maritime-specific data impedes the deployment of sophisticated visual perception techniques, akin to those utilized in autonomous vehicular systems, within the maritime context. To bridge this gap, we introduce Navigation12, a novel dataset annotated for 12 object categories under diverse maritime environments and weather conditions. Based upon this dataset, we propose HMPNet, a lightweight architecture tailored for shipborne object detection. HMPNet incorporates a hierarchical dynamic modulation backbone to bolster feature aggregation and expression, complemented by a matrix cascading poly-scale neck and a polymerization weight sharing detector, facilitating efficient multi-scale feature aggregation. Empirical evaluations indicate that HMPNet surpasses current state-of-the-art methods in terms of both accuracy and computational efficiency, realizing a 3.3% improvement in mean Average Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition</title>
<link>https://arxiv.org/abs/2505.08233</link>
<guid>https://arxiv.org/abs/2505.08233</guid>
<content:encoded><![CDATA[
<div> Keyword: contactless fingerprint recognition, G-MSGINet, minutiae localization, identity embedding, robust

Summary:
G-MSGINet is a new framework for contactless fingerprint recognition that integrates minutiae localization and identity embedding directly from raw input images. It introduces the GMSGI layer, which combines pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modeling. This approach eliminates the need for orientation supervision and adapts graph connectivity from learned kernel descriptors. Experimental results on benchmark datasets show that G-MSGINet achieves high minutiae F1-scores and Rank-1 identification accuracies, with an Equal Error Rate as low as 0.5%. It outperforms previous methods in terms of F1-score and accuracy while using fewer parameters and floating-point operations. The results demonstrate the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.

Summary:<br /><br />Keyword: contactless fingerprint recognition, G-MSGINet, minutiae localization, identity embedding, robust <div>
arXiv:2505.08233v1 Announce Type: new 
Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust contactless fingerprint recognition that jointly performs minutiae localization and identity embedding directly from raw input images. Existing approaches rely on multi-branch architectures, orientation labels, or complex preprocessing steps, which limit scalability and generalization across real-world acquisition scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a novel computational module that integrates grouped pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modelling into a single processing unit. Stacked GMSGI layers progressively refine both local minutiae-sensitive features and global topological representations through end-to-end optimization. The architecture eliminates explicit orientation supervision and adapts graph connectivity directly from learned kernel descriptors, thereby capturing meaningful structural relationships among fingerprint regions without fixed heuristics. Extensive experiments on three benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that G-MSGINet consistently achieves minutiae F1-scores in the range of $0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%, while maintaining an Equal Error Rate (EER) as low as 0.5%. These results correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1 accuracy when compared to prior methods, using only 0.38 million parameters and 6.63 giga floating-point operations, which represents up to ten times fewer parameters than competitive baselines. This highlights the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Removing Watermarks with Partial Regeneration using Semantic Information</title>
<link>https://arxiv.org/abs/2505.08234</link>
<guid>https://arxiv.org/abs/2505.08234</guid>
<content:encoded><![CDATA[
<div> Vulnerability, SemanticRegen, Watermarking, Adversarial Attack, Image Manipulation

Summary:
SemanticRegen is a three-stage, label-free attack that targets state-of-the-art semantic and invisible watermarks by erasing them while preserving an image's apparent meaning. The pipeline utilizes a vision-language model to obtain captions, zero-shot segmentation for foreground masks, and inpainting with an LLM-guided diffusion model to maintain salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems, SemanticRegen successfully defeats the semantic TreeRing watermark and reduces bit-accuracy for the remaining schemes while maintaining high perceptual quality. The attack achieves up to 12 percent higher masked SSIM in foreground regions compared to prior diffusion-based attackers. These results underscore the gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, emphasizing the need for watermarking algorithms resilient to content-preserving regenerative attacks.<br /><br />Summary: <div>
arXiv:2505.08234v1 Announce Type: new 
Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2505.08235</link>
<guid>https://arxiv.org/abs/2505.08235</guid>
<content:encoded><![CDATA[
<div> diffusion model, event camera, video frame interpolation, denoising, event-based

Summary:
EventDiff is a novel event-based diffusion model framework for Video Frame Interpolation (VFI). It incorporates a lightweight Spatial-Temporal Cross Attention (STCA) module to fuse dynamic event streams with static frames, allowing for direct interpolation in the latent space through a denoising diffusion process. The method surpasses existing event-based VFI approaches by up to 1.98dB in PSNR on Vimeo90K-Triplet dataset and demonstrates superior performance on SNU-FILM tasks. In comparison to diffusion-based VFI methods, EventDiff achieves up to 5.72dB PSNR improvement on Vimeo90K-Triplet and is 4.24 times faster during inference. Through a two-stage training strategy involving pretraining the Event-Frame Hybrid AutoEncoder (HAE) and then jointly optimizing it with the diffusion model, EventDiff achieves state-of-the-art performance on multiple synthetic and real-world event VFI datasets. <br /><br />Summary: <div>
arXiv:2505.08235v1 Announce Type: new 
Abstract: Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Congenital Heart Disease recognition using Deep Learning/Transformer models</title>
<link>https://arxiv.org/abs/2505.08242</link>
<guid>https://arxiv.org/abs/2505.08242</guid>
<content:encoded><![CDATA[
<div> Keywords: Congenital Heart Disease, deep learning, dual-modality, diagnosis, accuracy 

Summary:
Deep learning models are investigated for their effectiveness in diagnosing Congenital Heart Disease (CHD), a major cause of infant morbidity and mortality. The study focuses on utilizing dual-modality (sound and image) deep learning methods for CHD screening. The research achieves 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset. By leveraging the automatic feature extraction abilities of deep learning, doctors can improve the accuracy of CHD detection, reducing the risk of false negatives commonly seen in current non-invasive screening methods. This innovative approach showcases the potential of deep learning in assisting healthcare professionals in diagnosing CHD more effectively and accurately. <div>
arXiv:2505.08242v1 Announce Type: new 
Abstract: Congenital Heart Disease (CHD) remains a leading cause of infant morbidity and mortality, yet non-invasive screening methods often yield false negatives. Deep learning models, with their ability to automatically extract features, can assist doctors in detecting CHD more effectively. In this work, we investigate the use of dual-modality (sound and image) deep learning methods for CHD diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Memorization of Diffusion Models through p-Laplace Analysis</title>
<link>https://arxiv.org/abs/2505.08246</link>
<guid>https://arxiv.org/abs/2505.08246</guid>
<content:encoded><![CDATA[
<div> score function, gradient, diffusion models, p-Laplace operators, memorization identification

Summary:
This study explores the use of diffusion models in estimating score functions to compute higher-order differentials known as p-Laplace operators. By utilizing the estimated score functions, researchers were able to accurately identify memorized training data. A numerical p-Laplace approximation was proposed based on the learned score functions, proving effective in detecting important features of the probability landscape. The analysis focused on Gaussian mixture models but also extended to image generative models, marking the first instance of memorization identification using the p-Laplace operator in image generation. This research sheds light on the potential of leveraging score functions in diffusion models for advanced data analysis applications. <br /><br />Summary: <div>
arXiv:2505.08246v1 Announce Type: new 
Abstract: Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely p-Laplace operators. We show here these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. We analyze the structured case of Gaussian mixture models, and demonstrate the results carry-over to image generative models, where memorization identification based on the p-Laplace operator is performed for the first time.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets</title>
<link>https://arxiv.org/abs/2505.08259</link>
<guid>https://arxiv.org/abs/2505.08259</guid>
<content:encoded><![CDATA[
<div> Convolutional, transformer, image classification, ResNet-18, Vision Transformer <br />
Summary: <br />
This study compares convolutional and transformer-based architectures for image classification, focusing on reducing inference latency and model complexity while maintaining accuracy. Using ResNet-18 as a baseline, four Vision Transformer variants (Tiny, Small, Base, Large) were fine-tuned on DermatologyMNIST and TinyImageNet datasets. Through hyperparameter tuning, the study shows that optimally fine-tuned Vision Transformers can match or exceed baseline performance, achieve faster inference, and operate with fewer parameters. This highlights the potential of Vision Transformers for applications in resource-constrained environments. <div>
arXiv:2505.08259v1 Announce Type: new 
Abstract: This study evaluates the trade-offs between convolutional and transformer-based architectures on both medical and general-purpose image classification benchmarks. We use ResNet-18 as our baseline and introduce a fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small, Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce inference latency and model complexity with acceptable accuracy degradation. Through systematic hyperparameter variations, we demonstrate that appropriately fine-tuned Vision Transformers can match or exceed the baseline's performance, achieve faster inference, and operate with fewer parameters, highlighting their viability for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Novel Category Discovery</title>
<link>https://arxiv.org/abs/2505.08260</link>
<guid>https://arxiv.org/abs/2505.08260</guid>
<content:encoded><![CDATA[
<div> discovery, transductive learning, few-shot learning, clustering, model adaptability
Summary: 
This paper introduces the Few-Shot Novel Category Discovery (FSNCD) setting, where a trained agent can switch between identifying known classes and clustering novel classes with only a few support examples. Two methods, Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC), are proposed to enhance the model's reasoning capabilities. The framework extends prior-based clustering algorithms to real-world open set scenarios, improving model adaptability in few-shot learning. Extensive experiments on five datasets show that the methods achieve high performance across various task settings and scenarios. <div>
arXiv:2505.08260v1 Announce Type: new 
Abstract: The recently proposed Novel Category Discovery (NCD) adapt paradigm of transductive learning hinders its application in more real-world scenarios. In fact, few labeled data in part of new categories can well alleviate this burden, which coincides with the ease that people can label few of new category data. Therefore, this paper presents a new setting in which a trained agent is able to flexibly switch between the tasks of identifying examples of known (labelled) classes and clustering novel (completely unlabeled) classes as the number of query examples increases by leveraging knowledge learned from only a few (handful) support examples. Drawing inspiration from the discovery of novel categories using prior-based clustering algorithms, we introduce a novel framework that further relaxes its assumptions to the real-world open set level by unifying the concept of model adaptability in few-shot learning. We refer to this setting as Few-Shot Novel Category Discovery (FSNCD) and propose Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC) to examine the model's reasoning capabilities. Extensive experiments and detailed analysis on five commonly used datasets demonstrate that our methods can achieve leading performance levels across different task settings and scenarios.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
<link>https://arxiv.org/abs/2505.08266</link>
<guid>https://arxiv.org/abs/2505.08266</guid>
<content:encoded><![CDATA[
<div> MPNN, SFs, link prediction, Graph Vision Network, visual perception <br />
<br />
Summary: 
Message-passing graph neural networks (MPNNs) and structural features (SFs) are crucial for link prediction. However, the potential of visual perception has been overlooked in MPNNs. The Graph Vision Network (GVN) framework integrates vision structural awareness into MPNNs for the first time. An efficient variant, E-GVN, is also proposed. Empirical results show that GVN consistently benefits from visual enhancement across various link prediction datasets, including large-scale graphs. GVN outperforms existing state-of-the-art methods, indicating a promising direction for link prediction. <div>
arXiv:2505.08266v1 Announce Type: new 
Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping</title>
<link>https://arxiv.org/abs/2505.08273</link>
<guid>https://arxiv.org/abs/2505.08273</guid>
<content:encoded><![CDATA[
<div> satellite imagery, irrigation mapping, dataset, deep learning, agriculture <br />
Summary: 
IrrMap is a large-scale dataset comprising 1.1 million patches for irrigation method mapping across regions. The dataset includes multi-resolution satellite imagery from LandSat and Sentinel, along with auxiliary data such as crop type and vegetation indices. With standardized GeoTIFF patches and multiple input modalities, IrrMap is ML-ready for seamless deep learning model training. The dataset covers 1,687,899 farms and 14,117,330 acres in multiple western U.S. states from 2013 to 2023, enabling comprehensive irrigation analysis. The dataset's complete pipeline allows for easy extension to new regions for irrigation data collection. Analysis of irrigation method distribution, spatial patterns, and irrigated area variations provides insights into regional and resolution-based differences. IrrMap is openly released with benchmark models and pipeline code on GitHub and a data repository for further exploration in agricultural and geospatial analysis. <br /> <div>
arXiv:2505.08273v1 Announce Type: new 
Abstract: We introduce IrrMap, the first large-scale dataset (1.1 million patches) for irrigation method mapping across regions. IrrMap consists of multi-resolution satellite imagery from LandSat and Sentinel, along with key auxiliary data such as crop type, land use, and vegetation indices. The dataset spans 1,687,899 farms and 14,117,330 acres across multiple western U.S. states from 2013 to 2023, providing a rich and diverse foundation for irrigation analysis and ensuring geospatial alignment and quality control. The dataset is ML-ready, with standardized 224x224 GeoTIFF patches, the multiple input modalities, carefully chosen train-test-split data, and accompanying dataloaders for seamless deep learning model training andbenchmarking in irrigation mapping. The dataset is also accompanied by a complete pipeline for dataset generation, enabling researchers to extend IrrMap to new regions for irrigation data collection or adapt it with minimal effort for other similar applications in agricultural and geospatial analysis. We also analyze the irrigation method distribution across crop groups, spatial irrigation patterns (using Shannon diversity indices), and irrigated area variations for both LandSat and Sentinel, providing insights into regional and resolution-based differences. To promote further exploration, we openly release IrrMap, along with the derived datasets, benchmark models, and pipeline code, through a GitHub repository: https://github.com/Nibir088/IrrMap and Data repository: https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and implementation details.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion</title>
<link>https://arxiv.org/abs/2505.08281</link>
<guid>https://arxiv.org/abs/2505.08281</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal image compression, residual-guided, semantic retrieval, diffusion-based generation, compression-aware diffusion model

Summary:
ResULIC is a new residual-guided ultra-low-rate image compression framework that integrates semantic retrieval, residual signals, and a compression-aware diffusion model for improved reconstruction fidelity and coding efficiency. It introduces Semantic Residual Coding (SRC) to capture semantic disparities and uses a perceptual fidelity optimizer for high-quality reconstruction. The Compression-aware Diffusion Model (CDM) aligns bitrates with diffusion time steps to enhance compression-reconstruction synergy. Experimental results show ResULIC outperforms state-of-the-art methods, achieving significant improvements in LPIPS and FID metrics. This approach offers a comprehensive solution for high-quality image compression by incorporating residual signals, semantic retrieval, and a compression-aware diffusion model. <br /><br />Summary: <div>
arXiv:2505.08281v1 Announce Type: new 
Abstract: Existing multimodal large model-based image compression frameworks often rely on a fragmented integration of semantic retrieval, latent compression, and generative models, resulting in suboptimal performance in both reconstruction fidelity and coding efficiency. To address these challenges, we propose a residual-guided ultra lowrate image compression named ResULIC, which incorporates residual signals into both semantic retrieval and the diffusion-based generation process. Specifically, we introduce Semantic Residual Coding (SRC) to capture the semantic disparity between the original image and its compressed latent representation. A perceptual fidelity optimizer is further applied for superior reconstruction quality. Additionally, we present the Compression-aware Diffusion Model (CDM), which establishes an optimal alignment between bitrates and diffusion time steps, improving compression-reconstruction synergy. Extensive experiments demonstrate the effectiveness of ResULIC, achieving superior objective and subjective performance compared to state-of-the-art diffusion-based methods with - 80.7%, -66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at https: //njuvision.github.io/ResULIC/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks</title>
<link>https://arxiv.org/abs/2505.08284</link>
<guid>https://arxiv.org/abs/2505.08284</guid>
<content:encoded><![CDATA[
<div> quantitative analysis, Ukiyo-e, creativity, machine learning, cultural evolution
Summary: 
- The research focuses on using machine learning to conduct a quantitative analysis of creativity in Ukiyo-e, a traditional Japanese art form, using a large database of high-resolution images.
- The study reveals that the overall creativity of Ukiyo-e has decreased with cultural maturation, but the style has become more segmented and maintained a high level of creativity.
- The analysis provides new insights into the study of Ukiyo-e and demonstrates its role in the ongoing cultural history of Eastern art.
<br /><br />Summary: <div>
arXiv:2505.08284v1 Announce Type: new 
Abstract: Artwork research has long relied on human sensibility and subjective judgment, but recent developments in machine learning have enabled the quantitative assessment of features that humans could not discover. In Western paintings, comprehensive analyses have been conducted from various perspectives in conjunction with large databases, but such extensive analysis has not been sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a traditional Japanese art form, as a case study of Eastern paintings, and conduct a quantitative analysis of creativity in works of art using 11,000 high-resolution images. This involves using the concept of calculating creativity from networks to analyze both the creativity of the artwork and that of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that the creativity of its appearance has declined with the maturation of culture, but in terms of style, it has become more segmented with the maturation of culture and has maintained a high level of creativity. This not only provides new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved within the ongoing cultural history, playing a culturally significant role in the analysis of Eastern art.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units</title>
<link>https://arxiv.org/abs/2505.08294</link>
<guid>https://arxiv.org/abs/2505.08294</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, deepfakes, facial action units, multimodal manipulations, cross-modal fusion<br />
Summary: 
The article introduces FauForensics, a novel framework for detecting realistic audio-visual deepfakes by using biologically invariant facial action units (FAUs) to capture subtle dynamics disrupted in synthetic content. This approach reduces domain dependency and addresses multi-modal forgery issues. FauForensics computes frame-wise audiovisual similarities through a fusion module with learnable cross-modal queries, effectively aligning temporal-spatial lip-audio relationships. The experiments on FakeAVCeleb and LAV-DF datasets demonstrate state-of-the-art performance and superior cross-dataset generalizability, outperforming existing methods by up to an average of 4.83%. This innovative framework provides robust detection capabilities for multimodal manipulations in the evolving landscape of generative AI and deepfake threats.<br /><br />Summary: <div>
arXiv:2505.08294v1 Announce Type: new 
Abstract: The rapid evolution of generative AI has increased the threat of realistic audio-visual deepfakes, demanding robust detection methods. Existing solutions primarily address unimodal (audio or visual) forgeries but struggle with multimodal manipulations due to inadequate handling of heterogeneous modality features and poor generalization across datasets. To this end, we propose a novel framework called FauForensics by introducing biologically invariant facial action units (FAUs), which is a quantitative descriptor of facial muscle activity linked to emotion physiology. It serves as forgery-resistant representations that reduce domain dependency while capturing subtle dynamics often disrupted in synthetic content. Besides, instead of comparing entire video clips as in prior works, our method computes fine-grained frame-wise audiovisual similarities via a dedicated fusion module augmented with learnable cross-modal queries. It dynamically aligns temporal-spatial lip-audio relationships while mitigating multi-modal feature heterogeneity issues. Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance and superior cross-dataset generalizability with up to an average of 4.83\% than existing methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing</title>
<link>https://arxiv.org/abs/2505.08302</link>
<guid>https://arxiv.org/abs/2505.08302</guid>
<content:encoded><![CDATA[
<div> Swin-Transformer, irrigation mapping, agricultural landscapes, satellite imagery, sustainable practices 
Summary:
- The article introduces Knowledge-Informed Irrigation Mapping (KIIM), a novel approach using Swin-Transformer for accurate irrigation mapping in complex agricultural landscapes.
- KIIM incorporates a specialized projection matrix for crop to irrigation probability encoding, a spatial attention map for land identification, bi-directional cross-attention for information integration, and a weighted ensemble for prediction combination.
- Experimentation in five US states demonstrates significant improvement over baseline methods, particularly in identifying drip irrigation.
- A two-phase transfer learning approach enhances cross-state mapping, resulting in a substantial boost in performance in states with limited labeled data.
- KIIM shows efficiency by achieving baseline performance with only 40% of the training data, reducing the need for extensive manual labeling efforts and promoting cost-effective large-scale automated irrigation mapping. 

<br /><br />Summary: <div>
arXiv:2505.08302v1 Announce Type: new 
Abstract: Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An incremental algorithm for non-convex AI-enhanced medical image processing</title>
<link>https://arxiv.org/abs/2505.08324</link>
<guid>https://arxiv.org/abs/2505.08324</guid>
<content:encoded><![CDATA[
<div> deep learning, non-convex optimization, medical imaging, image reconstruction, incremental model-based optimization

Summary:
The article introduces incDG, a hybrid framework that combines deep learning with incremental model-based optimization to efficiently solve non-convex regularized inverse problems in medical imaging. By leveraging the Deep Guess strategy, incDG uses a deep neural network to generate initializations for a non-convex variational solver, improving reconstruction through incremental iterations. The integration of AI tools and model-based optimization enhances robustness and stability in solving imaging inverse problems. Validation on TpV-regularized optimization tasks, including medical image deblurring and tomographic reconstruction, shows superior accuracy and stability compared to conventional solvers and deep learning-based methods. Furthermore, training incDG without ground truth minimally affects performance, making it a practical and powerful tool for addressing non-convex inverse problems in various imaging applications and beyond. 

<br /><br />Summary: <div>
arXiv:2505.08324v1 Announce Type: new 
Abstract: Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computer vision-based model for occupancy detection using low-resolution thermal images</title>
<link>https://arxiv.org/abs/2505.08336</link>
<guid>https://arxiv.org/abs/2505.08336</guid>
<content:encoded><![CDATA[
<div> occupancy detection, thermal images, computer vision techniques, YOLOv5 model, privacy concerns <br />
Summary: <br />
Occupancy detection is crucial for efficient HVAC system operation, with Advanced occupant-centric control (OCC) being a more personalized approach. While traditional methods use RGB images for occupancy detection, privacy concerns arise due to captured facial and body features. This study presents a model using low-resolution thermal images and computer vision techniques, reducing privacy risks and computational resources. The model, incorporating transfer learning with YOLOv5, exhibited high performance with precision and recall values nearing perfection. By utilizing thermal images, this novel approach not only ensures privacy protection but also delivers accurate occupancy information for HVAC system optimization. <div>
arXiv:2505.08336v1 Announce Type: new 
Abstract: Occupancy plays an essential role in influencing the energy consumption and operation of heating, ventilation, and air conditioning (HVAC) systems. Traditional HVAC typically operate on fixed schedules without considering occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in regulating HVAC operations. RGB images combined with computer vision (CV) techniques are widely used for occupancy detection, however, the detailed facial and body features they capture raise significant privacy concerns. Low-resolution thermal images offer a non-invasive solution that mitigates privacy issues. The study developed an occupancy detection model utilizing low-resolution thermal images and CV techniques, where transfer learning was applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The developed model ultimately achieved satisfactory performance, with precision, recall, mAP50, and mAP50 values approaching 1.000. The contributions of this model lie not only in mitigating privacy concerns but also in reducing computing resource demands.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning</title>
<link>https://arxiv.org/abs/2505.08349</link>
<guid>https://arxiv.org/abs/2505.08349</guid>
<content:encoded><![CDATA[
<div> Frequency Adaptation, Diversion, Cross-domain few-shot learning, spectral representations, generalization <br />
Summary: <br />
The paper introduces Frequency Adaptation and Diversion (FAD) for Cross-domain few-shot learning (CD-FSL), which tackles the issue of spectral variations in spatially similar images across domains. FAD utilizes a Frequency Diversion Adapter to transform features into the frequency domain, partition them into low, mid, and high-frequency bands, and adapt each band separately to capture different levels of semantic information. Experiment results on the Meta-Dataset benchmark show that FAD outperforms existing methods in both seen and unseen domains, highlighting the effectiveness of frequency-aware frameworks and band-wise adaptation for enhancing generalization in CD-FSL. <div>
arXiv:2505.08349v1 Announce Type: new 
Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from limited labeled samples under significant distribution shifts. While recent methods enhance adaptability through lightweight task-specific modules, they operate solely in the spatial domain and overlook frequency-specific variations that are often critical for robust transfer. We observe that spatially similar images across domains can differ substantially in their spectral representations, with low and high frequencies capturing complementary semantic information at coarse and fine levels. This indicates that uniform spatial adaptation may overlook these spectral distinctions, thus constraining generalization. To address this, we introduce Frequency Adaptation and Diversion (FAD), a frequency-aware framework that explicitly models and modulates spectral components. At its core is the Frequency Diversion Adapter, which transforms intermediate features into the frequency domain using the discrete Fourier transform (DFT), partitions them into low, mid, and high-frequency bands via radial masks, and reconstructs each band using inverse DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional branch with a kernel size tailored to its spectral scale, enabling targeted and disentangled adaptation across frequencies. Extensive experiments on the Meta-Dataset benchmark demonstrate that FAD consistently outperforms state-of-the-art methods on both seen and unseen domains, validating the utility of frequency-domain representations and band-wise adaptation for improving generalization in CD-FSL.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives</title>
<link>https://arxiv.org/abs/2505.08350</link>
<guid>https://arxiv.org/abs/2505.08350</guid>
<content:encoded><![CDATA[
<div> Keywords: StoryAnchors, story frame generation, temporal consistency, scene diversity, narrative richness

Summary:
StoryAnchors introduces a framework for generating high-quality, multi-scene story frames with strong temporal consistency. The bidirectional story generator integrates past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions. Specific conditions are introduced to enhance scene diversity and narrative richness. The framework incorporates Multi-Event Story Frame Labeling and Progressive Story Frame Training to capture overarching narrative flow and event-level dynamics, supporting editable and expandable story frames. Extensive experiments demonstrate that StoryAnchors surpasses existing models in consistency, coherence, and scene diversity. Its performance in narrative consistency and story richness rivals that of GPT-4o. StoryAnchors provides a scalable, flexible, and highly editable foundation for advancing story-driven frame generation. 

<br /><br />Summary: <div>
arXiv:2505.08350v1 Announce Type: new 
Abstract: This paper introduces StoryAnchors, a unified framework for generating high-quality, multi-scene story frames with strong temporal consistency. The framework employs a bidirectional story generator that integrates both past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions throughout the narrative. Specific conditions are introduced to distinguish story frame generation from standard video synthesis, facilitating greater scene diversity and enhancing narrative richness. To further improve generation quality, StoryAnchors integrates Multi-Event Story Frame Labeling and Progressive Story Frame Training, enabling the model to capture both overarching narrative flow and event-level dynamics. This approach supports the creation of editable and expandable story frames, allowing for manual modifications and the generation of longer, more complex sequences. Extensive experiments show that StoryAnchors outperforms existing open-source models in key areas such as consistency, narrative coherence, and scene diversity. Its performance in narrative consistency and story richness is also on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of story-driven frame generation, offering a scalable, flexible, and highly editable foundation for future research.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DArFace: Deformation Aware Robustness for Low Quality Face Recognition</title>
<link>https://arxiv.org/abs/2505.08423</link>
<guid>https://arxiv.org/abs/2505.08423</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial recognition, deep neural networks, low-quality images, deformation modeling, robustness

Summary: 
DArFace is a new framework for robust face recognition that addresses the challenges posed by low-quality facial images in real-world scenarios. It incorporates global transformations and local elastic deformations during training to simulate realistic degraded conditions without the need for paired high- and low-quality samples. The method also includes a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on benchmarks such as TinyFace, IJB-B, and IJB-C showcase DArFace's superior performance compared to existing methods, with significant improvements attributed to its incorporation of local deformation modeling. The framework leverages deep neural networks, advanced loss functions, and large-scale datasets to overcome the domain gap between high-quality training data and low-quality real-world images. <div>
arXiv:2505.08423v1 Announce Type: new 
Abstract: Facial recognition systems have achieved remarkable success by leveraging deep neural networks, advanced loss functions, and large-scale datasets. However, their performance often deteriorates in real-world scenarios involving low-quality facial images. Such degradations, common in surveillance footage or standoff imaging include low resolution, motion blur, and various distortions, resulting in a substantial domain gap from the high-quality data typically used during training. While existing approaches attempt to address robustness by modifying network architectures or modeling global spatial transformations, they frequently overlook local, non-rigid deformations that are inherently present in real-world settings. In this work, we introduce DArFace, a Deformation-Aware robust Face recognition framework that enhances robustness to such degradations without requiring paired high- and low-quality training samples. Our method adversarially integrates both global transformations (e.g., rotation, translation) and local elastic deformations during training to simulate realistic low-quality conditions. Moreover, we introduce a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with significant gains attributed to the inclusion of local deformation modeling.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation</title>
<link>https://arxiv.org/abs/2505.08426</link>
<guid>https://arxiv.org/abs/2505.08426</guid>
<content:encoded><![CDATA[
<div> Keywords: gaze estimation, deep learning, super-resolution, cross-attention, dataset correction

Summary: 
- The paper introduces DHECA-SuperGaze, a deep learning-based method for improved gaze estimation in unconstrained environments.
- The method incorporates super-resolution and a dual head-eye cross-attention module to enhance gaze prediction accuracy.
- Errors in the Gaze360 dataset annotations were identified and corrected, leading to improved performance evaluation results.
- DHECA-SuperGaze outperforms state-of-the-art methods in both within-dataset and cross-dataset testing scenarios.
- The proposed approach demonstrates superior angular error reduction in static and temporal gaze estimation configurations, showcasing its robust generalization capabilities.

<br /><br />Summary: <div>
arXiv:2505.08426v1 Announce Type: new 
Abstract: Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48{\deg} (Gaze360) and 2.95{\deg} (GFIE) in static configurations, and 0.59{\deg} (Gaze360) and 3.00{\deg} (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53{\deg} (Gaze360) and 3.99{\deg} (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Image Reconstruction from Brain Activity via Latent Representation</title>
<link>https://arxiv.org/abs/2505.08429</link>
<guid>https://arxiv.org/abs/2505.08429</guid>
<content:encoded><![CDATA[
<div> Keywords: visual image reconstruction, deep neural networks, generative models, compositional strategies, ethical considerations

Summary: 
Visual image reconstruction has progressed with the integration of deep neural networks and generative models. The evolution of the field from early classification approaches to detailed reconstructions emphasizes hierarchical latent representations, compositional strategies, and modular architectures. Challenges include achieving zero-shot generalization for unseen images and modeling subjective aspects of perception accurately. Diverse datasets, refined evaluation metrics aligned with human judgments, and compositional representations are needed for model robustness and generalizability. Ethical considerations such as privacy, consent, and potential misuse must be addressed responsibly. Visual image reconstruction provides insights into neural coding, enables new psychological measurements of visual experiences, and has applications in clinical diagnostics and brain-machine interfaces. 

<br /><br />Summary: <div>
arXiv:2505.08429v1 Announce Type: new 
Abstract: Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection</title>
<link>https://arxiv.org/abs/2505.08437</link>
<guid>https://arxiv.org/abs/2505.08437</guid>
<content:encoded><![CDATA[
<div> Dataset, Forgery Detection, Deepfake, Human Body, TikTok-DeepFake<br />
<br />
Summary:<br />
The article introduces the TikTok-DeepFake (TT-DF) dataset, which focuses on body forgery detection, addressing the lack of datasets and detection methods in this area. TT-DF consists of 6,120 forged videos and 1,378,857 synthetic frames, offering a wide range of forgery methods using advanced human image animation models. The dataset aims to simulate various forged data scenarios comprehensively. Additionally, a new body forgery detection model, Temporal Optical Flow Network (TOF-Net), is proposed, leveraging spatiotemporal inconsistencies and optical flow distribution differences between natural and forged data. Experimental results show that TOF-Net outperforms existing facial forgery detection models on the TT-DF dataset. The work provides a significant contribution to advancing research in human body forgery detection and offers the TT-DF dataset for further exploration in this domain. <br /> <div>
arXiv:2505.08437v1 Announce Type: new 
Abstract: The emergence and popularity of facial deepfake methods spur the vigorous development of deepfake datasets and facial forgery detection, which to some extent alleviates the security concerns about facial-related artificial intelligence technologies. However, when it comes to human body forgery, there has been a persistent lack of datasets and detection methods, due to the later inception and complexity of human body generation methods. To mitigate this issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic frames, specifically tailored for body forgery detection. TT-DF offers a wide variety of forgery methods, involving multiple advanced human image animation models utilized for manipulation, two generative configurations based on the disentanglement of identity and pose information, as well as different compressed versions. The aim is to simulate any potential unseen forged data in the wild as comprehensively as possible, and we also furnish a benchmark on TT-DF. Additionally, we propose an adapted body forgery detection model, Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal inconsistencies and optical flow distribution differences between natural data and forged data. Our experiments demonstrate that TOF-Net achieves favorable performance on TT-DF, outperforming current state-of-the-art extendable facial forgery detection models. For our TT-DF dataset, please refer to https://github.com/HashTAG00002/TT-DF.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering</title>
<link>https://arxiv.org/abs/2505.08438</link>
<guid>https://arxiv.org/abs/2505.08438</guid>
<content:encoded><![CDATA[
<div> Event cameras, 3D reconstruction, stereo, monocular, multimodal systems<br />
Summary:<br />
Event cameras are a promising sensor for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. This survey categorizes existing works into three types based on input modality and further classifies them by reconstruction approach. Methods are organized chronologically and summarized public datasets relevant to event-based 3D reconstruction. Current research limitations in data availability, evaluation, representation, and dynamic scene handling are highlighted. Future research directions in event-driven 3D reconstruction are outlined. <br /> <div>
arXiv:2505.08438v1 Announce Type: new 
Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. Unlike conventional frame-based cameras, they produce sparse and temporally rich data streams, which enable more accurate 3D reconstruction and open up the possibility of performing reconstruction in extreme environments such as high-speed motion, low light, or high dynamic range scenes. In this survey, we provide the first comprehensive review focused exclusively on 3D reconstruction using event cameras. The survey categorises existing works into three major types based on input modality - stereo, monocular, and multimodal systems, and further classifies them by reconstruction approach, including geometry-based, deep learning-based, and recent neural rendering techniques such as Neural Radiance Fields and 3D Gaussian Splatting. Methods with a similar research focus were organised chronologically into the most subdivided groups. We also summarise public datasets relevant to event-based 3D reconstruction. Finally, we highlight current research limitations in data availability, evaluation, representation, and dynamic scene handling, and outline promising future research directions. This survey aims to serve as a comprehensive reference and a roadmap for future developments in event-driven 3D reconstruction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models</title>
<link>https://arxiv.org/abs/2505.08455</link>
<guid>https://arxiv.org/abs/2505.08455</guid>
<content:encoded><![CDATA[
<div> benchmark, Video-based long-form Causal Reasoning, LVLM, procedural videos, Recognition-Reasoning Decomposition<br />
<br />
Summary: Large Video Language Models (LVLMs) lack the ability to perform video-based causal reasoning due to the absence of benchmarks. To address this, a new benchmark called Video-based long-form Causal Reasoning (VCRBench) is introduced. VCRBench tests LVLMs on their capability to identify, reason about, and sequence events in videos to achieve a specific goal. State-of-the-art LVLMs struggle with long-form causal reasoning in videos due to difficulty in modeling long-range causal dependencies from visual observations. To address this issue, a modular approach called Recognition-Reasoning Decomposition (RRD) is proposed, breaking video-based causal reasoning into video recognition and causal reasoning sub-tasks. Experiments show that RRD significantly improves accuracy on VCRBench, with gains up to 25.2%. Analysis reveals that LVLMs heavily rely on language knowledge for complex video-based causal reasoning tasks. <br /><br /> <div>
arXiv:2505.08455v1 Announce Type: new 
Abstract: Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images</title>
<link>https://arxiv.org/abs/2505.08517</link>
<guid>https://arxiv.org/abs/2505.08517</guid>
<content:encoded><![CDATA[
<div> deep learning, inhalation injuries, bronchoscopy images, enhanced StarGAN, classification accuracy

Summary:
This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images. The traditional methods for diagnosing and grading inhalation injuries have limitations, leading to subjective assessments and weak correlations with clinical outcomes. To overcome the scarcity of medical imaging data, enhanced StarGAN, a generative model, was used to improve the quality and clinical relevance of synthetic images. The augmented dataset generated by enhanced StarGAN significantly improved classification performance, achieving an accuracy of 77.78%. The image quality was assessed using the Fr\'echet Inception Distance, with enhanced StarGAN achieving the lowest FID of 30.06. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly in preserving bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.<br /><br />Summary: <div>
arXiv:2505.08517v1 Announce Type: new 
Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to the limitations of traditional methods, such as Abbreviated Injury Score (AIS), which rely on subjective assessments and show weak correlations with clinical outcomes. This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images with the duration of mechanical ventilation as an objective metric. To address the scarcity of medical imaging data, we propose enhanced StarGAN, a generative model that integrates Patch Loss and SSIM Loss to improve synthetic images' quality and clinical relevance. The augmented dataset generated by enhanced StarGAN significantly improved classification performance when evaluated using the Swin Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the original dataset. Image quality was assessed using the Fr\'echet Inception Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06, outperforming baseline models. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly the preservation of bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis</title>
<link>https://arxiv.org/abs/2505.08524</link>
<guid>https://arxiv.org/abs/2505.08524</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole slide image classification, domain shift, continual learning, generative latent replay, attention mechanism

Summary: 
The article introduces a novel framework, Attention-based Generative Latent Replay Continual Learning (AGLR-CL), for Whole Slide Image (WSI) classification in the presence of domain shifts. By utilizing Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, the method preserves past domain knowledge without storing original data. An attention-based filtering step focuses on salient patch embeddings, generating high-quality synthetic samples. This privacy-aware approach eliminates the need for replay buffers and outperforms buffer-free methods while matching buffer-based solutions' performance. The AGLR-CL framework is validated on tasks such as biomarker detection and molecular status prediction across diverse datasets, demonstrating its ability to retain previous knowledge and adapt to new domains effectively. The framework provides a privacy-preserving solution for domain incremental continual learning in WSI classification. 

Summary: <div>
arXiv:2505.08524v1 Announce Type: new 
Abstract: Whole slide image (WSI) classification has emerged as a powerful tool in computational pathology, but remains constrained by domain shifts, e.g., due to different organs, diseases, or institution-specific variations. To address this challenge, we propose an Attention-based Generative Latent Replay Continual Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for domain incremental WSI classification. Our method employs Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, preserving knowledge of past domains without explicitly storing original data. A novel attention-based filtering step focuses on the most salient patch embeddings, ensuring high-quality synthetic samples. This privacy-aware strategy obviates the need for replay buffers and outperforms other buffer-free counterparts while matching the performance of buffer-based solutions. We validate AGLR-CL on clinically relevant biomarker detection and molecular status prediction across multiple public datasets with diverse centers, organs, and patient cohorts. Experimental results confirm its ability to retain prior knowledge and adapt to new domains, offering an effective, privacy-preserving avenue for domain incremental continual learning in WSI classification.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation</title>
<link>https://arxiv.org/abs/2505.08525</link>
<guid>https://arxiv.org/abs/2505.08525</guid>
<content:encoded><![CDATA[
<div> Keywords: tubular structures, semantic segmentation, super-resolution, dynamic snake upsampling, boundary-skeleton weighted loss

Summary:
This paper introduces a novel approach for accurate segmentation of tubular topological structures, addressing the challenges posed by the slenderness and curvature of such structures. The proposed method combines dynamic snake upsampling operators with a boundary-skeleton weighted loss to improve subpixel-level feature recovery and topological continuity. The snake upsampling operators adaptively adjust the sampling stride and select subpixel sampling points along the serpentine path, enabling more precise feature recovery for tubular structures. The boundary-skeleton weighted loss prioritizes boundary alignment precision and topological consistency by adjusting weight allocation based on the mask class ratio and distance field. Experimental results demonstrate that the proposed method significantly enhances both pixel-wise segmentation accuracy and topological consistency across various datasets and backbone networks. Overall, this dynamic snake upsampling operator and boundary-skeleton weighted loss offer a promising solution for accurate segmentation of tubular structures in various applications. 

<br /><br />Summary: <div>
arXiv:2505.08525v1 Announce Type: new 
Abstract: Accurate segmentation of tubular topological structures (e.g., fissures and vasculature) is critical in various fields to guarantee dependable downstream quantitative analysis and modeling. However, in dense prediction tasks such as semantic segmentation and super-resolution, conventional upsampling operators cannot accommodate the slenderness of tubular structures and the curvature of morphology. This paper introduces a dynamic snake upsampling operators and a boundary-skeleton weighted loss tailored for topological tubular structures. Specifically, we design a snake upsampling operators based on an adaptive sampling domain, which dynamically adjusts the sampling stride according to the feature map and selects a set of subpixel sampling points along the serpentine path, enabling more accurate subpixel-level feature recovery for tubular structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted loss that trades off main body and boundary weight allocation based on mask class ratio and distance field, preserving main body overlap while enhancing focus on target topological continuity and boundary alignment precision. Experiments across various domain datasets and backbone networks show that this plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted loss boost both pixel-wise segmentation accuracy and topological consistency of results.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting</title>
<link>https://arxiv.org/abs/2505.08527</link>
<guid>https://arxiv.org/abs/2505.08527</guid>
<content:encoded><![CDATA[
<div> SFDA, segmentation, Segment Anything Model, Dual Feature Guided, box prompt search<br />
Summary:<br />
The article discusses source-free domain adaptation (SFDA) for segmentation using the Segment Anything Model (SAM). Existing SFDA approaches generate defective bounding box prompts due to the domain gap. To address this, a Dual Feature Guided (DFG) auto-prompting approach is proposed. The approach involves training the source model in a feature aggregation phase to adapt to the target domain and build a suitable feature distribution for box prompt search. The box prompt is gradually expanded based on target model features and SAM features to handle different target feature distributions. Postprocessing the pseudo-labels produced by SAM further refines the results. Experimental results on 3D and 2D datasets show superior performance compared to conventional methods. The code for the approach is available on GitHub. <div>
arXiv:2505.08527v1 Announce Type: new 
Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a model trained in the source domain to perform well in the target domain with only the source model and unlabeled target data.Inspired by the recent success of Segment Anything Model (SAM) which exhibits the generality of segmenting images of various modalities and in different domains given human-annotated prompts like bounding boxes or points, we for the first time explore the potentials of Segment Anything Model for SFDA via automatedly finding an accurate bounding box prompt. We find that the bounding boxes directly generated with existing SFDA approaches are defective due to the domain gap.To tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting approach to search for the box prompt. Specifically, the source model is first trained in a feature aggregation phase, which not only preliminarily adapts the source model to the target domain but also builds a feature distribution well-prepared for box prompt search. In the second phase, based on two feature distribution observations, we gradually expand the box prompt with the guidance of the target model feature and the SAM feature to handle the class-wise clustered target features and the class-wise dispersed target features, respectively. To remove the potentially enlarged false positive regions caused by the over-confident prediction of the target model, the refined pseudo-labels produced by SAM are further postprocessed based on connectivity analysis. Experiments on 3D and 2D datasets indicate that our approach yields superior performance compared to conventional methods. Code is available at https://github.com/zheangh/DFG.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning</title>
<link>https://arxiv.org/abs/2505.08537</link>
<guid>https://arxiv.org/abs/2505.08537</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, food quality assessment, raspberry grading, instance segmentation, industrial environment<br />
<br />
Summary: <br />
This research explores the use of computer vision for assessing food quality rapidly and accurately in an industrial setting, specifically focusing on real-time raspberry grading. A dataset called RaspGrade was created and meticulously annotated for this purpose. The study found that while accurate fruit-level masks can be generated through instance segmentation, classifying certain raspberry grades poses challenges due to color similarities and occlusion, while others are more easily distinguishable based on color. The RaspGrade dataset is now available for access on HuggingFace. This research sheds light on the potential of computer vision for non-invasive and efficient food quality assessment in industrial processes, highlighting the complexities and opportunities in real-time fruit grading. <br /> <div>
arXiv:2505.08537v1 Announce Type: new 
Abstract: This research investigates the application of computer vision for rapid, accurate, and non-invasive food quality assessment, focusing on the novel challenge of real-time raspberry grading into five distinct classes within an industrial environment as the fruits move along a conveyor belt. To address this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and meticulously annotated. Instance segmentation experiments revealed that accurate fruit-level masks can be obtained; however, the classification of certain raspberry grades presents challenges due to color similarities and occlusion, while others are more readily distinguishable based on color. The acquired and annotated RaspGrade dataset is accessible on HuggingFace at: https://huggingface.co/datasets/FBK-TeV/RaspGrade.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</title>
<link>https://arxiv.org/abs/2505.08552</link>
<guid>https://arxiv.org/abs/2505.08552</guid>
<content:encoded><![CDATA[
<div> detect, copyright infringement, generative AI, visual artworks, contrastive learning 

Summary: 
The article discusses the issue of copyright infringement and forgery in AI-generated visual artworks due to the inherent memorization capabilities of generative models. The proposed DFA-CON framework utilizes contrastive learning to detect copyright violations in AI-generated art by creating a discriminative representation space that distinguishes between original artworks and their forged counterparts. The model is trained across various attack types, such as inpainting, style transfer, adversarial perturbation, and cutmix, to improve detection performance. Evaluation results show that DFA-CON outperforms existing pretrained foundation models in detecting copyright infringement. The code and model checkpoints will be released publicly upon acceptance. <div>
arXiv:2505.08552v1 Announce Type: new 
Abstract: Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</title>
<link>https://arxiv.org/abs/2505.08561</link>
<guid>https://arxiv.org/abs/2505.08561</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked video modeling, Trajectory-Aware Adaptive Token Sampler, MAE, Proximal Policy Optimization, action recognition

Summary: 
Masked video modeling (MVM) has been proven effective for pre-training visual foundation models by reconstructing masked spatiotemporal tokens. However, the challenge lies in selecting the appropriate masking strategy. This study introduces the Trajectory-Aware Adaptive Token Sampler (TATS), which models motion dynamics to select motion-centric tokens, integrated into the masked autoencoder framework. A unified training strategy enables joint optimization of MAE and TATS using Proximal Policy Optimization (PPO) from scratch. The model allows aggressive masking without compromising action recognition performance, maintaining memory efficiency in pre-training. Experiments across benchmarks demonstrate the effectiveness, transferability, generalization, and efficiency of the proposed approach compared to state-of-the-art methods. <div>
arXiv:2505.08561v1 Announce Type: new 
Abstract: Masked video modeling~(MVM) has emerged as a highly effective pre-training strategy for visual foundation models, whereby the model reconstructs masked spatiotemporal tokens using information from visible tokens. However, a key challenge in such approaches lies in selecting an appropriate masking strategy. Previous studies have explored predefined masking techniques, including random and tube-based masking, as well as approaches that leverage key motion priors, optical flow and semantic cues from externally pre-trained models. In this work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token Sampler (TATS), which models the motion dynamics of tokens and can be seamlessly integrated into the masked autoencoder (MAE) framework to select motion-centric tokens in videos. Additionally, we propose a unified training strategy that enables joint optimization of both MAE and TATS from scratch using Proximal Policy Optimization (PPO). We show that our model allows for aggressive masking without compromising performance on the downstream task of action recognition while also ensuring that the pre-training remains memory efficient. Extensive experiments of the proposed approach across four benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51, demonstrate the effectiveness, transferability, generalization, and efficiency of our work compared to other state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections</title>
<link>https://arxiv.org/abs/2505.08568</link>
<guid>https://arxiv.org/abs/2505.08568</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, thermal detector, traffic light system, mobility restrictions, pedestrian safety

Summary:
The article introduces a new thermal detector-based traffic light system designed to cater to the needs of individuals with mobility restrictions and visual impairments. Traditional RGB camera-based systems often overlook these users, but the proposed system dynamically adjusts signal durations for those with walking impairments and triggers auditory signals for the visually impaired. To address the challenges of thermal imaging, a new YOLO-Thermal detector is developed, enhancing detection accuracy and robustness in thermal imagery. The system also includes the creation of a thermal dataset for people with mobility restrictions, capturing diverse pedestrian scenarios in various environmental conditions. Experimental results show that the proposed system outperforms existing detectors and effectively enhances barrier-free intersection for all users. The source codes and dataset are available for further research and development. <br /><br />Summary: The new thermal detector-based traffic light system addresses the needs of individuals with mobility restrictions and visual impairments by dynamically adjusting signal durations and triggering auditory signals. Developed with a focus on thermal imaging, the YOLO-Thermal detector improves detection accuracy, and the created thermal dataset captures diverse pedestrian scenarios. Experimental results show superior performance compared to existing detectors, highlighting its effectiveness in enhancing barrier-free intersection for all users. <div>
arXiv:2505.08568v1 Announce Type: new 
Abstract: Rapid advances in deep learning for computer vision have driven the adoption of RGB camera-based adaptive traffic light systems to improve traffic safety and pedestrian comfort. However, these systems often overlook the needs of people with mobility restrictions. Moreover, the use of RGB cameras presents significant challenges, including limited detection performance under adverse weather or low-visibility conditions, as well as heightened privacy concerns. To address these issues, we propose a fully automated, thermal detector-based traffic light system that dynamically adjusts signal durations for individuals with walking impairments or mobility burden and triggers the auditory signal for visually impaired individuals, thereby advancing towards barrier-free intersection for all users. To this end, we build the thermal dataset for people with mobility restrictions (TD4PWMR), designed to capture diverse pedestrian scenarios, particularly focusing on individuals with mobility aids or mobility burden under varying environmental conditions, such as different lighting, weather, and crowded urban settings. While thermal imaging offers advantages in terms of privacy and robustness to adverse conditions, it also introduces inherent hurdles for object detection due to its lack of color and fine texture details and generally lower resolution of thermal images. To overcome these limitations, we develop YOLO-Thermal, a novel variant of the YOLO architecture that integrates advanced feature extraction and attention mechanisms for enhanced detection accuracy and robustness in thermal imaging. Experiments demonstrate that the proposed thermal detector outperforms existing detectors, while the proposed traffic light system effectively enhances barrier-free intersection. The source codes and dataset are available at https://github.com/leon2014dresden/YOLO-THERMAL.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking</title>
<link>https://arxiv.org/abs/2505.08581</link>
<guid>https://arxiv.org/abs/2505.08581</guid>
<content:encoded><![CDATA[
arXiv:2505.08581v1 Announce Type: new 
Abstract: Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior</title>
<link>https://arxiv.org/abs/2505.08585</link>
<guid>https://arxiv.org/abs/2505.08585</guid>
<content:encoded><![CDATA[
arXiv:2505.08585v1 Announce Type: new 
Abstract: Machine learning has taken a critical role in seismic interpretation workflows, especially in fault delineation tasks. However, despite the recent proliferation of pretrained models and synthetic datasets, the field still lacks a systematic understanding of the generalizability limits of these models across seismic data representing a variety of geologic, acquisition and processing settings. Distributional shifts between different data sources, limitations in fine-tuning strategies and labeled data accessibility, and inconsistent evaluation protocols all represent major roadblocks in the deployment of reliable and robust models in real-world exploration settings. In this paper, we present the first large-scale benchmarking study explicitly designed to provide answers and guidelines for domain shift strategies in seismic interpretation. Our benchmark encompasses over $200$ models trained and evaluated on three heterogeneous datasets (synthetic and real data) including FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining, fine-tuning, and joint training strategies under varying degrees of domain shift. Our analysis highlights the fragility of current fine-tuning practices, the emergence of catastrophic forgetting, and the challenges of interpreting performance in a systematic manner. We establish a robust experimental baseline to provide insights into the tradeoffs inherent to current fault delineation workflows, and shed light on directions for developing more generalizable, interpretable and effective machine learning models for seismic interpretation. The insights and analyses reported provide a set of guidelines on the deployment of fault delineation models within seismic interpretation workflows.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrePrompt: Predictive prompting for class incremental learning</title>
<link>https://arxiv.org/abs/2505.08586</link>
<guid>https://arxiv.org/abs/2505.08586</guid>
<content:encoded><![CDATA[
arXiv:2505.08586v1 Announce Type: new 
Abstract: Class Incremental Learning (CIL) based on pre-trained models offers a promising direction for open-world continual learning. Existing methods typically rely on correlation-based strategies, where an image's classification feature is used as a query to retrieve the most related key prompts and select the corresponding value prompts for training. However, these approaches face an inherent limitation: fitting the entire feature space of all tasks with only a few trainable prompts is fundamentally challenging. We propose Predictive Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based limitations by leveraging pre-trained models' natural classification ability to predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a two-stage prediction framework: task-specific prompt prediction followed by label prediction. While theoretically appealing, this framework risks bias toward recent classes due to missing historical data for older classifier calibration. PrePrompt then mitigates this by incorporating feature translation, dynamically balancing stability and plasticity. Experiments across multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art prompt-based CIL methods. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment</title>
<link>https://arxiv.org/abs/2505.08589</link>
<guid>https://arxiv.org/abs/2505.08589</guid>
<content:encoded><![CDATA[
arXiv:2505.08589v1 Announce Type: new 
Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI) dataset comprising 2525 images taken by a drone flying over dense urban environments. MESSI is unique in two main features. First, it contains images from various altitudes, allowing us to investigate the effect of depth on semantic segmentation. Second, it includes images taken from several different urban regions (at different altitudes). This is important since the variety covers the visual richness captured by a drone's 3D flight, performing horizontal and vertical maneuvers. MESSI contains images annotated with location, orientation, and the camera's intrinsic parameters and can be used to train a deep neural network for semantic segmentation or other applications of interest (e.g., localization, navigation, and tracking). This paper describes the dataset and provides annotation details. It also explains how semantic segmentation was performed using several neural network models and shows several relevant statistics. MESSI will be published in the public domain to serve as an evaluation benchmark for semantic segmentation using images captured by a drone or similar vehicle flying over a dense urban environment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejoining fragmented ancient bamboo slips with physics-driven deep learning</title>
<link>https://arxiv.org/abs/2505.08601</link>
<guid>https://arxiv.org/abs/2505.08601</guid>
<content:encoded><![CDATA[
arXiv:2505.08601v1 Announce Type: new 
Abstract: Bamboo slips are a crucial medium for recording ancient civilizations in East Asia, and offers invaluable archaeological insights for reconstructing the Silk Road, studying material culture exchanges, and global history. However, many excavated bamboo slips have been fragmented into thousands of irregular pieces, making their rejoining a vital yet challenging step for understanding their content. Here we introduce WisePanda, a physics-driven deep learning framework designed to rejoin fragmented bamboo slips. Based on the physics of fracture and material deterioration, WisePanda automatically generates synthetic training data that captures the physical properties of bamboo fragmentations. This approach enables the training of a matching network without requiring manually paired samples, providing ranked suggestions to facilitate the rejoining process. Compared to the leading curve matching method, WisePanda increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using WisePanda have experienced substantial efficiency improvements (approximately 20 times faster) when rejoining fragmented bamboo slips. This research demonstrates that incorporating physical principles into deep learning models can significantly enhance their performance, transforming how archaeologists restore and study fragmented artifacts. WisePanda provides a new paradigm for addressing data scarcity in ancient artifact restoration through physics-driven machine learning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking</title>
<link>https://arxiv.org/abs/2505.08604</link>
<guid>https://arxiv.org/abs/2505.08604</guid>
<content:encoded><![CDATA[
arXiv:2505.08604v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability of deep learning models in medical imaging applications. This work is motivated by the observation that class activation maps (CAMs) for in-distribution (ID) data typically emphasize regions that are highly relevant to the model's predictions, whereas OOD data often lacks such focused activations. By masking input images with inverted CAMs, the feature representations of ID data undergo more substantial changes compared to those of OOD data, offering a robust criterion for differentiation. In this paper, we introduce a novel unsupervised OOD detection framework, Multi-Exit Class Activation Map (MECAM), which leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks that combine CAMs from varying resolutions and depths, our method captures both global and local feature representations, thereby enhancing the robustness of OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and PathMNIST, and test its performance against three medical OOD datasets, RSNA Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN. Comprehensive comparisons with state-of-the-art OOD detection methods validate the effectiveness of our approach. Our findings emphasize the potential of multi-exit networks and feature masking for advancing unsupervised OOD detection in medical imaging, paving the way for more reliable and interpretable models in clinical practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-Modal Information to Enhance Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.08605</link>
<guid>https://arxiv.org/abs/2505.08605</guid>
<content:encoded><![CDATA[
arXiv:2505.08605v1 Announce Type: new 
Abstract: Dataset distillation aims to create a compact and highly representative synthetic dataset that preserves the knowledge of a larger real dataset. While existing methods primarily focus on optimizing visual representations, incorporating additional modalities and refining object-level information can significantly improve the quality of distilled datasets. In this work, we introduce two key enhancements to dataset distillation: caption-guided supervision and object-centric masking. To integrate textual information, we propose two strategies for leveraging caption features: the feature concatenation, where caption embeddings are fused with visual features at the classification stage, and caption matching, which introduces a caption-based alignment loss during training to ensure semantic coherence between real and synthetic data. Additionally, we apply segmentation masks to isolate target objects and remove background distractions, introducing two loss functions designed for object-centric learning: masked feature alignment loss and masked gradient matching loss. Comprehensive evaluations demonstrate that integrating caption-based guidance and object-centric masking enhances dataset distillation, leading to synthetic datasets that achieve superior performance on downstream tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</title>
<link>https://arxiv.org/abs/2505.08607</link>
<guid>https://arxiv.org/abs/2505.08607</guid>
<content:encoded><![CDATA[
arXiv:2505.08607v1 Announce Type: new 
Abstract: Stereo matching methods rely on dense pixel-wise ground truth labels, which are laborious to obtain, especially for real-world datasets. The scarcity of labeled data and domain gaps between synthetic and real-world images also pose notable challenges. In this paper, we propose a novel framework, \textbf{BooSTer}, that leverages both vision foundation models and large-scale mixed image sources, including synthetic, real, and single-view images. First, to fully unleash the potential of large-scale single-view images, we design a data generation strategy combining monocular depth estimation and diffusion models to generate dense stereo matching data from single-view images. Second, to tackle sparse labels in real-world datasets, we transfer knowledge from monocular depth estimation models, using pseudo-mono depth labels and a dynamic scale- and shift-invariant loss for additional supervision. Furthermore, we incorporate vision foundation model as an encoder to extract robust and transferable features, boosting accuracy and generalization. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving significant improvements in accuracy over existing methods, particularly in scenarios with limited labeled data and domain shifts.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.08614</link>
<guid>https://arxiv.org/abs/2505.08614</guid>
<content:encoded><![CDATA[
arXiv:2505.08614v1 Announce Type: new 
Abstract: Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08617</link>
<guid>https://arxiv.org/abs/2505.08617</guid>
<content:encoded><![CDATA[
arXiv:2505.08617v1 Announce Type: new 
Abstract: While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08644</link>
<guid>https://arxiv.org/abs/2505.08644</guid>
<content:encoded><![CDATA[
arXiv:2505.08644v1 Announce Type: new 
Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
<link>https://arxiv.org/abs/2505.08665</link>
<guid>https://arxiv.org/abs/2505.08665</guid>
<content:encoded><![CDATA[
arXiv:2505.08665v1 Announce Type: new 
Abstract: Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results</title>
<link>https://arxiv.org/abs/2505.08685</link>
<guid>https://arxiv.org/abs/2505.08685</guid>
<content:encoded><![CDATA[
arXiv:2505.08685v1 Announce Type: new 
Abstract: Deep learning (DL) has become the dominant approach for medical image segmentation, yet ensuring the reliability and clinical applicability of these models requires addressing key challenges such as annotation variability, calibration, and uncertainty estimation. This is why we created the Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS), which highlights the critical role of multiple annotators in establishing a more comprehensive ground truth, emphasizing that segmentation is inherently subjective and that leveraging inter-annotator variability is essential for robust model evaluation. Seven teams participated in the challenge, submitting a variety of DL models evaluated using metrics such as Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and Continuous Ranked Probability Score (CRPS). By incorporating consensus and dissensus ground truth, we assess how DL models handle uncertainty and whether their confidence estimates align with true segmentation performance. Our findings reinforce the importance of well-calibrated models, as better calibration is strongly correlated with the quality of the results. Furthermore, we demonstrate that segmentation models trained on diverse datasets and enriched with pre-trained knowledge exhibit greater robustness, particularly in cases deviating from standard anatomical structures. Notably, the best-performing models achieved high DSC and well-calibrated uncertainty estimates. This work underscores the need for multi-annotator ground truth, thorough calibration assessments, and uncertainty-aware evaluations to develop trustworthy and clinically reliable DL-based medical image segmentation models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model</title>
<link>https://arxiv.org/abs/2505.08695</link>
<guid>https://arxiv.org/abs/2505.08695</guid>
<content:encoded><![CDATA[
arXiv:2505.08695v1 Announce Type: new 
Abstract: Given an arbitrary content and style image, arbitrary style transfer aims to render a new stylized
  image which preserves the content image's structure and possesses the style image's style. Existing
  arbitrary style transfer methods are based on either small models or pre-trained large-scale models.
  The small model-based methods fail to generate high-quality stylized images, bringing artifacts and
  disharmonious patterns. The pre-trained large-scale model-based methods can generate high-quality
  stylized images but struggle to preserve the content structure and cost long inference time. To this
  end, we propose a new framework, called SPAST, to generate high-quality stylized images with
  less inference time. Specifically, we design a novel Local-global Window Size Stylization Module
  (LGWSSM)tofuse style features into content features. Besides, we introduce a novel style prior loss,
  which can dig out the style priors from a pre-trained large-scale model into the SPAST and motivate
  the SPAST to generate high-quality stylized images with short inference time.We conduct abundant
  experiments to verify that our proposed method can generate high-quality stylized images and less
  inference time compared with the SOTA arbitrary style transfer methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Image Colorization with Instance-aware Texts and Masks</title>
<link>https://arxiv.org/abs/2505.08705</link>
<guid>https://arxiv.org/abs/2505.08705</guid>
<content:encoded><![CDATA[
arXiv:2505.08705v1 Announce Type: new 
Abstract: Recently, the application of deep learning in image colorization has received widespread attention. The maturation of diffusion models has further advanced the development of image colorization models. However, current mainstream image colorization models still face issues such as color bleeding and color binding errors, and cannot colorize images at the instance level. In this paper, we propose a diffusion-based colorization method MT-Color to achieve precise instance-aware colorization with use-provided guidance. To tackle color bleeding issue, we design a pixel-level mask attention mechanism that integrates latent features and conditional gray image features through cross-attention. We use segmentation masks to construct cross-attention masks, preventing pixel information from exchanging between different instances. We also introduce an instance mask and text guidance module that extracts instance masks and text representations of each instance, which are then fused with latent features through self-attention, utilizing instance masks to form self-attention masks to prevent instance texts from guiding the colorization of other areas, thus mitigating color binding errors. Furthermore, we apply a multi-instance sampling strategy, which involves sampling each instance region separately and then fusing the results. Additionally, we have created a specialized dataset for instance-level colorization tasks, GPT-color, by leveraging large visual language models on existing image datasets. Qualitative and quantitative experiments show that our model and dataset outperform previous methods and datasets.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</title>
<link>https://arxiv.org/abs/2505.08723</link>
<guid>https://arxiv.org/abs/2505.08723</guid>
<content:encoded><![CDATA[
arXiv:2505.08723v1 Announce Type: new 
Abstract: Satellite image time series (SITS) provide continuous observations of the Earth's surface, making them essential for applications such as environmental management and disaster assessment. However, existing spatiotemporal foundation models rely on plain vision transformers, which encode entire temporal sequences without explicitly capturing multiscale spatiotemporal relationships between land objects. This limitation hinders their effectiveness in downstream tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision transformer foundation model tailored for SITS analysis. At its core, we introduce a spatiotemporal gyroscope attention mechanism that dynamically captures evolving multiscale patterns across both time and space. For pre-training, we curate MillionST, a large-scale dataset of one million images from 100,000 geographic locations, each captured across 10 temporal phases over five years, encompassing diverse geospatial changes and seasonal variations. Leveraging this dataset, we adapt masked image modeling to pre-train TiMo, enabling it to effectively learn and encode generalizable spatiotemporal representations.Extensive experiments across multiple spatiotemporal tasks-including deforestation monitoring, land cover segmentation, crop type classification, and flood detection-demonstrate TiMo's superiority over state-of-the-art methods. Code, model, and dataset will be released at https://github.com/MiliLab/TiMo.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.08725</link>
<guid>https://arxiv.org/abs/2505.08725</guid>
<content:encoded><![CDATA[
arXiv:2505.08725v1 Announce Type: new 
Abstract: The Large Visual-Language Models (LVLMs) have significantly advanced image understanding. Their comprehension and reasoning capabilities enable promising applications in autonomous driving scenarios. However, existing research typically focuses on front-view perspectives and partial objects within scenes, struggling to achieve comprehensive scene understanding. Meanwhile, existing LVLMs suffer from the lack of mapping relationship between 2D and 3D and insufficient integration of 3D object localization and instruction understanding. To tackle these limitations, we first introduce NuInteract, a large-scale dataset with over 1.5M multi-view image language pairs spanning dense scene captions and diverse interactive tasks. Furthermore, we propose DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs with a spatial processor using a series of learnable queries. The spatial processor, designed as a plug-and-play component, can be initialized with pre-trained 3D detectors to improve 3D perception. Our experiments show that DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable improvement on the 3D visual grounding task. The dataset and code will be released at https://github.com/zc-zhao/DriveMonkey.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion</title>
<link>https://arxiv.org/abs/2505.08747</link>
<guid>https://arxiv.org/abs/2505.08747</guid>
<content:encoded><![CDATA[
arXiv:2505.08747v1 Announce Type: new 
Abstract: Nutrition estimation is an important component of promoting healthy eating and mitigating diet-related health risks. Despite advances in tasks such as food classification and ingredient recognition, progress in nutrition estimation is limited due to the lack of datasets with nutritional annotations. To address this issue, we introduce FastFood, a dataset with 84,446 images across 908 fast food categories, featuring ingredient and nutritional annotations. In addition, we propose a new model-agnostic Visual-Ingredient Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating visual and ingredient features. Ingredient robustness is improved through synonym replacement and resampling strategies during training. The ingredient-aware visual feature fusion module combines ingredient features and visual representation to achieve accurate nutritional prediction. During testing, ingredient predictions are refined using large multimodal models by data augmentation and majority voting. Our experiments on both FastFood and Nutrition5k datasets validate the effectiveness of our proposed method built in different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the importance of ingredient information in nutrition estimation. https://huiyanqi.github.io/fastfood-nutrition-estimation/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology</title>
<link>https://arxiv.org/abs/2505.08765</link>
<guid>https://arxiv.org/abs/2505.08765</guid>
<content:encoded><![CDATA[
arXiv:2505.08765v1 Announce Type: new 
Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target objects using visual and textual cues without external guidance. Existing approaches struggle in complex urban environments due to redundant semantic processing, similar object distinction, and the exploration-exploitation dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS, the first benchmark dataset for autonomous search of common urban objects. This dataset comprises 2,420 tasks across six object categories with varying difficulty levels, enabling comprehensive evaluation of UAV agents' search capabilities. To solve the AVOS tasks, we also propose PRPSearcher (Perception-Reasoning-Planning Searcher), a novel agentic method powered by multi-modal large language models (MLLMs) that mimics human three-tier cognition. Specifically, PRPSearcher constructs three specialized maps: an object-centric dynamic semantic map enhancing spatial perception, a 3D cognitive map based on semantic attraction values for target reasoning, and a 3D uncertainty map for balanced exploration-exploitation search. Also, our approach incorporates a denoising mechanism to mitigate interference from similar objects and utilizes an Inspiration Promote Thought (IPT) prompting mechanism for adaptive action planning. Experimental results on CityAVOS demonstrate that PRPSearcher surpasses existing baselines in both success rate and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and -46.40% NE). While promising, the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities in AVOS tasks. This work establishes a foundation for future advances in embodied target search. Dataset and source code are available at https://anonymous.4open.science/r/CityAVOS-3DF8.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation</title>
<link>https://arxiv.org/abs/2505.07840</link>
<guid>https://arxiv.org/abs/2505.07840</guid>
<content:encoded><![CDATA[
arXiv:2505.07840v1 Announce Type: cross 
Abstract: Precision farming relies on accurate vegetation monitoring to enhance crop productivity and promote sustainable agricultural practices. This study presents a comprehensive evaluation of UAV-based imaging for vegetation health assessment in a palm tree cultivation region in Dubai. By comparing multispectral and RGB image data, we demonstrate that RGBbased vegetation indices offer performance comparable to more expensive multispectral indices, providing a cost-effective alternative for large-scale agricultural monitoring. Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI were computed to categorize vegetation into healthy, moderate, and stressed conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered similar results in vegetation classification and stress detection. Our findings highlight the practical benefits of integrating RGB imagery into precision farming, reducing operational costs while maintaining accuracy in plant health monitoring. This research underscores the potential of UAVbased RGB imaging as a powerful tool for precision agriculture, enabling broader adoption of data-driven decision-making in crop management. By leveraging the strengths of both multispectral and RGB imaging, this work advances the state of UAV applications in agriculture, paving the way for more efficient and scalable farming solutions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding</title>
<link>https://arxiv.org/abs/2505.07851</link>
<guid>https://arxiv.org/abs/2505.07851</guid>
<content:encoded><![CDATA[
arXiv:2505.07851v1 Announce Type: cross 
Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing high-resolution, real-time imaging of cardiac structures. However, existing navigation methods rely on electromagnetic (EM) tracking, which is susceptible to interference and position drift, or require manual adjustments based on operator expertise. To overcome these limitations, we propose a novel anatomy-aware pose estimation system that determines the ICE catheter position and orientation solely from ICE images, eliminating the need for external tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep learning model, which captures spatial relationships between ICE images and anatomical structures. The model is trained on a clinically acquired dataset of 851 subjects, including ICE images paired with position and orientation labels normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16 embeddings and processed through a transformer network, where a [CLS] token independently predicts position and orientation via separate linear layers. The model is optimized using a Mean Squared Error (MSE) loss function, balancing positional and orientational accuracy. Experimental results demonstrate an average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98 deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy. Qualitative assessments further validate alignment between predicted and target views within 3D cardiac meshes. This AI-driven system enhances procedural efficiency, reduces operator workload, and enables real-time ICE catheter localization for tracking-free procedures. The proposed method can function independently or complement existing mapping systems like CARTO, offering a transformative approach to ICE-guided interventions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding</title>
<link>https://arxiv.org/abs/2505.07864</link>
<guid>https://arxiv.org/abs/2505.07864</guid>
<content:encoded><![CDATA[
arXiv:2505.07864v1 Announce Type: cross 
Abstract: Flowcharts are indispensable tools in software design and business-process analysis, yet current vision-language models (VLMs) frequently misinterpret the directional arrows and graph topology that set these diagrams apart from natural images. We introduce a seven-stage pipeline grouped into three broader processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical character recognition (OCR) to extract node text; and (3) construction of a structured prompt that guides the VLMs. Tested on a 90-question benchmark distilled from 30 annotated flowcharts, the method raises overall accuracy from 80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp); branch-result questions improve more modestly, and before-step questions remain difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same trends, reinforcing the advantage of explicit arrow encoding. Limitations include dependence on detector and OCR precision, the small evaluation set, and residual errors at nodes with multiple incoming edges. Future work will enlarge the benchmark with synthetic and handwritten flowcharts and assess the approach on Business Process Model and Notation (BPMN) and Unified Modeling Language (UML).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2505.07866</link>
<guid>https://arxiv.org/abs/2505.07866</guid>
<content:encoded><![CDATA[
arXiv:2505.07866v1 Announce Type: cross 
Abstract: The diffusion model has recently emerged as a potent approach in computer vision, demonstrating remarkable performances in the field of generative artificial intelligence. Capable of producing high-quality synthetic images, diffusion models have been successfully applied across a range of applications. However, a significant challenge remains with the high computational cost associated with training and generating these models. This study focuses on the efficiency and inference time of diffusion-based generative models, highlighting their applications in both natural and medical imaging. We present the most recent advances in diffusion models by categorizing them into three key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play a crucial role in medical imaging, where producing fast, reliable, and high-quality medical images is essential for accurate analysis of abnormalities and disease diagnosis. We first investigate the general framework of DDPM, LDM, and WDM and discuss the computational complexity gap filled by these models in natural and medical imaging. We then discuss the current limitations of these models as well as the opportunities and future research directions in medical imaging.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2505.07879</link>
<guid>https://arxiv.org/abs/2505.07879</guid>
<content:encoded><![CDATA[
arXiv:2505.07879v1 Announce Type: cross 
Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monocular Online Reconstruction with Enhanced Detail Preservation</title>
<link>https://arxiv.org/abs/2505.07887</link>
<guid>https://arxiv.org/abs/2505.07887</guid>
<content:encoded><![CDATA[
arXiv:2505.07887v1 Announce Type: cross 
Abstract: We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the Hierarchical Gaussian Management Module for effective Gaussian distribution and the Global Consistency Optimization Module for maintaining alignment and coherence at all scales. In addition, we present the Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors</title>
<link>https://arxiv.org/abs/2505.07906</link>
<guid>https://arxiv.org/abs/2505.07906</guid>
<content:encoded><![CDATA[
arXiv:2505.07906v1 Announce Type: cross 
Abstract: Microstructure often dictates materials performance, yet it is rarely treated as an explicit design variable because microstructure is hard to quantify, predict, and optimize. Here, we introduce an image centric, closed-loop framework that makes microstructural morphology into a controllable objective and demonstrate its use case with Li- and Mn-rich layered oxide cathode precursors. This work presents an integrated, AI driven framework for the predictive design and optimization of lithium-ion battery cathode precursor synthesis. This framework integrates a diffusion-based image generation model, a quantitative image analysis pipeline, and a particle swarm optimization (PSO) algorithm. By extracting key morphological descriptors such as texture, sphericity, and median particle size (D50) from SEM images, the platform accurately predicts SEM like morphologies resulting from specific coprecipitation conditions, including reaction time-, solution concentration-, and pH-dependent structural changes. Optimization then pinpoints synthesis parameters that yield user defined target morphologies, as experimentally validated by the close agreement between predicted and synthesized structures. This framework offers a practical strategy for data driven materials design, enabling both forward prediction and inverse design of synthesis conditions and paving the way toward autonomous, image guided microstructure engineering.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny</title>
<link>https://arxiv.org/abs/2505.07908</link>
<guid>https://arxiv.org/abs/2505.07908</guid>
<content:encoded><![CDATA[
arXiv:2505.07908v1 Announce Type: cross 
Abstract: In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA (Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fr\'{e}chet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids</title>
<link>https://arxiv.org/abs/2505.08082</link>
<guid>https://arxiv.org/abs/2505.08082</guid>
<content:encoded><![CDATA[
arXiv:2505.08082v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fr\'{e}chet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Neighborhood Environments with Large Language Models</title>
<link>https://arxiv.org/abs/2505.08163</link>
<guid>https://arxiv.org/abs/2505.08163</guid>
<content:encoded><![CDATA[
arXiv:2505.08163v1 Announce Type: cross 
Abstract: Neighborhood environments include physical and environmental conditions such as housing quality, roads, and sidewalks, which significantly influence human health and well-being. Traditional methods for assessing these environments, including field surveys and geographic information systems (GIS), are resource-intensive and challenging to evaluate neighborhood environments at scale. Although machine learning offers potential for automated analysis, the laborious process of labeling training data and the lack of accessible models hinder scalability. This study explores the feasibility of large language models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood environments (e.g., sidewalk and powerline) at scale. We train a robust YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting six environmental indicators, including streetlight, sidewalk, powerline, apartment, single-lane road, and multilane road. We then evaluate four LLMs, including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility, robustness, and limitations in identifying these indicators, with a focus on the impact of prompting strategies and fine-tuning. We apply majority voting with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs could be a useful tool to decode the neighborhood environment without any training effort.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices</title>
<link>https://arxiv.org/abs/2505.08191</link>
<guid>https://arxiv.org/abs/2505.08191</guid>
<content:encoded><![CDATA[
arXiv:2505.08191v1 Announce Type: cross 
Abstract: Neural rendering has gained prominence for its high-quality output, which is crucial for AR/VR applications. However, its large voxel grid data size and irregular access patterns challenge real-time processing on edge devices. While previous works have focused on improving data locality, they have not adequately addressed the issue of large voxel grid sizes, which necessitate frequent off-chip memory access and substantial on-chip memory. This paper introduces SpNeRF, a software-hardware co-design solution tailored for sparse volumetric neural rendering. We first identify memory-bound rendering inefficiencies and analyze the inherent sparsity in the voxel grid data of neural rendering. To enhance efficiency, we propose novel preprocessing and online decoding steps, reducing the memory size for voxel grid. The preprocessing step employs hash mapping to support irregular data access while maintaining a minimal memory size. The online decoding step enables efficient on-chip sparse voxel grid processing, incorporating bitmap masking to mitigate PSNR loss caused by hash collisions. To further optimize performance, we design a dedicated hardware architecture supporting our sparse voxel grid processing technique. Experimental results demonstrate that SpNeRF achieves an average 21.07$\times$ reduction in memory size while maintaining comparable PSNR levels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and NeuRex.Edge, our design achieves speedups of 95.1$\times$, 63.5$\times$, 1.5$\times$ and 10.3$\times$, and improves energy efficiency by 625.6$\times$, 529.1$\times$, 4$\times$, and 4.4$\times$, respectively.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image</title>
<link>https://arxiv.org/abs/2505.08239</link>
<guid>https://arxiv.org/abs/2505.08239</guid>
<content:encoded><![CDATA[
arXiv:2505.08239v1 Announce Type: cross 
Abstract: We introduce adaptive view planning to multi-view synthesis, aiming to improve both occlusion revelation and 3D consistency for single-view 3D reconstruction. Instead of generating an unordered set of views independently or simultaneously, we generate a sequence of views, leveraging temporal consistency to enhance 3D coherence. Most importantly, our view sequence is not determined by a pre-determined camera setup. Instead, we compute an adaptive camera trajectory (ACT), specifically, an orbit of camera views, which maximizes the visibility of occluded regions of the 3D object to be reconstructed. Once the best orbit is found, we feed it to a video diffusion model to generate novel views around the orbit, which in turn, are passed to a multi-view 3D reconstruction model to obtain the final reconstruction. Our multi-view synthesis pipeline is quite efficient since it involves no run-time training/optimization, only forward inferences by applying the pre-trained models for occlusion analysis and multi-view synthesis. Our method predicts camera trajectories that reveal occlusions effectively and produce consistent novel views, significantly improving 3D reconstruction over SOTA on the unseen GSO dataset, both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis</title>
<link>https://arxiv.org/abs/2505.08247</link>
<guid>https://arxiv.org/abs/2505.08247</guid>
<content:encoded><![CDATA[
arXiv:2505.08247v1 Announce Type: cross 
Abstract: Medical image synthesis plays a crucial role in providing anatomically accurate images for diagnosis and treatment. Hallux valgus, which affects approximately 19% of the global population, requires frequent weight-bearing X-rays for assessment, placing additional strain on both patients and healthcare providers. Existing X-ray models often struggle to balance image fidelity, skeletal consistency, and physical constraints, particularly in diffusion-based methods that lack skeletal guidance. We propose the Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a foot evaluation method utilizing skeletal landmarks. SCCDM incorporates multi-scale feature extraction and attention mechanisms, improving the Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves an average score of 0.85, demonstrating strong clinical applicability. The code is available at https://github.com/midisec/SCCDM.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted</title>
<link>https://arxiv.org/abs/2505.08255</link>
<guid>https://arxiv.org/abs/2505.08255</guid>
<content:encoded><![CDATA[
arXiv:2505.08255v1 Announce Type: cross 
Abstract: With the advancement of AI generative techniques, Deepfake faces have become incredibly realistic and nearly indistinguishable to the human eye. To counter this, Deepfake detectors have been developed as reliable tools for assessing face authenticity. These detectors are typically developed on Deep Neural Networks (DNNs) and trained using third-party datasets. However, this protocol raises a new security risk that can seriously undermine the trustfulness of Deepfake detectors: Once the third-party data providers insert poisoned (corrupted) data maliciously, Deepfake detectors trained on these datasets will be injected ``backdoors'' that cause abnormal behavior when presented with samples containing specific triggers. This is a practical concern, as third-party providers may distribute or sell these triggers to malicious users, allowing them to manipulate detector performance and escape accountability.
  This paper investigates this risk in depth and describes a solution to stealthily infect Deepfake detectors. Specifically, we develop a trigger generator, that can synthesize passcode-controlled, semantic-suppression, adaptive, and invisible trigger patterns, ensuring both the stealthiness and effectiveness of these triggers. Then we discuss two poisoning scenarios, dirty-label poisoning and clean-label poisoning, to accomplish the injection of backdoors. Extensive experiments demonstrate the effectiveness, stealthiness, and practicality of our method compared to several baselines.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities</title>
<link>https://arxiv.org/abs/2505.08283</link>
<guid>https://arxiv.org/abs/2505.08283</guid>
<content:encoded><![CDATA[
arXiv:2505.08283v1 Announce Type: cross 
Abstract: Multimodal learning enhances deep learning models by enabling them to perceive and understand information from multiple data modalities, such as visual and textual inputs. However, most existing approaches assume the availability of all modalities, an assumption that often fails in real-world applications. Recent works have introduced learnable missing-case-aware prompts to mitigate performance degradation caused by missing modalities while reducing the need for extensive model fine-tuning. Building upon the effectiveness of missing-case-aware handling for missing modalities, we propose a novel decoupled prototype-based output head, which leverages missing-case-aware class-wise prototypes tailored for each individual modality. This approach dynamically adapts to different missing modality scenarios and can be seamlessly integrated with existing prompt-based methods. Extensive experiments demonstrate that our proposed output head significantly improves performance across a wide range of missing-modality scenarios and varying missing rates.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</title>
<link>https://arxiv.org/abs/2505.08293</link>
<guid>https://arxiv.org/abs/2505.08293</guid>
<content:encoded><![CDATA[
arXiv:2505.08293v1 Announce Type: cross 
Abstract: Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2505.08299</link>
<guid>https://arxiv.org/abs/2505.08299</guid>
<content:encoded><![CDATA[
arXiv:2505.08299v1 Announce Type: cross 
Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged as powerful alternatives to Transformers for sequence modeling, offering linear-time complexity and competitive performance across diverse tasks. However, their large parameter counts pose significant challenges for deployment in resource-constrained environments. We propose a novel unstructured pruning framework tailored for Mamba models that achieves up to 70\% parameter reduction while retaining over 95\% of the original performance. Our approach integrates three key innovations: (1) a gradient-aware magnitude pruning technique that combines weight magnitude and gradient information to identify less critical parameters, (2) an iterative pruning schedule that gradually increases sparsity to maintain model stability, and (3) a global pruning strategy that optimizes parameter allocation across the entire model. Through extensive experiments on WikiText-103, Long Range Arena, and ETT time-series benchmarks, we demonstrate significant efficiency gains with minimal performance degradation. Our analysis of pruning effects on Mamba's components reveals critical insights into the architecture's redundancy and robustness, enabling practical deployment in resource-constrained settings while broadening Mamba's applicability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
<link>https://arxiv.org/abs/2505.08316</link>
<guid>https://arxiv.org/abs/2505.08316</guid>
<content:encoded><![CDATA[
arXiv:2505.08316v1 Announce Type: cross 
Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</title>
<link>https://arxiv.org/abs/2505.08414</link>
<guid>https://arxiv.org/abs/2505.08414</guid>
<content:encoded><![CDATA[
arXiv:2505.08414v1 Announce Type: cross 
Abstract: Current deep learning models are mostly task specific and lack a user-friendly interface to operate. We present Meta-EyeFM, a multi-function foundation model that integrates a large language model (LLM) with vision foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a routing mechanism to enable accurate task-specific analysis based on text queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and systemic diseases, differentiate ocular disease severity, and identify common ocular signs. The model achieved 100% accuracy in routing fundus images to appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection, $\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification. Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o LMMs in detecting various eye diseases and comparable to an ophthalmologist. This system offers enhanced usability and diagnostic performance, making it a valuable decision support tool for primary eye care or an online LLM for fundus evaluation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI</title>
<link>https://arxiv.org/abs/2505.08430</link>
<guid>https://arxiv.org/abs/2505.08430</guid>
<content:encoded><![CDATA[
arXiv:2505.08430v1 Announce Type: cross 
Abstract: Tertiary lymphoid structures (TLS) are organized clusters of immune cells, whose maturity and area can be quantified in whole slide image (WSI) for various prognostic tasks. Existing methods for assessing these characteristics typically rely on cell proxy tasks and require additional post-processing steps. In this work, We focus on a novel task-TLS Semantic Segmentation (TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in an end-to-end manner. Due to the extensive scale of WSI and patch-based segmentation strategies, TLS-SS necessitates integrating from neighboring patches to guide target patch (target) segmentation. Previous techniques often employ on multi-resolution approaches, constraining the capacity to leverage the broader neighboring context while tend to preserve coarse-grained information. To address this, we propose a GNN-based Neighboring Context Aggregation Framework (GNCAF), which progressively aggregates multi-hop neighboring context from the target and employs a self-attention mechanism to guide the segmentation of the target. GNCAF can be integrated with various segmentation models to enhance their ability to perceive contextual information outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly available. Experiments on these datasets demonstrate the superiority of GNCAF, achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU, respectively. Additionally, we also validate the task scalability of GNCAF on segmentation of lymph node metastases.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?</title>
<link>https://arxiv.org/abs/2505.08468</link>
<guid>https://arxiv.org/abs/2505.08468</guid>
<content:encoded><![CDATA[
arXiv:2505.08468v1 Announce Type: cross 
Abstract: Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2505.08528</link>
<guid>https://arxiv.org/abs/2505.08528</guid>
<content:encoded><![CDATA[
arXiv:2505.08528v1 Announce Type: cross 
Abstract: In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A portable diagnosis model for Keratoconus using a smartphone</title>
<link>https://arxiv.org/abs/2505.08616</link>
<guid>https://arxiv.org/abs/2505.08616</guid>
<content:encoded><![CDATA[
arXiv:2505.08616v1 Announce Type: cross 
Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized thinning and protrusion, leading to visual distortion. While Placido disc-based topography remains a standard in clinical diagnostics, its dependence on specialized equipment limits accessibility. In this paper, we propose a portable, smartphone-based diagnostic framework that captures corneal reflections of a Placido disc displayed on a phone screen and applies a two-stage detection pipeline, then validate on 3D-printed emulated eyeball models that simulate normal, moderate, and severe KC stages based on anterior chamber depth (ACD). The first step of the two-stage detection pipeline is classifying different stages of KC with features including height and width of extracted reflections using weighted support vector machine (WSVM). It achieves a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16 Pro. For the second step, we visualize the KC-affected protrusion regions on the corneas with color maps based on inter-disc distance, that provides an intuitive representation of disease severity and localization. Moreover, we validate the ability of the extracted features to differentiate between KC stages with ANOVA and Omega Squared, with significant p-values (e.g., $p < 10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models</title>
<link>https://arxiv.org/abs/2505.08622</link>
<guid>https://arxiv.org/abs/2505.08622</guid>
<content:encoded><![CDATA[
arXiv:2505.08622v1 Announce Type: cross 
Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Claycode: Stylable and Deformable 2D Scannable Codes</title>
<link>https://arxiv.org/abs/2505.08666</link>
<guid>https://arxiv.org/abs/2505.08666</guid>
<content:encoded><![CDATA[
arXiv:2505.08666v1 Announce Type: cross 
Abstract: This paper introduces Claycode, a novel 2D scannable code designed for extensive stylization and deformation. Unlike traditional matrix-based codes (e.g., QR codes), Claycodes encode their message in a tree structure. During the encoding process, bits are mapped into a topology tree, which is then depicted as a nesting of color regions drawn within the boundaries of a target polygon shape. When decoding, Claycodes are extracted and interpreted in real-time from a camera stream. We detail the end-to-end pipeline and show that Claycodes allow for extensive stylization without compromising their functionality. We then empirically demonstrate Claycode's high tolerance to heavy deformations, outperforming traditional 2D scannable codes in scenarios where they typically fail.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Coder:Text-Guided CAD Files Code Generation</title>
<link>https://arxiv.org/abs/2505.08686</link>
<guid>https://arxiv.org/abs/2505.08686</guid>
<content:encoded><![CDATA[
arXiv:2505.08686v1 Announce Type: cross 
Abstract: Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D models of real-world products. Traditional CAD typically relies on hand-drawing by experts or modifications of existing library files, which doesn't allow for rapid personalization. With the emergence of generative artificial intelligence, convenient and efficient personalized CAD generation has become possible. However, existing generative methods typically produce outputs that lack interactive editability and geometric annotations, limiting their practical applications in manufacturing. To enable interactive generative CAD, we propose CAD-Coder, a framework that transforms natural language instructions into CAD script codes, which can be executed in Python environments to generate human-editable CAD files (.Dxf). To facilitate the generation of editable CAD sketches with annotation information, we construct a comprehensive dataset comprising 29,130 Dxf files with their corresponding script codes, where each sketch preserves both editability and geometric annotations. We evaluate CAD-Coder on various 2D/3D CAD generation tasks against existing methods, demonstrating superior interactive capabilities while uniquely providing editable sketches with geometric annotations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation</title>
<link>https://arxiv.org/abs/2505.08693</link>
<guid>https://arxiv.org/abs/2505.08693</guid>
<content:encoded><![CDATA[
arXiv:2505.08693v1 Announce Type: cross 
Abstract: Self-supervised pretrain techniques have been widely used to improve the downstream tasks' performance. However, real-world magnetic resonance (MR) studies usually consist of different sets of contrasts due to different acquisition protocols, which poses challenges for the current deep learning methods on large-scale pretrain and different downstream tasks with different input requirements, since these methods typically require a fixed set of input modalities or, contrasts. To address this challenge, we propose variable-input ViT (VIViT), a transformer-based framework designed for self-supervised pretraining and segmentation finetuning for variable contrasts in each study. With this ability, our approach can maximize the data availability in pretrain, and can transfer the learned knowledge from pretrain to downstream tasks despite variations in input requirements. We validate our method on brain infarct and brain tumor segmentation, where our method outperforms current CNN and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively. These results highlight the efficacy of our design for better adaptability and performance on tasks with real-world heterogeneous MR data.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aya Vision: Advancing the Frontier of Multilingual Multimodality</title>
<link>https://arxiv.org/abs/2505.08751</link>
<guid>https://arxiv.org/abs/2505.08751</guid>
<content:encoded><![CDATA[
arXiv:2505.08751v1 Announce Type: cross 
Abstract: Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</title>
<link>https://arxiv.org/abs/2505.08787</link>
<guid>https://arxiv.org/abs/2505.08787</guid>
<content:encoded><![CDATA[
arXiv:2505.08787v1 Announce Type: cross 
Abstract: Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Anytime Optical Flow Estimation with Event Cameras</title>
<link>https://arxiv.org/abs/2307.05033</link>
<guid>https://arxiv.org/abs/2307.05033</guid>
<content:encoded><![CDATA[
arXiv:2307.05033v3 Announce Type: replace 
Abstract: Event cameras respond to changes in log-brightness at the millisecond level, making them ideal for optical flow estimation. However, existing datasets from event cameras provide only low frame rate ground truth for optical flow, limiting the research potential of event-driven optical flow. To address this challenge, we introduce a low-latency event representation, Unified Voxel Grid, and propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. Furthermore, we propose the Rectified Flow Warp Loss (RFWL) for the unsupervised assessment of intermediate optical flow. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-based interactive segmentation in remote sensing</title>
<link>https://arxiv.org/abs/2308.13174</link>
<guid>https://arxiv.org/abs/2308.13174</guid>
<content:encoded><![CDATA[
arXiv:2308.13174v3 Announce Type: replace 
Abstract: Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal Consistency to Frame-Based Domain Translation Approaches</title>
<link>https://arxiv.org/abs/2310.00868</link>
<guid>https://arxiv.org/abs/2310.00868</guid>
<content:encoded><![CDATA[
arXiv:2310.00868v2 Announce Type: replace 
Abstract: Fourteen million colonoscopies are performed annually just in the U.S. However, the videos from these colonoscopies are not saved due to storage constraints (each video from a high-definition colonoscope camera can be in tens of gigabytes). Instead, a few relevant individual frames are saved for documentation/reporting purposes and these are the frames on which most current colonoscopy AI models are trained on. While developing new unsupervised domain translation methods for colonoscopy (e.g. to translate between real optical and virtual/CT colonoscopy), it is thus typical to start with approaches that initially work for individual frames without temporal consistency. Once an individual-frame model has been finalized, additional contiguous frames are added with a modified deep learning architecture to train a new model from scratch for temporal consistency. This transition to temporally-consistent deep learning models, however, requires significantly more computational and memory resources for training. In this paper, we present a lightweight solution with a tunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual frame-based approaches that reduces training requirements by a factor of 5. We demonstrate the effectiveness of our approach on two challenging use cases in colonoscopy: haustral fold segmentation (indicative of missed surface) and realistic colonoscopy simulator video generation. We also release a first-of-its kind temporal dataset for colonoscopy for the above use cases. The datasets, accompanying code, and pretrained models will be made available on our Computational Endoscopy Platform GitHub (https://github.com/nadeemlab/CEP). The supplementary video is available at https://youtu.be/UMVP-uIXwWk.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized View and Geometry Distillation from Multi-view Diffuser</title>
<link>https://arxiv.org/abs/2312.06198</link>
<guid>https://arxiv.org/abs/2312.06198</guid>
<content:encoded><![CDATA[
arXiv:2312.06198v4 Announce Type: replace 
Abstract: Generating multi-view images from a single input view using image-conditioned diffusion models is a recent advancement and has shown considerable potential. However, issues such as the lack of consistency in synthesized views and over-smoothing in extracted geometry persist. Previous methods integrate multi-view consistency modules or impose additional supervisory to enhance view consistency while compromising on the flexibility of camera positioning and limiting the versatility of view synthesis. In this study, we consider the radiance field optimized during geometry extraction as a more rigid consistency prior, compared to volume and ray aggregation used in previous works. We further identify and rectify a critical bias in the traditional radiance field optimization process through score distillation from a multi-view diffuser. We introduce an Unbiased Score Distillation (USD) that utilizes unconditioned noises from a 2D diffusion model, greatly refining the radiance field fidelity. We leverage the rendered views from the optimized radiance field as the basis and develop a two-step specialization process of a 2D diffusion model, which is adept at conducting object-specific denoising and generating high-quality multi-view images. Finally, we recover faithful geometry and texture directly from the refined multi-view images. Empirical evaluations demonstrate that our optimized geometry and view distillation technique generates comparable results to the state-of-the-art models trained on extensive datasets, all while maintaining freedom in camera positioning. Please see our project page at https://youjiazhang.github.io/USD/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation</title>
<link>https://arxiv.org/abs/2403.17525</link>
<guid>https://arxiv.org/abs/2403.17525</guid>
<content:encoded><![CDATA[
arXiv:2403.17525v2 Announce Type: replace 
Abstract: When benefiting graphic sketch representation with sketch drawing orders, recent studies have linked sketch patches as graph edges by drawing orders in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since the contextual relationships between patches may be inconsistent with the sequential positions in drawing orders, due to variants of sketch drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for sketch learning. We introduce a sinusoidal absolute PE to embed the sequential positions in drawing orders, and a learnable relative PE to encode the unseen contextual relationships between patches. Both types of PEs never attend the construction of graph edges, but are injected into graph nodes to cooperate with the visual patterns captured from patches. After linking nodes by semantic proximity, during message aggregation via graph convolutional networks, each node receives both semantic features from patches and contextual information from PEs from its neighbors, which equips local patch patterns with global contextual information, further obtaining drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis. The source codes could be found at https://github.com/SCZang/DC-gra2seq.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2404.11824</link>
<guid>https://arxiv.org/abs/2404.11824</guid>
<content:encoded><![CDATA[
arXiv:2404.11824v5 Announce Type: replace 
Abstract: Text-to-image (T2I) generation has made remarkable progress in producing high-quality images, but a fundamental challenge remains: creating backgrounds that naturally accommodate text placement without compromising image quality. This capability is non-trivial for real-world applications like graphic design, where clear visual hierarchy between content and text is essential. Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating text-friendly backgrounds. We present TextCenGen, a training-free dynamic background adaptation in the blank region for text-friendly image generation. Instead of directly reducing attention in text areas, which degrades image quality, we relocate conflicting objects before background optimization. Our method analyzes cross-attention maps to identify conflicting objects overlapping with text regions and uses a force-directed graph approach to guide their relocation, followed by attention excluding constraints to ensure smooth backgrounds. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality. Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across four seed datasets, TextCenGen outperforms existing methods by achieving 23% lower saliency overlap in text regions while maintaining 98% of the semantic fidelity measured by CLIP score and our proposed Visual-Textual Concordance Metric (VTCM).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminative and Consistent Representation Distillation</title>
<link>https://arxiv.org/abs/2407.11802</link>
<guid>https://arxiv.org/abs/2407.11802</guid>
<content:encoded><![CDATA[
arXiv:2407.11802v5 Announce Type: replace 
Abstract: Knowledge Distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. While contrastive learning has shown promise in self-supervised learning by creating discriminative representations, its application in knowledge distillation remains limited and focuses primarily on discrimination, neglecting the structural relationships captured by the teacher model. To address this limitation, we propose Discriminative and Consistent Distillation (DCD), which employs a contrastive loss along with a consistency regularization to minimize the discrepancy between the distributions of teacher and student representations. Our method introduces learnable temperature and bias parameters that adapt during training to balance these complementary objectives, replacing the fixed hyperparameters commonly used in contrastive learning approaches. Through extensive experiments on CIFAR-100 and ImageNet ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance, with the student model sometimes surpassing the teacher's accuracy. Furthermore, we show that DCD's learned representations exhibit superior cross-dataset generalization when transferred to Tiny ImageNet and STL-10.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Representation Distillation</title>
<link>https://arxiv.org/abs/2407.12073</link>
<guid>https://arxiv.org/abs/2407.12073</guid>
<content:encoded><![CDATA[
arXiv:2407.12073v5 Announce Type: replace 
Abstract: Knowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Feature Matching for Large-Scale Structure from Motion</title>
<link>https://arxiv.org/abs/2409.02310</link>
<guid>https://arxiv.org/abs/2409.02310</guid>
<content:encoded><![CDATA[
arXiv:2409.02310v4 Announce Type: replace 
Abstract: Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2410.01723</link>
<guid>https://arxiv.org/abs/2410.01723</guid>
<content:encoded><![CDATA[
arXiv:2410.01723v5 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR Scene Semantic Segmentation</title>
<link>https://arxiv.org/abs/2411.11935</link>
<guid>https://arxiv.org/abs/2411.11935</guid>
<content:encoded><![CDATA[
arXiv:2411.11935v2 Announce Type: replace 
Abstract: Reliable deep learning models require not only accurate predictions but also well-calibrated confidence estimates to ensure dependable uncertainty estimation. This is crucial in safety-critical applications like autonomous driving, which depend on rapid and precise semantic segmentation of LiDAR point clouds for real-time 3D scene understanding. In this work, we introduce a sampling-free approach for estimating well-calibrated confidence values for classification tasks, achieving alignment with true classification accuracy and significantly reducing inference time compared to sampling-based methods. Our evaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic segmentation shows that our approach maintains well-calibrated confidence values while achieving increased processing speed compared to a sampling baseline. Additionally, reliability diagrams reveal that our method produces underconfidence rather than overconfident predictions, an advantage for safety-critical applications. Our sampling-free approach offers well-calibrated and time-efficient predictions for LiDAR scene semantic segmentation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHOICE: Benchmarking the Remote Sensing Capabilities of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2411.18145</link>
<guid>https://arxiv.org/abs/2411.18145</guid>
<content:encoded><![CDATA[
arXiv:2411.18145v3 Announce Type: replace 
Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive benchmark designed to objectively evaluate the hierarchical remote sensing capabilities of VLMs. Focusing on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 23 leaf tasks to ensure a well-rounded assessment coverage. CHOICE guarantees the quality of all 10,507 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control. The newly curated data and the format of multiple-choice questions with definitive answers allow for an objective and straightforward performance assessment. Our evaluation of 3 proprietary and 21 open-source VLMs highlights their critical limitations within this specialized context. We hope that CHOICE will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing. We will release CHOICE at https://github.com/ShawnAn-WHU/CHOICE.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Strategies for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2412.11553</link>
<guid>https://arxiv.org/abs/2412.11553</guid>
<content:encoded><![CDATA[
arXiv:2412.11553v2 Announce Type: replace 
Abstract: Accurate recognition and interpretation of sign language are crucial for enhancing communication accessibility for deaf and hard of hearing individuals. However, current approaches of Isolated Sign Language Recognition (ISLR) often face challenges such as low data quality and variability in gesturing speed. This paper introduces a comprehensive model training pipeline for ISLR designed to accommodate the distinctive characteristics and constraints of the Sign Language (SL) domain. The constructed pipeline incorporates carefully selected image and video augmentations to tackle the challenges of low data quality and varying sign speeds. Including an additional regression head combined with IoU-balanced classification loss enhances the model's awareness of the gesture and simplifies capturing temporal information. Extensive experiments demonstrate that the developed training pipeline easily adapts to different datasets and architectures. Additionally, the ablation study shows that each proposed component expands the potential to consider ISLR task specifics. The presented strategies enhance recognition performance across various ISLR benchmarks and achieve state-of-the-art results on the WLASL and Slovo datasets.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2501.00958</link>
<guid>https://arxiv.org/abs/2501.00958</guid>
<content:encoded><![CDATA[
arXiv:2501.00958v4 Announce Type: replace 
Abstract: Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at https://github.com/DAMO-NLP-SG/multimodal_textbook.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding</title>
<link>https://arxiv.org/abs/2501.01645</link>
<guid>https://arxiv.org/abs/2501.01645</guid>
<content:encoded><![CDATA[
arXiv:2501.01645v3 Announce Type: replace 
Abstract: Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long-term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning</title>
<link>https://arxiv.org/abs/2501.04698</link>
<guid>https://arxiv.org/abs/2501.04698</guid>
<content:encoded><![CDATA[
arXiv:2501.04698v2 Announce Type: replace 
Abstract: Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges for this task: 1) the identity decoupling issue, where directly adopting existing customization methods inevitably mix identity attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training a model that can well represent and decouple various customized concepts in video generation. To address these challenges, we introduce ConceptMaster, a novel framework that effectively addresses the identity decoupling issues while maintaining concept fidelity in video customization. Specifically, we propose to learn decoupled multi-concept embeddings and inject them into diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To overcome the scarcity of high-quality MCVC data, we establish a data construction pipeline, which enables collection of high-quality multi-concept video-entity data pairs across diverse scenarios. A multi-concept video evaluation set is further devised to comprehensively validate our method from three dimensions, including concept fidelity, identity decoupling ability, and video generation quality, across six different concept composition scenarios. Extensive experiments demonstrate that ConceptMaster significantly outperforms previous methods for video customization tasks, showing great potential to generate personalized and semantically accurate content for video diffusion models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Do Not Understand Negation</title>
<link>https://arxiv.org/abs/2501.09425</link>
<guid>https://arxiv.org/abs/2501.09425</guid>
<content:encoded><![CDATA[
arXiv:2501.09425v2 Announce Type: replace 
Abstract: Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand negation? We introduce NegBench, a new benchmark designed to evaluate negation understanding across 18 task variations and $79$k examples spanning image, video, and medical datasets. The benchmark consists of two core tasks designed to evaluate negation understanding in diverse multimodal settings: Retrieval with Negation and Multiple Choice Questions with Negated Captions. Our evaluation reveals that modern VLMs struggle significantly with negation, often performing at chance level. To address these shortcomings, we explore a data-centric approach wherein we finetune CLIP models on large-scale synthetic datasets containing millions of negated captions. We show that this approach can result in a 10% increase in recall on negated queries and a 28% boost in accuracy on multiple-choice questions with negated captions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LP-DETR: Layer-wise Progressive Relations for Object Detection</title>
<link>https://arxiv.org/abs/2502.05147</link>
<guid>https://arxiv.org/abs/2502.05147</guid>
<content:encoded><![CDATA[
arXiv:2502.05147v3 Announce Type: replace 
Abstract: This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\% AP with 12 epochs and 52.5\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification</title>
<link>https://arxiv.org/abs/2502.07409</link>
<guid>https://arxiv.org/abs/2502.07409</guid>
<content:encoded><![CDATA[
arXiv:2502.07409v2 Announce Type: replace 
Abstract: Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this MGPATH.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image</title>
<link>https://arxiv.org/abs/2502.12894</link>
<guid>https://arxiv.org/abs/2502.12894</guid>
<content:encoded><![CDATA[
arXiv:2502.12894v2 Announce Type: replace 
Abstract: Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge</title>
<link>https://arxiv.org/abs/2502.13818</link>
<guid>https://arxiv.org/abs/2502.13818</guid>
<content:encoded><![CDATA[
arXiv:2502.13818v2 Announce Type: replace 
Abstract: Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed powerful Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe that are co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The AI4EO Challenge ESA MapYourCity was opened in 2024 for 4 months. In this paper, we present the Top-4 performing models of the challenge, and the evaluation results. During inference, the performance of the models using: i) both all three input modalities, and ii) only the two top-view modalities, i.e. without the street-view ground images, is examined. The evaluation results in this work show that the models to estimate the construction year of buildings are effective and can achieve good performance on this difficult important real-world task, even when inference is on previously unseen cities, as well as even when using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2503.01739</link>
<guid>https://arxiv.org/abs/2503.01739</guid>
<content:encoded><![CDATA[
arXiv:2503.01739v2 Announce Type: replace 
Abstract: Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset and code are publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO and https://github.com/WangWenhao0716/BenchUFO under the CC BY 4.0 License.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2503.09040</link>
<guid>https://arxiv.org/abs/2503.09040</guid>
<content:encoded><![CDATA[
arXiv:2503.09040v2 Announce Type: replace 
Abstract: Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.19703</link>
<guid>https://arxiv.org/abs/2503.19703</guid>
<content:encoded><![CDATA[
arXiv:2503.19703v2 Announce Type: replace 
Abstract: Highly accurate geometric precision and dense image features characterize True Digital Orthophoto Maps (TDOMs), which are in great demand for applications such as urban planning, infrastructure management, and environmental monitoring.Traditional TDOM generation methods need sophisticated processes, such as Digital Surface Models (DSM) and occlusion detection, which are computationally expensive and prone to errors.This work presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit DSM and occlusion detection. With depth map generation, spatial information for every pixel within the TDOM is retrieved and can reconstruct the scene with high precision. Divide-and-conquer strategy achieves excellent GS training and rendering with high-resolution TDOMs at a lower resource cost, which preserves higher quality of rendering on complex terrain and thin structure without a decrease in efficiency. Experimental results demonstrate the efficiency of large-scale scene reconstruction and high-precision terrain modeling. This approach provides accurate spatial data, which assists users in better planning and decision-making based on maps.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Adaptation For Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2503.23083</link>
<guid>https://arxiv.org/abs/2503.23083</guid>
<content:encoded><![CDATA[
arXiv:2503.23083v2 Announce Type: replace 
Abstract: Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schr\"odinger Diffusion Driven Signal Recovery in 3T BOLD fMRI Using Unmatched 7T Observations</title>
<link>https://arxiv.org/abs/2504.01004</link>
<guid>https://arxiv.org/abs/2504.01004</guid>
<content:encoded><![CDATA[
arXiv:2504.01004v2 Announce Type: replace 
Abstract: Ultra-high-field (7 Tesla) BOLD fMRI offers exceptional detail in both spatial and temporal domains, along with robust signal-to-noise characteristics, making it a powerful modality for studying visual information processing in the brain. However, due to the limited accessibility of 7T scanners, the majority of neuroimaging studies are still conducted using 3T systems, which inherently suffer from reduced fidelity in both resolution and SNR. To mitigate this limitation, we introduce a new computational approach designed to enhance the quality of 3T BOLD fMRI acquisitions. Specifically, we project both 3T and 7T datasets, sourced from different individuals and experimental setups, into a shared low-dimensional representation space. Within this space, we employ a lightweight, unsupervised Schr\"odinger Bridge framework to infer a high-SNR, high-resolution counterpart of the 3T data, without relying on paired supervision. This methodology is evaluated across multiple fMRI retinotopy datasets, including synthetically generated samples, and demonstrates a marked improvement in the reliability and fit of population receptive field (pRF) models applied to the enhanced 3T outputs. Our findings suggest that it is feasible to computationally approximate 7T-level quality from standard 3T acquisitions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation</title>
<link>https://arxiv.org/abs/2504.02697</link>
<guid>https://arxiv.org/abs/2504.02697</guid>
<content:encoded><![CDATA[
arXiv:2504.02697v2 Announce Type: replace 
Abstract: Atmospheric turbulence is a major source of image degradation in long-range imaging systems. Although numerous deep learning-based turbulence mitigation (TM) methods have been proposed, many are slow, memory-hungry, and do not generalize well. In the spatial domain, methods based on convolutional operators have a limited receptive field, so they cannot handle a large spatial dependency required by turbulence. In the temporal domain, methods relying on self-attention can, in theory, leverage the lucky effects of turbulence, but their quadratic complexity makes it difficult to scale to many frames. Traditional recurrent aggregation methods face parallelization challenges.
  In this paper, we present a new TM method based on two concepts: (1) A turbulence mitigation network based on the Selective State Space Model (MambaTM). MambaTM provides a global receptive field in each layer across spatial and temporal dimensions while maintaining linear computational complexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state space model. Unlike classical Zernike-based representations of phase distortion, the new LPD map uniquely captures the actual effects of turbulence, significantly improving the model's capability to estimate degradation by reducing the ill-posedness. Our proposed method exceeds current state-of-the-art networks on various synthetic and real-world TM benchmarks with significantly faster inference speed.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-event Interval Microscopy for Event Cameras</title>
<link>https://arxiv.org/abs/2504.04924</link>
<guid>https://arxiv.org/abs/2504.04924</guid>
<content:encoded><![CDATA[
arXiv:2504.04924v3 Announce Type: replace 
Abstract: Event cameras, an innovative bio-inspired sensor, differ from traditional cameras by sensing changes in intensity rather than directly perceiving intensity and recording these variations as a continuous stream of "events". The intensity reconstruction from these sparse events has long been a challenging problem. Previous approaches mainly focused on transforming motion-induced events into videos or achieving intensity imaging for static scenes by integrating modulation devices at the event camera acquisition end. In this paper, for the first time, we achieve event-to-intensity conversion using a static event camera for both static and dynamic scenes in fluorescence microscopy. Unlike conventional methods that primarily rely on event integration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the time interval between consecutive events at each pixel. With a fixed threshold in the event camera, the time interval can precisely represent the intensity. At the hardware level, the proposed IEIM integrates a pulse light modulation device within a microscope equipped with an event camera, termed Pulse Modulation-based Event-driven Fluorescence Microscopy. Additionally, we have collected IEIMat dataset under various scenes including high dynamic range and high-speed scenarios. Experimental results on the IEIMat dataset demonstrate that the proposed IEIM achieves superior spatial and temporal resolution, as well as a higher dynamic range, with lower bandwidth compared to other methods. The code and the IEIMat dataset will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</title>
<link>https://arxiv.org/abs/2504.07687</link>
<guid>https://arxiv.org/abs/2504.07687</guid>
<content:encoded><![CDATA[
arXiv:2504.07687v3 Announce Type: replace 
Abstract: News media, particularly video-based platforms, have become deeply embed-ded in daily life, concurrently amplifying the risks of misinformation dissem-ination. Consequently, multimodal fake news detection has garnered signifi-cant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public en-gagement, whereas professionally crafted fake news videos disseminated by media outlets-often politically or virally motivated-pose substantially greater societal harm. To address this gap, we construct FMNV, a novel da-taset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture that integrates spatio-temporal motion features from a 3D ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are fused via an attention-based mechanism, while co-attention modules refine the visual, textual, and audio features for effective multi-modal aggregation. Comparative experiments demonstrate both the generali-zation capability of FMNV across multiple baselines and the superior detec-tion efficacy of FMNVD. This work establishes critical benchmarks for de-tecting high-impact fake news in media ecosystems while advancing meth-odologies for cross-modal inconsistency analysis. Our dataset is available in https://github.com/DennisIW/FMNV.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach</title>
<link>https://arxiv.org/abs/2504.14131</link>
<guid>https://arxiv.org/abs/2504.14131</guid>
<content:encoded><![CDATA[
arXiv:2504.14131v3 Announce Type: replace 
Abstract: Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. We compare the U-Net with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models</title>
<link>https://arxiv.org/abs/2504.15026</link>
<guid>https://arxiv.org/abs/2504.15026</guid>
<content:encoded><![CDATA[
arXiv:2504.15026v2 Announce Type: replace 
Abstract: Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. Existing methods primarily focus on ensuring that watermark embedding does not degrade the model performance. However, they often overlook critical challenges in real-world deployment scenarios, such as the complexity of watermark key management, user-defined generation parameters, and the difficulty of verification by arbitrary third parties. To address this issue, we propose Gaussian Shading++, a diffusion model watermarking method tailored for real-world deployment. We propose a double-channel design that leverages pseudorandom error-correcting codes to encode the random seed required for watermark pseudorandomization, achieving performance-lossless watermarking under a fixed watermark key and overcoming key management challenges. Additionally, we model the distortions introduced during generation and inversion as an additive white Gaussian noise channel and employ a novel soft decision decoding strategy during extraction, ensuring strong robustness even when generation parameters vary. To enable third-party verification, we incorporate public key signatures, which provide a certain level of resistance against forgery attacks even when model inversion capabilities are fully disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only maintains performance losslessness but also outperforms existing methods in terms of robustness, making it a more practical solution for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiTPainter: Efficient Video Inpainting with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.15661</link>
<guid>https://arxiv.org/abs/2504.15661</guid>
<content:encoded><![CDATA[
arXiv:2504.15661v2 Announce Type: replace 
Abstract: Many existing video inpainting algorithms utilize optical flows to construct the corresponding maps and then propagate pixels from adjacent frames to missing areas by mapping. Despite the effectiveness of the propagation mechanism, they might encounter blurry and inconsistencies when dealing with inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT) has emerged as a revolutionary technique for video generation tasks. However, pretrained DiT models for video generation all contain a large amount of parameters, which makes it very time consuming to apply to video inpainting tasks. In this paper, we present DiTPainter, an end-to-end video inpainting model based on Diffusion Transformer (DiT). DiTPainter uses an efficient transformer network designed for video inpainting, which is trained from scratch instead of initializing from any large pretrained models. DiTPainter can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with an acceptable time cost. Experiments show that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection</title>
<link>https://arxiv.org/abs/2504.16404</link>
<guid>https://arxiv.org/abs/2504.16404</guid>
<content:encoded><![CDATA[
arXiv:2504.16404v2 Announce Type: replace 
Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior</title>
<link>https://arxiv.org/abs/2504.17551</link>
<guid>https://arxiv.org/abs/2504.17551</guid>
<content:encoded><![CDATA[
arXiv:2504.17551v2 Announce Type: replace 
Abstract: Urban land use classification and mapping are critical for urban planning, resource management, and environmental monitoring. Existing remote sensing techniques often lack precision in complex urban environments due to the absence of ground-level details. Unlike aerial perspectives, street view images provide a ground-level view that captures more human and social activities relevant to land use in complex urban scenes. Existing street view-based methods primarily rely on supervised classification, which is challenged by the scarcity of high-quality labeled data and the difficulty of generalizing across diverse urban landscapes. This study introduces an unsupervised contrastive clustering model for street view images with a built-in geographical prior, to enhance clustering performance. When combined with a simple visual assignment of the clusters, our approach offers a flexible and customizable solution to land use mapping, tailored to the specific needs of urban planners. We experimentally show that our method can generate land use maps from geotagged street view image datasets of two cities. As our methodology relies on the universal spatial coherence of geospatial data ("Tobler's law"), it can be adapted to various settings where street view images are available, to enable scalable, unsupervised land use mapping and updating. The code will be available at https://github.com/lin102/CCGP.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical and Multimodal Data for Daily Activity Understanding</title>
<link>https://arxiv.org/abs/2504.17696</link>
<guid>https://arxiv.org/abs/2504.17696</guid>
<content:encoded><![CDATA[
arXiv:2504.17696v3 Announce Type: replace 
Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced "Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to understand human activities in real-world settings. DARai consists of continuous scripted and unscripted recordings of 50 participants in 10 different environments, totaling over 200 hours of data from 20 sensors including multiple camera views, depth and radar sensors, wearable inertial measurement units (IMUs), electromyography (EMG), insole pressure sensors, biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three levels of hierarchy: (i) high-level activities (L1) that are independent tasks, (ii) lower-level actions (L2) that are patterns shared between activities, and (iii) fine-grained procedures (L3) that detail the exact execution steps for actions. The dataset annotations and recordings are designed so that 22.7% of L2 actions are shared between L1 activities and 14.2% of L3 procedures are shared between L2 actions. The overlap and unscripted nature of DARai allows counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai in uncovering important challenges in human-centered applications. Specifically, we conduct unimodal and multimodal sensor fusion experiments for recognition, temporal localization, and future action anticipation across all hierarchical annotation levels. To highlight the limitations of individual sensors, we also conduct domain-variant experiments that are enabled by DARai's multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai website: https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
arXiv:2504.18589v4 Announce Type: replace 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements. The project can be found at https://alibaba-damo-academy.github.io/VCBench/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
arXiv:2504.21435v3 Announce Type: replace 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on standalone videos and mainly assess "visual elements" like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a series. To address this challenge, we propose SeriesBench, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our SeriesBench and PC-DCoT highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
arXiv:2504.21650v2 Announce Type: replace 
Abstract: The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
arXiv:2505.00759v2 Announce Type: replace 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinearity Enhanced Adaptive Activation Functions</title>
<link>https://arxiv.org/abs/2403.19896</link>
<guid>https://arxiv.org/abs/2403.19896</guid>
<content:encoded><![CDATA[
arXiv:2403.19896v2 Announce Type: replace-cross 
Abstract: A general procedure for introducing parametric, learned, nonlinearity into activation functions is found to enhance the accuracy of representative neural networks without requiring significant additional computational resources. Examples are given based on the standard rectified linear unit (ReLU) as well as several other frequently employed activation functions. The associated accuracy improvement is quantified both in the context of the MNIST digit data set and a convolutional neural network (CNN) benchmark example.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures</title>
<link>https://arxiv.org/abs/2404.06080</link>
<guid>https://arxiv.org/abs/2404.06080</guid>
<content:encoded><![CDATA[
arXiv:2404.06080v3 Announce Type: replace-cross 
Abstract: This study presents a computer-aided diagnosis (CAD) system to assist early detection of lung metastases during endobronchial ultrasound (EBUS) procedures, significantly reducing follow-up time and enabling timely treatment. Due to limited cytology images and morphological similarities among cells, classifying lung metastases is challenging, and existing research rarely targets this issue directly.To overcome data scarcity and improve classification, the authors propose a few-shot learning model using a hybrid pretrained backbone with fine-grained classification and contrastive learning. Parameter-efficient fine-tuning on augmented support sets enhances generalization and transferability. The model achieved 49.59% accuracy, outperforming existing methods. With 20 image samples, accuracy improved to 55.48%, showing strong potential for identifying rare or novel cancer types in low-data clinical environments.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation</title>
<link>https://arxiv.org/abs/2407.00104</link>
<guid>https://arxiv.org/abs/2407.00104</guid>
<content:encoded><![CDATA[
arXiv:2407.00104v2 Announce Type: replace-cross 
Abstract: An AI tool has been developed to provide interpretable support for the diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing resource utilization. The interpretability is provided in two ways: on the one hand, the main BCC dermoscopic patterns are found in the image to justify the BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM, a clinically inspired visual explanation is developed where the relevant features for diagnosis are located. Since there is no established ground truth for BCC dermoscopic features, a standard reference is inferred from the diagnosis of four dermatologists using an Expectation Maximization (EM) based algorithm. The results demonstrate significant improvements in classification accuracy and interpretability, positioning this approach as a valuable tool for early BCC detection and referral to dermatologists. The BCC/non-BCC classification achieved an accuracy rate of 90%. For Clinically-inspired XAI results, the detection of BCC patterns useful to clinicians reaches 99% accuracy. As for the Clinically-inspired Visual XAI results, the mean of the Grad-CAM normalized value within the manually segmented clinical features is 0.57, while outside this region it is 0.16. This indicates that the model struggles to accurately identify the regions of the BCC patterns. These results prove the ability of the AI tool to provide a useful explanation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2409.12514</link>
<guid>https://arxiv.org/abs/2409.12514</guid>
<content:encoded><![CDATA[
arXiv:2409.12514v5 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMPERROR: A Flexible Generative Perception Error Model for Probing Self-Driving Planners</title>
<link>https://arxiv.org/abs/2411.07719</link>
<guid>https://arxiv.org/abs/2411.07719</guid>
<content:encoded><![CDATA[
arXiv:2411.07719v2 Announce Type: replace-cross 
Abstract: To handle the complexities of real-world traffic, learning planners for self-driving from data is a promising direction. While recent approaches have shown great progress, they typically assume a setting in which the ground-truth world state is available as input. However, when deployed, planning needs to be robust to the long-tail of errors incurred by a noisy perception system, which is often neglected in evaluation. To address this, previous work has proposed drawing adversarial samples from a perception error model (PEM) mimicking the noise characteristics of a target object detector. However, these methods use simple PEMs that fail to accurately capture all failure modes of detection. In this paper, we present EMPERROR, a novel transformer-based generative PEM, apply it to stress-test an imitation learning (IL)-based planner and show that it imitates modern detectors more faithfully than previous work. Furthermore, it is able to produce realistic noisy inputs that increase the planner's collision rate by up to 85%, demonstrating its utility as a valuable tool for a more complete evaluation of self-driving planners.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression</title>
<link>https://arxiv.org/abs/2412.03293</link>
<guid>https://arxiv.org/abs/2412.03293</guid>
<content:encoded><![CDATA[
arXiv:2412.03293v2 Announce Type: replace-cross 
Abstract: In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Scene Coordinate Regression with Efficient Keypoint Detection and Sequential Information</title>
<link>https://arxiv.org/abs/2412.06488</link>
<guid>https://arxiv.org/abs/2412.06488</guid>
<content:encoded><![CDATA[
arXiv:2412.06488v2 Announce Type: replace-cross 
Abstract: Scene Coordinate Regression (SCR) is a visual localization technique that utilizes deep neural networks (DNN) to directly regress 2D-3D correspondences for camera pose estimation. However, current SCR methods often face challenges in handling repetitive textures and meaningless areas due to their reliance on implicit triangulation. In this paper, we propose an efficient and accurate SCR system. Compared to existing SCR methods, we propose a unified architecture for both scene encoding and salient keypoint detection, allowing our system to prioritize the encoding of informative regions. This design significantly improves computational efficiency. Additionally, we introduce a mechanism that utilizes sequential information during both mapping and relocalization. The proposed method enhances the implicit triangulation, especially in environments with repetitive textures. Comprehensive experiments conducted across indoor and outdoor datasets demonstrate that the proposed system outperforms state-of-the-art (SOTA) SCR methods. Our single-frame relocalization mode improves the recall rate of our baseline by 6.4% and increases the running speed from 56Hz to 90Hz. Furthermore, our sequence-based mode increases the recall rate by 11% while maintaining the original efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation</title>
<link>https://arxiv.org/abs/2501.05014</link>
<guid>https://arxiv.org/abs/2501.05014</guid>
<content:encoded><![CDATA[
arXiv:2501.05014v2 Announce Type: replace-cross 
Abstract: The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control</title>
<link>https://arxiv.org/abs/2502.05855</link>
<guid>https://arxiv.org/abs/2502.05855</guid>
<content:encoded><![CDATA[
arXiv:2502.05855v2 Announce Type: replace-cross 
Abstract: Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Data Transformation Effects on Segment Anything 2</title>
<link>https://arxiv.org/abs/2503.00042</link>
<guid>https://arxiv.org/abs/2503.00042</guid>
<content:encoded><![CDATA[
arXiv:2503.00042v2 Announce Type: replace-cross 
Abstract: Video object segmentation (VOS) is a critical task in the development of video perception and understanding. The Segment-Anything Model 2 (SAM 2), released by Meta AI, is the current state-of-the-art architecture for end-to-end VOS. SAM 2 performs very well on both clean video data and augmented data, and completely intelligent video perception requires an understanding of how this architecture is capable of achieving such quality results. To better understand how each step within the SAM 2 architecture permits high-quality video segmentation, a variety of complex video transformations are passed through the architecture, and the impact at each stage of the process is measured. It is observed that each progressive stage enables the filtering of complex transformation noise and the emphasis of the object of interest. Contributions include the creation of complex transformation video datasets, an analysis of how each stage of the SAM 2 architecture interprets these transformations, and visualizations of segmented objects through each stage. By better understanding how each model structure impacts overall video understanding, VOS development can work to improve real-world applicability and performance tracking, localizing, and segmenting objects despite complex cluttered scenes and obscurations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI Data</title>
<link>https://arxiv.org/abs/2503.04325</link>
<guid>https://arxiv.org/abs/2503.04325</guid>
<content:encoded><![CDATA[
arXiv:2503.04325v3 Announce Type: replace-cross 
Abstract: Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6\% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAM's potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at https://github.com/vpulab/med-sam-brain .
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decadal analysis of sea surface temperature patterns, climatology, and anomalies in temperate coastal waters with Landsat-8 TIRS observations</title>
<link>https://arxiv.org/abs/2503.05843</link>
<guid>https://arxiv.org/abs/2503.05843</guid>
<content:encoded><![CDATA[
arXiv:2503.05843v2 Announce Type: replace-cross 
Abstract: Sea surface temperature (SST) is a fundamental physical parameter characterising the thermal state of sea surface. Due to the intricate thermal interactions between land, sea, and atmosphere, the spatial gradients of SST in coastal waters often appear at finer spatial scales than those in open ocean waters. The Thermal Infrared Sensor (TIRS) onboard Landsat-8, with its 100-meter spatial resolution, offers a unique opportunity to uncover fine-scale coastal SST patterns that would otherwise be overlooked by coarser-resolution thermal sensors. In this study, we first analysed the spatiotemporal patterns of SST in South Australia's temperate coastal waters from 2014 to 2023 by developing an operational approach for SST retrieval from the Landsat-8 TIRS sensor. A buoy was deployed off the coast of Port Lincoln, South Australia, to validate the quality of SST retrievals. Then the daily baseline climatology of SST with 100 m resolution was constructed, which allowed for the detection and analysis of anomalous SST events. Our results suggest the following: (1) the satellite-derived SST data aligned well with the in-situ measured SST values; (2) the semi-enclosed, shallow regions of Upper Spencer Gulf and Upper St Vincent Gulf showed higher temperatures during summer and cooler temperatures during winter than waters closer to the open ocean, resulting in a higher seasonal variation in SST; (3) the near-shore shallow areas in Spencer Gulf and St Vincent Gulf, and regions surrounding Kangaroo Island, were identified to have a higher probability of SST anomalies compared to the rest of the study area; and (4) anomalous SST events were more likely to happen during the warm months than the cool months. We hope these findings would be helpful in supporting the fishing and aquaculture industries in the coastal waters of South Australia.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic quality control in multi-centric fetal brain MRI super-resolution reconstruction</title>
<link>https://arxiv.org/abs/2503.10156</link>
<guid>https://arxiv.org/abs/2503.10156</guid>
<content:encoded><![CDATA[
arXiv:2503.10156v3 Announce Type: replace-cross 
Abstract: Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC = 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45\%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Integrated Layered Attention (AILA)</title>
<link>https://arxiv.org/abs/2503.22742</link>
<guid>https://arxiv.org/abs/2503.22742</guid>
<content:encoded><![CDATA[
arXiv:2503.22742v2 Announce Type: replace-cross 
Abstract: We propose Adaptive Integrated Layered Attention (AILA), a neural network architecture that combines dense skip connections with different mechanisms for adaptive feature reuse across network layers. We evaluate AILA on three challenging tasks: price forecasting for various commodities and indices (S&amp;P 500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In all cases, AILA matches strong deep learning baselines (LSTMs, Transformers, and ResNets), achieving it at a fraction of the training and inference time. Notably, we implement and test two versions of the model - AILA-Architecture 1, which uses simple linear layers as the connection mechanism between layers, and AILA-Architecture 2, which implements an attention mechanism to selectively focus on outputs from previous layers. Both architectures are applied in a single-task learning setting, with each model trained separately for individual tasks. Results confirm that AILA's adaptive inter-layer connections yield robust gains by flexibly reusing pertinent features at multiple network depths. The AILA approach thus presents an extension to existing architectures, improving long-range sequence modeling, image recognition with optimised computational speed, and SOTA classification performance in practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2504.01953</link>
<guid>https://arxiv.org/abs/2504.01953</guid>
<content:encoded><![CDATA[
arXiv:2504.01953v2 Announce Type: replace-cross 
Abstract: Understanding the complex myocardial architecture is critical for diagnosing and treating heart disease. However, existing methods often struggle to accurately capture this intricate structure from Diffusion Tensor Imaging (DTI) data, particularly due to the lack of ground truth labels and the ambiguous, intertwined nature of fiber trajectories. We present a novel deep learning framework for unsupervised clustering of myocardial fibers, providing a data-driven approach to identifying distinct fiber bundles. We uniquely combine a Bidirectional Long Short-Term Memory network to capture local sequential information along fibers, with a Transformer autoencoder to learn global shape features, with pointwise incorporation of essential anatomical context. Clustering these representations using a density-based algorithm identifies 33 to 62 robust clusters, successfully capturing the subtle distinctions in fiber trajectories with varying levels of granularity. Our framework offers a new, flexible, and quantitative way to analyze myocardial structure, achieving a level of delineation that, to our knowledge, has not been previously achieved, with potential applications in improving surgical planning, characterizing disease-related remodeling, and ultimately, advancing personalized cardiac care.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</title>
<link>https://arxiv.org/abs/2504.21414</link>
<guid>https://arxiv.org/abs/2504.21414</guid>
<content:encoded><![CDATA[
<div> Adaptation, informative model structures, few-shot segmentation, cross-domain, domain characteristics <br />
<br />
Summary: The proposed method, Informative Structure Adaptation (ISA), addresses the challenges of cross-domain few-shot segmentation by adapting well-trained FSS models for target domains without the need for retraining. By identifying domain-specific model structures and measuring parameter importance using a novel structure Fisher score, ISA effectively learns domain characteristics from few-shot labeled support samples during inference. The method progressively trains selected model structures with hierarchically constructed training samples, accommodating shifts in domain characteristics. Extensive experiments validate the superior performance of ISA across multiple cross-domain few-shot segmentation benchmarks. This approach eliminates the costly retraining process and equips existing FSS models with flexible adaptation capabilities for new domains, demonstrating the effectiveness of leveraging informative model structures for domain adaptation in segmentation tasks. <div>
arXiv:2504.21414v2 Announce Type: replace 
Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.21476</link>
<guid>https://arxiv.org/abs/2504.21476</guid>
<content:encoded><![CDATA[
<div> Keywords: GarmentDiffusion, generative modeling, sewing patterns, multimodal inputs, 3D sewing patterns

Summary: 
GarmentDiffusion is introduced as a new generative model for creating precise 3D sewing patterns from various input modalities such as text, image, and incomplete sewing patterns. The model efficiently encodes sewing pattern parameters into compact edge token representations, significantly reducing the sequence length compared to existing methods. By utilizing a diffusion transformer, the model can denoise all edge tokens along the temporal axis while maintaining a constant number of denoising steps across different datasets. GarmentDiffusion accelerates sewing pattern generation speed by 100 times compared to previous models like SewingGPT. The model achieves state-of-the-art results on DressCodeData and GarmentCodeData datasets, demonstrating its effectiveness in generating diverse and accurate sewing patterns. More information about the project can be found on the project website at https://shenfu-research.github.io/Garment-Diffusion/.<br /><br />Summary: <div>
arXiv:2504.21476v2 Announce Type: replace 
Abstract: Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present GarmentDiffusion, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is 10 times shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by 100 times compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</title>
<link>https://arxiv.org/abs/2504.21497</link>
<guid>https://arxiv.org/abs/2504.21497</guid>
<content:encoded><![CDATA[
<div> 3D face parametric model, video face reenactment, latent diffusion framework, FLAME model, motion control <br />
<br />
Summary: 
This study presents a novel method for video face reenactment by integrating a 3D face parametric model into a latent diffusion framework. The use of the FLAME model allows for improved shape consistency and motion control in generating face animations. Depth maps, normal maps, and rendering maps derived from FLAME sequences are incorporated into the denoising UNet using a Geometric Guidance Encoder. A multi-layer feature fusion module with self-attention mechanisms combines facial appearance and motion latent features. By utilizing the 3D face parametric model as motion guidance, precise expression and head pose modeling are achieved. Experimental results demonstrate the method's ability to generate high-quality face animations with accurate expression and head pose variation modeling, as well as strong generalization performance on out-of-domain images. <div>
arXiv:2504.21497v2 Announce Type: replace 
Abstract: In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</title>
<link>https://arxiv.org/abs/2505.00308</link>
<guid>https://arxiv.org/abs/2505.00308</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Quality Assessment, Auto-contours, Radiotherapy, Bayesian Ordinal Classification<br />
<br />
Summary: 
This study presents a Deep Learning-based quality assessment approach for evaluating auto-generated contours in radiotherapy, specifically focusing on Online Adaptive Radiotherapy (OART). By leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. The BOC model classifies auto-contour quality and quantifies prediction uncertainty, delivering robust performance across scenarios with varying amounts of manual labels. Fine-tuning with minimal manual labels and calibration resulted in high accuracy on test data. The use of calibrated thresholds accurately predicted over 93% of auto-contours' qualities, reducing the need for manual reviews and identifying cases requiring correction. This QA model enhances contouring efficiency in OART by reducing manual workload and facilitating informed clinical decisions, ensuring safer and more reliable radiotherapy workflows through uncertainty quantification. <br /><br /> <div>
arXiv:2505.00308v2 Announce Type: replace 
Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports</title>
<link>https://arxiv.org/abs/2505.00228</link>
<guid>https://arxiv.org/abs/2505.00228</guid>
<content:encoded><![CDATA[
<div> Dataset, chest X-ray, AI systems, medical imaging, radiological reports
Summary:
- ReXGradient-160K is introduced as the largest chest X-ray dataset publicly available, containing 160,000 studies from 109,487 patients across 3 U.S. health systems.
- The dataset includes multiple images per study and detailed radiology reports, making it valuable for AI system development and report generation models in medical imaging.
- It is partitioned into training, validation, public test sets, and a private test set for model evaluation.
- The dataset aims to prompt research in medical imaging AI and enhance automated radiological analysis.
- The dataset will be open-sourced for public access at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.<br /><br />Summary: <div>
arXiv:2505.00228v2 Announce Type: replace-cross 
Abstract: We present ReXGradient-160K, representing the largest publicly available chest X-ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients across 3 U.S. health systems (79 medical sites). This comprehensive dataset includes multiple images per study and detailed radiology reports, making it particularly valuable for the development and evaluation of AI systems for medical imaging and automated report generation models. The dataset is divided into training (140,000 studies), validation (10,000 studies), and public test (10,000 studies) sets, with an additional private test set (10,000 studies) reserved for model evaluation on the ReXrank benchmark. By providing this extensive dataset, we aim to accelerate research in medical imaging AI and advance the state-of-the-art in automated radiological analysis. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA</title>
<link>https://arxiv.org/abs/2505.06356</link>
<guid>https://arxiv.org/abs/2505.06356</guid>
<content:encoded><![CDATA[
<div> Keywords: pretraining datasets, multimodal models, toxicity, mitigation strategies, open source

<br /><br />Summary: This paper investigates the presence of toxic content in the LLaVA image-text pretraining dataset, which is crucial for developing multimodal models. The authors analyze various categories of toxicity and identify how harmful content appears across different modalities. They reveal that the dataset contains a significant amount of toxic image-text pairs. To address this issue, the paper proposes targeted mitigation strategies, resulting in the removal of 7,531 toxic pairs from the original dataset. Additionally, the authors provide guidelines for establishing effective toxicity detection pipelines to prevent the incorporation of harmful content in future models. They emphasize the importance of actively identifying and filtering out various forms of toxic content, such as hate speech, explicit imagery, and targeted harassment, to foster more responsible and equitable multimodal systems. To facilitate further research, the refined toxicity-mitigated dataset is made available as open source, promoting transparency and encouraging other researchers to build upon these findings. This study highlights critical issues surrounding the ethical implications of AI and the importance of creating safer data environments for multimodal learning. <div>
arXiv:2505.06356v1 Announce Type: new 
Abstract: Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering</title>
<link>https://arxiv.org/abs/2505.06370</link>
<guid>https://arxiv.org/abs/2505.06370</guid>
<content:encoded><![CDATA[
<div> Keywords: Lung Cancer, CT Images, Deep Learning, Nodule Classification, Semi-Supervised Learning

Summary:
Lung cancer is a major cause of mortality worldwide, highlighting the importance of early detection of malignant pulmonary nodules in CT images. The LMLCC-Net is a novel deep learning framework proposed in this study, utilizing a 3D CNN with Hounsfield Unit-based intensity filtering to classify nodules. This approach considers intensity patterns and textures, leveraging the significant differences between benign and malignant nodules. Multiple branches extract features using learnable filters, with different combinations and ranges explored for optimal performance. Additionally, a semi-supervised learning scheme aids in labeling ambiguous cases, while a lightweight model enhances classification efficiency. Experimental results on the LUNA16 dataset demonstrate superior accuracy, sensitivity, and AUC compared to existing methods. The proposed method shows promise in assisting radiologists in nodule classification, ultimately improving patient care. 

<br /><br />Summary: <div>
arXiv:2505.06370v1 Announce Type: new 
Abstract: Lung cancer is the leading cause of patient mortality in the world. Early diagnosis of malignant pulmonary nodules in CT images can have a significant impact on reducing disease mortality and morbidity. In this work, we propose LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity filtering. Benign and malignant nodules have significant differences in their intensity profile of HU, which was not exploited in the literature. Our method considers the intensity pattern as well as the texture for the prediction of malignancies. LMLCC-Net extracts features from multiple branches that each use a separate learnable HU-based intensity filtering stage. Various combinations of branches and learnable ranges of filters were explored to finally produce the best-performing model. In addition, we propose a semi-supervised learning scheme for labeling ambiguous cases and also developed a lightweight model to classify the nodules. The experimental evaluations are carried out on the LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of 91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of 91.87%, showing improved performance compared to existing methods. The proposed method can have a significant impact in helping radiologists in the classification of pulmonary nodules and improving patient care.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust &amp; Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal</title>
<link>https://arxiv.org/abs/2505.06381</link>
<guid>https://arxiv.org/abs/2505.06381</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical disease prediction, deep learning, Knowledge Distillation, Ant Colony Optimization, context-aware predictor<br />
Summary:<br />
- Medical disease prediction using imaging data is challenging due to variability and complexity.
- Deep learning models, including Knowledge Distillation, show promise but struggle with uncertainty and generalization.
- A novel framework combining Ant Colony Optimization and context-aware predictor addresses these limitations.
- The framework adjusts temperature based on image quality, disease complexity, and model confidence.
- Ant Colony Optimization efficiently selects teacher-student model pairs, outperforming current optimization methods.
- Evaluation on three benchmark datasets demonstrates significant improvement in accuracy, surpassing existing benchmarks.
Summary: <div>
arXiv:2505.06381v1 Announce Type: new 
Abstract: Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms</title>
<link>https://arxiv.org/abs/2505.06389</link>
<guid>https://arxiv.org/abs/2505.06389</guid>
<content:encoded><![CDATA[
<div> Keywords: Sensor-based guidance, deep network, bimodal scene, registration, long-range platforms

Summary:
Sensor-based guidance is crucial for long-range platforms, but traditional registration methods face limitations in handling reference images. To address this, a new approach is proposed in this paper, where a deep network is utilized to encode a stack of images of the scene. This approach is particularly useful for bimodal scenes, where the appearance of the scene can vary (e.g., snowy or non-snowy conditions). By using a stack of images, the deep network can effectively capture the varying characteristics of the scene and provide more robust guidance for the platform. This innovative method offers a promising solution for improving the accuracy and reliability of sensor-based guidance systems in long-range applications. <div>
arXiv:2505.06389v1 Announce Type: new 
Abstract: Sensor-based guidance is required for long-range platforms. To bypass the structural limitation of classical registration on reference image framework, we offer in this paper to encode a stack of images of the scene into a deep network. Relying on a stack is showed to be relevant on bimodal scene (e.g. when the scene can or can not be snowy).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2505.06393</link>
<guid>https://arxiv.org/abs/2505.06393</guid>
<content:encoded><![CDATA[
<div> Dataset, Super-resolution, License Plate Recognition, OCR, Fusion<br />
Summary:<br />
The article introduces the UFPR-SR-Plates dataset, consisting of 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. The study establishes a benchmark using multiple LR and HR images per vehicle and evaluates two super-resolution models for license plates. Three fusion strategies are investigated to enhance performance by combining predictions from an OCR model for multiple super-resolved license plates. The results show significant improvements in LPR performance with super-resolution, especially when using the LCDNet model and MVCP fusion strategy. Recognition rates increased from 1.7% with LR images to 31.1% with super-resolution, reaching 44.7% when combining OCR outputs from five super-resolved images. The findings highlight the importance of super-resolution and temporal information in enhancing LPR accuracy in challenging real-world scenarios. The dataset is openly available to support further research. <br />Summary: <div>
arXiv:2505.06393v1 Announce Type: new 
Abstract: Recent advancements in super-resolution for License Plate Recognition (LPR) have sought to address challenges posed by low-resolution (LR) and degraded images in surveillance, traffic monitoring, and forensic applications. However, existing studies have relied on private datasets and simplistic degradation models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset containing 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. We establish a benchmark using multiple sequential LR and high-resolution (HR) images per vehicle -- five of each -- and two state-of-the-art models for super-resolution of license plates. We also investigate three fusion strategies to evaluate how combining predictions from a leading Optical Character Recognition (OCR) model for multiple super-resolved license plates enhances overall performance. Our findings demonstrate that super-resolution significantly boosts LPR performance, with further improvements observed when applying majority vote-based fusion techniques. Specifically, the Layout-Aware and Character-Driven Network (LCDNet) model combined with the Majority Vote by Character Position (MVCP) strategy led to the highest recognition rates, increasing from 1.7% with low-resolution images to 31.1% with super-resolution, and up to 44.7% when combining OCR outputs from five super-resolved images. These findings underscore the critical role of super-resolution and temporal information in enhancing LPR accuracy under real-world, adverse conditions. The proposed dataset is publicly available to support further research and can be accessed at: https://valfride.github.io/nascimento2024toward/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGE:A Multi-stage Avatar Generator with Sparse Observations</title>
<link>https://arxiv.org/abs/2505.06411</link>
<guid>https://arxiv.org/abs/2505.06411</guid>
<content:encoded><![CDATA[
<div> Keywords: full-body poses, Head Mounted Devices, Multi-stage Avatar GEnerator, motion mapping, realistic motion completion

Summary:
MAGE, a Multi-stage Avatar GEnerator, tackles the challenge of inferring full-body poses from limited 3-joint observations captured by Head Mounted Devices. Unlike previous methods, MAGE adopts a progressive prediction strategy, gradually refining predictions from a 6-part body representation to 22 joints, leveraging motion context priors and improving realism. This approach results in more accurate and consistent lower-body predictions, enhancing the overall quality of motion sequences in AR/VR applications. Extensive experiments demonstrate that MAGE outperforms existing methods in terms of accuracy and continuity, making it a promising solution for full-body pose inference from constrained input observations. 

<br /><br />Summary: <div>
arXiv:2505.06411v1 Announce Type: new 
Abstract: Inferring full-body poses from Head Mounted Devices, which capture only 3-joint observations from the head and wrists, is a challenging task with wide AR/VR applications. Previous attempts focus on learning one-stage motion mapping and thus suffer from an over-large inference space for unobserved body joint motions. This often leads to unsatisfactory lower-body predictions and poor temporal consistency, resulting in unrealistic or incoherent motion sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator named MAGE that factorizes this one-stage direct motion mapping learning with a progressive prediction strategy. Specifically, given initial 3-joint motions, MAGE gradually inferring multi-scale body part poses at different abstract granularity levels, starting from a 6-part body representation and gradually refining to 22 joints. With decreasing abstract levels step by step, MAGE introduces more motion context priors from former prediction stages and thus improves realistic motion completion with richer constraint conditions and less ambiguity. Extensive experiments on large-scale datasets verify that MAGE significantly outperforms state-of-the-art methods with better accuracy and continuity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.06413</link>
<guid>https://arxiv.org/abs/2505.06413</guid>
<content:encoded><![CDATA[
<div> backdoor attack, Vision-Language Models, autonomous driving, response delays, trigger<br />
<br />
Summary:<br />
Vision-Language Models (VLMs) are integrated into autonomous driving systems for enhanced reasoning, particularly in tasks like Visual Question Answering (VQA). This study introduces a reflection-based backdoor attack on VLM systems in autonomous driving, causing significant response delays by embedding triggers in images. By incorporating reflection patterns and irrelevant text in the dataset, models are trained to produce delayed responses when triggered. Fine-tuning state-of-the-art VLMs, the study shows normal performance on clean inputs but increased latency when triggered, posing risks in real-world driving decisions. Analysis considers poisoning rates, camera perspectives, and transferability across different views, revealing vulnerabilities in VLM-augmented driving systems and emphasizing the urgent need for robust security measures. <br /> <div>
arXiv:2505.06413v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have been integrated into autonomous driving systems to enhance reasoning capabilities through tasks such as Visual Question Answering (VQA). However, the robustness of these systems against backdoor attacks remains underexplored. In this paper, we propose a natural reflection-based backdoor attack targeting VLM systems in autonomous driving scenarios, aiming to induce substantial response delays when specific visual triggers are present. We embed faint reflection patterns, mimicking natural surfaces such as glass or water, into a subset of images in the DriveLM dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories or system update notifications) to the corresponding textual labels. This strategy trains the model to generate abnormally long responses upon encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and LLaMA-Adapter, using parameter-efficient methods. Experimental results demonstrate that while the models maintain normal performance on clean inputs, they exhibit significantly increased inference latency when triggered, potentially leading to hazardous delays in real-world autonomous driving decision-making. Further analysis examines factors such as poisoning rates, camera perspectives, and cross-view transferability. Our findings uncover a new class of attacks that exploit the stringent real-time requirements of autonomous driving, posing serious challenges to the security and reliability of VLM-augmented driving systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing</title>
<link>https://arxiv.org/abs/2505.06436</link>
<guid>https://arxiv.org/abs/2505.06436</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Adversarial Network, StyleGAN, facial keypoints, data augmentation, facial expression  

<br /><br />Summary:  
Generative Adversarial Networks (GANs), particularly StyleGAN/2, facilitate the creation of photo-realistic human face images with a semantically structured latent space. Numerous methods have been devised to edit these images by navigating this latent space to modify specific features, such as gender and age, while preserving others. However, the issue of entanglement arises when altering one feature inadvertently affects others, notably facial expressions. To tackle this problem, the authors propose augmenting the loss function of an existing Facial Keypoint Detection model with a new Human Face Landmark Detection (HFLD) loss. This modification aims to minimize unwanted changes to facial expressions during the image transformation process. Through both quantitative and qualitative evaluations, the proposed method demonstrated a significant improvement, achieving up to a 49% reduction in alterations to emotions compared to previous models. The study compares the extended model to state-of-the-art techniques, showing enhanced capability in maintaining facial gestures and expressions. Consequently, this approach offers a reliable data augmentation method for research focused on facial gestures and expressions, generating images with fixed emotional content but varying appearances. <div>
arXiv:2505.06436v1 Announce Type: new 
Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors in the latent space of a pre-trained StyleGAN/2 models by identifying semantically meaningful directions (e.g., gender or age) in the latent space. By moving the vector in a specific direction, the ideal result would only change the target feature while preserving all the other features. Providing an ideal data augmentation approach for gesture research as it could be used to generate numerous image variations whilst keeping the facial expressions intact. However, entanglement issues, where changing one feature inevitably affects other features, impacts the ability to preserve facial expressions. To address this, we propose the use of an addition to the loss function of a Facial Keypoint Detection model to restrict changes to the facial expressions. Building on top of an existing model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided by a pre-trained Facial Keypoint Detection model, to the original loss function. We quantitatively and qualitatively evaluate the existing and our extended model, showing the effectiveness of our approach in addressing the entanglement issue and maintaining the facial expression. Our approach achieves up to 49% reduction in the change of emotion in our experiments. Moreover, we show the benefit of our approach by comparing with state-of-the-art models. By increasing the ability to preserve the facial gesture and expression during facial transformation, we present a way to create human face images with fixed expression but different appearances, making it a reliable data augmentation approach for Facial Gesture and Expression research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation</title>
<link>https://arxiv.org/abs/2505.06467</link>
<guid>https://arxiv.org/abs/2505.06467</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, PromptIQ, Component-Aware Similarity, image quality, prompt engineering expertise

Summary: 
PromptIQ is a new framework designed to improve the quality of images generated by text-to-image models without requiring specialized prompt engineering knowledge. The framework utilizes a novel Component-Aware Similarity (CAS) metric to detect and penalize structural errors in the generated images. Unlike traditional methods, PromptIQ automates prompt refinement and image evaluation processes, iterating until the user is satisfied with the results. This iterative approach eliminates the need for trial-and-error prompt tuning and significantly enhances the quality of image generation while improving evaluation accuracy. By addressing the limitations of current evaluation methods, PromptIQ makes text-to-image models more accessible and user-friendly for individuals with limited prompt engineering expertise. <div>
arXiv:2505.06467v1 Announce Type: new 
Abstract: Generating high-quality images without prompt engineering expertise remains a challenge for text-to-image (T2I) models, which often misinterpret poorly structured prompts, leading to distortions and misalignments. While humans easily recognize these flaws, metrics like CLIP fail to capture structural inconsistencies, exposing a key limitation in current evaluation methods. To address this, we introduce PromptIQ, an automated framework that refines prompts and assesses image quality using our novel Component-Aware Similarity (CAS) metric, which detects and penalizes structural errors. Unlike conventional methods, PromptIQ iteratively generates and evaluates images until the user is satisfied, eliminating trial-and-error prompt tuning. Our results show that PromptIQ significantly improves generation quality and evaluation accuracy, making T2I models more accessible for users with little to no prompt engineering expertise.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.06512</link>
<guid>https://arxiv.org/abs/2505.06512</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image synthesis, Hierarchical Cross-Modal Alignment, diffusion sampling, semantic fidelity, spatial control<br /><br />Summary: The paper introduces a new framework called Hierarchical Cross-Modal Alignment (HCMA) to enhance text-to-image synthesis. This approach addresses the challenge of achieving high-level semantic fidelity while providing explicit spatial control, especially when generating scenes with multiple objects and intricate relationships. HCMA consists of two key alignment modules integrated into each diffusion sampling step. The global module aligns latent representations with textual descriptions, ensuring overall scene coherence. In contrast, the local module employs bounding-box layouts to accurately position objects, allowing for detailed spatial control. The authors conducted extensive experiments using the MS-COCO 2014 validation set to evaluate HCMA's effectiveness. The results indicate that HCMA outperforms existing state-of-the-art methods, with significant improvements reflected in a 0.69 increase in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These findings highlight HCMA's capability to faithfully capture complex textual semantics while adhering to spatial constraints defined by the user, marking it as a strong solution for grounded image generation. The code for HCMA is publicly available, further promoting research in this area. <div>
arXiv:2505.06512v1 Announce Type: new 
Abstract: Text-to-image synthesis has progressed to the point where models can generate visually compelling images from natural language prompts. Yet, existing methods often fail to reconcile high-level semantic fidelity with explicit spatial control, particularly in scenes involving multiple objects, nuanced relations, or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal Alignment (HCMA) framework for grounded text-to-image generation. HCMA integrates two alignment modules into each diffusion sampling step: a global module that continuously aligns latent representations with textual descriptions to ensure scene-level coherence, and a local module that employs bounding-box layouts to anchor objects at specified locations, enabling fine-grained spatial control. Extensive experiments on the MS-COCO 2014 validation set show that HCMA surpasses state-of-the-art baselines, achieving a 0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These results demonstrate HCMA's effectiveness in faithfully capturing intricate textual semantics while adhering to user-defined spatial constraints, offering a robust solution for semantically grounded image generation.Our code is available at https://github.com/hwang-cs-ime/HCMA
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation</title>
<link>https://arxiv.org/abs/2505.06515</link>
<guid>https://arxiv.org/abs/2505.06515</guid>
<content:encoded><![CDATA[
<div> Keywords: BEV, semantic segmentation, RESAR-BEV, autonomous driving, nuScenes  

<br /><br />Summary: Bird's-Eye-View (BEV) semantic segmentation enhances environmental perception for autonomous driving, but faces challenges like multi-modal misalignment and sensor noise. To address these, the proposed framework, RESAR-BEV, utilizes a progressive refinement method through residual autoregressive learning, decomposing BEV segmentation into interpretable stages via the Drive-Transformer and Modifier-Transformer architecture. It features a robust BEV representation that integrates ground-proximity voxels with adaptive height offsets and employs dual-path voxel feature encoding to facilitate efficient feature extraction. Additionally, RESAR-BEV implements decoupled supervision, allowing for offline Ground Truth decomposition and online joint optimization, which helps mitigate overfitting while maintaining structural coherence in the predictions. Extensive experiments conducted on the nuScenes dataset have shown that RESAR-BEV achieves a state-of-the-art performance with a mean Intersection over Union (mIoU) of 54.0% across seven crucial driving scene categories, while also ensuring real-time processing capabilities at 14.6 frames per second. The framework demonstrates robust performance even in challenging scenarios, including long-range perception and adverse weather conditions. <div>
arXiv:2505.06515v1 Announce Type: new 
Abstract: Bird's-Eye-View (BEV) semantic segmentation provides comprehensive environmental perception for autonomous driving but suffers multi-modal misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement framework that advances beyond single-step end-to-end approaches: (1) progressive refinement through residual autoregressive learning that decomposes BEV segmentation into interpretable coarse-to-fine stages via our Drive-Transformer and Modifier-Transformer residual prediction cascaded architecture, (2) robust BEV representation combining ground-proximity voxels with adaptive height offsets and dual-path voxel feature encoding (max+attention pooling) for efficient feature extraction, and (3) decoupled supervision with offline Ground Truth decomposition and online joint optimization to prevent overfitting while ensuring structural coherence. Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art performance with 54.0% mIoU across 7 essential driving-scene categories while maintaining real-time capability at 14.6 FPS. The framework exhibits robustness in challenging scenarios of long-range perception and adverse weather conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.06516</link>
<guid>https://arxiv.org/abs/2505.06516</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Dempster-Shafer Theory, Quantum Conflict Indicator, Conflict Fusion, Out-of-Distribution Detection, Quantum Mass Function

Summary:
Quantum Dempster-Shafer Theory (QDST) utilizes quantum interference effects to derive a Quantum Mass Function (QMF) from various data sources and employs quantum parallel computing for faster computation. This study addresses the challenge of managing conflicts between multiple QMFs by introducing a Quantum Conflict Indicator (QCI) that measures conflicts between two QMFs in decision-making. The QCI exhibits desirable properties such as non-negativity, symmetry, boundedness, extreme consistency, and insensitivity to refinement. It is then applied in conflict fusion methods, showing superior performance compared to existing approaches. The Class Description Domain Space (C-DDS) and its optimized version C-DDS+ utilize the QCI-based fusion method for Out-of-Distribution (OOD) detection, yielding better performance than state-of-the-art baseline methods. Experimental results demonstrate an increase in AUC and a decrease in FPR95, highlighting the effectiveness of the proposed approach in OOD detection tasks. 

<br /><br />Summary: <div>
arXiv:2505.06516v1 Announce Type: new 
Abstract: Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to derive a quantum mass function (QMF) as a fuzzy metric type from information obtained from various data sources. In addition, QDST uses quantum parallel computing to speed up computation. Nevertheless, the effective management of conflicts between multiple QMFs in QDST is a challenging question. This work aims to address this problem by proposing a Quantum Conflict Indicator (QCI) that measures the conflict between two QMFs in decision-making. Then, the properties of the QCI are carefully investigated. The obtained results validate its compliance with desirable conflict measurement properties such as non-negativity, symmetry, boundedness, extreme consistency and insensitivity to refinement. We then apply the proposed QCI in conflict fusion methods and compare its performance with several commonly used fusion approaches. This comparison demonstrates the superiority of the QCI-based conflict fusion method. Moreover, the Class Description Domain Space (C-DDS) and its optimized version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to address the Out-of-Distribution (OOD) detection task. The experimental results show that the proposed approach gives better OOD performance with respect to several state-of-the-art baseline OOD detection methods. Specifically, it achieves an average increase in Area Under the Receiver Operating Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the optimal baseline method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation</title>
<link>https://arxiv.org/abs/2505.06517</link>
<guid>https://arxiv.org/abs/2505.06517</guid>
<content:encoded><![CDATA[
<div> Keywords: visual-inertial odometry, long-tracked features, accumulated matching errors, real-time performance, IoT navigation

Summary: 
This paper introduces a visual-inertial odometry (VIO) method that utilizes long-tracked features to reduce localization drift. Long-tracked features can provide constraints on more visual frames but may also lead to accumulated matching errors. To address this, the proposed method includes an active decoupling mechanism to handle accumulated errors in long-tracked feature utilization. Additionally, a visual reference frame reset strategy eliminates tracking errors, while a depth prediction strategy leverages long-term constraints. To ensure real-time performance, the system incorporates efficient state estimation strategies, such as a parallel elimination strategy and an elimination skipping strategy. Experimental results demonstrate that the method offers higher positioning accuracy with short consumption time, making it well-suited for edge-enabled low-altitude IoT navigation applications. The code for the method will be made available on GitHub.<br /><br />Summary: <div>
arXiv:2505.06517v1 Announce Type: new 
Abstract: This paper presents a visual-inertial odometry (VIO) method using long-tracked features. Long-tracked features can constrain more visual frames, reducing localization drift. However, they may also lead to accumulated matching errors and drift in feature tracking. Current VIO methods adjust observation weights based on re-projection errors, yet this approach has flaws. Re-projection errors depend on estimated camera poses and map points, so increased errors might come from estimation inaccuracies, not actual feature tracking errors. This can mislead the optimization process and make long-tracked features ineffective for suppressing localization drift. Furthermore, long-tracked features constrain a larger number of frames, which poses a significant challenge to real-time performance of the system. To tackle these issues, we propose an active decoupling mechanism for accumulated errors in long-tracked feature utilization. We introduce a visual reference frame reset strategy to eliminate accumulated tracking errors and a depth prediction strategy to leverage the long-term constraint. To ensure real time preformane, we implement three strategies for efficient system state estimation: a parallel elimination strategy based on predefined elimination order, an inverse-depth elimination simplification strategy, and an elimination skipping strategy. Experiments on various datasets show that our method offers higher positioning accuracy with relatively short consumption time, making it more suitable for edge-enabled low-altitude IoT navigation, where high-accuracy positioning and real-time operation on edge device are required. The code will be published at github.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation</title>
<link>https://arxiv.org/abs/2505.06524</link>
<guid>https://arxiv.org/abs/2505.06524</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, open-vocabulary multi-entity segmentation, prompt bias, causal prompt, CPC-SAM

Summary:<br /><br /> The Segment Anything Model (SAM) faces generalization challenges in open-vocabulary multi-entity segmentation (OVMS) due to prompt bias. This bias stems from task-irrelevant generating factors in prompts, affecting generalization. To address this issue, a method is proposed to calibrate prompts for accurate OVMS by generating causal prompts containing only task-relevant factors. The proposed CPC-SAM integrates a causal prompt learner (CaPL) into SAM to obtain causal prompts. CaPL optimizes prompts by enforcing causal multi-distribution consistency, alternating between optimizing CaPL and SAM for accurate OVMS. The method is proven superior through extensive experiments. <div>
arXiv:2505.06524v1 Announce Type: new 
Abstract: Despite the strength of the Segment Anything Model (SAM), it struggles with generalization issues in open-vocabulary multi-entity segmentation (OVMS). Through empirical and causal analyses, we find that (i) the prompt bias is the primary cause of the generalization issues; (ii) this bias is closely tied to the task-irrelevant generating factors within the prompts, which act as confounders and affect generalization. To address the generalization issues, we aim to propose a method that can calibrate prompts to eliminate confounders for accurate OVMS. Building upon the causal analysis, we propose that the optimal prompt for OVMS should contain only task-relevant causal factors. We define it as the causal prompt, serving as the goal of calibration. Next, our theoretical analysis, grounded by causal multi-distribution consistency theory, proves that this prompt can be obtained by enforcing segmentation consistency and optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration method for SAM to achieve accurate OVMS. It integrates a lightweight causal prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first generate multiple prompts using random annotations to simulate diverse distributions and then reweight them via CaPL by enforcing causal multi-distribution consistency in both task and entity levels. To ensure obtaining causal prompts, CaPL is optimized by minimizing the cumulative segmentation loss across the reweighted prompts to achieve consistency and optimality. A bi-level optimization strategy alternates between optimizing CaPL and SAM, ensuring accurate OVMS. Extensive experiments validate its superiority.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization of Medical Image Registration Foundation Model</title>
<link>https://arxiv.org/abs/2505.06527</link>
<guid>https://arxiv.org/abs/2505.06527</guid>
<content:encoded><![CDATA[
<div> Keywords: Deformable registration, deep learning, medical image processing, foundation models, Sharpness-Aware Minimization

Summary:
Deformable registration in medical image processing is essential for aligning images accurately through establishing nonlinear correspondences. Traditional methods provide adaptability and interpretability but lack computational efficiency. On the other hand, deep learning approaches have improved speed and accuracy but struggle with flexibility and generalizability. Foundation models have shown promise in learning universal features for image registration but still face challenges in generalization. This paper introduces Sharpness-Aware Minimization (SAM) to enhance foundation models' generalization and robustness in medical image registration. By optimizing the loss landscape, SAM improves model stability across diverse data distributions and complex clinical scenarios. The experimental results demonstrate that integrating SAM with foundation models leads to significant improvements in cross-dataset registration performance, providing new pathways for advancing medical image registration technology.

<br /><br />Summary: <div>
arXiv:2505.06527v1 Announce Type: new 
Abstract: Deformable registration is a fundamental task in medical image processing, aiming to achieve precise alignment by establishing nonlinear correspondences between images. Traditional methods offer good adaptability and interpretability but are limited by computational efficiency. Although deep learning approaches have significantly improved registration speed and accuracy, they often lack flexibility and generalizability across different datasets and tasks. In recent years, foundation models have emerged as a promising direction, leveraging large and diverse datasets to learn universal features and transformation patterns for image registration, thus demonstrating strong cross-task transferability. However, these models still face challenges in generalization and robustness when encountering novel anatomical structures, varying imaging conditions, or unseen modalities. To address these limitations, this paper incorporates Sharpness-Aware Minimization (SAM) into foundation models to enhance their generalization and robustness in medical image registration. By optimizing the flatness of the loss landscape, SAM improves model stability across diverse data distributions and strengthens its ability to handle complex clinical scenarios. Experimental results show that foundation models integrated with SAM achieve significant improvements in cross-dataset registration performance, offering new insights for the advancement of medical image registration technology. Our code is available at https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection</title>
<link>https://arxiv.org/abs/2505.06528</link>
<guid>https://arxiv.org/abs/2505.06528</guid>
<content:encoded><![CDATA[
<div> Keywords: Deepfake, Convolutional Neural Networks, MTCNN, EfficientNet-B5, Kaggle DFDC 

Summary: 
Deepfake videos, created using advanced AI techniques, present a significant challenge to the authenticity of digital media. Detecting these videos requires sophisticated methods capable of identifying subtle inconsistencies. This paper focuses on using deep learning, specifically convolutional neural networks, to detect deepfake videos. The approach includes MTCNN as a face detector and EfficientNet-B5 as an encoder model to predict the authenticity of videos. The model was trained and evaluated on the Kaggle DFDC dataset, achieving a log loss of 42.78%, an AUC of 93.80%, and an F1 score of 86.82%. The results demonstrate the effectiveness of the deep learning approach in detecting deepfakes with high accuracy and reliability. The use of these advanced techniques highlights the importance of staying ahead of the evolving landscape of digital manipulation to preserve the integrity of media content.<br /><br />Summary: <div>
arXiv:2505.06528v1 Announce Type: new 
Abstract: Deepfake videos, produced through advanced artificial intelligence methods now a days, pose a new challenge to the truthfulness of the digital media. As Deepfake becomes more convincing day by day, detecting them requires advanced methods capable of identifying subtle inconsistencies. The primary motivation of this paper is to recognize deepfake videos using deep learning techniques, specifically by using convolutional neural networks. Deep learning excels in pattern recognition, hence, makes it an ideal approach for detecting the intricate manipulations in deepfakes. In this paper, we consider using MTCNN as a face detector and EfficientNet-B5 as encoder model to predict if a video is deepfake or not. We utilize training and evaluation dataset from Kaggle DFDC. The results shows that our deepfake detection model acquired 42.78% log loss, 93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition</title>
<link>https://arxiv.org/abs/2505.06536</link>
<guid>https://arxiv.org/abs/2505.06536</guid>
<content:encoded><![CDATA[
<div> Transformer-based; Adaptive Cross-modal Fusion Network; multimodal emotion recognition; cross-modal attention; feature reinforcement <br />
Summary: <br />
The study introduces a novel Transformer-based Adaptive Cross-modal Fusion Network (TACFN) for multimodal emotion recognition. TACFN addresses the issue of redundant features in cross-modal attention by implementing intra-modal feature selection through self-attention. This allows for adaptive and efficient interaction between modalities. Additionally, TACFN captures complementary information by splicing and using fused weight vectors for feature reinforcement. Experiments on RAVDESS and IEMOCAP datasets demonstrate the effectiveness of TACFN, showing significant performance improvement compared to other methods and achieving state-of-the-art results. The proposed approach, available on GitHub, showcases the potential of using selective and adaptive fusion techniques for enhancing multimodal emotion recognition tasks. <br /> <div>
arXiv:2505.06536v1 Announce Type: new 
Abstract: The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</title>
<link>https://arxiv.org/abs/2505.06537</link>
<guid>https://arxiv.org/abs/2505.06537</guid>
<content:encoded><![CDATA[
<div> Keywords: Fashion video generation, multiple reference images, Pose-aware Prototype Aggregator, Flow-enhanced Prototype Instantiator, MRFashion-7K dataset  

<br /><br />Summary:  
Fashion video generation involves creating temporally consistent videos based on reference images of a specific character. Existing diffusion-based approaches are limited to using a single reference image, constraining their ability to produce view-consistent videos, particularly when clothing patterns vary from different angles. Additionally, current motion modules fail to adequately model human body movement, leading to issues with spatiotemporal consistency. To resolve these challenges, we introduce ProFashion, a framework that employs multiple reference images to enhance both view consistency and temporal coherence. ProFashion features a Pose-aware Prototype Aggregator which selects and combines reference features based on pose information, generating frame-wise prototypes that guide the denoising process while maintaining efficiency. Moreover, we implement a Flow-enhanced Prototype Instantiator that utilizes human keypoint motion flow to direct an advanced spatiotemporal attention process within the denoiser, improving motion consistency. We validate the efficacy of ProFashion through comprehensive evaluations on the MRFashion-7K dataset, which we compiled from online sources. The results indicate that ProFashion surpasses existing methods on the UBC Fashion dataset, demonstrating its effectiveness in fashion video generation. <div>
arXiv:2505.06537v1 Announce Type: new 
Abstract: Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fashion videos, especially when there are different patterns on the clothes from different perspectives. Moreover, the widely adopted motion module does not sufficiently model human body movement, leading to sub-optimal spatiotemporal consistency. To address these issues, we propose ProFashion, a fashion video generation framework leveraging multiple reference images to achieve improved view consistency and temporal coherency. To effectively leverage features from multiple reference images while maintaining a reasonable computational cost, we devise a Pose-aware Prototype Aggregator, which selects and aggregates global and fine-grained reference features according to pose information to form frame-wise prototypes, which serve as guidance in the denoising process. To further enhance motion consistency, we introduce a Flow-enhanced Prototype Instantiator, which exploits the human keypoint motion flow to guide an extra spatiotemporal attention process in the denoiser. To demonstrate the effectiveness of ProFashion, we extensively evaluate our method on the MRFashion-7K dataset we collected from the Internet. ProFashion also outperforms previous methods on the UBC Fashion dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06543</link>
<guid>https://arxiv.org/abs/2505.06543</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual text rendering, Hierarchical Disentangled Glyph-Based framework, Multi-Linguistic GlyphNet, Glyph-Aware Perceptual Loss, LD-TSR

Summary:
The article introduces a novel approach called Hierarchical Disentangled Glyph-Based framework (HDGlyph) for improved visual text rendering in images. The framework addresses challenges with long-tail text cases, particularly for unseen or small-sized text. At the training stage, HDGlyph disentangles pixel-level representations using Multi-Linguistic GlyphNet and Glyph-Aware Perceptual Loss. This ensures robust rendering even for unseen characters. At inference time, HDGlyph employs Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme to refine background and small-sized text. Extensive evaluations demonstrate that HDGlyph outperforms existing methods, achieving accuracy gains of 5.08% in English and 11.7% in Chinese text rendering while maintaining high image quality. The framework excels in long-tail scenarios, showcasing strong accuracy and visual performance.

<br /><br />Summary: <div>
arXiv:2505.06543v1 Announce Type: new 
Abstract: Visual text rendering, which aims to accurately integrate specified textual content within generated images, is critical for various applications such as commercial design. Despite recent advances, current methods struggle with long-tail text cases, particularly when handling unseen or small-sized text. In this work, we propose a novel Hierarchical Disentangled Glyph-Based framework (HDGlyph) that hierarchically decouples text generation from non-text visual synthesis, enabling joint optimization of both common and long-tail text rendering. At the training stage, HDGlyph disentangles pixel-level representations via the Multi-Linguistic GlyphNet and the Glyph-Aware Perceptual Loss, ensuring robust rendering even for unseen characters. At inference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both background and small-sized text. Extensive evaluations show our model consistently outperforms others, with 5.08% and 11.7% accuracy gains in English and Chinese text rendering while maintaining high image quality. It also excels in long-tail scenarios with strong accuracy and visual performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</title>
<link>https://arxiv.org/abs/2505.06557</link>
<guid>https://arxiv.org/abs/2505.06557</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised temporal sentence grounding, positive sample mining, contrastive loss, rank loss, video-language correspondence

Summary: 
The article introduces a novel framework called Positive Sample Mining (PSM) for weakly supervised temporal sentence grounding (WSTSG) in untrimmed videos. Existing methods struggle with negative sample generation, especially when dealing with highly similar samples to an anchor sample. PSM addresses this by mining positive samples from the training set based on semantic similarity. By partitioning the training set into similar and dissimilar subsets, PSM provides more discriminative supervision. The framework incorporates PSM-guided contrastive and rank losses to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Experimental results on WSTSG and VideoQA tasks demonstrate the effectiveness and superiority of PSM in enhancing performance in temporal sentence grounding. <div>
arXiv:2505.06557v1 Announce Type: new 
Abstract: The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search</title>
<link>https://arxiv.org/abs/2505.06566</link>
<guid>https://arxiv.org/abs/2505.06566</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, person search, noise reduction, DURA framework, retrieval performance

<br /><br />Summary: This article addresses the challenge of text-to-image person search, which involves identifying individuals based on textual descriptions. To mitigate data collection costs, large-scale text-image datasets are generated from online co-occurrence pairs. However, this method can introduce noise, particularly through mismatched pairs, which negatively impacts retrieval performance. Many existing approaches emphasize the issue of negative samples, potentially exacerbating the noise problem. To overcome these challenges, the authors introduce the Dynamic Uncertainty and Relational Alignment (DURA) framework. This framework comprises the Key Feature Selector (KFS), which effectively captures and models noise uncertainty, thereby enhancing retrieval reliability. Additionally, it incorporates a novel loss function, Dynamic Softmax Hinge Loss (DSH-Loss), which intelligently adjusts the difficulty of negative samples, improving robustness against noisy environments. The researchers conduct experiments on three different datasets, demonstrating that their proposed method exhibits strong resistance to noise while significantly enhancing retrieval performance in both low- and high-noise scenarios. Overall, the DURA framework offers a promising solution for improving text-to-image person search in challenging data conditions. <div>
arXiv:2505.06566v1 Announce Type: new 
Abstract: Text-to-image person search aims to identify an individual based on a text description. To reduce data collection costs, large-scale text-image datasets are created from co-occurrence pairs found online. However, this can introduce noise, particularly mismatched pairs, which degrade retrieval performance. Existing methods often focus on negative samples, amplifying this noise. To address these issues, we propose the Dynamic Uncertainty and Relational Alignment (DURA) framework, which includes the Key Feature Selector (KFS) and a new loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and models noise uncertainty, improving retrieval reliability. The bidirectional evidence from cross-modal similarity is modeled as a Dirichlet distribution, enhancing adaptability to noisy data. DSH adjusts the difficulty of negative samples to improve robustness in noisy environments. Our experiments on three datasets show that the method offers strong noise resistance and improves retrieval performance in both low- and high-noise scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</title>
<link>https://arxiv.org/abs/2505.06573</link>
<guid>https://arxiv.org/abs/2505.06573</guid>
<content:encoded><![CDATA[
<div> sensor-based methods, distance measurement, power transmission lines, 3D lasers, ElectricSight<br />
<br />
Summary:
ElectricSight is introduced as a system for measuring and monitoring 3D distances between power lines and potential threats, addressing the challenge of balancing accuracy and cost in power line protection. By combining real-time images with environmental point cloud priors, ElectricSight provides cost-effective and precise measurements. A monocular depth estimation method integrates 3D point cloud data into image-based estimates, enhancing accuracy and reliability. Experimental tests in a real-world scenario show ElectricSight achieves an average distance measurement accuracy of 1.08m and an early warning accuracy of 92%, demonstrating its effectiveness in protecting power transmission lines and identifying hazards. <div>
arXiv:2505.06573v1 Announce Type: new 
Abstract: Protecting power transmission lines from potential hazards involves critical tasks, one of which is the accurate measurement of distances between power lines and potential threats, such as large cranes. The challenge with this task is that the current sensor-based methods face challenges in balancing accuracy and cost in distance measurement. A common practice is to install cameras on transmission towers, which, however, struggle to measure true 3D distances due to the lack of depth information. Although 3D lasers can provide accurate depth data, their high cost makes large-scale deployment impractical.
  To address this challenge, we present ElectricSight, a system designed for 3D distance measurement and monitoring of potential hazards to power transmission lines. This work's key innovations lie in both the overall system framework and a monocular depth estimation method. Specifically, the system framework combines real-time images with environmental point cloud priors, enabling cost-effective and precise 3D distance measurements. As a core component of the system, the monocular depth estimation method enhances the performance by integrating 3D point cloud data into image-based estimates, improving both the accuracy and reliability of the system.
  To assess ElectricSight's performance, we conducted tests with data from a real-world power transmission scenario. The experimental results demonstrate that ElectricSight achieves an average accuracy of 1.08 m for distance measurements and an early warning accuracy of 92%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</title>
<link>https://arxiv.org/abs/2505.06575</link>
<guid>https://arxiv.org/abs/2505.06575</guid>
<content:encoded><![CDATA[
<div> encoder-decoder, hierarchical feature extraction, 3D human contact estimation, point cloud, geometric structures
Summary: 
GRACE introduces a new approach called Geometry-level Reasoning for 3D Human-scene Contact Estimation that combines a point cloud encoder-decoder architecture with hierarchical feature extraction to accurately estimate contact points between humans and scenes. The framework integrates 3D human geometric structures with 2D interaction semantics derived from images, enabling the effective modeling of contact regions. By establishing an implicit mapping from geometric features to the vertex space of the 3D human mesh, GRACE achieves high prediction accuracy and demonstrates strong generalization capabilities across diverse human geometries. Extensive experiments on benchmark datasets show that GRACE outperforms existing methods in contact estimation, with additional results confirming its robust generalization to unstructured human point clouds. <div>
arXiv:2505.06575v1 Announce Type: new 
Abstract: Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Random Alternation Framework for Zero-Shot Pansharpening</title>
<link>https://arxiv.org/abs/2505.06576</link>
<guid>https://arxiv.org/abs/2505.06576</guid>
<content:encoded><![CDATA[
<div> Keywords: pansharpening, deep learning, two-stage framework, degradation-aware modeling, random alternation optimization

Summary: 
The article introduces a new approach called TRA-PAN for pansharpening, leveraging deep learning techniques. The proposed method addresses the challenge of acquiring real high-resolution images by utilizing reduced-resolution images and physical characteristics of full-resolution images. The TRA-PAN framework consists of two stages: a pre-training stage featuring Degradation-Aware Modeling (DAM) and a warm-up procedure, and a second stage employing Random Alternation Optimization (RAO) for model optimization. By primarily focusing on full-resolution images, the TRA-PAN method allows zero-shot training with just one image pair, eliminating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms current state-of-the-art methods in both quantitative metrics and visual quality, showcasing its practical applicability in real-world scenarios. <br /><br />Summary: <div>
arXiv:2505.06576v1 Announce Type: new 
Abstract: In recent years, pansharpening has seen rapid advancements with deep learning methods, which have demonstrated impressive fusion quality. However, the challenge of acquiring real high-resolution images limits the practical applicability of these methods. To address this, we propose a two-stage random alternating framework (TRA-PAN) that effectively integrates strong supervision constraints from reduced-resolution images with the physical characteristics of full-resolution images. The first stage introduces a pre-training procedure, which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral degradation mappings, alongside a warm-up procedure designed to reduce training time and mitigate the negative effects of reduced-resolution data. In the second stage, Random Alternation Optimization (RAO) is employed, where random alternating training leverages the strengths of both reduced- and full-resolution images, further optimizing the fusion model. By primarily relying on full-resolution images, our method enables zero-shot training with just a single image pair, obviating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in both quantitative metrics and visual quality in real-world scenarios, highlighting its strong practical applicability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform</title>
<link>https://arxiv.org/abs/2505.06578</link>
<guid>https://arxiv.org/abs/2505.06578</guid>
<content:encoded><![CDATA[
<div> FC layer weights sharing, two-dimensional separable transform, neural network architecture, image recognition, model parameter reduction <br />
Summary:<br />
The paper introduces a new two-dimensional separable transform (LST) for neural network architectures in image recognition tasks. The LST leverages weight sharing in fully connected (FC) layers to process rows and columns of an image efficiently. By using LST layers, the model significantly reduces the number of parameters compared to traditional stacked FC layers. A neural network classifier incorporating a single LST layer and an FC layer achieves a high accuracy of 98.02% on the MNIST dataset with just 9.5k parameters. The study also demonstrates the effectiveness of the LST approach by implementing a classifier for handwritten digit recognition on an FPGA platform. This work opens up new possibilities for designing compact and high-performance neural network models for image recognition tasks. <br /> <div>
arXiv:2505.06578v1 Announce Type: new 
Abstract: The paper presents a learned two-dimensional separable transform (LST) that can be considered as a new type of computational layer for constructing neural network (NN) architecture for image recognition tasks. The LST based on the idea of sharing the weights of one fullyconnected (FC) layer to process all rows of an image. After that, a second shared FC layer is used to process all columns of image representation obtained from the first layer. The use of LST layers in a NN architecture significantly reduces the number of model parameters compared to models that use stacked FC layers. We show that a NN-classifier based on a single LST layer followed by an FC layer achieves 98.02\% accuracy on the MNIST dataset, while having only 9.5k parameters. We also implemented a LST-based classifier for handwritten digit recognition on the FPGA platform to demonstrate the efficiency of the suggested approach for designing a compact and high-performance implementation of NN models. Git repository with supplementary materials: https://github.com/Mak-Sim/LST-2d
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning</title>
<link>https://arxiv.org/abs/2505.06592</link>
<guid>https://arxiv.org/abs/2505.06592</guid>
<content:encoded><![CDATA[
<div> Keywords: batch augmentation, fine-tuning, multimodal data, neural networks, dataloader<br />
Summary:<br />
This paper introduces a novel approach for detecting fetal organs in ultrasound images and clinical text data using batch augmentation and unimodal fine-tuning. The method involves pre-training initial layers with medical data before multimodal training, transferring initialization with batch augmentation for image data, and fine-tuning neural networks for feature extraction. By combining image features with textual information, the proposed approach achieves state-of-the-art performance on the UPMC Food-101 dataset. A custom dataloader script is implemented to load and augment the multimodal data, enhancing generalization capabilities. The multimodal large language model (LLM) with the proposed training methodology outperforms existing methods. The scripts for the proposed method are available on GitHub for comparative analysis with traditional approaches.<br /> 
Summary: <div>
arXiv:2505.06592v1 Announce Type: new 
Abstract: This paper proposes batch augmentation with unimodal fine-tuning to detect the fetus's organs from ultrasound images and associated clinical textual information. We also prescribe pre-training initial layers with investigated medical data before the multimodal training. At first, we apply a transferred initialization with the unimodal image portion of the dataset with batch augmentation. This step adjusts the initial layer weights for medical data. Then, we apply neural networks (NNs) with fine-tuned initial layers to images in batches with batch augmentation to obtain features. We also extract information from descriptions of images. We combine this information with features obtained from images to train the head layer. We write a dataloader script to load the multimodal data and use existing unimodal image augmentation techniques with batch augmentation for the multimodal data. The dataloader brings a new random augmentation for each batch to get a good generalization. We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The multimodal large language model (LLM) with the proposed training provides the best results among the investigated methods. We receive near state-of-the-art (SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the proposed method with traditional counterparts at the following repository: github.com/dipuk0506/multimodal
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.06603</link>
<guid>https://arxiv.org/abs/2505.06603</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Anomaly Detection, ReplayCAD, diffusion-driven generative replay, segmentation, pixel-level detailed features

Summary:
ReplayCAD is introduced as a novel approach to Continual Anomaly Detection (CAD), addressing challenges such as catastrophic forgetting and segmentation of small anomalous regions. The framework utilizes diffusion-driven generative replay to preserve pixel-level detailed features by compressing historical data based on class semantic embeddings. This allows for the accurate segmentation of anomalies. Additionally, ReplayCAD leverages spatial features to enhance spatial diversity in the compressed data, leading to improved performance in both classification and segmentation tasks. The method achieves state-of-the-art results, with significant enhancements in segmentation accuracy on benchmark datasets like VisA and MVTec. By making the source code available on GitHub, ReplayCAD offers a valuable resource for future research in anomaly detection. 

<br /><br />Summary: <div>
arXiv:2505.06603v1 Announce Type: new 
Abstract: Continual Anomaly Detection (CAD) enables anomaly detection models in learning new classes while preserving knowledge of historical classes. CAD faces two key challenges: catastrophic forgetting and segmentation of small anomalous regions. Existing CAD methods store image distributions or patch features to mitigate catastrophic forgetting, but they fail to preserve pixel-level detailed features for accurate segmentation. To overcome this limitation, we propose ReplayCAD, a novel diffusion-driven generative replay framework that replay high-quality historical data, thus effectively preserving pixel-level detailed features. Specifically, we compress historical data by searching for a class semantic embedding in the conditional space of the pre-trained diffusion model, which can guide the model to replay data with fine-grained pixel details, thus improving the segmentation performance. However, relying solely on semantic features results in limited spatial diversity. Hence, we further use spatial features to guide data compression, achieving precise control of sample space, thereby generating more diverse data. Our method achieves state-of-the-art performance in both classification and segmentation, with notable improvements in segmentation: 11.5% on VisA and 8.1% on MVTec. Our source code is available at https://github.com/HULEI7/ReplayCAD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization</title>
<link>https://arxiv.org/abs/2505.06635</link>
<guid>https://arxiv.org/abs/2505.06635</guid>
<content:encoded><![CDATA[
<div> regularization term, functional entropy, multi-modal learning, semantic segmentation, unimodal dominance

Summary:<br />
- A challenge in dense prediction tasks is fusing multi-modal inputs effectively, as models tend to rely on easily learnable modalities, resulting in unimodal dominance.
- A new regularization term based on functional entropy is proposed to balance the contribution of each visual modality to segmentation results without adding parameters or modules.
- The log-Sobolev inequality is utilized to bound functional entropy using functional-Fisher-information, maximizing the information from each visual modality to mitigate unimodal dominance.
- A multi-scale regularization module applies the proposed term on high-level features and segmentation predictions for more balanced multi-modal learning.
- Experimental results on three datasets show superior performance of the proposed method, achieving significant improvements without additional parameters. 

<br /><br />Summary: <div>
arXiv:2505.06635v1 Announce Type: new 
Abstract: Fusing and balancing multi-modal inputs from novel sensors for dense prediction tasks, particularly semantic segmentation, is critically important yet remains a significant challenge. One major limitation is the tendency of multi-modal frameworks to over-rely on easily learnable modalities, a phenomenon referred to as unimodal dominance or bias. This issue becomes especially problematic in real-world scenarios where the dominant modality may be unavailable, resulting in severe performance degradation. To this end, we apply a simple but effective plug-and-play regularization term based on functional entropy, which introduces no additional parameters or modules. This term is designed to intuitively balance the contribution of each visual modality to the segmentation results. Specifically, we leverage the log-Sobolev inequality to bound functional entropy using functional-Fisher-information. By maximizing the information contributed by each visual modality, our approach mitigates unimodal dominance and establishes a more balanced and robust segmentation framework. A multi-scale regularization module is proposed to apply our proposed plug-and-play term on high-level features and also segmentation predictions for more balanced multi-modal learning. Extensive experiments on three datasets demonstrate that our proposed method achieves superior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing any additional parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Distillation with Probabilistic Latent Features</title>
<link>https://arxiv.org/abs/2505.06647</link>
<guid>https://arxiv.org/abs/2505.06647</guid>
<content:encoded><![CDATA[
<div> Keywords: dataset distillation, synthetic data, low-rank multivariate normal, computational efficiency, classification tasks

<br /><br />Summary:  
As deep learning models become more complex and require larger datasets, reducing storage and computational costs is crucial. Dataset distillation offers a solution by synthesizing a smaller set of data to replace the original dataset in classification tasks. Traditional methods often map data from pixel space to the latent space of a generative model, but this study introduces a novel stochastic approach that models the joint distribution of latent features, enhancing the capture of spatial structures and diversity in the synthetic samples. The method employs a low-rank multivariate normal distribution, parameterized by a lightweight network, ensuring low computational complexity and compatibility with various matching networks in dataset distillation. Following the distillation process, synthetic images are generated by inputting the learned latent features into a pretrained generator. These images are then utilized to train classification models, with performance evaluated against real test sets. The proposed method was validated across various benchmarks, including subsets of ImageNet, CIFAR-10, and the MedMNIST histopathological dataset, achieving state-of-the-art performance across different backbone architectures, highlighting its generality and effectiveness in practical applications. <div>
arXiv:2505.06647v1 Announce Type: new 
Abstract: As deep learning models grow in complexity and the volume of training data increases, reducing storage and computational costs becomes increasingly important. Dataset distillation addresses this challenge by synthesizing a compact set of synthetic data that can effectively replace the original dataset in downstream classification tasks. While existing methods typically rely on mapping data from pixel space to the latent space of a generative model, we propose a novel stochastic approach that models the joint distribution of latent features. This allows our method to better capture spatial structures and produce diverse synthetic samples, which benefits model training. Specifically, we introduce a low-rank multivariate normal distribution parameterized by a lightweight network. This design maintains low computational complexity and is compatible with various matching networks used in dataset distillation. After distillation, synthetic images are generated by feeding the learned latent features into a pretrained generator. These synthetic images are then used to train classification models, and performance is evaluated on real test set. We validate our method on several benchmarks, including ImageNet subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach achieves state-of-the-art cross architecture performance across a range of backbone architectures, demonstrating its generality and effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection</title>
<link>https://arxiv.org/abs/2505.06663</link>
<guid>https://arxiv.org/abs/2505.06663</guid>
<content:encoded><![CDATA[
<div> Keywords: open-vocabulary, video visual relationship detection, object detection, relationship classification, mutual enhancement<br />
<br />
Summary: 
The paper introduces a framework called METOR for open-vocabulary video visual relationship detection. METOR aims to detect objects and their relationships in videos without predefined categories, by jointly modeling and mutually enhancing object detection and relationship classification. The framework includes a contextual refinement encoding module that refines text features and object queries using visual contexts. An iterative enhancement module is proposed to improve recognition performance by enhancing object and relationship representations alternately, exploiting their interdependence. METOR outperforms existing methods on the VidVRD and VidOR datasets, showcasing state-of-the-art performance in open-vocabulary video visual relationship detection. <div>
arXiv:2505.06663v1 Announce Type: new 
Abstract: Open-vocabulary video visual relationship detection aims to detect objects and their relationships in videos without being restricted by predefined object or relationship categories. Existing methods leverage the rich semantic knowledge of pre-trained vision-language models such as CLIP to identify novel categories. They typically adopt a cascaded pipeline to first detect objects and then classify relationships based on the detected objects, which may lead to error propagation and thus suboptimal performance. In this paper, we propose Mutual EnhancemenT of Objects and Relationships (METOR), a query-based unified framework to jointly model and mutually enhance object detection and relationship classification in open-vocabulary scenarios. Under this framework, we first design a CLIP-based contextual refinement encoding module that extracts visual contexts of objects and relationships to refine the encoding of text features and object queries, thus improving the generalization of encoding to novel categories. Then we propose an iterative enhancement module to alternatively enhance the representations of objects and relationships by fully exploiting their interdependence to improve recognition performance. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate that our framework achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning</title>
<link>https://arxiv.org/abs/2505.06665</link>
<guid>https://arxiv.org/abs/2505.06665</guid>
<content:encoded><![CDATA[
<div> Keywords: image fusion, semantic information, multi-task learning, segmentation, MultiTaskVIF

<br /><br />Summary:  
Visible and infrared image fusion (VIF) has gained significant attention, particularly with the focus on improving visual quality and incorporating semantic information into fusion models. Traditional segmentation-oriented VIF methods often rely on a cascade structure, which utilizes separate models for fusion and segmentation, leading to complexity and redundancy. The proposed solution addresses this issue by introducing MultiTaskVIF, a concise and universal training framework that allows for the simultaneous generation of both fused images and segmentation results. This innovative approach is inspired by multi-task learning, integrating semantic information directly into the fusion process without requiring a complete segmentation model. The framework features a multi-task head decoder (MTH) that replaces the conventional decoder of the fusion model, facilitating the efficient learning of semantic features during training. Extensive experimental evaluations have demonstrated the effectiveness and efficiency of MultiTaskVIF, proving it to be a simpler alternative to existing cascade frameworks. The authors plan to release the code for this method upon acceptance, thereby contributing to the field of image fusion and offering a streamlined approach for future research and applications. <div>
arXiv:2505.06665v1 Announce Type: new 
Abstract: Visible and infrared image fusion (VIF) has attracted significant attention in recent years. Traditional VIF methods primarily focus on generating fused images with high visual quality, while recent advancements increasingly emphasize incorporating semantic information into the fusion model during training. However, most existing segmentation-oriented VIF methods adopt a cascade structure comprising separate fusion and segmentation models, leading to increased network complexity and redundancy. This raises a critical question: can we design a more concise and efficient structure to integrate semantic information directly into the fusion model during training-Inspired by multi-task learning, we propose a concise and universal training framework, MultiTaskVIF, for segmentation-oriented VIF models. In this framework, we introduce a multi-task head decoder (MTH) to simultaneously output both the fused image and the segmentation result during training. Unlike previous cascade training frameworks that necessitate joint training with a complete segmentation model, MultiTaskVIF enables the fusion model to learn semantic features by simply replacing its decoder with MTH. Extensive experimental evaluations validate the effectiveness of the proposed method. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation</title>
<link>https://arxiv.org/abs/2505.06668</link>
<guid>https://arxiv.org/abs/2505.06668</guid>
<content:encoded><![CDATA[
<div> Keywords: StableMotion, image rectification, diffusion models, Adaptive Ensemble Strategy, Sampling Steps Disaster 

Summary: 
StableMotion is a new framework that utilizes pretrained large-scale image diffusion models to perform motion estimation for image rectification tasks like Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). It repurposes text-to-image Stable Diffusion (SD) models as an image-to-motion estimator and uses an Adaptive Ensemble Strategy (AES) to ensure high-quality results by consolidating multiple outputs. The framework leverages the concept of Sampling Steps Disaster (SSD) to achieve fast one-step inference and overcome inconsistencies in diffusion model outputs. StableMotion outperforms previous methods in image rectification tasks, demonstrating strong generalizability and a 200 times speedup. <div>
arXiv:2505.06668v1 Announce Type: new 
Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and content priors) from pretrained large-scale image diffusion models to perform motion estimation, solving single-image-based image rectification tasks such as Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD) models as backbone and repurposes it into an image-to-motion estimator. To mitigate inconsistent output produced by diffusion models, we propose Adaptive Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive, high-fidelity result. Additionally, we present the concept of Sampling Steps Disaster (SSD), the counterintuitive scenario where increasing the number of sampling steps can lead to poorer outcomes, which enables our framework to achieve one-step inference. StableMotion is verified on two image rectification tasks and delivers state-of-the-art performance in both, as well as showing strong generalizability. Supported by SSD, StableMotion offers a speedup of 200 times compared to previous diffusion model-based methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Dataset Condensation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06670</link>
<guid>https://arxiv.org/abs/2505.06670</guid>
<content:encoded><![CDATA[
<div> video dataset distillation, synthetic dataset, video diffusion model, VST-UNet, TAC-DT

Summary:
- Dataset distillation is used to generate compact synthetic datasets from large real datasets to save computational resources.
- Video dataset distillation is challenging due to limited performance and poor data quality.
- A video diffusion model is introduced alongside VST-UNet and TAC-DT to improve video dataset distillation.
- VST-UNet helps select diverse and informative subset of videos from the original dataset.
- TAC-DT efficiently selects representative videos without additional training overhead.
- Experimental results on benchmark datasets show up to 10.61% performance improvement over existing methods.
- The proposed method consistently outperforms other approaches, setting a new benchmark for video dataset distillation.

<br /><br />Summary: <div>
arXiv:2505.06670v1 Announce Type: new 
Abstract: In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance and poor data quality, particularly in the video domain. In this paper, we focus on video dataset distillation by employing a video diffusion model to generate high-quality synthetic videos. To enhance representativeness, we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking the Text-to-Video Generative Models</title>
<link>https://arxiv.org/abs/2505.06679</link>
<guid>https://arxiv.org/abs/2505.06679</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, generative models, jailbreak attack, safety concerns, optimization-based

<br /><br />Summary: This paper addresses the vulnerabilities of text-to-video generative models, which have advanced significantly through diffusion models like Pika, Luma, Kling, and Sora. While these models excel at generating content, they are susceptible to jailbreak attacks that can produce unsafe material such as pornography and violence. Previous efforts, like T2VSafetyBench, have evaluated these models against unsafe prompts but lacked systematic analysis of their vulnerabilities. The authors propose the first optimization-based jailbreak attack tailored for text-to-video models, framing prompt generation as an optimization problem with three objectives: maximizing semantic similarity between input and generated prompts, evading safety filters, and ensuring generated videos are semantically aligned with the original input. To improve the robustness of prompts, they introduce a mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on averaged scores. Extensive experiments conducted on various models, including Open-Sora, Pika, Luma, and Kling, show that their approach not only achieves a higher success rate in bypassing safety measures but also generates videos that maintain greater semantic relevance to the input prompts. <div>
arXiv:2505.06679v1 Announce Type: new 
Abstract: Text-to-video generative models have achieved significant progress, driven by the rapid advancements in diffusion models, with notable examples including Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their vulnerability to jailbreak attack, i.e. to generate unsafe content, including pornography, violence, and discrimination, raises serious safety concerns. Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for evaluating the safety of text-to-video models against unsafe prompts but lack systematic studies for exploiting their vulnerabilities effectively. In this paper, we propose the \textit{first} optimization-based jailbreak attack against text-to-video models, which is specifically designed. Our approach formulates the prompt generation task as an optimization problem with three key objectives: (1) maximizing the semantic similarity between the input and generated prompts, (2) ensuring that the generated prompts can evade the safety filter of the text-to-video model, and (3) maximizing the semantic similarity between the generated videos and the original input prompts. To further enhance the robustness of the generated prompts, we introduce a prompt mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on the averaged score. This strategy not only improves the attack success rate but also boosts the semantic relevance of the generated video. We conduct extensive experiments across multiple text-to-video models, including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our method not only achieves a higher attack success rate compared to baseline methods but also generates videos with greater semantic similarity to the original input prompts.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration</title>
<link>https://arxiv.org/abs/2505.06683</link>
<guid>https://arxiv.org/abs/2505.06683</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep unfolding networks, illumination degradation image restoration, UnfoldIR, reflectance-assisted illumination correction, illumination-guided reflectance enhancement

Summary:
UnfoldIR is a novel Deep Unfolding Network (DUN) method for illumination degradation image restoration (IDIR) tasks. It addresses limitations by introducing a new task-specific restoration model with dedicated regularization terms for illumination smoothing and texture enhancement. The iterative optimized solution is unfolded into a multistage network consisting of Reflectance-Assisted Illumination Correction (RAIC) and Illumination-Guided Reflectance Enhancement (IGRE) modules. RAIC enforces illumination smoothness using a visual state space (VSS), while IGRE globally aligns similar textures to enhance details in degraded regions. An inter-stage information consistent loss is proposed to ensure network stability in later stages and maintain structural preservation. Experiments validate the effectiveness of UnfoldIR across various IDIR tasks and downstream problems. <div>
arXiv:2505.06683v1 Announce Type: new 
Abstract: Deep unfolding networks (DUNs) are widely employed in illumination degradation image restoration (IDIR) to merge the interpretability of model-based approaches with the generalization of learning-based methods. However, the performance of DUN-based methods remains considerably inferior to that of state-of-the-art IDIR solvers. Our investigation indicates that this limitation does not stem from structural shortcomings of DUNs but rather from the limited exploration of the unfolding structure, particularly for (1) constructing task-specific restoration models, (2) integrating advanced network architectures, and (3) designing DUN-specific loss functions. To address these issues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR first introduces a new IDIR model with dedicated regularization terms for smoothing illumination and enhancing texture. We unfold the iterative optimized solution of this model into a multistage network, with each stage comprising a reflectance-assisted illumination correction (RAIC) module and an illumination-guided reflectance enhancement (IGRE) module. RAIC employs a visual state space (VSS) to extract non-local features, enforcing illumination smoothness, while IGRE introduces a frequency-aware VSS to globally align similar textures, enabling mildly degraded regions to guide the enhancement of details in more severely degraded areas. This suppresses noise while enhancing details. Furthermore, given the multistage structure, we propose an inter-stage information consistent loss to maintain network stability in the final stages. This loss contributes to structural preservation and sustains the model's performance even in unsupervised settings. Experiments verify our effectiveness across 5 IDIR tasks and 3 downstream problems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNBench: Benchmarking Robust Federated Learning against Noisy Labels</title>
<link>https://arxiv.org/abs/2505.06684</link>
<guid>https://arxiv.org/abs/2505.06684</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, label noise, benchmark study, robustness, regularization

Summary:
Federated learning faces the challenge of label noise within distributed datasets, affecting performance. The FNBench benchmark study evaluates state-of-the-art methods under unified settings, considering synthetic label noise, human-annotation errors, and systematic errors. This study sheds light on why noisy labels degrade federated learning performance. A representation-aware regularization method is proposed to enhance robustness against label noise. The open-sourced source code and observations provided in this study contribute to understanding and addressing the impact of label noise in federated learning. Future directions for research are also outlined to further advance the field. <div>
arXiv:2505.06684v1 Announce Type: new 
Abstract: Robustness to label noise within data is a significant challenge in federated learning (FL). From the data-centric perspective, the data quality of distributed datasets can not be guaranteed since annotations of different clients contain complicated label noise of varying degrees, which causes the performance degradation. There have been some early attempts to tackle noisy labels in FL. However, there exists a lack of benchmark studies on comprehensively evaluating their practical performance under unified settings. To this end, we propose the first benchmark study FNBench to provide an experimental investigation which considers three diverse label noise patterns covering synthetic label noise, imperfect human-annotation errors and systematic errors. Our evaluation incorporates eighteen state-of-the-art methods over five image recognition datasets and one text classification dataset. Meanwhile, we provide observations to understand why noisy labels impair FL, and additionally exploit a representation-aware regularization method to enhance the robustness of existing methods against noisy labels based on our observations. Finally, we discuss the limitations of this work and propose three-fold future directions. To facilitate related communities, our source code is open-sourced at https://github.com/Sprinter1999/FNBench.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search</title>
<link>https://arxiv.org/abs/2505.06694</link>
<guid>https://arxiv.org/abs/2505.06694</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater object detection, sonar imagery, Detection Transformer, Neural Architecture Search, NAS-DETR

Summary:
- Proposal of a Detection Transformer architecture optimized with a Neural Architecture Search approach for object detection in sonar images.
- Introduction of an improved Zero-shot Neural Architecture Search method based on the maximum entropy principle to identify a high-performance CNN-Transformer backbone for sonar image detection.
- Combination of the selected backbone with a Feature Pyramid Network and a deformable attention-based Transformer decoder to construct a complete network architecture for enhanced performance.
- Extensive experiments showcasing state-of-the-art performance on two representative datasets while maintaining real-time efficiency and minimal computational complexity.
- Correlation analysis between key parameters and differential entropy-based fitness function to enhance the interpretability of the proposed framework.

<br /><br />Summary: In this study, a novel approach to underwater object detection using sonar imagery is introduced. By combining a Detection Transformer architecture with a Neural Architecture Search method, the authors achieve state-of-the-art performance on representative datasets. The proposed method incorporates an improved Zero-shot Neural Architecture Search technique and advanced network components to enhance detection accuracy. Through extensive experiments, the effectiveness of the approach is demonstrated, highlighting its real-time efficiency and minimal computational complexity. Additionally, correlation analysis is performed to improve the interpretability of the framework, making this work a significant contribution to the field of sonar object detection. <div>
arXiv:2505.06694v1 Announce Type: new 
Abstract: Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</title>
<link>https://arxiv.org/abs/2505.06710</link>
<guid>https://arxiv.org/abs/2505.06710</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-instance learning, feature extractor, weakly-supervised learning, data augmentation, pathological images<br />
Summary: <br />
This paper introduces a novel approach to pre-train feature extractors for multi-instance learning (MIL) using weakly-supervised learning on whole-slide pathological images (WSI). The method focuses on learning instance-level representations by propagating weak bag-level labels to instances for supervised learning. It incorporates strong data augmentation, a non-linear prediction head, and a robust loss function to enhance feature learning for MIL. Experimental results demonstrate superior performance compared to other pre-training methods like ImageNet and self-supervised learning in various downstream tasks on large-scale WSI datasets. The proposed scheme is also scalable and compatible with fine-tuning pathological-specific models and pre-training on multiple merged datasets. This work, the first to emphasize representation learning for MIL, showcases the importance of optimizing feature extraction for improved performance in WSI analysis. <br /> <div>
arXiv:2505.06710v1 Announce Type: new 
Abstract: Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers</title>
<link>https://arxiv.org/abs/2505.06745</link>
<guid>https://arxiv.org/abs/2505.06745</guid>
<content:encoded><![CDATA[
<div> framework, symbolic rule extraction, Vision Transformers, sparse concept layer, logic programs <br />
 <br />Summary: 
This paper introduces a framework for extracting symbolic rules from Vision Transformers (ViTs). It addresses the challenge posed by ViTs' lack of modular concept detectors and reliance on global self-attention mechanisms by proposing a sparse concept layer inspired by Sparse Autoencoders. This layer learns a binarized representation of high-level visual concepts through a combination of sparsity, entropy minimization, and supervised contrastive loss. The resulting binarized concept activations are used as input to the FOLD-SE-M algorithm to generate logic programs, enabling symbolic reasoning. The method achieves higher classification accuracy than standard ViTs while providing interpretable and semantically meaningful rule-sets that serve as a logic-based decision layer operating directly on sparse concept representations. This work represents a significant advancement in neuro-symbolic AI by bridging the gap between transformer-based vision models and symbolic logic programming. <div>
arXiv:2505.06745v1 Announce Type: new 
Abstract: Recent neuro-symbolic approaches have successfully extracted symbolic rule-sets from CNN-based models to enhance interpretability. However, applying similar techniques to Vision Transformers (ViTs) remains challenging due to their lack of modular concept detectors and reliance on global self-attention mechanisms. We propose a framework for symbolic rule extraction from ViTs by introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This linear layer operates on attention-weighted patch representations and learns a disentangled, binarized representation in which individual neurons activate for high-level visual concepts. To encourage interpretability, we apply a combination of L1 sparsity, entropy minimization, and supervised contrastive loss. These binarized concept activations are used as input to the FOLD-SE-M algorithm, which generates a rule-set in the form of logic programs. Our method achieves a 5.14% better classification accuracy than the standard ViT while enabling symbolic reasoning. Crucially, the extracted rule-set is not merely post-hoc but acts as a logic-based decision layer that operates directly on the sparse concept representations. The resulting programs are concise and semantically meaningful. This work is the first to extract executable logic programs from ViTs using sparse symbolic representations. It bridges the gap between transformer-based vision models and symbolic logic programming, providing a step forward in interpretable and verifiable neuro-symbolic AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</title>
<link>https://arxiv.org/abs/2505.06796</link>
<guid>https://arxiv.org/abs/2505.06796</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Fake News Detection, Dataset, Shallow-Deep Multitask Learning, Image-text Generation, Deepfake Modeling

Summary:
The article introduces a new dataset called Multimodal Fake News Detection (MFND) to identify and localize authentic fake news that is easily manipulated by deepfake modeling attacks. A Shallow-Deep Multitask Learning (SDML) model is proposed to effectively detect fake news by leveraging unimodal and mutual modal features to capture the underlying semantics of news. The model employs momentum distillation-based contrastive learning for fine-grained image and text alignment and an adaptive cross-modal fusion module to enhance mutual modal features. It also incorporates a two-branch framework for image and text features to make four predictions through detection and localization projections. Experimental results show the model's superiority in detecting fake news on both mainstream and newly proposed datasets. The code and dataset are available for further research. 

<br /><br />Summary: 
- Introduction of Multimodal Fake News Detection dataset (MFND) to identify and localize authentic fake news affected by deepfake modeling attacks.
- Proposal of a Shallow-Deep Multitask Learning (SDML) model leveraging unimodal and mutual modal features for effective fake news detection.
- Utilization of momentum distillation-based contrastive learning and adaptive cross-modal fusion module for image-text alignment and feature enhancement.
- Implementation of a two-branch framework for image and text features to make four predictions through detection and localization projections.
- Superiority of the model demonstrated through experiments on both mainstream and newly proposed datasets. Availability of code and dataset for further research. <div>
arXiv:2505.06796v1 Announce Type: new 
Abstract: Multimodal news contains a wealth of information and is easily affected by deepfake modeling attacks. To combat the latest image and text generation methods, we present a new Multimodal Fake News Detection dataset (MFND) containing 11 manipulated types, designed to detect and localize highly authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning (SDML) model for fake news, which fully uses unimodal and mutual modal features to mine the intrinsic semantics of news. Under shallow inference, we propose the momentum distillation-based light punishment contrastive learning for fine-grained uniform spatial image and text semantic alignment, and an adaptive cross-modal fusion module to enhance mutual modal features. Under deep inference, we design a two-branch framework to augment the image and text unimodal features, respectively merging with mutual modalities features, for four predictions via dedicated detection and localization projections. Experiments on both mainstream and our proposed datasets demonstrate the superiority of the model. Codes and dataset are released at https://github.com/yunan-wang33/sdml.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
<link>https://arxiv.org/abs/2505.06814</link>
<guid>https://arxiv.org/abs/2505.06814</guid>
<content:encoded><![CDATA[
<div> Keywords: M4IVQA, multi-modal, multilingual, medical instructional videos, reasoning  

<br /><br />Summary:  
The M4IVQA challenge has been introduced following the successful CMIVQA and MMIVQA challenges. It aims to advance research in multi-modal, multilingual, and multi-hop medical instructional question answering systems, specifically focusing on medical instructional videos. The challenge features three tracks: M4TAGSV, which centers on Temporal Answer Grounding in a Single Video; M4VCR, focused on Video Corpus Retrieval; and M4TAGVC, which involves Temporal Answer Grounding in a Video Corpus. Participants are tasked with creating algorithms that can effectively process both video and text data, understand queries in multiple languages, and accurately respond to complex multi-hop medical questions. The challenge is expected to foster innovations in multimodal reasoning systems applicable to healthcare, enhancing both emergency response capabilities and medical education within multilingual communities. The official website for more information is https://cmivqa.github.io/. <div>
arXiv:2505.06814v1 Announce Type: new 
Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Multi-class Image Classification</title>
<link>https://arxiv.org/abs/2505.06825</link>
<guid>https://arxiv.org/abs/2505.06825</guid>
<content:encoded><![CDATA[
<div> active learning, image classification, CNN classifier, uncertainty metrics, training set size

Summary:
Active learning is proposed as a method to reduce the number of training examples needed for image classification tasks, particularly for CNN classifiers. By assigning values to image examples using different uncertainty metrics, high-value examples can be strategically selected in a smaller training set size. Results on digit recognition and fruit classification datasets demonstrate the effectiveness of active learning, with formal comparisons across four different uncertainty metrics. Active learning shows marked improvement over random sampling, especially for more difficult classification tasks. The study also indicates the viability of active learning for simpler binary classification tasks, showcasing its potential for a wide range of image classification problems. <div>
arXiv:2505.06825v1 Announce Type: new 
Abstract: A principle bottleneck in image classification is the large number of training examples needed to train a classifier. Using active learning, we can reduce the number of training examples to teach a CNN classifier by strategically selecting examples. Assigning values to image examples using different uncertainty metrics allows the model to identify and select high-value examples in a smaller training set size. We demonstrate results for digit recognition and fruit classification on the MNIST and Fruits360 data sets. We formally compare results for four different uncertainty metrics. Finally, we observe active learning is also effective on simpler (binary) classification tasks, but marked improvement from random sampling is more evident on more difficult tasks. We show active learning is a viable algorithm for image classification problems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification</title>
<link>https://arxiv.org/abs/2505.06831</link>
<guid>https://arxiv.org/abs/2505.06831</guid>
<content:encoded><![CDATA[
<div> Keywords: group-robust generalization, spurious correlations, Class-Conditional Distribution Balancing (CCDB), Bias Exploration via Overfitting (BEO), fine-grained variant

Summary: 
- Achieving group-robust generalization without bias annotations is challenging
- Spurious correlations often arise from mismatches in distributions of bias attributes
- CCDB addresses this issue through simple distribution matching but uses a single Gaussian for approximation
- BEO proposes modeling distributions as mixtures of latent groups for more detailed representation
- FG-CCDB, a fine-grained variant, performs precise distribution matching and balancing within each group
- FG-CCDB achieves stronger mitigation of spurious correlations through group-level reweighting
- BEO acts as a proxy for bias annotations and can be integrated with bias-supervised methods
- The combination of BEO and FG-CCDB performs comparably to bias-supervised approaches in binary classification and outperforms them in highly biased multi-class scenarios

<br /><br />Summary: <div>
arXiv:2505.06831v1 Announce Type: new 
Abstract: Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable. Recent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. However, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. To address this limitation, we propose a novel method called Bias Exploration via Overfitting (BEO), which captures each distribution in greater detail by modeling it as a mixture of latent groups. Building on these group-level descriptions, we introduce a fine-grained variant of CCDB, termed FG-CCDB, which performs more precise distribution matching and balancing within each group. Through group-level reweighting, FG-CCDB learns sample weights from a global perspective, achieving stronger mitigation of spurious correlations without incurring substantial storage or computational costs. Extensive experiments demonstrate that BEO serves as a strong proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods. Moreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Instruction Tuning with Chain of Region-of-Interest</title>
<link>https://arxiv.org/abs/2505.06840</link>
<guid>https://arxiv.org/abs/2505.06840</guid>
<content:encoded><![CDATA[
<div> Keywords: high-resolution images, multimodal large language models, Chain of Region-of-Interest, visual instruction tuning, computational efficiency 

Summary: 
The study introduces a novel method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning to address the computational challenges of high-resolution images in multimodal large language models. Inspired by the selective nature of the human visual system, CoRoI prioritizes informative regions in images to improve visual comprehension and recognition without processing lengthy high-resolution tokens. Extensive experiments across 11 benchmarks validate the effectiveness of CoRoI across various model sizes, consistently outperforming existing methods like LLaVA-NeXT and proprietary models like Gemini Pro 1.0 and GPT-4V on multiple benchmarks. The proposed method demonstrates superior performance, especially with a finetuned 34B model surpassing proprietary methods on six benchmarks and outperforming GPT-4V on specific tasks. Overall, CoRoI enhances multimodal visual comprehension while providing computational efficiency for large language models. 

<br /><br />Summary: <div>
arXiv:2505.06840v1 Announce Type: new 
Abstract: High-resolution (HR) images are pivotal for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs). However, directly increasing image resolution can significantly escalate computational demands. In this study, we propose a method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating the computational burden associated with high-resolution images for MLLMs. Drawing inspiration from the selective nature of the human visual system, we recognize that not all regions within high-resolution images carry equal importance. CoRoI seeks to identify and prioritize the most informative regions, thereby enhancing multimodal visual comprehension and recognition while circumventing the need for processing lengthy HR image tokens. Through extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI across varying sizes, ranging from 7B to 34B in parameters. Our models consistently demonstrate superior performance across diverse multimodal benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all benchmarks and our finetuned 34B model surpasses proprietary methods like Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB, SEED-I, and MME.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach</title>
<link>https://arxiv.org/abs/2505.06853</link>
<guid>https://arxiv.org/abs/2505.06853</guid>
<content:encoded><![CDATA[
<div> Pan American Health Organization, cancer cases, Latin America, osteosarcoma, surgical safety margins

Summary:
The Pan American Health Organization reports a rising trend in cancer cases in Latin America, with osteosarcoma being a common and lethal bone cancer affecting the youth. Detection of osteosarcoma poses challenges due to its unique characteristics. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while sparing healthy tissue. A novel approach is proposed in this study for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The method utilizes MRI and X-ray data, digital processing techniques, and unsupervised learning algorithms like k-means clustering to delineate tumor boundaries. Results from experiments demonstrate the potential of automated, patient-specific determination of safety margins, which could enhance the efficiency and accuracy of osteosarcoma surgery.<br /><br />Summary: <div>
arXiv:2505.06853v1 Announce Type: new 
Abstract: According to the Pan American Health Organization, the number of cancer cases in Latin America was estimated at 4.2 million in 2022 and is projected to rise to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone cancers affecting young people, is difficult to detect due to its unique texture and intensity. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while preserving healthy tissue. Therefore, this study proposes a method for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The proposed approach uses MRI and X-ray data from open-source repositories, digital processing techniques, and unsupervised learning algorithms (such as k-means clustering) to define tumor boundaries. Experimental results highlight the potential for automated, patient-specific determination of safety margins.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</title>
<link>https://arxiv.org/abs/2505.06855</link>
<guid>https://arxiv.org/abs/2505.06855</guid>
<content:encoded><![CDATA[
<div> Keywords: text recognition, self-supervised learning, masked image modeling, multi-masking strategy, real-world datasets

Summary:
The study focuses on improving text recognition methods by addressing the limitations of training on synthetic datasets. Existing techniques suffer from performance disparities when handling complex real-world images due to their inability to replicate real-world scenarios accurately. The researchers propose a Multi-Masking Strategy (MMS) that integrates different masking techniques, including random patch, blockwise, and span masking, into the Masked Image Modeling (MIM) framework. By combining low and high-level textual representations, MMS outperforms existing self-supervised methods after fine-tuning with real data. The approach enhances performance in various text-related tasks such as recognition, segmentation, and text-image super-resolution. This novel strategy aims to bridge the gap between synthetic and real-world data, leading to more accurate and robust text recognition systems.<br /><br />Summary: <div>
arXiv:2505.06855v1 Announce Type: new 
Abstract: Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuRN: Neuro-inspired Domain Generalization for Image Classification</title>
<link>https://arxiv.org/abs/2505.06881</link>
<guid>https://arxiv.org/abs/2505.06881</guid>
<content:encoded><![CDATA[
<div> Neural Response Normalization, domain generalization, image classification, deep learning architectures, neuro-inspired

Summary: 
Neural Response Normalization (NeuRN) layer is introduced to improve domain generalization in image classification tasks. Inspired by neurons in the visual cortex, NeuRN aims to enhance deep learning models' performance on unseen datasets. Experimenting with various deep learning architectures, including those from Neural Architecture Search and Vision Transformer, models integrated with NeuRN show improved performance compared to baseline models. A novel method that uses the Needleman-Wunsch algorithm is proposed to compute similarity between deep learning architectures, aiding in selecting models for experimentation. Results demonstrate the effectiveness of NeuRN in cross-domain image classification, paving the way for future neuro-inspired deep learning models. 

<br /><br />Summary: <div>
arXiv:2505.06881v1 Announce Type: new 
Abstract: Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.06886</link>
<guid>https://arxiv.org/abs/2505.06886</guid>
<content:encoded><![CDATA[
<div> Keywords: mouse, visual cortex, deep learning, NeuRN, neural representations  

<br /><br />Summary: The mouse serves as a vital model in systems neuroscience, particularly in understanding how its visual cortex responds to various natural scene stimuli. This study examines the functional alignment of the mouse visual cortex with deep learning models in object classification tasks. A novel representational learning strategy reveals a significant resemblance between the mouse's visual processing and top-performing deep learning models, observed at both population and single-cell levels. The research introduces a Neural Response Normalization (NeuRN) layer, inspired by the activation profiles of excitatory and inhibitory neurons, which enhances the representational similarity. Testing NeuRN within deep learning frameworks demonstrates marked improvements in robustness, especially against data shifts in domain generalization tasks. This work outlines a transformative approach for comparing the mouse visual cortex's architecture with advanced AI models, enabling a deeper understanding of neural representations. The findings suggest that deep learning models can benefit from insights gained from mouse vision, potentially leading to enhanced performance in real-world applications. Overall, this research holds significant implications for advancing AI models by integrating insights from biological systems. <div>
arXiv:2505.06886v1 Announce Type: new 
Abstract: The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.06894</link>
<guid>https://arxiv.org/abs/2505.06894</guid>
<content:encoded><![CDATA[
<div> NeuGen, brain-inspired normalization, NeRF architectures, generalization, image rendering

Summary:
Neural Radiance Fields (NeRF) have revolutionized novel view synthesis but face challenges in generalizing across diverse scenes. This study proposes integrating a brain-inspired normalization technique, NeuGen, into leading NeRF architectures like MVSNeRF and GeoNeRF. NeuGen extracts domain-invariant features to enhance models' generalization capabilities, improving accuracy and robustness in image rendering. Integration of NeuGen shows enhanced performance in benchmarks across diverse datasets, enabling better generalization across varied scenes. Comprehensive evaluations, both quantitative and qualitative, demonstrate that this approach outperforms existing models in generalizability and rendering quality. By merging neuroscientific principles with deep learning frameworks, this work sets a new standard for improved generalizability and efficiency in novel view synthesis. A demo showcasing the study is available at https://neugennerf.github.io.

<br /><br />Summary: <div>
arXiv:2505.06894v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2505.06898</link>
<guid>https://arxiv.org/abs/2505.06898</guid>
<content:encoded><![CDATA[
<div> Keywords: Generalist Medical AI, multi-modal explainability, prognostic capabilities, uncertainty quantification, clinician-centric  

<br /><br />Summary: This article introduces XMedGPT, a multi-modal AI assistant designed for clinical use, addressing limitations in existing Generalist Medical AI (GMAI) systems. XMedGPT enhances medical decision-making by integrating textual and visual interpretability, providing accurate diagnostic outputs while grounding anatomical references in medical images. The system introduces a reliability indexing mechanism to quantify uncertainty through interactive question-answering and produces validated results across four key areas: multi-modal interpretability, uncertainty quantification, prognostic modeling, and rigorous benchmarking. The model achieves a notable IoU of 0.703 across anatomical regions and a Kendall's tau-b of 0.479, reflecting strong alignment between visual rationales and clinical outcomes. It excels in uncertainty estimation with AUC scores of 0.862 for visual question answering and 0.764 for radiology report generation. In cancer prognosis, it outperforms prior models by 26.9% and GPT-4o by 25.0%. Availability of extensive benchmarking across 347 datasets and validation across four anatomical systems indicates the model's exceptional generalizability, evidenced by performance improvements of 20.7% in-domain and 16.7% on a large in-house dataset. XMedGPT signifies a notable advancement in clinician-centered AI integration for diverse healthcare applications. <div>
arXiv:2505.06898v1 Announce Type: new 
Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection</title>
<link>https://arxiv.org/abs/2505.06903</link>
<guid>https://arxiv.org/abs/2505.06903</guid>
<content:encoded><![CDATA[
arXiv:2505.06903v1 Announce Type: new 
Abstract: Temporal medical image analysis is essential for clinical decision-making, yet existing methods either align images and text at a coarse level - causing potential semantic mismatches - or depend solely on visual information, lacking medical semantic integration. We present CheXLearner, the first end-to-end framework that unifies anatomical region detection, Riemannian manifold-based structure alignment, and fine-grained regional semantic guidance. Our proposed Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to robustly align anatomical structures and capture pathologically meaningful discrepancies across temporal chest X-rays. By introducing regional progression descriptions as supervision, CheXLearner achieves enhanced cross-modal representation learning and supports dynamic low-level feature optimization. Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and 80.32% (+11.05%) F1-score on anatomical region progression detection - substantially outperforming state-of-the-art baselines, especially in structurally complex regions. Additionally, our model attains a 91.52% average AUC score in downstream disease classification, validating its superior feature representation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</title>
<link>https://arxiv.org/abs/2505.06905</link>
<guid>https://arxiv.org/abs/2505.06905</guid>
<content:encoded><![CDATA[
arXiv:2505.06905v1 Announce Type: new 
Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. Recently, models trained on synthetic data and refined through domain adaptation have shown remarkable performance in MHE, yet it remains unclear how these models make predictions or how reliable they truly are. In this paper, we investigate a state-of-the-art MHE model trained purely on synthetic data to explore where the model looks when making height predictions. Through systematic analyses, we find that the model relies heavily on shadow cues, a factor that can lead to overestimation or underestimation of heights when shadows deviate from expected norms. Furthermore, the inherent difficulty of evaluating regression tasks with the human eye underscores additional limitations of purely synthetic training. To address these issues, we propose a novel correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep-learning outputs to improve local accuracy and achieve spatially consistent corrections. Our method comprises two stages: pre-processing raw ICESat-2 data, followed by a random forest-based approach to densely refine height estimates. Experiments in three representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal substantial error reductions, with mean absolute error (MAE) decreased by 22.8\%, 6.9\%, and 4.9\%, respectively. These findings highlight the critical role of shadow awareness in synthetic data-driven models and demonstrate how fusing imperfect real-world LiDAR data can bolster the robustness of MHE, paving the way for more reliable and scalable 3D mapping solutions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI</title>
<link>https://arxiv.org/abs/2505.06912</link>
<guid>https://arxiv.org/abs/2505.06912</guid>
<content:encoded><![CDATA[
arXiv:2505.06912v1 Announce Type: new 
Abstract: Despite strong performance in medical question-answering, the clinical adoption of Large Language Models (LLMs) is critically hampered by their opaque 'black-box' reasoning, limiting clinician trust. This challenge is compounded by the predominant reliance of current medical LLMs on corpora from scientific literature or synthetic data, which often lack the granular expert validation and high clinical relevance essential for advancing their specialized medical capabilities. To address these critical gaps, we introduce a highly clinically relevant dataset with 31,247 medical question-answer pairs, each accompanied by expert-validated chain-of-thought (CoT) explanations. This resource, spanning multiple clinical domains, was curated via a scalable human-LLM hybrid pipeline: LLM-generated rationales were iteratively reviewed, scored, and refined by medical experts against a structured rubric, with substandard outputs revised through human effort or guided LLM regeneration until expert consensus. This publicly available dataset provides a vital source for the development of medical LLMs that capable of transparent and verifiable reasoning, thereby advancing safer and more interpretable AI in medicine.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion</title>
<link>https://arxiv.org/abs/2505.06920</link>
<guid>https://arxiv.org/abs/2505.06920</guid>
<content:encoded><![CDATA[
arXiv:2505.06920v1 Announce Type: new 
Abstract: Acquiring accurately aligned multi-modal image pairs is fundamental for achieving high-quality multi-modal image fusion. To address the lack of ground truth in current multi-modal image registration and fusion methods, we propose a novel self-supervised \textbf{B}i-directional \textbf{S}elf-\textbf{R}egistration framework (\textbf{B-SR}). Specifically, B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator (IPDG) to achieve self-supervised global-local registration. Visible-infrared image pairs with spatially misaligned differences are aligned to obtain global differences through the registration module. The same image pairs are processed by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain local differences. IPDG converts the obtained local differences into pseudo-global differences, which are used to perform global-local difference consistency with the global differences. Furthermore, aiming at eliminating the effect of modal gaps on the registration module, we design a neighborhood dynamic alignment loss to achieve cross-modal image edge alignment. Extensive experiments on misaligned multi-modal images demonstrate the effectiveness of the proposed method in multi-modal image alignment and fusion against the competing methods. Our code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network</title>
<link>https://arxiv.org/abs/2505.06937</link>
<guid>https://arxiv.org/abs/2505.06937</guid>
<content:encoded><![CDATA[
arXiv:2505.06937v1 Announce Type: new 
Abstract: In this paper, the dual-optical attention fusion crowd head point counting model (TAPNet) is proposed to address the problem of the difficulty of accurate counting in complex scenes such as crowd dense occlusion and low light in crowd counting tasks under UAV view. The model designs a dual-optical attention fusion module (DAFP) by introducing complementary information from infrared images to improve the accuracy and robustness of all-day crowd counting. In order to fully utilize different modal information and solve the problem of inaccurate localization caused by systematic misalignment between image pairs, this paper also proposes an adaptive two-optical feature decomposition fusion module (AFDF). In addition, we optimize the training strategy to improve the model robustness through spatial random offset data augmentation. Experiments on two challenging public datasets, DroneRGBT and GAIIC2, show that the proposed method outperforms existing techniques in terms of performance, especially in challenging dense low-light scenes. Code is available at https://github.com/zz-zik/TAPNet
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning for Class Distribution Mismatch</title>
<link>https://arxiv.org/abs/2505.06948</link>
<guid>https://arxiv.org/abs/2505.06948</guid>
<content:encoded><![CDATA[
arXiv:2505.06948v1 Announce Type: new 
Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an "other" category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDM's superiority over previous semi-supervised methods. Specifically, with a 60% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.06951</link>
<guid>https://arxiv.org/abs/2505.06951</guid>
<content:encoded><![CDATA[
arXiv:2505.06951v1 Announce Type: new 
Abstract: In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.06975</link>
<guid>https://arxiv.org/abs/2505.06975</guid>
<content:encoded><![CDATA[
arXiv:2505.06975v1 Announce Type: new 
Abstract: The primary challenge in accelerating image super-resolution lies in reducing computation while maintaining performance and adaptability. Motivated by the observation that high-frequency regions (e.g., edges and textures) are most critical for reconstruction, we propose a training-free adaptive masking module for acceleration that dynamically focuses computation on these challenging areas. Specifically, our method first extracts high-frequency components via Gaussian blur subtraction and adaptively generates binary masks using K-means clustering to identify regions requiring intensive processing. Our method can be easily integrated with both CNNs and Transformers. For CNN-based architectures, we replace standard $3 \times 3$ convolutions with an unfold operation followed by $1 \times 1$ convolutions, enabling pixel-wise sparse computation guided by the mask. For Transformer-based models, we partition the mask into non-overlapping windows and selectively process tokens based on their average values. During inference, unnecessary pixels or windows are pruned, significantly reducing computation. Moreover, our method supports dilation-based mask adjustment to control the processing scope without retraining, and is robust to unseen degradations (e.g., noise, compression). Extensive experiments on benchmarks demonstrate that our method reduces FLOPs by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving comparable or better quantitative metrics. The source code is available at https://github.com/shangwei5/AMSR
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition</title>
<link>https://arxiv.org/abs/2505.06982</link>
<guid>https://arxiv.org/abs/2505.06982</guid>
<content:encoded><![CDATA[
arXiv:2505.06982v1 Announce Type: new 
Abstract: Recent progress in image-based medical disease detection encounters challenges such as limited annotated data sets, inadequate spatial feature analysis, data security issues, and inefficient training frameworks. This study introduces a data-efficient image transformer (DeIT)-based approach that overcomes these challenges by utilizing multiscale patch embedding for better feature extraction and stratified weighted random sampling to address class imbalance. The model also incorporates a LoRA-enhanced transformer encoder, a distillation framework, and federated learning for decentralized training, improving both efficiency and data security. Consequently, it achieves state-of-the-art performance, with the highest AUC, F1 score, precision, minimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations improve interpretability by highlighting critical pathological regions, enhancing the model's clinical relevance. These results highlight the potential of this approach to advance AI-powered medical imaging and disease detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation</title>
<link>https://arxiv.org/abs/2505.06985</link>
<guid>https://arxiv.org/abs/2505.06985</guid>
<content:encoded><![CDATA[
arXiv:2505.06985v1 Announce Type: new 
Abstract: Both zero-shot and tuning-based customized text-to-image (CT2I) generation have made significant progress for storytelling content creation. In contrast, research on customized text-to-video (CT2V) generation remains relatively limited. Existing zero-shot CT2V methods suffer from poor generalization, while another line of work directly combining tuning-based T2I models with temporal motion modules often leads to the loss of structural and texture information. To bridge this gap, we propose an autoregressive structure and texture propagation module (STPM), which extracts key structural and texture features from the reference subject and injects them autoregressively into each video frame to enhance consistency. Additionally, we introduce a test-time reward optimization (TTRO) method to further refine fine-grained details. Quantitative and qualitative experiments validate the effectiveness of STPM and TTRO, demonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency metrics over the baseline, respectively.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</title>
<link>https://arxiv.org/abs/2505.06991</link>
<guid>https://arxiv.org/abs/2505.06991</guid>
<content:encoded><![CDATA[
arXiv:2505.06991v1 Announce Type: new 
Abstract: This report presents our semantic segmentation framework developed by team ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which focuses on parsing outdoor scenes into nine semantic categories under real-world conditions. Our method integrates a Swin Transformer backbone enhanced with Rotary Position Embedding (RoPE) for improved spatial generalization, alongside a Color Shift Estimation-and-Correction module designed to compensate for illumination inconsistencies in natural environments. To further improve training stability, we adopt a quantile-based denoising strategy that downweights the top 2.5\% of highest-error pixels, treating them as noise and suppressing their influence during optimization. Evaluated on the official GOOSE test set, our approach achieved a mean Intersection over Union (mIoU) of 0.848, demonstrating the effectiveness of combining color correction, positional encoding, and error-aware denoising in robust semantic segmentation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.06995</link>
<guid>https://arxiv.org/abs/2505.06995</guid>
<content:encoded><![CDATA[
arXiv:2505.06995v1 Announce Type: new 
Abstract: Recent advancements in text-to-image diffusion models are hindered by high computational demands, limiting accessibility and scalability. This paper introduces KDC-Diff, a novel stable diffusion framework that enhances efficiency while maintaining image quality. KDC-Diff features a streamlined U-Net architecture with nearly half the parameters of the original U-Net (482M), significantly reducing model complexity. We propose a dual-layered distillation strategy to ensure high-fidelity generation, transferring semantic and structural insights from a teacher to a compact student model while minimizing quality degradation. Additionally, replay-based continual learning is integrated to mitigate catastrophic forgetting, allowing the model to retain prior knowledge while adapting to new data. Despite operating under extremely low computational resources, KDC-Diff achieves state-of-the-art performance on the Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly reduces inference time compared to existing models. These results establish KDC-Diff as a highly efficient and adaptable solution for text-to-image generation, particularly in computationally constrained environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07001</link>
<guid>https://arxiv.org/abs/2505.07001</guid>
<content:encoded><![CDATA[
arXiv:2505.07001v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination--the tendency to generate descriptions that are inconsistent with the visual content--remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: https://github.com/bhattarailab/Hallucination-Aware-VLM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation</title>
<link>https://arxiv.org/abs/2505.07003</link>
<guid>https://arxiv.org/abs/2505.07003</guid>
<content:encoded><![CDATA[
arXiv:2505.07003v1 Announce Type: new 
Abstract: Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</title>
<link>https://arxiv.org/abs/2505.07007</link>
<guid>https://arxiv.org/abs/2505.07007</guid>
<content:encoded><![CDATA[
arXiv:2505.07007v1 Announce Type: new 
Abstract: Micro-expressions (MEs) are crucial psychological responses with significant potential for affective computing. However, current automatic micro-expression recognition (MER) research primarily focuses on discrete emotion classification, neglecting a convincing analysis of the subtle dynamic movements and inherent emotional cues. The rapid progress in multimodal large language models (MLLMs), known for their strong multimodal comprehension and language generation abilities, offers new possibilities. MLLMs have shown success in various vision-language tasks, indicating their potential to understand MEs comprehensively, including both fine-grained motion patterns and underlying emotional semantics. Nevertheless, challenges remain due to the subtle intensity and short duration of MEs, as existing MLLMs are not designed to capture such delicate frame-level facial dynamics. In this paper, we propose a novel Micro-Expression Large Language Model (MELLM), which incorporates a subtle facial motion perception strategy with the strong inference capabilities of MLLMs, representing the first exploration of MLLMs in the domain of ME analysis. Specifically, to explicitly guide the MLLM toward motion-sensitive regions, we construct an interpretable motion-enhanced color map by fusing onset-apex optical flow dynamics with the corresponding grayscale onset frame as the model input. Additionally, specialized fine-tuning strategies are incorporated to further enhance the model's visual perception of MEs. Furthermore, we construct an instruction-description dataset based on Facial Action Coding System (FACS) annotations and emotion labels to train our MELLM. Comprehensive evaluations across multiple benchmark datasets demonstrate that our model exhibits superior robustness and generalization capabilities in ME understanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization</title>
<link>https://arxiv.org/abs/2505.07013</link>
<guid>https://arxiv.org/abs/2505.07013</guid>
<content:encoded><![CDATA[
arXiv:2505.07013v1 Announce Type: new 
Abstract: Remote physiological sensing using camera-based technologies offers transformative potential for non-invasive vital sign monitoring across healthcare and human-computer interaction domains. Although deep learning approaches have advanced the extraction of physiological signals from video data, existing methods have not been sufficiently assessed for their robustness to domain shifts. These shifts in remote physiological sensing include variations in ambient conditions, camera specifications, head movements, facial poses, and physiological states which often impact real-world performance significantly. Cross-dataset evaluation provides an objective measure to assess generalization capabilities across these domain shifts. We introduce Target Signal Constrained Factorization module (TSFM), a novel multidimensional attention mechanism that explicitly incorporates physiological signal characteristics as factorization constraints, allowing more precise feature extraction. Building on this innovation, we present MMRPhys, an efficient dual-branch 3D-CNN architecture designed for simultaneous multitask estimation of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly outperforms state-of-the-art methods in generalization across domain shifts for rPPG and rRSP estimation, while maintaining a minimal inference latency suitable for real-time applications. Our approach establishes new benchmarks for robust multitask and multimodal physiological sensing and offers a computationally efficient framework for practical deployment in unconstrained environments. The web browser-based application featuring on-device real-time inference of MMRPhys model is available at https://physiologicailab.github.io/mmrphys-live
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision-Language Foundation Model for Leaf Disease Identification</title>
<link>https://arxiv.org/abs/2505.07019</link>
<guid>https://arxiv.org/abs/2505.07019</guid>
<content:encoded><![CDATA[
arXiv:2505.07019v1 Announce Type: new 
Abstract: Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD's effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at https://huggingface.co/enalis/scold
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MarkMatch: Same-Hand Stuffing Detection</title>
<link>https://arxiv.org/abs/2505.07032</link>
<guid>https://arxiv.org/abs/2505.07032</guid>
<content:encoded><![CDATA[
arXiv:2505.07032v1 Announce Type: new 
Abstract: We present MarkMatch, a retrieval system for detecting whether two paper ballot marks were filled by the same hand. Unlike the previous SOTA method BubbleSig, which used binary classification on isolated mark pairs, MarkMatch ranks stylistic similarity between a query mark and a mark in the database using contrastive learning. Our model is trained with a dense batch similarity matrix and a dual loss objective. Each sample is contrasted against many negatives within each batch, enabling the model to learn subtle handwriting difference and improve generalization under handwriting variation and visual noise, while diagonal supervision reinforces high confidence on true matches. The model achieves an F1 score of 0.943, surpassing BubbleSig's best performance. MarkMatch also integrates Segment Anything Model for flexible mark extraction via box- or point-based prompts. The system offers election auditors a practical tool for visual, non-biometric investigation of suspicious ballots.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection</title>
<link>https://arxiv.org/abs/2505.07040</link>
<guid>https://arxiv.org/abs/2505.07040</guid>
<content:encoded><![CDATA[
arXiv:2505.07040v1 Announce Type: new 
Abstract: Fabric defect detection confronts two fundamental challenges. First, conventional non-maximum suppression disrupts gradient flow, which hinders genuine end-to-end learning. Second, acquiring pixel-level annotations at industrial scale is prohibitively costly. Addressing these limitations, we propose a differentiable NMS framework for fabric defect detection that achieves superior localization precision through end-to-end optimization. We reformulate NMS as a differentiable bipartite matching problem solved through the Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow throughout the network. This approach specifically targets the irregular morphologies and ambiguous boundaries of fabric defects by integrating proposal quality, feature similarity, and spatial relationships. Our entropy-constrained mask refinement mechanism further enhances localization precision through principled uncertainty modeling. Extensive experiments on the Tianchi fabric defect dataset demonstrate significant performance improvements over existing methods while maintaining real-time speeds suitable for industrial deployment. The framework exhibits remarkable adaptability across different architectures and generalizes effectively to general object detection tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.07050</link>
<guid>https://arxiv.org/abs/2505.07050</guid>
<content:encoded><![CDATA[
arXiv:2505.07050v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation (UDA) aims to align source and target domain distributions to close the domain gap, but still struggles with obtaining the target data. Fortunately, Domain Generalization (DG) excels without the need for any target data. Recent works expose that depth maps contribute to improved generalized performance in the UDA tasks, but they ignore the noise and holes in depth maps due to device and environmental factors, failing to sufficiently and effectively learn domain-invariant representation. Although high-sensitivity region suppression has shown promising results in learning domain-invariant features, existing methods cannot be directly applicable to depth maps due to their unique characteristics. Hence, we propose a novel framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal stylization flow (DSSS), focusing on learning domain-invariant features from depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D inter-modal stylization flow to generate stylized depth maps for sensitivity detection, cleverly utilizing RGB information as the stylization source. Then, a class-wise soft spatial sensitivity suppression is designed to identify and emphasize non-sensitive depth features that contain more domain-invariant information. Furthermore, an RGB-D soft alignment loss is proposed to ensure that the stylized depth maps only align part of the RGB features while still retaining the unique depth information. To our best knowledge, our DSSS framework is the first work to integrate RGB and Depth information in the multi-class DG semantic segmentation task. Extensive experiments over multiple backbone networks show that our framework achieves remarkable performance improvement.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.07057</link>
<guid>https://arxiv.org/abs/2505.07057</guid>
<content:encoded><![CDATA[
arXiv:2505.07057v1 Announce Type: new 
Abstract: Video generation based on diffusion models presents a challenging multimodal task, with video editing emerging as a pivotal direction in this field. Recent video editing approaches primarily fall into two categories: training-required and training-free methods. While training-based methods incur high computational costs, training-free alternatives often yield suboptimal performance. To address these limitations, we propose DAPE, a high-quality yet cost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for video editing. In the first stage, we design an efficient norm-tuning method to enhance temporal consistency in generated videos. The second stage introduces a vision-friendly adapter to improve visual quality. Additionally, we identify critical shortcomings in existing benchmarks, including limited category diversity, imbalanced object distribution, and inconsistent frame counts. To mitigate these issues, we curate a large dataset benchmark comprising 232 videos with rich annotations and 6 editing prompts, enabling objective and comprehensive evaluation of advanced methods. Extensive experiments on existing datasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate that DAPE significantly improves temporal coherence and text-video alignment while outperforming previous state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed1.5-VL Technical Report</title>
<link>https://arxiv.org/abs/2505.07062</link>
<guid>https://arxiv.org/abs/2505.07062</guid>
<content:encoded><![CDATA[
arXiv:2505.07062v1 Announce Type: new 
Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.07071</link>
<guid>https://arxiv.org/abs/2505.07071</guid>
<content:encoded><![CDATA[
arXiv:2505.07071v1 Announce Type: new 
Abstract: Diffusion-based image super-resolution (SR) methods have demonstrated remarkable performance. Recent advancements have introduced deterministic sampling processes that reduce inference from 15 iterative steps to a single step, thereby significantly improving the inference speed of existing diffusion models. However, their efficiency remains limited when handling complex semantic regions due to the single-step inference. To address this limitation, we propose SAMSR, a semantic-guided diffusion framework that incorporates semantic segmentation masks into the sampling process. Specifically, we introduce the SAM-Noise Module, which refines Gaussian noise using segmentation masks to preserve spatial and semantic features. Furthermore, we develop a pixel-wise sampling strategy that dynamically adjusts the residual transfer rate and noise strength based on pixel-level semantic weights, prioritizing semantically rich regions during the diffusion process. To enhance model training, we also propose a semantic consistency loss, which aligns pixel-wise semantic weights between predictions and ground truth. Extensive experiments on both real-world and synthetic datasets demonstrate that SAMSR significantly improves perceptual quality and detail recovery, particularly in semantically complex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering</title>
<link>https://arxiv.org/abs/2505.07073</link>
<guid>https://arxiv.org/abs/2505.07073</guid>
<content:encoded><![CDATA[
arXiv:2505.07073v1 Announce Type: new 
Abstract: Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. Recently, the Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework, introduced by Varshney et al. (2025), attempts to identify concepts via dimension-wise traversal of the latent space of a Variational Autoencoder trained on counterfactual trajectories. Extending the CDCT framework, this work introduces Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC substantially reduces computational complexity by eliminating the exhaustive latent dimension traversal required in CDCT and enables the extraction of multidimensional semantic concepts encoded across the latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression</title>
<link>https://arxiv.org/abs/2505.07119</link>
<guid>https://arxiv.org/abs/2505.07119</guid>
<content:encoded><![CDATA[
arXiv:2505.07119v1 Announce Type: new 
Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where minimizing waste and operational costs is essential. Deploying deep learning models within Internet of Things (IoT) environments introduces specific challenges due to the limited computational power and bandwidth of edge devices. This study investigates how to perform VAD effectively under such constraints by leveraging compact and efficient processing strategies. We evaluate several data compression techniques, examining the trade-off between system latency and detection accuracy. Experiments on the MVTec AD benchmark demonstrate that significant compression can be achieved with minimal loss in anomaly detection performance compared to uncompressed data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework</title>
<link>https://arxiv.org/abs/2505.07165</link>
<guid>https://arxiv.org/abs/2505.07165</guid>
<content:encoded><![CDATA[
arXiv:2505.07165v1 Announce Type: new 
Abstract: Recently, numerous pancreas segmentation methods have achieved promising performance on local single-source datasets. However, these methods don't adequately account for generalizability issues, and hence typically show limited performance and low stability on test data from other sources. Considering the limited availability of distinct data sources, we seek to improve the generalization performance of a pancreas segmentation model trained with a single-source dataset, i.e., the single source generalization task. In particular, we propose a dual self-supervised learning model that incorporates both global and local anatomical contexts. Our model aims to fully exploit the anatomical features of the intra-pancreatic and extra-pancreatic regions, and hence enhance the characterization of the high-uncertainty regions for more robust generalization. Specifically, we first construct a global-feature contrastive self-supervised learning module that is guided by the pancreatic spatial structure. This module obtains complete and consistent pancreatic features through promoting intra-class cohesion, and also extracts more discriminative features for differentiating between pancreatic and non-pancreatic tissues through maximizing inter-class separation. It mitigates the influence of surrounding tissue on the segmentation outcomes in high-uncertainty regions. Subsequently, a local-image restoration self-supervised learning module is introduced to further enhance the characterization of the high uncertainty regions. In this module, informative anatomical contexts are actually learned to recover randomly corrupted appearance patterns in those regions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.07172</link>
<guid>https://arxiv.org/abs/2505.07172</guid>
<content:encoded><![CDATA[
arXiv:2505.07172v1 Announce Type: new 
Abstract: Despite significant advancements in multimodal reasoning tasks, existing Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded responses when interpreting associated images. In contrast, when humans embark on learning new knowledge, they often rely on a set of fundamental pre-study principles: reviewing outlines to grasp core concepts, summarizing key points to guide their focus and enhance understanding. However, such preparatory actions are notably absent in the current instruction tuning processes. This paper presents Re-Critic, an easily scalable rationale-augmented framework designed to incorporate fundamental rules and chain-of-thought (CoT) as a bridge to enhance reasoning abilities. Specifically, Re-Critic develops a visual rationale synthesizer that scalably augments raw instructions with rationale explanation. To probe more contextually grounded responses, Re-Critic employs an in-context self-critic mechanism to select response pairs for preference tuning. Experiments demonstrate that models fine-tuned with our rationale-augmented dataset yield gains that extend beyond hallucination-specific tasks to broader multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranking-aware Continual Learning for LiDAR Place Recognition</title>
<link>https://arxiv.org/abs/2505.07198</link>
<guid>https://arxiv.org/abs/2505.07198</guid>
<content:encoded><![CDATA[
arXiv:2505.07198v1 Announce Type: new 
Abstract: Place recognition plays a significant role in SLAM, robot navigation, and autonomous driving applications. Benefiting from deep learning, the performance of LiDAR place recognition (LPR) has been greatly improved. However, many existing learning-based LPR methods suffer from catastrophic forgetting, which severely harms the performance of LPR on previously trained places after training on a new environment. In this paper, we introduce a continual learning framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate forgetting. Inspired by the ranking process of place recognition retrieval, we present a ranking-aware knowledge distillation loss that encourages the network to preserve the high-level place recognition knowledge. We also introduce a knowledge fusion module to integrate the knowledge of old and new models for LiDAR place recognition. Our extensive experiments demonstrate that KDF can be applied to different networks to overcome catastrophic forgetting, surpassing the state-of-the-art methods in terms of mean Recall@1 and forgetting score.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2505.07209</link>
<guid>https://arxiv.org/abs/2505.07209</guid>
<content:encoded><![CDATA[
arXiv:2505.07209v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) try to make the decision-making process transparent by exploring an intermediate concept space between the input image and the output prediction. Existing CBMs just learn coarse-grained relations between the whole image and the concepts, less considering local image information, leading to two main drawbacks: i) they often produce spurious visual-concept relations, hence decreasing model reliability; and ii) though CBMs could explain the importance of every concept to the final prediction, it is still challenging to tell which visual region produces the prediction. To solve these problems, this paper proposes a Disentangled Optimal Transport CBM (DOT-CBM) framework to explore fine-grained visual-concept relations between local image patches and concepts. Specifically, we model the concept prediction process as a transportation problem between the patches and concepts, thereby achieving explicit fine-grained feature alignment. We also incorporate orthogonal projection losses within the modality to enhance local feature disentanglement. To further address the shortcut issues caused by statistical biases in the data, we utilize the visual saliency map and concept label statistics as transportation priors. Thus, DOT-CBM can visualize inversion heatmaps, provide more reliable concept predictions, and produce more accurate class predictions. Comprehensive experiments demonstrate that our proposed DOT-CBM achieves SOTA performance on several tasks, including image classification, local part detection and out-of-distribution generalization.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection</title>
<link>https://arxiv.org/abs/2505.07219</link>
<guid>https://arxiv.org/abs/2505.07219</guid>
<content:encoded><![CDATA[
arXiv:2505.07219v1 Announce Type: new 
Abstract: Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at https://github.com/qinhongda8/LDDS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dance Video Archives Challenge Computer Vision</title>
<link>https://arxiv.org/abs/2505.07249</link>
<guid>https://arxiv.org/abs/2505.07249</guid>
<content:encoded><![CDATA[
arXiv:2505.07249v1 Announce Type: new 
Abstract: The accuracy and efficiency of human body pose estimation depend on the quality of the data to be processed and of the particularities of these data. To demonstrate how dance videos can challenge pose estimation techniques, we proposed a new 3D human body pose estimation pipeline which combined up-to-date techniques and methods that had not been yet used in dance analysis. Second, we performed tests and extensive experimentations from dance video archives, and used visual analytic tools to evaluate the impact of several data parameters on human body pose. Our results are publicly available for research at https://www.couleur.org/articles/arXiv-1-2025/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incomplete In-context Learning</title>
<link>https://arxiv.org/abs/2505.07251</link>
<guid>https://arxiv.org/abs/2505.07251</guid>
<content:encoded><![CDATA[
arXiv:2505.07251v1 Announce Type: new 
Abstract: Large vision language models (LVLMs) achieve remarkable performance through Vision In-context Learning (VICL), a process that depends significantly on demonstrations retrieved from an extensive collection of annotated examples (retrieval database). Existing studies often assume that the retrieval database contains annotated examples for all labels. However, in real-world scenarios, delays in database updates or incomplete data annotation may result in the retrieval database containing labeled samples for only a subset of classes. We refer to this phenomenon as an \textbf{incomplete retrieval database} and define the in-context learning under this condition as \textbf{Incomplete In-context Learning (IICL)}. To address this challenge, we propose \textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage framework designed to mitigate the limitations of IICL. The Iterative Judgments Stage reformulates an \(\boldsymbol{m}\)-class classification problem into a series of \(\boldsymbol{m}\) binary classification tasks, effectively converting the IICL setting into a standard VICL scenario. The Integrated Prediction Stage further refines the classification process by leveraging both the input image and the predictions from the Iterative Judgments Stage to enhance overall classification accuracy. IJIP demonstrates considerable performance across two LVLMs and two datasets under three distinct conditions of label incompleteness, achieving the highest accuracy of 93.9\%. Notably, even in scenarios where labels are fully available, IJIP still achieves the best performance of all six baselines. Furthermore, IJIP can be directly applied to \textbf{Prompt Learning} and is adaptable to the \textbf{text domain}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2505.07254</link>
<guid>https://arxiv.org/abs/2505.07254</guid>
<content:encoded><![CDATA[
arXiv:2505.07254v1 Announce Type: new 
Abstract: This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and 0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\% in higher order tracking accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Similarity Search in Automotive Production</title>
<link>https://arxiv.org/abs/2505.07256</link>
<guid>https://arxiv.org/abs/2505.07256</guid>
<content:encoded><![CDATA[
arXiv:2505.07256v1 Announce Type: new 
Abstract: Visual quality inspection in automotive production is essential for ensuring the safety and reliability of vehicles. Computer vision (CV) has become a popular solution for these inspections due to its cost-effectiveness and reliability. However, CV models require large, annotated datasets, which are costly and time-consuming to collect. To reduce the need for extensive training data, we propose a novel image classification pipeline that combines similarity search using a vision-based foundation model with synthetic data. Our approach leverages a DINOv2 model to transform input images into feature vectors, which are then compared to pre-classified reference images using cosine distance measurements. By utilizing synthetic data instead of real images as references, our pipeline achieves high classification accuracy without relying on real data. We evaluate this approach in eight real-world inspection scenarios and demonstrate that it meets the high performance requirements of production environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.07263</link>
<guid>https://arxiv.org/abs/2505.07263</guid>
<content:encoded><![CDATA[
arXiv:2505.07263v1 Announce Type: new 
Abstract: We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers</title>
<link>https://arxiv.org/abs/2505.07300</link>
<guid>https://arxiv.org/abs/2505.07300</guid>
<content:encoded><![CDATA[
arXiv:2505.07300v1 Announce Type: new 
Abstract: Training-free Neural Architecture Search (NAS) efficiently identifies high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the need for model training, and (ii) interpretable, with proxy designs often theoretically grounded. Despite rapid developments in the field, current SOTA ZC proxies are typically constrained to well-established convolutional search spaces. With the rise of Large Language Models shaping the future of deep learning, this work extends ZC proxy applicability to Vision Transformers (ViTs). We present a new benchmark using the Autoformer search space evaluated on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients information (L-SWAG), a novel, generalizable metric that characterizes both convolutional and transformer architectures across 14 tasks. Additionally, previous works highlighted how different proxies contain complementary information, motivating the need for a ML model to identify useful combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low Information gain and Bias Re-Alignment), a method that strategically combines proxies to best represent a specific benchmark. Integrated into the NAS search, LIBRA-NAS outperforms evolution and gradient-based NAS techniques by identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1 GPU days.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
<link>https://arxiv.org/abs/2505.07301</link>
<guid>https://arxiv.org/abs/2505.07301</guid>
<content:encoded><![CDATA[
arXiv:2505.07301v1 Announce Type: new 
Abstract: In 3D Human Motion Prediction (HMP), conventional methods train HMP models with expensive motion capture data. However, the data collection cost of such motion capture data limits the data diversity, which leads to poor generalizability to unseen motions or subjects. To address this issue, this paper proposes to enhance HMP with additional learning using estimated poses from easily available videos. The 2D poses estimated from the monocular videos are carefully transformed into motion capture-style 3D motions through our pipeline. By additional learning with the obtained motions, the HMP model is adapted to the test domain. The experimental results demonstrate the quantitative and qualitative impact of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Privacy-Aware AI-Based Ergonomic Analysis</title>
<link>https://arxiv.org/abs/2505.07306</link>
<guid>https://arxiv.org/abs/2505.07306</guid>
<content:encoded><![CDATA[
arXiv:2505.07306v1 Announce Type: new 
Abstract: Musculoskeletal disorders (MSDs) are a leading cause of injury and productivity loss in the manufacturing industry, incurring substantial economic costs. Ergonomic assessments can mitigate these risks by identifying workplace adjustments that improve posture and reduce strain. Camera-based systems offer a non-intrusive, cost-effective method for continuous ergonomic tracking, but they also raise significant privacy concerns. To address this, we propose a privacy-aware ergonomic assessment framework utilizing machine learning techniques. Our approach employs adversarial training to develop a lightweight neural network that obfuscates video data, preserving only the essential information needed for human pose estimation. This obfuscation ensures compatibility with standard pose estimation algorithms, maintaining high accuracy while protecting privacy. The obfuscated video data is transmitted to a central server, where state-of-the-art keypoint detection algorithms extract body landmarks. Using multi-view integration, 3D keypoints are reconstructed and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system provides a secure, effective solution for ergonomic monitoring in industrial environments, addressing both privacy and workplace safety concerns.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning</title>
<link>https://arxiv.org/abs/2505.07322</link>
<guid>https://arxiv.org/abs/2505.07322</guid>
<content:encoded><![CDATA[
arXiv:2505.07322v1 Announce Type: new 
Abstract: High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming increasingly prevalent, intensifying the demand for converting Standard Dynamic Range (SDR) content to HDR. Existing methods primarily rely on fixed tone mapping operators, which are inadequate for handling SDR inputs with diverse styles commonly found in real-world scenarios. To address this challenge, we propose a generalized SDR-to-HDR method that handles diverse styles in real-world SDR content, termed Realistic Style Disentangled Representation Learning (RealRep). By disentangling luminance and chrominance, we analyze the intrinsic differences between contents with varying styles and propose a disentangled multi-view style representation learning method. This approach captures the guidance prior of true luminance and chrominance distributions across different styles, even when the SDR style distributions exhibit significant variations, thereby establishing a robust embedding space for inverse tone mapping. Motivated by the difficulty of directly utilizing degradation representation priors, we further introduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a two-stage framework that performs adaptive hierarchical mapping guided by a control-aware normalization mechanism. DDACMNet dynamically modulates the mapping process via degradation-conditioned hierarchical features, enabling robust adaptation across diverse degradation domains. Extensive experiments show that RealRep consistently outperforms state-of-the-art methods with superior generalization and perceptually faithful HDR color gamut reconstruction.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video</title>
<link>https://arxiv.org/abs/2505.07333</link>
<guid>https://arxiv.org/abs/2505.07333</guid>
<content:encoded><![CDATA[
arXiv:2505.07333v1 Announce Type: new 
Abstract: Fast 3D clothed human reconstruction from monocular video remains a significant challenge in computer vision, particularly in balancing computational efficiency with reconstruction quality. Current approaches are either focused on static image reconstruction but too computationally intensive, or achieve high quality through per-video optimization that requires minutes to hours of processing, making them unsuitable for real-time applications. To this end, we present TemPoFast3D, a novel method that leverages temporal coherency of human appearance to reduce redundant computation while maintaining reconstruction quality. Our approach is a "plug-and play" solution that uniquely transforms pixel-aligned reconstruction networks to handle continuous video streams by maintaining and refining a canonical appearance representation through efficient coordinate mapping. Extensive experiments demonstrate that TemPoFast3D matches or exceeds state-of-the-art methods across standard metrics while providing high-quality textured reconstruction across diverse pose and appearance, with a maximum speed of 12 FPS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction</title>
<link>https://arxiv.org/abs/2505.07336</link>
<guid>https://arxiv.org/abs/2505.07336</guid>
<content:encoded><![CDATA[
arXiv:2505.07336v1 Announce Type: new 
Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Pre-trained Autoregressive Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.07344</link>
<guid>https://arxiv.org/abs/2505.07344</guid>
<content:encoded><![CDATA[
arXiv:2505.07344v1 Announce Type: new 
Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography</title>
<link>https://arxiv.org/abs/2505.07347</link>
<guid>https://arxiv.org/abs/2505.07347</guid>
<content:encoded><![CDATA[
arXiv:2505.07347v1 Announce Type: new 
Abstract: Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practicality for timely diagnosis and monitoring of pulmonary hypertension progression. Here, we propose MePH, a multi-view, multi-modal vision-language model to accurately assess pulmonary hypertension progression using non-invasive echocardiography. We constructed a large dataset comprising paired standardized echocardiogram videos, spectral images and RHC data, covering 1,237 patient cases from 12 medical centers. For the first time, MePH precisely models the correlation between non-invasive multi-view, multi-modal echocardiography and the pressure and resistance obtained via RHC. We show that MePH significantly outperforms echocardiographers' assessments using echocardiography, reducing the mean absolute error in estimating mean pulmonary arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and 43.81%, respectively. In eight independent external hospitals, MePH achieved a mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an area under the curve of 0.921, surpassing echocardiographers (area under the curve of 0.842) in accurately predicting the severity of pulmonary hypertension, whether mild or severe. A prospective study demonstrated that MePH can predict treatment efficacy for patients. Our work provides pulmonary hypertension patients with a non-invasive and timely method for monitoring disease progression, improving the accuracy and efficiency of pulmonary hypertension management while enabling earlier interventions and more personalized treatment decisions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild</title>
<link>https://arxiv.org/abs/2505.07373</link>
<guid>https://arxiv.org/abs/2505.07373</guid>
<content:encoded><![CDATA[
arXiv:2505.07373v1 Announce Type: new 
Abstract: Neural implicit surface reconstruction using volume rendering techniques has recently achieved significant advancements in creating high-fidelity surfaces from multiple 2D images. However, current methods primarily target scenes with consistent illumination and struggle to accurately reconstruct 3D geometry in uncontrolled environments with transient occlusions or varying appearances. While some neural radiance field (NeRF)-based variants can better manage photometric variations and transient objects in complex scenes, they are designed for novel view synthesis rather than precise surface reconstruction due to limited surface constraints. To overcome this limitation, we introduce a novel approach that applies multiple geometric constraints to the implicit surface optimization process, enabling more accurate reconstructions from unconstrained image collections. First, we utilize sparse 3D points from structure-from-motion (SfM) to refine the signed distance function estimation for the reconstructed surface, with a displacement compensation to accommodate noise in the sparse points. Additionally, we employ robust normal priors derived from a normal predictor, enhanced by edge prior filtering and multi-view consistency constraints, to improve alignment with the actual surface geometry. Extensive testing on the Heritage-Recon benchmark and other datasets has shown that the proposed method can accurately reconstruct surfaces from in-the-wild images, yielding geometries with superior accuracy and granularity compared to existing techniques. Our approach enables high-quality 3D reconstruction of various landmarks, making it applicable to diverse scenarios such as digital preservation of cultural heritage sites.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.07375</link>
<guid>https://arxiv.org/abs/2505.07375</guid>
<content:encoded><![CDATA[
arXiv:2505.07375v1 Announce Type: new 
Abstract: Point cloud anomaly detection is essential for various industrial applications. The huge computation and storage costs caused by the increasing product classes limit the application of single-class unsupervised methods, necessitating the development of multi-class unsupervised methods. However, the feature similarity between normal and anomalous points from different class data leads to the feature confusion problem, which greatly hinders the performance of multi-class methods. Therefore, we introduce a multi-class point cloud anomaly detection method, named GLFM, leveraging global-local feature matching to progressively separate data that are prone to confusion across multiple classes. Specifically, GLFM is structured into three stages: Stage-I proposes an anomaly synthesis pipeline that stretches point clouds to create abundant anomaly data that are utilized to adapt the point cloud feature extractor for better feature representation. Stage-II establishes the global and local memory banks according to the global and local feature distributions of all the training data, weakening the impact of feature confusion on the establishment of the memory bank. Stage-III implements anomaly detection of test data leveraging its feature distance from global and local memory banks. Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts dataset showcase our proposed GLFM's superior point cloud anomaly detection performance. The code is available at https://github.com/hustCYQ/GLFM-Multi-class-3DAD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications</title>
<link>https://arxiv.org/abs/2505.07380</link>
<guid>https://arxiv.org/abs/2505.07380</guid>
<content:encoded><![CDATA[
arXiv:2505.07380v1 Announce Type: new 
Abstract: iPhone portrait-mode images contain a distinctive pattern in out-of-focus regions simulating the bokeh effect, which we term Apple's Synthetic Defocus Noise Pattern (SDNP). If overlooked, this pattern can interfere with blind forensic analyses, especially PRNU-based camera source verification, as noted in earlier works. Since Apple's SDNP remains underexplored, we provide a detailed characterization, proposing a method for its precise estimation, modeling its dependence on scene brightness, ISO settings, and other factors. Leveraging this characterization, we explore forensic applications of the SDNP, including traceability of portrait-mode images across iPhone models and iOS versions in open-set scenarios, assessing its robustness under post-processing. Furthermore, we show that masking SDNP-affected regions in PRNU-based camera source verification significantly reduces false positives, overcoming a critical limitation in camera attribution, and improving state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Semantic Encoding and Decoding for Video Surveillance</title>
<link>https://arxiv.org/abs/2505.07381</link>
<guid>https://arxiv.org/abs/2505.07381</guid>
<content:encoded><![CDATA[
arXiv:2505.07381v1 Announce Type: new 
Abstract: With the continuous increase in the number and resolution of video surveillance cameras, the burden of transmitting and storing surveillance video is growing. Traditional communication methods based on Shannon's theory are facing optimization bottlenecks. Semantic communication, as an emerging communication method, is expected to break through this bottleneck and reduce the storage and transmission consumption of video. Existing semantic decoding methods often require many samples to train the neural network for each scene, which is time-consuming and labor-intensive. In this study, a semantic encoding and decoding method for surveillance video is proposed. First, the sketch was extracted as semantic information, and a sketch compression method was proposed to reduce the bit rate of semantic information. Then, an image translation network was proposed to translate the sketch into a video frame with a reference frame. Finally, a few-shot sketch decoding network was proposed to reconstruct video from sketch. Experimental results showed that the proposed method achieved significantly better video reconstruction performance than baseline methods. The sketch compression method could effectively reduce the storage and transmission consumption of semantic information with little compromise on video quality. The proposed method provides a novel semantic encoding and decoding method that only needs a few training samples for each surveillance scene, thus improving the practicality of the semantic communication system.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Visualization in 3D Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2505.07387</link>
<guid>https://arxiv.org/abs/2505.07387</guid>
<content:encoded><![CDATA[
arXiv:2505.07387v1 Announce Type: new 
Abstract: Understanding the computations of convolutional neural networks requires effective visualization of their kernels. While maximal activation methods have proven successful in highlighting the preferred features of 2D convolutional kernels, directly applying these techniques to 3D convolutions often leads to uninterpretable results due to the higher dimensionality and complexity of 3D features. To address this challenge, we propose a novel visualization approach for 3D convolutional kernels that disentangles their texture and motion preferences. Our method begins with a data-driven decomposition of the optimal input that maximally activates a given kernel. We then introduce a two-stage optimization strategy to extract distinct texture and motion components from this input. Applying our approach to visualize kernels at various depths of several pre-trained models, we find that the resulting visualizations--particularly those capturing motion--clearly reveal the preferred dynamic patterns encoded by 3D kernels. These results demonstrate the effectiveness of our method in providing interpretable insights into 3D convolutional operations. Code is available at https://github.com/YatangLiLab/3DKernelVisualizer.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.07396</link>
<guid>https://arxiv.org/abs/2505.07396</guid>
<content:encoded><![CDATA[
arXiv:2505.07396v1 Announce Type: new 
Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.07398</link>
<guid>https://arxiv.org/abs/2505.07398</guid>
<content:encoded><![CDATA[
arXiv:2505.07398v1 Announce Type: new 
Abstract: State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture</title>
<link>https://arxiv.org/abs/2505.07444</link>
<guid>https://arxiv.org/abs/2505.07444</guid>
<content:encoded><![CDATA[
arXiv:2505.07444v1 Announce Type: new 
Abstract: Efficient crop-weed segmentation is critical for site-specific weed control in precision agriculture. Conventional CNN-based methods struggle to generalize and rely on RGB imagery, limiting performance under complex field conditions. To address these challenges, we propose a lightweight transformer-CNN hybrid. It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using specialized encoders and dynamic modality integration. Evaluated on the WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7 million parameters, the model offers high accuracy, computational efficiency, and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and edge devices, advancing precision weed management.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing degeneracies in latent interpolation for diffusion models</title>
<link>https://arxiv.org/abs/2505.07481</link>
<guid>https://arxiv.org/abs/2505.07481</guid>
<content:encoded><![CDATA[
arXiv:2505.07481v1 Announce Type: new 
Abstract: There is an increasing interest in using image-generating diffusion models for deep data augmentation and image morphing. In this context, it is useful to interpolate between latents produced by inverting a set of input images, in order to generate new images representing some mixture of the inputs. We observe that such interpolation can easily lead to degenerate results when the number of inputs is large. We analyze the cause of this effect theoretically and experimentally, and suggest a suitable remedy. The suggested approach is a relatively simple normalization scheme that is easy to use whenever interpolation between latents is needed. We measure image quality using FID and CLIP embedding distance and show experimentally that baseline interpolation methods lead to a drop in quality metrics long before the degeneration issue is clearly visible. In contrast, our method significantly reduces the degeneration effect and leads to improved quality metrics also in non-degenerate situations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocVXQA: Context-Aware Visual Explanations for Document Question Answering</title>
<link>https://arxiv.org/abs/2505.07496</link>
<guid>https://arxiv.org/abs/2505.07496</guid>
<content:encoded><![CDATA[
arXiv:2505.07496v1 Announce Type: new 
Abstract: We propose DocVXQA, a novel framework for visually self-explainable document question answering. The framework is designed not only to produce accurate answers to questions but also to learn visual heatmaps that highlight contextually critical regions, thereby offering interpretable justifications for the model's decisions. To integrate explanations into the learning process, we quantitatively formulate explainability principles as explicit learning objectives. Unlike conventional methods that emphasize only the regions pertinent to the answer, our framework delivers explanations that are \textit{contextually sufficient} while remaining \textit{representation-efficient}. This fosters user trust while achieving a balance between predictive performance and interpretability in DocVQA applications. Extensive experiments, including human evaluation, provide strong evidence supporting the effectiveness of our method. The code is available at https://github.com/dali92002/DocVXQA.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models</title>
<link>https://arxiv.org/abs/2505.07500</link>
<guid>https://arxiv.org/abs/2505.07500</guid>
<content:encoded><![CDATA[
arXiv:2505.07500v1 Announce Type: new 
Abstract: The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as "bring me a spoon", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAIS: Memory-Attention for Interactive Segmentation</title>
<link>https://arxiv.org/abs/2505.07511</link>
<guid>https://arxiv.org/abs/2505.07511</guid>
<content:encoded><![CDATA[
arXiv:2505.07511v1 Announce Type: new 
Abstract: Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images</title>
<link>https://arxiv.org/abs/2505.07530</link>
<guid>https://arxiv.org/abs/2505.07530</guid>
<content:encoded><![CDATA[
arXiv:2505.07530v1 Announce Type: new 
Abstract: Synthetic face datasets are increasingly used to overcome the limitations of real-world biometric data, including privacy concerns, demographic imbalance, and high collection costs. However, many existing methods lack fine-grained control over identity attributes and fail to produce paired, identity-consistent images under structured capture conditions. We introduce FLUXSynID, a framework for generating high-resolution synthetic face datasets with user-defined identity attribute distributions and paired document-style and trusted live capture images. The dataset generated using the FLUXSynID framework shows improved alignment with real-world identity distributions and greater inter-set diversity compared to prior work. The FLUXSynID framework for generating custom datasets, along with a dataset of 14,889 synthetic identities, is publicly released to support biometric research, including face recognition and morphing attack detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability</title>
<link>https://arxiv.org/abs/2505.07533</link>
<guid>https://arxiv.org/abs/2505.07533</guid>
<content:encoded><![CDATA[
arXiv:2505.07533v1 Announce Type: new 
Abstract: Monitoring and analyzing electrocardiogram (ECG) signals, even under varying physiological conditions, including those influenced by physical activity, drugs and stress, is crucial to accurately assess cardiac health. However, current AI-based methods often fail to account for how these factors interact and alter ECG patterns, ultimately limiting their applicability in real-world settings. This study introduces IKrNet, a novel neural network model, which identifies drug-specific patterns in ECGs amidst certain physiological conditions. IKrNet's architecture incorporates spatial and temporal dynamics by using a convolutional backbone with varying receptive field size to capture spatial features. A bi-directional Long Short-Term Memory module is also employed to model temporal dependencies. By treating heart rate variability as a surrogate for physiological fluctuations, we evaluated IKrNet's performance across diverse scenarios, including conditions with physical stress, drug intake alone, and a baseline without drug presence. Our assessment follows a clinical protocol in which 990 healthy volunteers were administered 80mg of Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art models' accuracy and stability in varying physiological conditions, underscoring its clinical viability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning</title>
<link>https://arxiv.org/abs/2505.07538</link>
<guid>https://arxiv.org/abs/2505.07538</guid>
<content:encoded><![CDATA[
arXiv:2505.07538v1 Announce Type: new 
Abstract: We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</title>
<link>https://arxiv.org/abs/2505.07539</link>
<guid>https://arxiv.org/abs/2505.07539</guid>
<content:encoded><![CDATA[
arXiv:2505.07539v1 Announce Type: new 
Abstract: Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynID: Passport Synthetic Dataset for Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2505.07540</link>
<guid>https://arxiv.org/abs/2505.07540</guid>
<content:encoded><![CDATA[
arXiv:2505.07540v1 Announce Type: new 
Abstract: The demand for Presentation Attack Detection (PAD) to identify fraudulent ID documents in remote verification systems has significantly risen in recent years. This increase is driven by several factors, including the rise of remote work, online purchasing, migration, and advancements in synthetic images. Additionally, we have noticed a surge in the number of attacks aimed at the enrolment process. Training a PAD to detect fake ID documents is very challenging because of the limited number of ID documents available due to privacy concerns. This work proposes a new passport dataset generated from a hybrid method that combines synthetic data and open-access information using the ICAO requirement to obtain realistic training and testing images.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies</title>
<link>https://arxiv.org/abs/2505.07552</link>
<guid>https://arxiv.org/abs/2505.07552</guid>
<content:encoded><![CDATA[
arXiv:2505.07552v1 Announce Type: new 
Abstract: Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs</title>
<link>https://arxiv.org/abs/2505.07556</link>
<guid>https://arxiv.org/abs/2505.07556</guid>
<content:encoded><![CDATA[
arXiv:2505.07556v1 Announce Type: new 
Abstract: Event cameras offer significant advantages over traditional frame-based sensors. These include microsecond temporal resolution, robustness under varying lighting conditions and low power consumption. Nevertheless, the effective processing of their sparse, asynchronous event streams remains challenging. Existing approaches to this problem can be categorised into two distinct groups. The first group involves the direct processing of event data with neural models, such as Spiking Neural Networks or Graph Convolutional Neural Networks. However, this approach is often accompanied by a compromise in terms of qualitative performance. The second group involves the conversion of events into dense representations with handcrafted aggregation functions, which can boost accuracy at the cost of temporal fidelity. This paper introduces a novel Self-Supervised Event Representation (SSER) method leveraging Gated Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event timestamps and polarities without temporal discretisation. The recurrent layers are trained in a self-supervised manner to maximise the fidelity of event-time encoding. The inference is performed with event representations generated asynchronously, thus ensuring compatibility with high-throughput sensors. The experimental validation demonstrates that SSER outperforms aggregation-based baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx object detection datasets. Furthermore, the paper presents the first hardware implementation of recurrent representation for event data on a System-on-Chip FPGA, achieving sub-microsecond latency and power consumption between 1-2 W, suitable for real-time, power-efficient applications. Code is available at https://github.com/vision-agh/RecRepEvent.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework</title>
<link>https://arxiv.org/abs/2505.07573</link>
<guid>https://arxiv.org/abs/2505.07573</guid>
<content:encoded><![CDATA[
arXiv:2505.07573v1 Announce Type: new 
Abstract: Kidney abnormality segmentation has important potential to enhance the clinical workflow, especially in settings requiring quantitative assessments. Kidney volume could serve as an important biomarker for renal diseases, with changes in volume correlating directly with kidney function. Currently, clinical practice often relies on subjective visual assessment for evaluating kidney size and abnormalities, including tumors and cysts, which are typically staged based on diameter, volume, and anatomical location. To support a more objective and reproducible approach, this research aims to develop a robust, thoroughly validated kidney abnormality segmentation algorithm, made publicly available for clinical and research use. We employ publicly available training datasets and leverage the state-of-the-art medical image segmentation framework nnU-Net. Validation is conducted using both proprietary and public test datasets, with segmentation performance quantified by Dice coefficient and the 95th percentile Hausdorff distance. Furthermore, we analyze robustness across subgroups based on patient sex, age, CT contrast phases, and tumor histologic subtypes. Our findings demonstrate that our segmentation algorithm, trained exclusively on publicly available data, generalizes effectively to external test sets and outperforms existing state-of-the-art models across all tested datasets. Subgroup analyses reveal consistent high performance, indicating strong robustness and reliability. The developed algorithm and associated code are publicly accessible at https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study</title>
<link>https://arxiv.org/abs/2505.07576</link>
<guid>https://arxiv.org/abs/2505.07576</guid>
<content:encoded><![CDATA[
arXiv:2505.07576v1 Announce Type: new 
Abstract: Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions</title>
<link>https://arxiv.org/abs/2505.07611</link>
<guid>https://arxiv.org/abs/2505.07611</guid>
<content:encoded><![CDATA[
arXiv:2505.07611v1 Announce Type: new 
Abstract: Traffic accident prediction and detection are critical for enhancing road safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged as a promising approach in the era of deep learning.This paper reviews 147 recent studies,focusing on the application of supervised,unsupervised,and hybrid deep learning models for accident prediction,alongside the use of real-world and synthetic datasets.Current methodologies are categorized into four key approaches: image and video feature-based prediction, spatiotemporal feature-based prediction, scene understanding,and multimodal data fusion.While these methods demonstrate significant potential,challenges such as data scarcity,limited generalization to complex scenarios,and real-time performance constraints remain prevalent. This review highlights opportunities for future research,including the integration of multimodal data fusion, self-supervised learning,and Transformer-based architectures to enhance prediction accuracy and scalability.By synthesizing existing advancements and identifying critical gaps, this paper provides a foundational reference for developing robust and adaptive Vision-TAA systems,contributing to road safety and traffic management.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Convolution Improves Neural Predictivity in the Retina</title>
<link>https://arxiv.org/abs/2505.07620</link>
<guid>https://arxiv.org/abs/2505.07620</guid>
<content:encoded><![CDATA[
arXiv:2505.07620v1 Announce Type: new 
Abstract: We present a novel approach to neural response prediction that incorporates higher-order operations directly within convolutional neural networks (CNNs). Our model extends traditional 3D CNNs by embedding higher-order operations within the convolutional operator itself, enabling direct modeling of multiplicative interactions between neighboring pixels across space and time. Our model increases the representational power of CNNs without increasing their depth, therefore addressing the architectural disparity between deep artificial networks and the relatively shallow processing hierarchy of biological visual systems. We evaluate our approach on two distinct datasets: salamander retinal ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC responses to controlled geometric transformations. Our higher-order CNN (HoCNN) achieves superior performance while requiring only half the training data compared to standard architectures, demonstrating correlation coefficients up to 0.75 with neural responses (against 0.80$\pm$0.02 retinal reliability). When integrated into state-of-the-art architectures, our approach consistently improves performance across different species and stimulus conditions. Analysis of the learned representations reveals that our network naturally encodes fundamental geometric transformations, particularly scaling parameters that characterize object expansion and contraction. This capability is especially relevant for specific cell types, such as transient OFF-alpha and transient ON cells, which are known to detect looming objects and object motion respectively, and where our model shows marked improvement in response prediction. The correlation coefficients for scaling parameters are more than twice as high in HoCNN (0.72) compared to baseline models (0.32).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios</title>
<link>https://arxiv.org/abs/2505.07622</link>
<guid>https://arxiv.org/abs/2505.07622</guid>
<content:encoded><![CDATA[
arXiv:2505.07622v1 Announce Type: new 
Abstract: Cross-view geo-localization is a promising solution for large-scale localization problems, requiring the sequential execution of retrieval and metric localization tasks to achieve fine-grained predictions. However, existing methods typically focus on designing standalone models for these two tasks, resulting in inefficient collaboration and increased training overhead. In this paper, we propose UnifyGeo, a novel unified hierarchical geo-localization framework that integrates retrieval and metric localization tasks into a single network. Specifically, we first employ a unified learning strategy with shared parameters to jointly learn multi-granularity representation, facilitating mutual reinforcement between these two tasks. Subsequently, we design a re-ranking mechanism guided by a dedicated loss function, which enhances geo-localization performance by improving both retrieval accuracy and metric localization references. Extensive experiments demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in both task-isolated and task-associated settings. Remarkably, on the challenging VIGOR benchmark, which supports fine-grained localization evaluation, the 1-meter-level localization recall rate improves from 1.53\% to 39.64\% and from 0.43\% to 25.58\% under same-area and cross-area evaluations, respectively. Code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.07652</link>
<guid>https://arxiv.org/abs/2505.07652</guid>
<content:encoded><![CDATA[
arXiv:2505.07652v1 Announce Type: new 
Abstract: Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token's effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in https://shotadapter.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomical Attention Alignment representation for Radiology Report Generation</title>
<link>https://arxiv.org/abs/2505.07689</link>
<guid>https://arxiv.org/abs/2505.07689</guid>
<content:encoded><![CDATA[
arXiv:2505.07689v1 Announce Type: new 
Abstract: Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologists' workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can limit the understanding of spatial structures and semantic relationships, often resulting in suboptimal text generation. To address this, we propose Anatomical Attention Alignment Network (A3Net), a framework that enhance visual-textual understanding by constructing hyper-visual representations. Our approach integrates a knowledge dictionary of anatomical structures with patch-level visual features, enabling the model to effectively associate image regions with their corresponding anatomical entities. This structured representation improves semantic reasoning, interpretability, and cross-modal alignment, ultimately enhancing the accuracy and clinical relevance of generated reports. Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net significantly improves both visual perception and text generation quality. Our code is available at \href{https://github.com/Vinh-AI/A3Net}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter for Continual Learning of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07690</link>
<guid>https://arxiv.org/abs/2505.07690</guid>
<content:encoded><![CDATA[
arXiv:2505.07690v1 Announce Type: new 
Abstract: This study aims to address the problem of multi-domain task incremental learning~(MTIL), which requires that vision-language models~(VLMs) continuously acquire new knowledge while maintaining their inherent zero-shot recognition capability. Existing paradigms delegate the testing of unseen-domain samples to the original CLIP, which only prevents the degradation of the model's zero-shot capability but fails to enhance the generalization of the VLM further. To this end, we propose a novel MTIL framework, named AFA, which comprises two core modules: (1) an against forward-forgetting adapter that learns task-invariant information for each dataset in the incremental tasks to enhance the zero-shot recognition ability of VLMs; (2) an against backward-forgetting adapter that strengthens the few-shot learning capability of VLMs while supporting incremental learning. Extensive experiments demonstrate that the AFA method significantly outperforms existing state-of-the-art approaches, especially in few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP in terms of transferability. The code is provided in the Supplementary Material.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.07691</link>
<guid>https://arxiv.org/abs/2505.07691</guid>
<content:encoded><![CDATA[
arXiv:2505.07691v1 Announce Type: new 
Abstract: Semi-supervised learning leverages unlabeled data to enhance model performance, addressing the limitations of fully supervised approaches. Among its strategies, pseudo-supervision has proven highly effective, typically relying on one or multiple teacher networks to refine pseudo-labels before training a student network. A common practice in pseudo-supervision is filtering pseudo-labels based on pre-defined confidence thresholds or entropy. However, selecting optimal thresholds requires large labeled datasets, which are often scarce in real-world semi-supervised scenarios. To overcome this challenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic feedback-driven thresholding strategy for pseudo-label selection. Instead of relying on static confidence thresholds, ENCORE estimates class-wise true-positive confidence within the unlabeled dataset and continuously adjusts thresholds based on the model's response to different levels of pseudo-label filtering. This feedback-driven mechanism ensures the retention of informative pseudo-labels while filtering unreliable ones, enhancing model training without manual threshold tuning. Our method seamlessly integrates into existing pseudo-supervision frameworks and significantly improves segmentation performance, particularly in data-scarce conditions. Extensive experiments demonstrate that integrating ENCORE with existing pseudo-supervision frameworks enhances performance across multiple datasets and network architectures, validating its effectiveness in semi-supervised learning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images</title>
<link>https://arxiv.org/abs/2505.07704</link>
<guid>https://arxiv.org/abs/2505.07704</guid>
<content:encoded><![CDATA[
arXiv:2505.07704v1 Announce Type: new 
Abstract: Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Spiking Vision Transformer for Object Detection with Event Cameras</title>
<link>https://arxiv.org/abs/2505.07715</link>
<guid>https://arxiv.org/abs/2505.07715</guid>
<content:encoded><![CDATA[
arXiv:2505.07715v1 Announce Type: new 
Abstract: Event-based object detection has gained increasing attention due to its advantages such as high temporal resolution, wide dynamic range, and asynchronous address-event representation. Leveraging these advantages, Spiking Neural Networks (SNNs) have emerged as a promising approach, offering low energy consumption and rich spatiotemporal dynamics. To further enhance the performance of event-based object detection, this study proposes a novel hybrid spike vision Transformer (HsVT) model. The HsVT model integrates a spatial feature extraction module to capture local and global features, and a temporal feature extraction module to model time dependencies and long-term patterns in event sequences. This combination enables HsVT to capture spatiotemporal features, improving its capability to handle complex event-based object detection tasks. To support research in this area, we developed and publicly released The Fall Detection Dataset as a benchmark for event-based object detection tasks. This dataset, captured using an event-based camera, ensures facial privacy protection and reduces memory usage due to the event representation format. We evaluated the HsVT model on GEN1 and Fall Detection datasets across various model sizes. Experimental results demonstrate that HsVT achieves significant performance improvements in event detection with fewer parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gameplay Highlights Generation</title>
<link>https://arxiv.org/abs/2505.07721</link>
<guid>https://arxiv.org/abs/2505.07721</guid>
<content:encoded><![CDATA[
arXiv:2505.07721v1 Announce Type: new 
Abstract: In this work, we enable gamers to share their gaming experience on social media by automatically generating eye-catching highlight reels from their gameplay session Our automation will save time for gamers while increasing audience engagement. We approach the highlight generation problem by first identifying intervals in the video where interesting events occur and then concatenate them. We developed an in-house gameplay event detection dataset containing interesting events annotated by humans using VIA video annotator. Traditional techniques for highlight detection such as game engine integration requires expensive collaboration with game developers. OCR techniques which detect patches of specific images or texts require expensive per game engineering and may not generalize across game UI and different language. We finetuned a multimodal general purpose video understanding model such as X-CLIP using our dataset which generalizes across multiple games in a genre without per game engineering. Prompt engineering was performed to improve the classification performance of this multimodal model. Our evaluation showed that such a finetuned model can detect interesting events in first person shooting games from unseen gameplay footage with more than 90% accuracy. Moreover, our model performed significantly better on low resource games (small dataset) when trained along with high resource games, showing signs of transfer learning. To make the model production ready, we used ONNX libraries to enable cross platform inference. These libraries also provide post training quantization tools to reduce model size and inference time for deployment. ONNX runtime libraries with DirectML backend were used to perform efficient inference on Windows OS. We show that natural language supervision in the X-CLIP model leads to data efficient and highly performant video recognition models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention</title>
<link>https://arxiv.org/abs/2505.07734</link>
<guid>https://arxiv.org/abs/2505.07734</guid>
<content:encoded><![CDATA[
arXiv:2505.07734v1 Announce Type: new 
Abstract: Detecting AI-synthetic faces presents a critical challenge: it is hard to capture consistent structural relationships between facial regions across diverse generation techniques. Current methods, which focus on specific artifacts rather than fundamental inconsistencies, often fail when confronted with novel generative models. To address this limitation, we introduce Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer designed for robust facial forgery detection. This model integrates distinct Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation (LAMM) components within each layer. RG-MHA utilizes facial landmarks to create regional attention masks, guiding the model to scrutinize architectural inconsistencies across different facial areas. Crucially, the separate LAMM module dynamically generates layer-specific parameters, including mask weights and gating values, based on network context. These parameters then modulate the behavior of RG-MHA, enabling adaptive adjustment of regional focus across network depths. This architecture facilitates the capture of subtle, hierarchical forgery cues ubiquitous among diverse generation techniques, such as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT demonstrates superior performance, achieving 94.09% mean ACC (a +5.45% improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results demonstrate LAMM-ViT's exceptional ability to generalize and its potential for reliable deployment against evolving synthetic media threats.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BodyGPS: Anatomical Positioning System</title>
<link>https://arxiv.org/abs/2505.07744</link>
<guid>https://arxiv.org/abs/2505.07744</guid>
<content:encoded><![CDATA[
arXiv:2505.07744v1 Announce Type: new 
Abstract: We introduce a new type of foundational model for parsing human anatomy in medical images that works for different modalities. It supports supervised or unsupervised training and can perform matching, registration, classification, or segmentation with or without user interaction. We achieve this by training a neural network estimator that maps query locations to atlas coordinates via regression. Efficiency is improved by sparsely sampling the input, enabling response times of less than 1 ms without additional accelerator hardware. We demonstrate the utility of the algorithm in both CT and MRI modalities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</title>
<link>https://arxiv.org/abs/2505.07747</link>
<guid>https://arxiv.org/abs/2505.07747</guid>
<content:encoded><![CDATA[
arXiv:2505.07747v1 Announce Type: new 
Abstract: While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Visual Autoregressive Generation via Score Maximization</title>
<link>https://arxiv.org/abs/2505.07812</link>
<guid>https://arxiv.org/abs/2505.07812</guid>
<content:encoded><![CDATA[
arXiv:2505.07812v1 Announce Type: new 
Abstract: Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
<link>https://arxiv.org/abs/2505.07818</link>
<guid>https://arxiv.org/abs/2505.07818</guid>
<content:encoded><![CDATA[
arXiv:2505.07818v1 Announce Type: new 
Abstract: Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion</title>
<link>https://arxiv.org/abs/2505.06250</link>
<guid>https://arxiv.org/abs/2505.06250</guid>
<content:encoded><![CDATA[
arXiv:2505.06250v1 Announce Type: cross 
Abstract: Digital Predistortion (DPD) is a popular technique to enhance signal quality in wideband RF power amplifiers (PAs). With increasing bandwidth and data rates, DPD faces significant energy consumption challenges during deployment, contrasting with its efficiency goals. State-of-the-art DPD models rely on recurrent neural networks (RNN), whose computational complexity hinders system efficiency. This paper introduces DeltaDPD, exploring the dynamic temporal sparsity of input signals and neuronal hidden states in RNNs for energy-efficient DPD, reducing arithmetic operations and memory accesses while preserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW 256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03 dBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square Error (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal sparsity, leading to a 1.8X reduction in estimated inference power. The DeltaDPD code will be released after formal publication at https://www.opendpd.com.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attonsecond Streaking Phase Retrieval Via Deep Learning Methods</title>
<link>https://arxiv.org/abs/2505.06275</link>
<guid>https://arxiv.org/abs/2505.06275</guid>
<content:encoded><![CDATA[
arXiv:2505.06275v1 Announce Type: cross 
Abstract: Attosecond streaking phase retrieval is essential for resolving electron dynamics on sub-femtosecond time scales yet traditional algorithms rely on iterative minimization and central momentum approximations that degrade accuracy for broadband pulses. In this work phase retrieval is reformulated as a supervised computer-vision problem and four neural architectures are systematically compared. A convolutional network demonstrates strong sensitivity to local streak edges but lacks global context; a vision transformer captures long-range delay-energy correlations at the expense of local inductive bias; a hybrid CNN-ViT model unites local feature extraction and full-graph attention; and a capsule network further enforces spatial pose agreement through dynamic routing. A theoretical analysis introduces local, global and positional sensitivity measures and derives surrogate error bounds that predict the strict ordering $CNN<Capsule$. Controlled experiments on synthetic streaking spectrograms confirm this hierarchy, with the capsule network achieving the highest retrieval fidelity. Looking forward, embedding the strong-field integral into physics-informed neural networks and exploring photonic hardware implementations promise pathways toward real-time attosecond pulse characterization under demanding experimental conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field</title>
<link>https://arxiv.org/abs/2505.06277</link>
<guid>https://arxiv.org/abs/2505.06277</guid>
<content:encoded><![CDATA[
arXiv:2505.06277v1 Announce Type: cross 
Abstract: Terahertz (THz) communication is a key enabler for 6G systems, offering ultra-wide bandwidth and unprecedented data rates. However, THz signal propagation differs significantly from lower-frequency bands due to severe free space path loss, minimal diffraction and specular reflection, and prominent scattering, making conventional channel modeling and pilot-based estimation approaches inefficient. In this work, we investigate the feasibility of applying radio radiance field (RRF) framework to the THz band. This method reconstructs a continuous RRF using visual-based geometry and sparse THz RF measurements, enabling efficient spatial channel state information (Spatial-CSI) modeling without dense sampling. We first build a fine simulated THz scenario, then we reconstruct the RRF and evaluate the performance in terms of both reconstruction quality and effectiveness in THz communication, showing that the reconstructed RRF captures key propagation paths with sparse training samples. Our findings demonstrate that RRF modeling remains effective in the THz regime and provides a promising direction for scalable, low-cost spatial channel reconstruction in future 6G networks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments</title>
<link>https://arxiv.org/abs/2505.06285</link>
<guid>https://arxiv.org/abs/2505.06285</guid>
<content:encoded><![CDATA[
arXiv:2505.06285v1 Announce Type: cross 
Abstract: Rolling bearings are critical components of rotating machinery, and their proper functioning is essential for industrial production. Most existing condition monitoring methods focus on extracting discriminative features from time-domain signals to assess bearing health status. However, under complex operating conditions, periodic impulsive characteristics related to fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn distinctive fault-related features in such scenarios. To address this issue, this paper proposes a novel CNN-based model named FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL) is introduced as an input denoising layer to enhance key features while filtering out irrelevant information. Subsequently, a Multiscale Time-Frequency Fusion (MSTFF) module is employed to extract fused time-frequency features, further improving the model robustness and nonlinear representation capability. Additionally, a distillation layer is incorporated to expand the receptive field. Based on these advancements, a novel deep lightweight CNN model, termed the Frequency-Enhanced Multiscale Network (FEMSN), is developed. The effectiveness of FEMSN and FADEL in machine health monitoring and stability assessment is validated through two case studies.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments</title>
<link>https://arxiv.org/abs/2505.06483</link>
<guid>https://arxiv.org/abs/2505.06483</guid>
<content:encoded><![CDATA[
arXiv:2505.06483v1 Announce Type: cross 
Abstract: Robot autonomy in unknown, GPS-denied, and complex underground environments requires real-time, robust, and accurate onboard pose estimation and mapping for reliable operations. This becomes particularly challenging in perception-degraded subterranean conditions under harsh environmental factors, including darkness, dust, and geometrically self-similar structures. This paper details CompSLAM, a highly resilient and hierarchical multi-modal localization and mapping framework designed to address these challenges. Its flexible architecture achieves resilience through redundancy by leveraging the complementary nature of pose estimates derived from diverse sensor modalities. Developed during the DARPA Subterranean Challenge, CompSLAM was successfully deployed on all aerial, legged, and wheeled robots of Team Cerberus during their competition-winning final run. Furthermore, it has proven to be a reliable odometry and mapping solution in various subsequent projects, with extensions enabling multi-robot map sharing for marsupial robotic deployments and collaborative mapping. This paper also introduces a comprehensive dataset acquired by a manually teleoperated quadrupedal robot, covering a significant portion of the DARPA Subterranean Challenge finals course. This dataset evaluates CompSLAM's robustness to sensor degradations as the robot traverses 740 meters in an environment characterized by highly variable geometries and demanding lighting conditions. The CompSLAM code and the DARPA SubT Finals dataset are made publicly available for the benefit of the robotics community
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
arXiv:2505.06502v1 Announce Type: cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities</title>
<link>https://arxiv.org/abs/2505.06507</link>
<guid>https://arxiv.org/abs/2505.06507</guid>
<content:encoded><![CDATA[
arXiv:2505.06507v1 Announce Type: cross 
Abstract: Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: https://github.com/Text-to-CadQuery/Text-to-CadQuery.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation</title>
<link>https://arxiv.org/abs/2505.06594</link>
<guid>https://arxiv.org/abs/2505.06594</guid>
<content:encoded><![CDATA[
arXiv:2505.06594v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Representation Transferring to Lightweight Models via Perception Coherence</title>
<link>https://arxiv.org/abs/2505.06595</link>
<guid>https://arxiv.org/abs/2505.06595</guid>
<content:encoded><![CDATA[
arXiv:2505.06595v1 Announce Type: cross 
Abstract: In this paper, we propose a method for transferring feature representation to lightweight student models from larger teacher models. We mathematically define a new notion called \textit{perception coherence}. Based on this notion, we propose a loss function, which takes into account the dissimilarities between data points in feature space through their ranking. At a high level, by minimizing this loss function, the student model learns to mimic how the teacher model \textit{perceives} inputs. More precisely, our method is motivated by the fact that the representational capacity of the student model is weaker than the teacher model. Hence, we aim to develop a new method allowing for a better relaxation. This means that, the student model does not need to preserve the absolute geometry of the teacher one, while preserving global coherence through dissimilarity ranking. Our theoretical insights provide a probabilistic perspective on the process of feature representation transfer. Our experiments results show that our method outperforms or achieves on-par performance compared to strong baseline methods for representation transferring.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models</title>
<link>https://arxiv.org/abs/2505.06621</link>
<guid>https://arxiv.org/abs/2505.06621</guid>
<content:encoded><![CDATA[
arXiv:2505.06621v1 Announce Type: cross 
Abstract: The distribution of child sexual abuse imagery (CSAI) is an ever-growing concern of our modern world; children who suffered from this heinous crime are revictimized, and the growing amount of illegal imagery distributed overwhelms law enforcement agents (LEAs) with the manual labor of categorization. To ease this burden researchers have explored methods for automating data triage and detection of CSAI, but the sensitive nature of the data imposes restricted access and minimal interaction between real data and learning algorithms, avoiding leaks at all costs. In observing how these restrictions have shaped the literature we formalize a definition of "Proxy Tasks", i.e., the substitute tasks used for training models for CSAI without making use of CSA data. Under this new terminology we review current literature and present a protocol for making conscious use of Proxy Tasks together with consistent input from LEAs to design better automation in this field. Finally, we apply this protocol to study -- for the first time -- the task of Few-shot Indoor Scene Classification on CSAI, showing a final model that achieves promising results on a real-world CSAI dataset whilst having no weights actually trained on sensitive data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification</title>
<link>https://arxiv.org/abs/2505.06646</link>
<guid>https://arxiv.org/abs/2505.06646</guid>
<content:encoded><![CDATA[
arXiv:2505.06646v1 Announce Type: cross 
Abstract: Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2505.06685</link>
<guid>https://arxiv.org/abs/2505.06685</guid>
<content:encoded><![CDATA[
arXiv:2505.06685v1 Announce Type: cross 
Abstract: Emotion understanding in videos aims to accurately recognize and interpret individuals' emotional states by integrating contextual, visual, textual, and auditory cues. While Large Multimodal Models (LMMs) have demonstrated significant progress in general vision-language (VL) tasks, their performance in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on emotion-related tasks often leads to catastrophic forgetting, hindering their ability to generalize across diverse tasks. To address these challenges, we present Emotion-Qwen, a tailored multimodal framework designed to enhance both emotion understanding and general VL reasoning. Emotion-Qwen incorporates a sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm, which dynamically routes inputs to balance emotion-specific and general-purpose processing. The model is pre-trained in a three-stage pipeline on large-scale general and emotional image datasets to support robust multimodal representations. Furthermore, we construct the Video Emotion Reasoning (VER) dataset, comprising more than 40K bilingual video clips with fine-grained descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning capability. Experimental results demonstrate that Emotion-Qwen achieves state-of-the-art performance on multiple emotion recognition benchmarks, while maintaining competitive results on general VL tasks. Code and models are available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark</title>
<link>https://arxiv.org/abs/2505.06746</link>
<guid>https://arxiv.org/abs/2505.06746</guid>
<content:encoded><![CDATA[
arXiv:2505.06746v1 Announce Type: cross 
Abstract: We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HistDiST: Histopathological Diffusion-based Stain Transfer</title>
<link>https://arxiv.org/abs/2505.06793</link>
<guid>https://arxiv.org/abs/2505.06793</guid>
<content:encoded><![CDATA[
arXiv:2505.06793v1 Announce Type: cross 
Abstract: Hematoxylin and Eosin (H&amp;E) staining is the cornerstone of histopathology but lacks molecular specificity. While Immunohistochemistry (IHC) provides molecular insights, it is costly and complex, motivating H&amp;E-to-IHC translation as a cost-effective alternative. Existing translation methods are mainly GAN-based, often struggling with training instability and limited structural fidelity, while diffusion-based approaches remain underexplored. We propose HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity H&amp;E-to-IHC translation. HistDiST introduces a dual-conditioning strategy, utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&amp;E representations to ensure pathology-relevant context and structural consistency. To overcome brightness biases, we incorporate a rescaled noise schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition at the final timestep. During inference, DDIM inversion preserves the morphological structure, while an eta-cosine noise schedule introduces controlled stochasticity, balancing structural consistency and molecular fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel pathology-aware metric leveraging GigaPath embeddings to assess molecular relevance. Extensive evaluations on MIST and BCI datasets demonstrate that HistDiST significantly outperforms existing methods, achieving a 28% improvement in MRA on the H&amp;E-to-Ki67 translation task, highlighting its effectiveness in capturing true IHC semantics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation</title>
<link>https://arxiv.org/abs/2505.06803</link>
<guid>https://arxiv.org/abs/2505.06803</guid>
<content:encoded><![CDATA[
arXiv:2505.06803v1 Announce Type: cross 
Abstract: Audio large language models (LLMs) are considered experts at recognizing sound objects, yet their performance relative to LLMs in other sensory modalities, such as visual or audio-visual LLMs, and to humans using their ears, eyes, or both remains unexplored. To investigate this, we systematically evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of different classes from audio-only, silent video, or sounded video inputs. We uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the sensory discrepancy between human ears and eyes. To reduce this gap, we introduce a cross-modal distillation framework, where an LLM in one modality serves as the teacher and another as the student, with knowledge transfer in sound classes predicted as more challenging to the student by a heuristic model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice versa, leads to notable improvements, particularly in challenging classes. This work highlights the sensory gap in LLMs from a human-aligned perspective and proposes a principled approach to enhancing modality-specific perception in multimodal LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods</title>
<link>https://arxiv.org/abs/2505.06811</link>
<guid>https://arxiv.org/abs/2505.06811</guid>
<content:encoded><![CDATA[
arXiv:2505.06811v1 Announce Type: cross 
Abstract: Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for non-invasive mapping of brain metabolites, providing critical insights into neurological conditions. However, its utility is often limited by missing or corrupted data due to motion artifacts, magnetic field inhomogeneities, or failed spectral fitting-especially in high resolution 3D acquisitions. To address this, we propose the first deep learning-based, mask-free framework for estimating missing data in MRSI metabolic maps. Unlike conventional restoration methods that rely on explicit masks to identify missing regions, our approach implicitly detects and estimates these areas using contextual spatial features through 2D and 3D U-Net architectures. We also introduce a progressive training strategy to enhance robustness under varying levels of data degradation. Our method is evaluated on both simulated and real patient datasets and consistently outperforms traditional interpolation techniques such as cubic and linear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97 with 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM of 0.98 with 15% missing voxels. Qualitative results show improved fidelity in estimating missing data, particularly in metabolically heterogeneous regions and ventricular regions. Importantly, our model generalizes well to real-world datasets without requiring retraining or mask input. These findings demonstrate the effectiveness and broad applicability of mask-free deep learning for MRSI restoration, with strong potential for clinical and research integration.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Robotic Policy Learning via Latent Space Backward Planning</title>
<link>https://arxiv.org/abs/2505.06861</link>
<guid>https://arxiv.org/abs/2505.06861</guid>
<content:encoded><![CDATA[
arXiv:2505.06861v1 Announce Type: cross 
Abstract: Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Classification Using a Diffusion Model as a Pre-Training Model</title>
<link>https://arxiv.org/abs/2505.06890</link>
<guid>https://arxiv.org/abs/2505.06890</guid>
<content:encoded><![CDATA[
arXiv:2505.06890v1 Announce Type: cross 
Abstract: In this paper, we propose a diffusion model that integrates a representation-conditioning mechanism, where the representations derived from a Vision Transformer (ViT) are used to condition the internal process of a Transformer-based diffusion model. This approach enables representation-conditioned data generation, addressing the challenge of requiring large-scale labeled datasets by leveraging self-supervised learning on unlabeled data. We evaluate our method through a zero-shot classification task for hematoma detection in brain imaging. Compared to the strong contrastive learning baseline, DINOv2, our method achieves a notable improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its effectiveness in image classification.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence</title>
<link>https://arxiv.org/abs/2505.06907</link>
<guid>https://arxiv.org/abs/2505.06907</guid>
<content:encoded><![CDATA[
arXiv:2505.06907v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-AIMS: AI-Powered Microscopy Image Analysis</title>
<link>https://arxiv.org/abs/2505.06918</link>
<guid>https://arxiv.org/abs/2505.06918</guid>
<content:encoded><![CDATA[
arXiv:2505.06918v1 Announce Type: cross 
Abstract: This paper presents a systematic solution for the intelligent recognition and automatic analysis of microscopy images. We developed a data engine that generates high-quality annotated datasets through a combination of the collection of diverse microscopy images from experiments, synthetic data generation and a human-in-the-loop annotation process. To address the unique challenges of microscopy images, we propose a segmentation model capable of robustly detecting both small and large objects. The model effectively identifies and separates thousands of closely situated targets, even in cluttered visual environments. Furthermore, our solution supports the precise automatic recognition of image scale bars, an essential feature in quantitative microscopic analysis. Building upon these components, we have constructed a comprehensive intelligent analysis platform and validated its effectiveness and practicality in real-world applications. This study not only advances automatic recognition in microscopy imaging but also ensures scalability and generalizability across multiple application domains, offering a powerful tool for automated microscopic analysis in interdisciplinary research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whitened CLIP as a Likelihood Surrogate of Images and Captions</title>
<link>https://arxiv.org/abs/2505.06934</link>
<guid>https://arxiv.org/abs/2505.06934</guid>
<content:encoded><![CDATA[
arXiv:2505.06934v1 Announce Type: cross 
Abstract: Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent space via an invertible linear operation. This transformation ensures that each feature in the embedding space has zero mean, unit standard deviation, and no correlation with all other features, resulting in an identity covariance matrix. We show that the whitened embeddings statistics can be well approximated as a standard normal distribution, thus, the log-likelihood is estimated simply by the square Euclidean norm in the whitened embedding space. The whitening procedure is completely training-free and performed using a pre-computed whitening matrix, hence, is very fast. We present several preliminary experiments demonstrating the properties and applicability of these likelihood scores to images and captions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing</title>
<link>https://arxiv.org/abs/2505.06963</link>
<guid>https://arxiv.org/abs/2505.06963</guid>
<content:encoded><![CDATA[
arXiv:2505.06963v1 Announce Type: cross 
Abstract: This paper introduces an innovative approach for the autonomous landing of Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera, therefore obviating the requirement for depth estimation cameras. Drawing on the inherent human estimating process, the proposed method reframes the landing task as an optimization problem. The UAV employs variations in the visual characteristics of a specially designed lenticular circle on the landing pad, where the perceived color and form provide critical information for estimating both altitude and depth. Reinforcement learning algorithms are utilized to approximate the functions governing these estimations, enabling the UAV to ascertain ideal landing settings via training. This method's efficacy is assessed by simulations and experiments, showcasing its potential for robust and accurate autonomous landing without dependence on complex sensor setups. This research contributes to the advancement of cost-effective and efficient UAV landing solutions, paving the way for wider applicability across various fields.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving</title>
<link>https://arxiv.org/abs/2505.06980</link>
<guid>https://arxiv.org/abs/2505.06980</guid>
<content:encoded><![CDATA[
arXiv:2505.06980v1 Announce Type: cross 
Abstract: Perception is a core capability of automated vehicles and has been significantly advanced through modern sensor technologies and artificial intelligence. However, perception systems still face challenges in complex real-world scenarios. To improve robustness against various external factors, multi-sensor fusion techniques are essential, combining the strengths of different sensor modalities. With recent developments in Vehicle-to-Everything (V2X communication, sensor fusion can now extend beyond a single vehicle to a cooperative multi-agent system involving Connected Automated Vehicle (CAV) and intelligent infrastructure. This paper presents VALISENS, an innovative multi-sensor system distributed across multiple agents. It integrates onboard and roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance situational awareness and support cooperative automated driving. The thermal camera adds critical redundancy for perceiving Vulnerable Road User (VRU), while fusion with roadside sensors mitigates visual occlusions and extends the perception range beyond the limits of individual vehicles. We introduce the corresponding perception module built on this sensor system, which includes object detection, tracking, motion forecasting, and high-level data fusion. The proposed system demonstrates the potential of cooperative perception in real-world test environments and lays the groundwork for future Cooperative Intelligent Transport Systems (C-ITS) applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Three-Phase Dynamics of Generalization Power of a DNN</title>
<link>https://arxiv.org/abs/2505.06993</link>
<guid>https://arxiv.org/abs/2505.06993</guid>
<content:encoded><![CDATA[
arXiv:2505.06993v1 Announce Type: cross 
Abstract: This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy of Groups in Dense Street Imagery</title>
<link>https://arxiv.org/abs/2505.07085</link>
<guid>https://arxiv.org/abs/2505.07085</guid>
<content:encoded><![CDATA[
arXiv:2505.07085v1 Announce Type: cross 
Abstract: Spatially and temporally dense street imagery (DSI) datasets have grown unbounded. In 2024, individual companies possessed around 3 trillion unique images of public streets. DSI data streams are only set to grow as companies like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze collisions. Academic researchers leverage DSI to explore novel approaches to urban analysis. Despite good-faith efforts by DSI providers to protect individual privacy through blurring faces and license plates, these measures fail to address broader privacy concerns. In this work, we find that increased data density and advancements in artificial intelligence enable harmful group membership inferences from supposedly anonymized data. We perform a penetration test to demonstrate how easily sensitive group affiliations can be inferred from obfuscated pedestrians in 25,232,608 dashcam images taken in New York City. We develop a typology of identifiable groups within DSI and analyze privacy implications through the lens of contextual integrity. Finally, we discuss actionable recommendations for researchers working with data from DSI providers.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</title>
<link>https://arxiv.org/abs/2505.07110</link>
<guid>https://arxiv.org/abs/2505.07110</guid>
<content:encoded><![CDATA[
arXiv:2505.07110v1 Announce Type: cross 
Abstract: Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skull stripping with purely synthetic data</title>
<link>https://arxiv.org/abs/2505.07159</link>
<guid>https://arxiv.org/abs/2505.07159</guid>
<content:encoded><![CDATA[
arXiv:2505.07159v1 Announce Type: cross 
Abstract: While many skull stripping algorithms have been developed for multi-modal and multi-species cases, there is still a lack of a fundamentally generalizable approach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain extrAction), a strategy to train a model for brain extraction with no real brain images or labels. Our results show that even without any real images or anatomical priors, the model achieves comparable accuracy in multi-modal, multi-species and pathological cases. This work presents a new direction of research for any generalizable medical image segmentation task.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metrics that matter: Evaluating image quality metrics for medical image generation</title>
<link>https://arxiv.org/abs/2505.07175</link>
<guid>https://arxiv.org/abs/2505.07175</guid>
<content:encoded><![CDATA[
arXiv:2505.07175v1 Announce Type: cross 
Abstract: Evaluating generative models for synthetic medical imaging is crucial yet challenging, especially given the high standards of fidelity, anatomical accuracy, and safety required for clinical applications. Standard evaluation of generated images often relies on no-reference image quality metrics when ground truth images are unavailable, but their reliability in this complex domain is not well established. This study comprehensively assesses commonly used no-reference image quality metrics using brain MRI data, including tumour and vascular images, providing a representative exemplar for the field. We systematically evaluate metric sensitivity to a range of challenges, including noise, distribution shifts, and, critically, localised morphological alterations designed to mimic clinically relevant inaccuracies. We then compare these metric scores against model performance on a relevant downstream segmentation task, analysing results across both controlled image perturbations and outputs from different generative model architectures. Our findings reveal significant limitations: many widely-used no-reference image quality metrics correlate poorly with downstream task suitability and exhibit a profound insensitivity to localised anatomical details crucial for clinical validity. Furthermore, these metrics can yield misleading scores regarding distribution shifts, e.g. data memorisation. This reveals the risk of misjudging model readiness, potentially leading to the deployment of flawed tools that could compromise patient safety. We conclude that ensuring generative models are truly fit for clinical purpose requires a multifaceted validation framework, integrating performance on relevant downstream tasks with the cautious interpretation of carefully selected no-reference image quality metrics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards user-centered interactive medical image segmentation in VR with an assistive AI agent</title>
<link>https://arxiv.org/abs/2505.07214</link>
<guid>https://arxiv.org/abs/2505.07214</guid>
<content:encoded><![CDATA[
arXiv:2505.07214v1 Announce Type: cross 
Abstract: Crucial in disease analysis and surgical planning, manual segmentation of volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and challenging to master, while fully automatic algorithms can benefit from user-feedback. Therefore, with the complementary power of the latest radiological AI foundation models and virtual reality (VR)'s intuitive data interaction, we propose SAMIRA, a novel conversational AI agent that assists users with localizing, segmenting, and visualizing 3D medical concepts in VR. Through speech-based interaction, the agent helps users understand radiological features, locate clinical targets, and generate segmentation masks that can be refined with just a few point prompts. The system also supports true-to-scale 3D visualization of segmented pathology to enhance patient-specific anatomical understanding. Furthermore, to determine the optimal interaction paradigm under near-far attention-switching for refining segmentation masks in an immersive, human-in-the-loop workflow, we compare VR controller pointing, head pointing, and eye tracking as input modes. With a user study, evaluations demonstrated a high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as strong support for the proposed VR system's guidance, training potential, and integration of AI in radiological segmentation tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data</title>
<link>https://arxiv.org/abs/2505.07349</link>
<guid>https://arxiv.org/abs/2505.07349</guid>
<content:encoded><![CDATA[
arXiv:2505.07349v1 Announce Type: cross 
Abstract: Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a critical task for healthcare professionals. The diverse nature of MRI acquisitions with varying contrasts and orientation introduce complexity in identifying hemorrhage using neural networks. For acquisitions with varying orientations, traditional methods often involve resampling images to a fixed plane, which can lead to information loss. To address this, we propose a 3D multi-plane vision transformer (MP-ViT) for hemorrhage classification with varying orientation data. It employs two separate transformer encoders for axial and sagittal contrasts, using cross-attention to integrate information across orientations. MP-ViT also includes a modality indication vector to provide missing contrast information to the model. The effectiveness of the proposed model is demonstrated with extensive experiments on real world clinical dataset consists of 10,084 training, 1,289 validation and 1,496 test subjects. MP-ViT achieved substantial improvement in area under the curve (AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based architectures by 1.8%. These results highlight the potential of MP-ViT in improving performance for hemorrhage detection when different orientation contrasts are needed.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.07411</link>
<guid>https://arxiv.org/abs/2505.07411</guid>
<content:encoded><![CDATA[
arXiv:2505.07411v1 Announce Type: cross 
Abstract: Pruning is a widely used method for compressing Deep Neural Networks (DNNs), where less relevant parameters are removed from a DNN model to reduce its size. However, removing parameters reduces model accuracy, so pruning is typically combined with fine-tuning, and sometimes other operations such as rewinding weights, to recover accuracy. A common approach is to repeatedly prune and then fine-tune, with increasing amounts of model parameters being removed in each step. While straightforward to implement, pruning pipelines that follow this approach are computationally expensive due to the need for repeated fine-tuning.
  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs that significantly decreases the time required for pruning by reducing the overall cost of fine-tuning, while maintaining a similar accuracy to existing pruning pipelines. ICE-Pruning is based on three main components: i) an automatic mechanism to determine after which pruning steps fine-tuning should be performed; ii) a freezing strategy for faster fine-tuning in each pruning step; and iii) a custom pruning-aware learning rate scheduler to further improve the accuracy of each pruning step and reduce the overall time consumption. We also propose an efficient auto-tuning stage for the hyperparameters (e.g., freezing percentage) introduced by the three components. We evaluate ICE-Pruning on several DNN models and datasets, showing that it can accelerate pruning by up to 9.61x. Code is available at https://github.com/gicLAB/ICE-Pruning
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Continuous Generative Models</title>
<link>https://arxiv.org/abs/2505.07447</link>
<guid>https://arxiv.org/abs/2505.07447</guid>
<content:encoded><![CDATA[
arXiv:2505.07447v1 Announce Type: cross 
Abstract: Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model</title>
<link>https://arxiv.org/abs/2505.07449</link>
<guid>https://arxiv.org/abs/2505.07449</guid>
<content:encoded><![CDATA[
arXiv:2505.07449v1 Announce Type: cross 
Abstract: In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at https://github.com/mar-cry/Ophora.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts</title>
<link>https://arxiv.org/abs/2505.07477</link>
<guid>https://arxiv.org/abs/2505.07477</guid>
<content:encoded><![CDATA[
arXiv:2505.07477v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by $\sim 90\%$ while maintaining superior performance. Code is available at https://github.com/deng-ai-lab/SDO.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Optimized Conditional Diffusion for Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.07548</link>
<guid>https://arxiv.org/abs/2505.07548</guid>
<content:encoded><![CDATA[
arXiv:2505.07548v1 Announce Type: cross 
Abstract: Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples (\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical alignment, causing DA failures. To address this challenge, we propose \textbf{N}oise \textbf{O}ptimized \textbf{C}onditional \textbf{D}iffusion for \textbf{D}omain \textbf{A}daptation (\textbf{NOCDDA}), which seamlessly integrates the generative capabilities of conditional diffusion models with the decision-making requirements of DA to achieve task-coupled optimization for efficient adaptation. For robust cross-domain consistency, we modify the DA classifier to align with the conditional diffusion classifier within a unified optimization framework, enabling forward training on noise-varying cross-domain samples. Furthermore, we argue that the conventional \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \) initialization in diffusion models often generates class-confused hcpl-tds, compromising discriminative DA. To resolve this, we introduce a class-aware noise optimization strategy that refines sampling regions for reverse class-specific hcpl-tds generation, effectively enhancing cross-domain alignment. Extensive experiments across 5 benchmark datasets and 29 DA tasks demonstrate significant performance gains of \textbf{NOCDDA} over 31 state-of-the-art methods, validating its robustness and effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding</title>
<link>https://arxiv.org/abs/2505.07600</link>
<guid>https://arxiv.org/abs/2505.07600</guid>
<content:encoded><![CDATA[
arXiv:2505.07600v1 Announce Type: cross 
Abstract: Manipulating clothes is challenging due to their complex dynamics, high deformability, and frequent self-occlusions. Garments exhibit a nearly infinite number of configurations, making explicit state representations difficult to define. In this paper, we analyze BiFold, a model that predicts language-conditioned pick-and-place actions from visual observations, while implicitly encoding garment state through end-to-end learning. To address scenarios such as crumpled garments or recovery from failed manipulations, BiFold leverages temporal context to improve state estimation. We examine the internal representations of the model and present evidence that its fine-tuning and temporal context enable effective alignment between text and image regions, as well as temporal consistency.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Brain: A Neuroscience-inspired Framework for Embodied Agents</title>
<link>https://arxiv.org/abs/2505.07634</link>
<guid>https://arxiv.org/abs/2505.07634</guid>
<content:encoded><![CDATA[
arXiv:2505.07634v1 Announce Type: cross 
Abstract: The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework</title>
<link>https://arxiv.org/abs/2505.07654</link>
<guid>https://arxiv.org/abs/2505.07654</guid>
<content:encoded><![CDATA[
arXiv:2505.07654v1 Announce Type: cross 
Abstract: Breast-conserving surgery (BCS) aims to completely remove malignant lesions while maximizing healthy tissue preservation. Intraoperative margin assessment is essential to achieve a balance between thorough cancer resection and tissue conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM) enables rapid acquisition of whole surface images (WSIs) for excised tissue, providing contrast between malignant and normal tissues. However, breast cancer classification with DUV WSIs is challenged by high resolutions and complex histopathological features. This study introduces a DUV WSI classification framework using a patch-level vision transformer (ViT) model, capturing local and global features. Grad-CAM++ saliency weighting highlights relevant spatial regions, enhances result interpretability, and improves diagnostic accuracy for benign and malignant tissue classification. A comprehensive 5-fold cross-validation demonstrates the proposed approach significantly outperforms conventional deep learning methods, achieving a classification accuracy of 98.33%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells</title>
<link>https://arxiv.org/abs/2505.07661</link>
<guid>https://arxiv.org/abs/2505.07661</guid>
<content:encoded><![CDATA[
arXiv:2505.07661v1 Announce Type: cross 
Abstract: We present SparseAttnNet, a new hierarchical attention-driven framework for efficient image classification that adaptively selects and processes only the most informative pixels from images. Traditional convolutional neural networks typically process the entire images regardless of information density, leading to computational inefficiency and potential focus on irrelevant features. Our approach leverages a dynamic selection mechanism that uses coarse attention distilled by fine multi-head attention from the downstream layers of the model, allowing the model to identify and extract the most salient k pixels, where k is adaptively learned during training based on loss convergence trends. Once the top-k pixels are selected, the model processes only these pixels, embedding them as words in a language model to capture their semantics, followed by multi-head attention to incorporate global context. For biological cell images, we demonstrate that SparseAttnNet can process approximately 15% of the pixels instead of the full image. Applied to cell classification tasks using white blood cells images from the following modalities: optical path difference (OPD) images from digital holography for stain-free cells, images from motion-sensitive (event) camera from stain-free cells, and brightfield microscopy images of stained cells, For all three imaging modalities, SparseAttnNet achieves competitive accuracy while drastically reducing computational requirements in terms of both parameters and floating-point operations per second, compared to traditional CNNs and Vision Transformers. Since the model focuses on biologically relevant regions, it also offers improved explainability. The adaptive and lightweight nature of SparseAttnNet makes it ideal for deployment in resource-constrained and high-throughput settings, including imaging flow cytometry.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization</title>
<link>https://arxiv.org/abs/2505.07675</link>
<guid>https://arxiv.org/abs/2505.07675</guid>
<content:encoded><![CDATA[
arXiv:2505.07675v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that $\texttt{DHO}$ consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation</title>
<link>https://arxiv.org/abs/2505.07687</link>
<guid>https://arxiv.org/abs/2505.07687</guid>
<content:encoded><![CDATA[
arXiv:2505.07687v1 Announce Type: cross 
Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba's selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2's image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at https://github.com/gatina-yone/ABS-Mamba
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeletonization of neuronal processes using Discrete Morse techniques from computational topology</title>
<link>https://arxiv.org/abs/2505.07754</link>
<guid>https://arxiv.org/abs/2505.07754</guid>
<content:encoded><![CDATA[
arXiv:2505.07754v1 Announce Type: cross 
Abstract: To understand biological intelligence we need to map neuronal networks in vertebrate brains. Mapping mesoscale neural circuitry is done using injections of tracers that label groups of neurons whose axons project to different brain regions. Since many neurons are labeled, it is difficult to follow individual axons. Previous approaches have instead quantified the regional projections using the total label intensity within a region. However, such a quantification is not biologically meaningful. We propose a new approach better connected to the underlying neurons by skeletonizing labeled axon fragments and then estimating a volumetric length density. Our approach uses a combination of deep nets and the Discrete Morse (DM) technique from computational topology. This technique takes into account nonlocal connectivity information and therefore provides noise-robustness. We demonstrate the utility and scalability of the approach on whole-brain tracer injected data. We also define and illustrate an information theoretic measure that quantifies the additional information obtained, compared to the skeletonized tracer injection fragments, when individual axon morphologies are available. Our approach is the first application of the DM technique to computational neuroanatomy. It can help bridge between single-axon skeletons and tracer injections, two important data types in mapping neural networks in vertebrates.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution</title>
<link>https://arxiv.org/abs/2505.07766</link>
<guid>https://arxiv.org/abs/2505.07766</guid>
<content:encoded><![CDATA[
arXiv:2505.07766v1 Announce Type: cross 
Abstract: User privacy is a crucial concern in robotic applications, especially when mobile service robots are deployed in personal or sensitive environments. However, many robotic downstream tasks require the use of cameras, which may raise privacy risks. To better understand user perceptions of privacy in relation to visual data, we conducted a user study investigating how different image modalities and image resolutions affect users' privacy concerns. The results show that depth images are broadly viewed as privacy-safe, and a similarly high proportion of respondents feel the same about semantic segmentation images. Additionally, the majority of participants consider 32*32 resolution RGB images to be almost sufficiently privacy-preserving, while most believe that 16*16 resolution can fully guarantee privacy protection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies</title>
<link>https://arxiv.org/abs/2505.07813</link>
<guid>https://arxiv.org/abs/2505.07813</guid>
<content:encoded><![CDATA[
arXiv:2505.07813v1 Announce Type: cross 
Abstract: Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous manipulation policies to generalize to novel environments, but acquiring such datasets presents many challenges. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DexWild-System, a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. This combination results in robust robot policies capable of generalizing to novel environments, tasks, and embodiments with minimal additional robot-specific data. Experimental results demonstrate that DexWild significantly improves performance, achieving a 68.5% success rate in unseen environments-nearly four times higher than policies trained with robot data only-and offering 5.8x better cross-embodiment generalization. Video results, codebases, and instructions at https://dexwild.github.io
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07815</link>
<guid>https://arxiv.org/abs/2505.07815</guid>
<content:encoded><![CDATA[
arXiv:2505.07815v1 Announce Type: cross 
Abstract: Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel Motion as Universal Representation for Robot Control</title>
<link>https://arxiv.org/abs/2505.07817</link>
<guid>https://arxiv.org/abs/2505.07817</guid>
<content:encoded><![CDATA[
arXiv:2505.07817v1 Announce Type: cross 
Abstract: We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo for visualizations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^{\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning</title>
<link>https://arxiv.org/abs/2505.07819</link>
<guid>https://arxiv.org/abs/2505.07819</guid>
<content:encoded><![CDATA[
arXiv:2505.07819v1 Announce Type: cross 
Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\textbf{Triply-Hierarchical Diffusion Policy}~(\textbf{H$^{\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\mathbf{+27.5\%}$ average relative improvement over baselines across $\mathbf{44}$ simulation tasks and achieves superior performance in $\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review helps learn better: Temporal Supervised Knowledge Distillation</title>
<link>https://arxiv.org/abs/2307.00811</link>
<guid>https://arxiv.org/abs/2307.00811</guid>
<content:encoded><![CDATA[
arXiv:2307.00811v3 Announce Type: replace 
Abstract: Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and advantages of our method over existing knowledge distillation methods, including various network architectures and different tasks (image classification and object detection) .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC</title>
<link>https://arxiv.org/abs/2310.02719</link>
<guid>https://arxiv.org/abs/2310.02719</guid>
<content:encoded><![CDATA[
arXiv:2310.02719v2 Announce Type: replace 
Abstract: In this paper, we introduce a general framework for analyzing the numerical conditioning of minimal problems in multiple view geometry, using tools from computational algebra and Riemannian geometry. Special motivation comes from the fact that relative pose estimation, based on standard 5-point or 7-point Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are present and there is enough data to support a hypothesis. We argue that these cases arise due to the intrinsic instability of the 5- and 7-point minimal problems. We apply our framework to characterize the instabilities, both in terms of the world scenes that lead to infinite condition number, and directly in terms of ill-conditioned image data. The approach produces computational tests for assessing the condition number before solving the minimal problem. Lastly, synthetic and real data experiments suggest that RANSAC serves not only to remove outliers, but in practice it also selects for well-conditioned image data, which is consistent with our theory.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic and Expressive Variation in Image Captions Across Languages</title>
<link>https://arxiv.org/abs/2310.14356</link>
<guid>https://arxiv.org/abs/2310.14356</guid>
<content:encoded><![CDATA[
arXiv:2310.14356v5 Announce Type: replace 
Abstract: Computer vision often treats human perception as homogeneous: an implicit assumption that visual stimuli are perceived similarly by everyone. This assumption is reflected in the way researchers collect datasets and train vision models. By contrast, literature in cross-cultural psychology and linguistics has provided evidence that people from different cultural backgrounds observe vastly different concepts even when viewing the same visual stimuli. In this paper, we study how these differences manifest themselves in vision-language datasets and models, using language as a proxy for culture. By comparing textual descriptions generated across 7 languages for the same images, we find significant differences in the semantic content and linguistic expression. When datasets are multilingual as opposed to monolingual, descriptions have higher semantic coverage on average, where coverage is measured using scene graphs, model embeddings, and linguistic taxonomies. For example, multilingual descriptions have on average 29.9% more objects, 24.5% more relations, and 46.0% more attributes than a set of monolingual captions. When prompted to describe images in different languages, popular models (e.g. LLaVA) inherit this bias and describe different parts of the image. Moreover, finetuning models on captions from one language performs best on corresponding test data from that language, while finetuning on multilingual data performs consistently well across all test data compositions. Our work points towards the need to account for and embrace the diversity of human perception in the computer vision community.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Feature-Guided Diffusion Models for Shadow Removal</title>
<link>https://arxiv.org/abs/2312.02156</link>
<guid>https://arxiv.org/abs/2312.02156</guid>
<content:encoded><![CDATA[
arXiv:2312.02156v2 Announce Type: replace 
Abstract: Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process. Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Habitat Information for Fine-grained Bird Identification</title>
<link>https://arxiv.org/abs/2312.14999</link>
<guid>https://arxiv.org/abs/2312.14999</guid>
<content:encoded><![CDATA[
arXiv:2312.14999v2 Announce Type: replace 
Abstract: Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: https://anonymous.4open.science/r/reasoning-8B7E/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction</title>
<link>https://arxiv.org/abs/2401.13174</link>
<guid>https://arxiv.org/abs/2401.13174</guid>
<content:encoded><![CDATA[
arXiv:2401.13174v4 Announce Type: replace 
Abstract: It has been revealed that small efficient dense image prediction (EDIP) models, trained using the knowledge distillation (KD) framework, encounter two key challenges, including maintaining boundary region completeness and preserving target region connectivity, despite their favorable capacity to recognize main object regions. In this work, we propose a complementary boundary and context distillation (BCD) method within the KD framework for EDIPs, which facilitates the targeted knowledge transfer from large accurate teacher models to compact efficient student models. Specifically, the boundary distillation component focuses on extracting explicit object-level semantic boundaries from the hierarchical feature maps of the backbone network to enhance the student model's mask quality in boundary regions. Concurrently, the context distillation component leverages self-relations as a bridge to transfer implicit pixel-level contexts from the teacher model to the student model, ensuring strong connectivity in target regions. Our proposed BCD method is specifically designed for EDIP tasks and is characterized by its simplicity and efficiency. Extensive experimental results across semantic segmentation, object detection, and instance segmentation on various representative datasets demonstrate that our method can outperform existing methods without requiring extra supervisions or incurring increased inference costs, resulting in well-defined object boundaries and smooth connecting regions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Msmsfnet: a multi-stream and multi-scale fusion net for edge detection</title>
<link>https://arxiv.org/abs/2404.04856</link>
<guid>https://arxiv.org/abs/2404.04856</guid>
<content:encoded><![CDATA[
arXiv:2404.04856v3 Announce Type: replace 
Abstract: Edge detection is a long-standing problem in computer vision. Despite the efficiency of existing algorithms, their performance, however, rely heavily on the pre-trained weights of the backbone network on the ImageNet dataset. The use of pre-trained weights in previous methods significantly increases the difficulty to design new models for edge detection without relying on existing well-trained ImageNet models, as pre-training the model on the ImageNet dataset is expensive and becomes compulsory to ensure the fairness of comparison. Besides, the pre-training and fine-tuning strategy is not always useful and sometimes even inaccessible. For instance, the pre-trained weights on the ImageNet dataset are unlikely to be helpful for edge detection in Synthetic Aperture Radar (SAR) images due to strong differences in the statistics between optical images and SAR images. Moreover, no dataset has comparable size to the ImageNet dataset for SAR image processing. In this work, we study the performance achievable by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi-scale fusion net (msmsfnet), for edge detection. We show in our experiments that by training all models from scratch, our model outperforms state-of-the-art edge detectors in three publicly available datasets. We also demonstrate the efficiency of our model for edge detection in SAR images, where no useful pre-trained weight is available. Finally, We show that our model is able to achieve competitive performance on the BSDS500 dataset when the pre-trained weights are used.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention</title>
<link>https://arxiv.org/abs/2404.17484</link>
<guid>https://arxiv.org/abs/2404.17484</guid>
<content:encoded><![CDATA[
arXiv:2404.17484v2 Announce Type: replace 
Abstract: Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging technique. The fundamental unit of ODT is the 1D depth-resolved trace named raw A-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a cross-sectional flow image via Doppler phase-subtraction of raw A-scans along B-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required currently, leading to prolonged scanning time and increased storage demands. Addressing this issue, we propose a novel sparse ODT reconstruction framework with an Alternative State Space Attention Network (ASSAN) that effectively reduces raw A-scans needed. Inspired by the distinct distributions of information along A-line and B-line, ASSAN applies 1D State Space Model (SSM) to each A-line to learn the intra-A-scan representation, while using 1D gated self-attention along B-line to capture the inter-A-scan features. In addition, an effective feedforward network based on sequential 1D convolutions along different axes is employed to enhance the local feature. In validation experiments on real animal data, ASSAN shows clear effectiveness in the reconstruction in comparison with state-of-the-art reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPL: Memory Augmentation and Pseudo-Labeling for Semi-Supervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2405.06198</link>
<guid>https://arxiv.org/abs/2405.06198</guid>
<content:encoded><![CDATA[
arXiv:2405.06198v3 Announce Type: replace 
Abstract: Large unlabeled data and difficult-to-identify anomalies are the urgent issues need to overcome in most industrial scene. In order to address this issue, a new meth-odology for detecting surface defects in in-dustrial settings is introduced, referred to as Memory Augmentation and Pseudo-Labeling(MAPL). The methodology first in-troduces an anomaly simulation strategy, which significantly improves the model's ability to recognize rare or unknown anom-aly types by generating simulated anomaly samples. To cope with the problem of the lack of labeling of anomalous simulated samples, a pseudo-labeler method based on a one-classifier ensemble was employed in this study, which enhances the robustness of the model in the case of limited labeling data by automatically selecting key pseudo-labeling hyperparameters. Meanwhile, a memory-enhanced learning mechanism is introduced to effectively predict abnormal regions by analyzing the difference be-tween the input samples and the normal samples in the memory pool. An end-to-end learning framework is employed by MAPL to identify the abnormal regions directly from the input data, which optimizes the ef-ficiency and real-time performance of de-tection. By conducting extensive trials on the recently developed BHAD dataset (in-cluding MVTec AD [1], Visa [2], and MDPP [3]), MAPL achieves an average im-age-level AUROC score of 86.2%, demon-strating a 5.1% enhancement compared to the original MemSeg [4] model. The source code is available at https://github.com/jzc777/MAPL.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2405.17456</link>
<guid>https://arxiv.org/abs/2405.17456</guid>
<content:encoded><![CDATA[
arXiv:2405.17456v3 Announce Type: replace 
Abstract: We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions</title>
<link>https://arxiv.org/abs/2407.01330</link>
<guid>https://arxiv.org/abs/2407.01330</guid>
<content:encoded><![CDATA[
arXiv:2407.01330v2 Announce Type: replace 
Abstract: Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large 3D shape datasets, which is costly and necessitates re-training for new datasets. This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns in localized regions, prompting us to develop a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. Despite being highly lightweight, with only 653 KB of trainable parameters and a modest-sized training dataset with 0.5 GB storage, our method enables efficient and robust surface reconstruction from point clouds without requiring for shape-specific training. Furthermore, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We conduct comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy. Notably, our lightweight framework offers rapid and reliable initialization for other unsupervised iterative approaches, improving both the efficiency and accuracy of their reconstructions. Our project and code are available at https://jbhu67.github.io/LoSF-UDF.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration</title>
<link>https://arxiv.org/abs/2407.13120</link>
<guid>https://arxiv.org/abs/2407.13120</guid>
<content:encoded><![CDATA[
arXiv:2407.13120v3 Announce Type: replace 
Abstract: Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert space and lacks accelerated variants. To address these issues, we propose a Halpern-type PPP (HPPP) algorithm, which leverages the strong convergence and acceleration properties of Halpern's iteration method. Moreover, we propose a novel algorithm for image restoration by combining HPPP with denoiser priors such as Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method. Finally, numerical experiments including several toy examples and image restoration validate the effectiveness of our proposed algorithms.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation</title>
<link>https://arxiv.org/abs/2407.18715</link>
<guid>https://arxiv.org/abs/2407.18715</guid>
<content:encoded><![CDATA[
arXiv:2407.18715v3 Announce Type: replace 
Abstract: Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency through end-to-end learning. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, which restricts effective information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization in a semantic-aligned space for SGG, enabling efficient and generalizable interaction between entities and predicates. Specifically, we introduce an end-to-end scene graph generation model, the Bidirectional Conditioning Transformer (BCTR), to implement this factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) performs multi-stage interactive feature augmentation between entities and predicates, enabling mutual enhancement between these predictions. Second, Random Feature Alignment (RFA) is present to regularize feature space by distilling multi-modal knowledge from pre-trained models. Within this regularized feature space, BCG is feasible to capture interaction patterns across diverse relationships during training, and the learned interaction patterns can generalize to unseen but semantically related relationships during inference. Extensive experiments on Visual Genome and Open Image V6 show that BCTR achieves state-of-the-art performance on both benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking</title>
<link>https://arxiv.org/abs/2408.12232</link>
<guid>https://arxiv.org/abs/2408.12232</guid>
<content:encoded><![CDATA[
arXiv:2408.12232v2 Announce Type: replace 
Abstract: Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camouflaged Object Tracking: A Benchmark</title>
<link>https://arxiv.org/abs/2408.13877</link>
<guid>https://arxiv.org/abs/2408.13877</guid>
<content:encoded><![CDATA[
arXiv:2408.13877v3 Announce Type: replace 
Abstract: Visual tracking has seen remarkable advancements, largely driven by the availability of large-scale training datasets that have enabled the development of highly accurate and robust algorithms. While significant progress has been made in tracking general objects, research on more challenging scenarios, such as tracking camouflaged objects, remains limited. Camouflaged objects, which blend seamlessly with their surroundings or other objects, present unique challenges for detection and tracking in complex environments. This challenge is particularly critical in applications such as military, security, agriculture, and marine monitoring, where precise tracking of camouflaged objects is essential. To address this gap, we introduce the Camouflaged Object Tracking Dataset (COTD), a specialized benchmark designed specifically for evaluating camouflaged object tracking methods. The COTD dataset comprises 200 sequences and approximately 80,000 frames, each annotated with detailed bounding boxes. Our evaluation of 20 existing tracking algorithms reveals significant deficiencies in their performance with camouflaged objects. To address these issues, we propose a novel tracking framework, HiPTrack-MLS, which demonstrates promising results in improving tracking performance for camouflaged objects. COTD and code are avialable at https://github.com/openat25/HIPTrack-MLS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning</title>
<link>https://arxiv.org/abs/2409.00020</link>
<guid>https://arxiv.org/abs/2409.00020</guid>
<content:encoded><![CDATA[
arXiv:2409.00020v2 Announce Type: replace 
Abstract: Crop phenology describes the physiological development stages of crops from planting to harvest which is valuable information for decision makers to plan and adapt agricultural management strategies. In the era of big Earth observation data ubiquity, attempts have been made to accurately detect crop phenology using Remote Sensing (RS) and high resolution weather data. However, most studies have focused on large scale predictions of phenology or developed methods which are not adequate to help crop modeler communities on leveraging Sentinel-1 and Sentinal-2 data and fusing them with high resolution climate data, using a novel framework. For this, we trained a Machine Learning (ML) LightGBM model to predict 13 phenological stages for eight major crops across Germany at 20 m scale. Observed phonologies were taken from German national phenology network (German Meteorological Service; DWD) between 2017 and 2021. We proposed a thorough feature selection analysis to find the best combination of RS and climate data to detect phenological stages. At national scale, predicted phenology resulted in a reasonable precision of R2 > 0.43 and a low Mean Absolute Error of 6 days, averaged over all phenological stages and crops. The spatio-temporal analysis of the model predictions demonstrates its transferability across different spatial and temporal context of Germany. The results indicated that combining radar sensors with climate data yields a very promising performance for a multitude of practical applications. Moreover, these improvements are expected to be useful to generate highly valuable input for crop model calibrations and evaluations, facilitate informed agricultural decisions, and contribute to sustainable food production to address the increasing global food demand.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGEV++: Iterative Multi-range Geometry Encoding Volumes for Stereo Matching</title>
<link>https://arxiv.org/abs/2409.00638</link>
<guid>https://arxiv.org/abs/2409.00638</guid>
<content:encoded><![CDATA[
arXiv:2409.00638v3 Announce Type: replace 
Abstract: Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23\% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9\% and 54.8\% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks. The code is publicly available at https://github.com/gangweix/IGEV and https://github.com/gangweix/IGEV-plusplus.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance</title>
<link>https://arxiv.org/abs/2409.06002</link>
<guid>https://arxiv.org/abs/2409.06002</guid>
<content:encoded><![CDATA[
arXiv:2409.06002v4 Announce Type: replace 
Abstract: Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this https URL}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation</title>
<link>https://arxiv.org/abs/2409.18653</link>
<guid>https://arxiv.org/abs/2409.18653</guid>
<content:encoded><![CDATA[
arXiv:2409.18653v2 Announce Type: replace 
Abstract: This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code is available at https://github.com/zhoustan/SAM2-VCOS
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction</title>
<link>https://arxiv.org/abs/2409.20223</link>
<guid>https://arxiv.org/abs/2409.20223</guid>
<content:encoded><![CDATA[
arXiv:2409.20223v2 Announce Type: replace 
Abstract: Understanding and predicting pedestrian crossing behavioral intention is crucial for the driving safety of autonomous vehicles. Nonetheless, challenges emerge when using promising images or environmental context masks to extract various factors for time-series network modeling, causing pre-processing errors or a loss of efficiency. Typically, pedestrian positions captured by onboard cameras are often distorted and do not accurately reflect their actual movements. To address these issues, GTransPDM -- a Graph-embedded Transformer with a Position Decoupling Module -- was developed for pedestrian crossing intention prediction by leveraging multi-modal features. First, a positional decoupling module was proposed to decompose pedestrian lateral motion and encode depth cues in the image view. Then, a graph-embedded Transformer was designed to capture the spatio-temporal dynamics of human pose skeletons, integrating essential factors such as position, skeleton, and ego-vehicle motion. Experimental results indicate that the proposed method achieves 92% accuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a processing speed of 0.05ms. It outperforms the state-of-the-art in comparison.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Large Motion Models with Million-Level Human Motions</title>
<link>https://arxiv.org/abs/2410.03311</link>
<guid>https://arxiv.org/abs/2410.03311</guid>
<content:encoded><![CDATA[
arXiv:2410.03311v2 Announce Type: replace 
Abstract: Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named Being-M0, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://github.com/BeingBeyond/Being-M0.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotating-star Pattern for Camera Calibration</title>
<link>https://arxiv.org/abs/2410.13371</link>
<guid>https://arxiv.org/abs/2410.13371</guid>
<content:encoded><![CDATA[
arXiv:2410.13371v3 Announce Type: replace 
Abstract: Camera calibration is fundamental to 3D vision, and the choice of calibration pattern greatly affects the accuracy. To address aberration issue, star-shaped pattern has been proposed as alternatives to traditional checkerboard. However, such pattern suffers from aliasing artifacts. In this paper, we present a novel solution by employing a series of checkerboard patterns rotated around a central point instead of a single star-shaped pattern. We further propose a complete feature extraction algorithm tailored for this design. Experimental results demonstrate that our approach offers improved accuracy over the conventional star-shaped pattern and achieves high stability across varying exposure levels.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2410.19794</link>
<guid>https://arxiv.org/abs/2410.19794</guid>
<content:encoded><![CDATA[
arXiv:2410.19794v2 Announce Type: replace 
Abstract: Deep Neural Networks (DNNs) are increasingly deployed across applications. However, ensuring their reliability remains a challenge, and in many situations, alternative models with similar functionality and accuracy are available. Traditional accuracy-based evaluations often fail to capture behavioral differences between models, especially with limited test datasets, making it difficult to select or combine models effectively. Differential testing addresses this by generating test inputs that expose discrepancies in DNN model behavior. However, existing approaches face significant limitations: many rely on model internals or are constrained by available seed inputs. To address these challenges, we propose DiffGAN, a black-box test image generation approach for differential testing of DNN models. DiffGAN leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to generate diverse and valid triggering inputs that reveal behavioral discrepancies between models. DiffGAN employs two custom fitness functions, focusing on diversity and divergence, to guide the exploration of the GAN input space and identify discrepancies between models' outputs. By strategically searching this space, DiffGAN generates inputs with specific features that trigger differences in model behavior. DiffGAN is black-box, making it applicable in more situations. We evaluate DiffGAN on eight DNN model pairs trained on widely used image datasets. Our results show DiffGAN significantly outperforms a SOTA baseline, generating four times more triggering inputs, with greater diversity and validity, within the same budget. Additionally, the generated inputs improve the accuracy of a machine learning-based model selection mechanism, which selects the best-performing model based on input characteristics and can serve as a smart output voting mechanism when using alternative models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veri-Car: Towards Open-world Vehicle Information Retrieval</title>
<link>https://arxiv.org/abs/2411.06864</link>
<guid>https://arxiv.org/abs/2411.06864</guid>
<content:encoded><![CDATA[
arXiv:2411.06864v4 Announce Type: replace 
Abstract: Many industrial and service sectors require tools to extract vehicle characteristics from images. This is a complex task not only by the variety of noise, and large number of classes, but also by the constant introduction of new vehicle models to the market. In this paper, we present Veri-Car, an information retrieval integrated approach designed to help on this task. It leverages supervised learning techniques to accurately identify the make, type, model, year, color, and license plate of cars. The approach also addresses the challenge of handling open-world problems, where new car models and variations frequently emerge, by employing a sophisticated combination of pre-trained models, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust performance, achieving high precision and accuracy in classifying both seen and unseen data. Additionally, it integrates an ensemble license plate detection, and an OCR model to extract license plate numbers with impressive accuracy.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning</title>
<link>https://arxiv.org/abs/2411.07742</link>
<guid>https://arxiv.org/abs/2411.07742</guid>
<content:encoded><![CDATA[
arXiv:2411.07742v4 Announce Type: replace 
Abstract: This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive point cloud sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of point cloud sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Without incurring additional computational overhead, we increase the number of point cloud sweeps from 10, a common practice, to as many as 40. Consequently, there is a significant enhancement in perception performance. For instance, in nuScenes 3D object detection and BEV map segmentation tasks, our pruning strategy improves several 3D perception baseline methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules</title>
<link>https://arxiv.org/abs/2411.11011</link>
<guid>https://arxiv.org/abs/2411.11011</guid>
<content:encoded><![CDATA[
arXiv:2411.11011v2 Announce Type: replace 
Abstract: Fire incidents in urban and forested areas pose serious threats,underscoring the need for more effective detection technologies. To address these challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted improvements for detecting small fires and smoke. The model integrates the CARAFE up-sampling operator and a context-guided module to reduce information loss during up-sampling and down-sampling, thereby retaining richer feature representations. Additionally, an inverted residual mobile block enhanced C2f module captures small targets and fine smoke patterns, a critical improvement over the original model's detection capacity.For validation, we introduce Web-Fire, a dataset curated for fire and smoke detection across diverse real-world scenarios. Experimental results indicate that CCi-YOLOv8n outperforms YOLOv8n in detection precision, confirming its effectiveness for robust fire detection tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transmission Line Defect Detection Based on UAV Patrol Images and Vision-language Pretraining</title>
<link>https://arxiv.org/abs/2411.11370</link>
<guid>https://arxiv.org/abs/2411.11370</guid>
<content:encoded><![CDATA[
arXiv:2411.11370v2 Announce Type: replace 
Abstract: Unmanned aerial vehicle (UAV) patrol inspection has emerged as a predominant approach in transmission line monitoring owing to its cost-effectiveness. Detecting defects in transmission lines is a critical task during UAV patrol inspection. However, due to imaging distance and shooting angles, UAV patrol images often suffer from insufficient defect-related visual information, which has an adverse effect on detection accuracy. In this article, we propose a novel method for detecting defects in UAV patrol images, which is based on vision-language pretraining for transmission line (VLP-TL) and a progressive transfer strategy (PTS). Specifically, VLP-TL contains two novel pretraining tasks tailored for the transmission line scenario, aimimg at pretraining an image encoder with abundant knowledge acquired from both visual and linguistic information. Transferring the pretrained image encoder to the defect detector as its backbone can effectively alleviate the insufficient visual information problem. In addition, the PTS further improves transfer performance by progressively bridging the gap between pretraining and downstream defection detection. Experimental results demonstrate that the proposed method significantly improves defect detection accuracy by jointly utilizing multimodal information, overcoming the limitations of insufficient defect-related visual information provided by UAV patrol images.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2411.11904</link>
<guid>https://arxiv.org/abs/2411.11904</guid>
<content:encoded><![CDATA[
arXiv:2411.11904v3 Announce Type: replace 
Abstract: Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph</title>
<link>https://arxiv.org/abs/2411.14494</link>
<guid>https://arxiv.org/abs/2411.14494</guid>
<content:encoded><![CDATA[
arXiv:2411.14494v3 Announce Type: replace 
Abstract: A facial morph is an image created by combining two face images pertaining to two distinct identities. Face demorphing inverts the process and tries to recover the original images constituting a facial morph. While morph attack detection (MAD) techniques can be used to flag morph images, they do not divulge any visual information about the faces used to create them. Demorphing helps address this problem. Existing demorphing techniques are either very restrictive (assume identities during testing) or produce feeble outputs (both outputs look very similar). In this paper, we overcome these issues by proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph images. Our method overcomes morph-replication and produces high quality reconstructions of the bonafide images used to create the morphs. Moreover, our method is highly generalizable across demorphing paradigms (differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and MorDiff datasets to showcase the efficacy of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opt-In Art: Learning Art Styles Only from Few Examples</title>
<link>https://arxiv.org/abs/2412.00176</link>
<guid>https://arxiv.org/abs/2412.00176</guid>
<content:encoded><![CDATA[
arXiv:2412.00176v2 Announce Type: replace 
Abstract: We explore whether pre-training on datasets with paintings is necessary for a model to learn an artistic style with only a few examples. To investigate this, we train a text-to-image model exclusively on photographs, without access to any painting-related content. We show that it is possible to adapt a model that is trained without paintings to an artistic style, given only few examples. User studies and automatic evaluations confirm that our model (post-adaptation) performs on par with state-of-the-art models trained on massive datasets that contain artistic content like paintings, drawings or illustrations. Finally, using data attribution techniques, we analyze how both artistic and non-artistic datasets contribute to generating artistic-style images. Surprisingly, our findings suggest that high-quality artistic outputs can be achieved without prior exposure to artistic data, indicating that artistic style generation can occur in a controlled, opt-in manner using only a limited, carefully selected set of training examples.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SerialGen: Personalized Image Generation by First Standardization Then Personalization</title>
<link>https://arxiv.org/abs/2412.01485</link>
<guid>https://arxiv.org/abs/2412.01485</guid>
<content:encoded><![CDATA[
arXiv:2412.01485v2 Announce Type: replace 
Abstract: In this work, we are interested in achieving both high text controllability and whole-body appearance consistency in the generation of personalized human characters. We propose a novel framework, named SerialGen, which is a serial generation method consisting of two stages: first, a standardization stage that standardizes reference images, and then a personalized generation stage based on the standardized reference. Furthermore, we introduce two modules aimed at enhancing the standardization process. Our experimental results validate the proposed framework's ability to produce personalized images that faithfully recover the reference image's whole-body appearance while accurately responding to a wide range of text prompts. Through thorough analysis, we highlight the critical contribution of the proposed serial generation method and standardization model, evidencing enhancements in appearance consistency between reference and output images and across serial outputs generated from diverse text prompts. The term "Serial" in this work carries a double meaning: it refers to the two-stage method and also underlines our ability to generate serial images with consistent appearance throughout.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs</title>
<link>https://arxiv.org/abs/2412.01818</link>
<guid>https://arxiv.org/abs/2412.01818</guid>
<content:encoded><![CDATA[
arXiv:2412.01818v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis</title>
<link>https://arxiv.org/abs/2412.09521</link>
<guid>https://arxiv.org/abs/2412.09521</guid>
<content:encoded><![CDATA[
arXiv:2412.09521v2 Announce Type: replace 
Abstract: Pathological diagnosis is vital for determining disease characteristics, guiding treatment, and assessing prognosis, relying heavily on detailed, multi-scale analysis of high-resolution whole slide images (WSI). However, traditional pure vision models face challenges of redundant feature extraction, whereas existing large vision-language models (LVLMs) are limited by input resolution constraints, hindering their efficiency and accuracy. To overcome these issues, we propose two innovative strategies: the mixed task-guided feature enhancement, which directs feature extraction toward lesion-related details across scales, and the prompt-guided detail feature completion, which integrates coarse- and fine-grained features from WSI based on specific prompts without compromising inference speed. Leveraging a comprehensive dataset of 490,000 samples from diverse pathology tasks-including cancer detection, grading, vascular and neural invasion identification, and so on-we trained the pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that this model significantly outperforms existing methods in diagnostic accuracy and efficiency, offering an interactive, clinically aligned approach for auxiliary diagnosis in a wide range of pathology applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-QuAD: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification</title>
<link>https://arxiv.org/abs/2412.14489</link>
<guid>https://arxiv.org/abs/2412.14489</guid>
<content:encoded><![CDATA[
arXiv:2412.14489v3 Announce Type: replace 
Abstract: Multimodal machine learning has achieved remarkable progress in many scenarios, but its reliability is undermined by varying sample quality. This paper finds that existing reliable multimodal classification methods not only fail to provide robust estimation of data quality, but also lack dynamic networks for sample-specific depth and parameters to achieve reliable inference. To this end, a novel framework for multimodal reliable classification termed \textit{Multi-level Quality-Adaptive Dynamic multimodal network} (Multi-QuAD) is proposed. Multi-QuAD first adopts a novel approach based on noise-free prototypes and a classifier-free design to reliably estimate the quality of each sample at both modality and feature levels. It then achieves sample-specific network depth via the \textbf{\textit{Global Confidence Normalized Depth (GCND)}} mechanism. By normalizing depth across modalities and samples, \textit{\textbf{GCND}} effectively mitigates the impact of challenging modality inputs on dynamic depth reliability. Furthermore, Multi-QuAD provides sample-adaptive network parameters via the \textbf{\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by feature-level quality. The cross-modality layer-wise greedy strategy in \textbf{\textit{LGP}} designs a reliable parameter prediction paradigm for multimodal networks with variable architecture for the first time. Experiments conducted on four datasets demonstrate that Multi-QuAD significantly outperforms state-of-the-art methods in classification performance and reliability, exhibiting strong adaptability to data with diverse quality.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</title>
<link>https://arxiv.org/abs/2501.00843</link>
<guid>https://arxiv.org/abs/2501.00843</guid>
<content:encoded><![CDATA[
arXiv:2501.00843v3 Announce Type: replace 
Abstract: In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Tropical Cyclone Forecasting With Video Diffusion Models</title>
<link>https://arxiv.org/abs/2501.16003</link>
<guid>https://arxiv.org/abs/2501.16003</guid>
<content:encoded><![CDATA[
arXiv:2501.16003v5 Announce Type: replace 
Abstract: Tropical cyclone (TC) forecasting is crucial for disaster preparedness and mitigation. While recent deep learning approaches have shown promise, existing methods often treat TC evolution as a series of independent frame-to-frame predictions, limiting their ability to capture long-term dynamics. We present a novel application of video diffusion models for TC forecasting that explicitly models temporal dependencies through additional temporal layers. Our approach enables the model to generate multiple frames simultaneously, better capturing cyclone evolution patterns. We introduce a two-stage training strategy that significantly improves individual-frame quality and performance in low-data regimes. Experimental results show our method outperforms the previous approach of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably, we extend the reliable forecasting horizon from 36 to 50 hours. Through comprehensive evaluation using both traditional metrics and Fr\'echet Video Distance (FVD), we demonstrate that our approach produces more temporally coherent forecasts while maintaining competitive single-frame quality. Code accessible at https://github.com/Ren-creater/forecast-video-diffmodels.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning</title>
<link>https://arxiv.org/abs/2502.00848</link>
<guid>https://arxiv.org/abs/2502.00848</guid>
<content:encoded><![CDATA[
arXiv:2502.00848v2 Announce Type: replace 
Abstract: Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title>
<link>https://arxiv.org/abs/2502.02283</link>
<guid>https://arxiv.org/abs/2502.02283</guid>
<content:encoded><![CDATA[
arXiv:2502.02283v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling</title>
<link>https://arxiv.org/abs/2502.02590</link>
<guid>https://arxiv.org/abs/2502.02590</guid>
<content:encoded><![CDATA[
arXiv:2502.02590v2 Announce Type: replace 
Abstract: 3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is https://articulate-anymesh.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</title>
<link>https://arxiv.org/abs/2502.04847</link>
<guid>https://arxiv.org/abs/2502.04847</guid>
<content:encoded><![CDATA[
arXiv:2502.04847v4 Announce Type: replace 
Abstract: Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.14908</link>
<guid>https://arxiv.org/abs/2502.14908</guid>
<content:encoded><![CDATA[
arXiv:2502.14908v2 Announce Type: replace 
Abstract: Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lacks systematic investigation of cross-modal knowledge conflicts. This research introduces \segsub, a framework for applying targeted image perturbations to investigate VLM resilience against knowledge conflicts. Our analysis reveals distinct vulnerability patterns: while VLMs are robust to parametric conflicts (20% adherence rates), they exhibit significant weaknesses in identifying counterfactual conditions (<30% accuracy) and resolving source conflicts (<1% accuracy). Correlations between contextual richness and hallucination rate (r = -0.368, p = 0.003) reveal the kinds of images that are likely to cause hallucinations. Through targeted fine-tuning on our benchmark dataset, we demonstrate improvements in VLM knowledge conflict detection, establishing a foundation for developing hallucination-resilient multimodal systems in information-sensitive environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical Inspired MRI Lesion Segmentation</title>
<link>https://arxiv.org/abs/2502.16032</link>
<guid>https://arxiv.org/abs/2502.16032</guid>
<content:encoded><![CDATA[
arXiv:2502.16032v2 Announce Type: replace 
Abstract: Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting pathological tissues in various diseases. Different MRI sequences have different contrast mechanisms and sensitivities for different types of lesions, which pose challenges to accurate and consistent lesion segmentation. In clinical practice, radiologists commonly use the sub-sequence feature, i.e. the difference between post contrast-enhanced T1-weighted (post) and pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we propose a residual fusion method to learn subsequence representation for MRI lesion segmentation. Specifically, we iteratively and adaptively fuse features from pre- and post-contrast sequences at multiple resolutions, using dynamic weights to achieve optimal fusion and address diverse lesion enhancement patterns. Our method achieves state-of-the-art performances on BraTS2023 dataset for brain tumor segmentation and our in-house breast MRI dataset for breast lesion segmentation. Our method is clinically inspired and has the potential to facilitate lesion segmentation in various applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs</title>
<link>https://arxiv.org/abs/2502.19159</link>
<guid>https://arxiv.org/abs/2502.19159</guid>
<content:encoded><![CDATA[
arXiv:2502.19159v2 Announce Type: replace 
Abstract: Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. However, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the ``Patch-like'' feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we propose a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</title>
<link>https://arxiv.org/abs/2503.01103</link>
<guid>https://arxiv.org/abs/2503.01103</guid>
<content:encoded><![CDATA[
arXiv:2503.01103v2 Announce Type: replace 
Abstract: While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection</title>
<link>https://arxiv.org/abs/2503.01234</link>
<guid>https://arxiv.org/abs/2503.01234</guid>
<content:encoded><![CDATA[
arXiv:2503.01234v3 Announce Type: replace 
Abstract: Metal defect detection is critical in industrial quality assurance, yet existing methods struggle with grayscale variations and complex defect states, limiting its robustness. To address these challenges, this paper proposes a Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced detection framework integrating a Dynamic Gamma Correction (GC) module to enhance grayscale representation and optimize feature extraction for precise defect reconstruction. A State-Space Search Management (SSM) architecture captures robust multi-scale features, effectively handling defects of varying shapes and scales. Focal Loss is employed to mitigate class imbalance and refine detection accuracy. Additionally, the CD5-DET dataset is introduced, specifically designed for port container maintenance, featuring significant grayscale variations and intricate defect patterns. Experimental results demonstrate that the proposed model achieves substantial improvements, with mAP@0.5 gains of 27.6\%, 6.6\%, and 2.6\% on the CD5-DET, NEU-DET, and GC10-DET datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals</title>
<link>https://arxiv.org/abs/2503.06473</link>
<guid>https://arxiv.org/abs/2503.06473</guid>
<content:encoded><![CDATA[
arXiv:2503.06473v4 Announce Type: replace 
Abstract: Growing evidence suggests that layer attention mechanisms, which enhance interaction among layers in deep neural networks, have significantly advanced network architectures. However, existing layer attention methods suffer from redundancy, as attention weights learned by adjacent layers often become highly similar. This redundancy causes multiple layers to extract nearly identical features, reducing the model's representational capacity and increasing training time. To address this issue, we propose a novel approach to quantify redundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent layers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM) method that accurately identifies and skips redundant layers, thereby maintaining model stability. Our proposed Efficient Layer Attention (ELA) architecture, improves both training efficiency and overall performance, achieving a 30% reduction in training time while enhancing performance in tasks such as image classification and object detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction</title>
<link>https://arxiv.org/abs/2503.06587</link>
<guid>https://arxiv.org/abs/2503.06587</guid>
<content:encoded><![CDATA[
arXiv:2503.06587v2 Announce Type: replace 
Abstract: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We found the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectified the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Referring to Any Person</title>
<link>https://arxiv.org/abs/2503.08507</link>
<guid>https://arxiv.org/abs/2503.08507</guid>
<content:encoded><![CDATA[
arXiv:2503.08507v2 Announce Type: replace 
Abstract: Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-camera orientation tracking method for anisotropic particles in particle-laden flows</title>
<link>https://arxiv.org/abs/2503.08694</link>
<guid>https://arxiv.org/abs/2503.08694</guid>
<content:encoded><![CDATA[
arXiv:2503.08694v2 Announce Type: replace 
Abstract: A method for particle orientation tracking is developed and demonstrated specifically for anisotropic particles. Using (high-speed) multi-camera recordings of anisotropic particles from different viewpoints, we reconstruct the 3D location and orientation of these particles using their known shape. This paper describes an algorithm which tracks the location and orientation of multiple anisotropic particles over time, enabling detailed investigations of location, orientation, and rotation statistics. The robustness and error of this method is quantified, and we explore the effects of noise, image size, the number of used cameras, and the camera arrangement by applying the algorithm to synthetic images. We showcase several use-cases of this method in several experiments (in both quiescent and turbulent fluids), demonstrating the effectiveness and broad applicability of the described tracking method. The proposed method is shown to work for widely different particle shapes, successfully tracks multiple particles simultaneously, and the method can distinguish between different types of particles.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network</title>
<link>https://arxiv.org/abs/2503.13179</link>
<guid>https://arxiv.org/abs/2503.13179</guid>
<content:encoded><![CDATA[
arXiv:2503.13179v2 Announce Type: replace 
Abstract: This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Improving Motion Diffusion Models with Sparse Keyframes</title>
<link>https://arxiv.org/abs/2503.13859</link>
<guid>https://arxiv.org/abs/2503.13859</guid>
<content:encoded><![CDATA[
arXiv:2503.13859v2 Announce Type: replace 
Abstract: Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis. However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames. The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks. Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes. Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps. Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps. We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection</title>
<link>https://arxiv.org/abs/2503.13903</link>
<guid>https://arxiv.org/abs/2503.13903</guid>
<content:encoded><![CDATA[
arXiv:2503.13903v2 Announce Type: replace 
Abstract: Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object detection methods solely rely on CNNs or ViTs to conduct feature aggregation, which hampers their capability to simultaneously leverage global and local information, thereby resulting in limited detection performance. In this paper, we propose a Transformer-GraphFormer Blender Network (TGBFormer) for video object detection, with three key technical improvements to fully exploit the advantages of transformers and graph convolutional networks while compensating for their limitations. First, we develop a spatial-temporal transformer module to aggregate global contextual information, constituting global representations with long-range feature dependencies. Second, we introduce a spatial-temporal GraphFormer module that utilizes local spatial and temporal relationships to aggregate features, generating new local representations that are complementary to the transformer outputs. Third, we design a global-local feature blender module to adaptively couple transformer-based global representations and GraphFormer-based local representations. Extensive experiments demonstrate that our TGBFormer establishes new state-of-the-art results on the ImageNet VID dataset. Particularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS on a single Tesla A100 GPU.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision Centric Remote Sensing Benchmark</title>
<link>https://arxiv.org/abs/2503.15816</link>
<guid>https://arxiv.org/abs/2503.15816</guid>
<content:encoded><![CDATA[
arXiv:2503.15816v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks but their remote sensing (RS) counterpart are relatively under explored. Unlike natural images, RS imagery presents unique challenges that current MLLMs struggle to handle, particularly in visual grounding and spatial reasoning. This study investigates the limitations of CLIP-based MLLMs in RS, highlighting their failure to differentiate visually distinct yet semantically similar RS images. To address this, we introduce a remote sensing multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models incorrectly assign high similarity scores to visually distinct RS images. Through a visual question answering (VQA) evaluation, we analyze the performance of state-of-the-art MLLMs, revealing significant limitations in RS specific representation learning. The results provide valuable insights into the weaknesses of CLIP-based visual encoding and offer a foundation for future research to develop more effective MLLMs tailored for remote sensing applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2503.15831</link>
<guid>https://arxiv.org/abs/2503.15831</guid>
<content:encoded><![CDATA[
arXiv:2503.15831v2 Announce Type: replace 
Abstract: Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2503.15970</link>
<guid>https://arxiv.org/abs/2503.15970</guid>
<content:encoded><![CDATA[
arXiv:2503.15970v2 Announce Type: replace 
Abstract: Facial Expression Recognition (FER) plays a crucial role in human affective analysis and has been widely applied in computer vision tasks such as human-computer interaction and psychological assessment. The 8th Affective Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions using the video-based Aff-Wild2 dataset. This challenge includes various tasks, including the video-based EXPR recognition track, which is our primary focus. In this paper, we demonstrate that addressing label ambiguity and class imbalance, which are known to cause performance degradation, can lead to meaningful performance improvements. Specifically, we propose Video-based Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to each frame in a clip to address label ambiguity and effectively capture temporal variations in facial expressions. Furthermore, we introduce a simple and effective augmentation strategy to reduce redundancy between consecutive frames, which is a primary cause of overfitting. Through extensive experiments, we validate the effectiveness of our approach, demonstrating significant improvements in video-based FER performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2503.16188</link>
<guid>https://arxiv.org/abs/2503.16188</guid>
<content:encoded><![CDATA[
arXiv:2503.16188v4 Announce Type: replace 
Abstract: This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM image classification, using verifiable rewards for fine-tuning. Experiments show CLS-RL significantly outperforms SFT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RL on 6 diverse tasks across different model sizes and types. Experimental results reveal three key findings: 1). Visual perception tasks do not require thinking during RFT, as No-Thinking-RL consistently outperforms or matches Thinking-based RFT across model sizes. 2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-based RFT less effective than No-Thinking-RL. 3). There are inconsistencies between the answers in the thinking and answer tags for some responses of thinking-based RFT, which show lower accuracy than the overall accuracy. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance. To test this hypothesis, we propose Think-After-Answer, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an Adaptive-Thinking method. Experiments show that it converges to a specific prompt depending on model capability and task complexity, achieving comparable or better performance than both Thinking and No-Thinking-RL. This suggests MLLMs can adaptively decide to think or not based on their capabilities and task complexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2504.04893</link>
<guid>https://arxiv.org/abs/2504.04893</guid>
<content:encoded><![CDATA[
arXiv:2504.04893v3 Announce Type: replace 
Abstract: Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FANeRV: Frequency Separation and Augmentation based Neural Representation for Video</title>
<link>https://arxiv.org/abs/2504.06755</link>
<guid>https://arxiv.org/abs/2504.06755</guid>
<content:encoded><![CDATA[
arXiv:2504.06755v3 Announce Type: replace 
Abstract: Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block. This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene</title>
<link>https://arxiv.org/abs/2504.09455</link>
<guid>https://arxiv.org/abs/2504.09455</guid>
<content:encoded><![CDATA[
arXiv:2504.09455v2 Announce Type: replace 
Abstract: A common dilemma while photographing a scene is whether to capture it at a wider angle, allowing more of the scene to be covered but in less detail or to click in a narrow angle that captures better details but leaves out portions of the scene. We propose a novel method in this paper that infuses wider shots with finer quality details that is usually associated with an image captured by the primary lens by capturing the same scene using both narrow and wide field of view (FoV) lenses. We do so by training a Generative Adversarial Network (GAN)-based model to learn to extract the visual quality parameters from a narrow-angle shot and to transfer these to the corresponding wide-angle image of the scene using residual connections and an attention-based fusion module. We have mentioned in details the proposed technique to isolate the visual essence of an image and to transfer it into another image. We have also elaborately discussed our implementation details and have presented the results of evaluation over several benchmark datasets and comparisons with contemporary advancements in the field.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocusedAD: Character-centric Movie Audio Description</title>
<link>https://arxiv.org/abs/2504.12157</link>
<guid>https://arxiv.org/abs/2504.12157</guid>
<content:encoded><![CDATA[
arXiv:2504.12157v3 Announce Type: replace 
Abstract: Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForgetMe: Evaluating Selective Forgetting in Generative Models</title>
<link>https://arxiv.org/abs/2504.12574</link>
<guid>https://arxiv.org/abs/2504.12574</guid>
<content:encoded><![CDATA[
arXiv:2504.12574v2 Announce Type: replace 
Abstract: The widespread adoption of diffusion models in image generation has increased the demand for privacy-compliant unlearning. However, due to the high-dimensional nature and complex feature representations of diffusion models, achieving selective unlearning remains challenging, as existing methods struggle to remove sensitive information while preserving the consistency of non-sensitive regions. To address this, we propose an Automatic Dataset Creation Framework based on prompt-based layered editing and training-free local feature removal, constructing the ForgetMe dataset and introducing the Entangled evaluation metric. The Entangled metric quantifies unlearning effectiveness by assessing the similarity and consistency between the target and background regions and supports both paired (Entangled-D) and unpaired (Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe dataset encompasses a diverse set of real and synthetic scenarios, including CUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We apply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on this dataset and validate the effectiveness of both the ForgetMe dataset and the Entangled metric, establishing them as benchmarks for selective unlearning. Our work provides a scalable and adaptable solution for advancing privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compile Scene Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13617</link>
<guid>https://arxiv.org/abs/2504.13617</guid>
<content:encoded><![CDATA[
arXiv:2504.13617v3 Announce Type: replace 
Abstract: Next-token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. We design a set of graph-centric rewards, including three recall-based variants -- Hard Recall, Hard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial alignment between predictions and ground truth at the object and relation levels. A format consistency reward further ensures that outputs follow the expected structural schema. Extensive experiments on the VG150 and PSG benchmarks show that R1-SGG substantially reduces failure rates and achieves strong performance in Recall and mean Recall, surpassing traditional SGG models and existing multimodal language models. Our code is available at https://github.com/gpt4vision/R1-SGG
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Sound Source Localization with Joint Slot Attention on Image and Audio</title>
<link>https://arxiv.org/abs/2504.15118</link>
<guid>https://arxiv.org/abs/2504.15118</guid>
<content:encoded><![CDATA[
arXiv:2504.15118v2 Announce Type: replace 
Abstract: Sound source localization (SSL) is the task of locating the source of sound within an image. Due to the lack of localization labels, the de facto standard in SSL has been to represent an image and audio as a single embedding vector each, and use them to learn SSL via contrastive learning. To this end, previous work samples one of local image features as the image embedding and aggregates all local audio features to obtain the audio embedding, which is far from optimal due to the presence of noise and background irrelevant to the actual target in the input. We present a novel SSL method that addresses this chronic issue by joint slot attention on image and audio. To be specific, two slots competitively attend image and audio features to decompose them into target and off-target representations, and only target representations of image and audio are used for contrastive learning. Also, we introduce cross-modal attention matching to further align local features of image and audio. Our method achieved the best in almost all settings on three public benchmarks for SSL, and substantially outperformed all the prior work in cross-modal retrieval.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs</title>
<link>https://arxiv.org/abs/2504.17040</link>
<guid>https://arxiv.org/abs/2504.17040</guid>
<content:encoded><![CDATA[
arXiv:2504.17040v2 Announce Type: replace 
Abstract: We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableCenterNet: A one-stage network for table structure recognition</title>
<link>https://arxiv.org/abs/2504.17522</link>
<guid>https://arxiv.org/abs/2504.17522</guid>
<content:encoded><![CDATA[
arXiv:2504.17522v2 Announce Type: replace 
Abstract: Table structure recognition aims to parse tables in unstructured data into machine-understandable formats. Recent methods address this problem through a two-stage process or optimized one-stage approaches. However, these methods either require multiple networks to be serially trained and perform more time-consuming sequential decoding, or rely on complex post-processing algorithms to parse the logical structure of tables. They struggle to balance cross-scenario adaptability, robustness, and computational efficiency. In this paper, we propose a one-stage end-to-end table structure parsing network called TableCenterNet. This network unifies the prediction of table spatial and logical structure into a parallel regression task for the first time, and implicitly learns the spatial-logical location mapping laws of cells through a synergistic architecture of shared feature extraction layers and task-specific decoding. Compared with two-stage methods, our method is easier to train and faster to infer. Experiments on benchmark datasets show that TableCenterNet can effectively parse table structures in diverse scenarios and achieve state-of-the-art performance on the TableGraph-24k dataset. Code is available at https://github.com/dreamy-xay/TableCenterNet.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</title>
<link>https://arxiv.org/abs/2504.18864</link>
<guid>https://arxiv.org/abs/2504.18864</guid>
<content:encoded><![CDATA[
arXiv:2504.18864v2 Announce Type: replace 
Abstract: The need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems</title>
<link>https://arxiv.org/abs/2311.11796</link>
<guid>https://arxiv.org/abs/2311.11796</guid>
<content:encoded><![CDATA[
arXiv:2311.11796v2 Announce Type: replace-cross 
Abstract: As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini: A Family of Highly Capable Multimodal Models</title>
<link>https://arxiv.org/abs/2312.11805</link>
<guid>https://arxiv.org/abs/2312.11805</guid>
<content:encoded><![CDATA[
arXiv:2312.11805v5 Announce Type: replace-cross 
Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning</title>
<link>https://arxiv.org/abs/2402.06223</link>
<guid>https://arxiv.org/abs/2402.06223</guid>
<content:encoded><![CDATA[
arXiv:2402.06223v2 Announce Type: replace-cross 
Abstract: Directed acyclic graphs (DAGs) are fundamental graph structures in causal modeling, but identifying the desired DAG from observational data often requires strong assumptions that may not hold in real-world scenarios, especially for latent causal models and complex multimodal data. This raises the question of whether we can relax or bypass the DAG assumption while maintaining practical utility. In this work, we propose a novel latent partial causal model for multimodal data, featuring two latent coupled variables, connected by an undirected edge, to represent the transfer of knowledge across modalities. Under specific statistical assumptions, we establish an identifiability result, demonstrating that representations learned by multimodal contrastive learning correspond to the latent coupled variables up to a trivial transformation. This result deepens our understanding of the why multimodal contrastive learning works, highlights its potential for disentanglement, and expands the utility of pre-trained models like CLIP. Synthetic experiments confirm the robustness of our findings, even when the assumptions are partially violated. Most importantly, experiments on a pre-trained CLIP model embodies disentangled representations, enabling few-shot learning and improving domain generalization across diverse real-world datasets. Together, these contributions push the boundaries of multimodal contrastive learning, both theoretically and, crucially, in practical applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples</title>
<link>https://arxiv.org/abs/2404.19460</link>
<guid>https://arxiv.org/abs/2404.19460</guid>
<content:encoded><![CDATA[
arXiv:2404.19460v3 Announce Type: replace-cross 
Abstract: Adversarial examples are typically optimized with gradient-based attacks. While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models. This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others. In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks. To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences. We then introduce our framework, which evaluates their effectiveness and efficiency. We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget. Our extensive experimental analysis compares more than $100$ attack implementations with a total of over $800$ different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches. Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all. We release AttackBench as a publicly-available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2408.14419</link>
<guid>https://arxiv.org/abs/2408.14419</guid>
<content:encoded><![CDATA[
arXiv:2408.14419v2 Announce Type: replace-cross 
Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance. We benchmark leading LLMs as of late 2024 - including GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset and found that our benchmark was challenging to all of them, suggesting room for future large language models to improve.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering</title>
<link>https://arxiv.org/abs/2409.02426</link>
<guid>https://arxiv.org/abs/2409.02426</guid>
<content:encoded><![CDATA[
arXiv:2409.02426v3 Announce Type: replace-cross 
Abstract: Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.13439</link>
<guid>https://arxiv.org/abs/2410.13439</guid>
<content:encoded><![CDATA[
arXiv:2410.13439v4 Announce Type: replace-cross 
Abstract: Supervised contrastive learning has achieved remarkable success by leveraging label information; however, determining positive samples in multi-label scenarios remains a critical challenge. In multi-label supervised contrastive learning (MSCL), multi-label relations are not yet fully defined, leading to ambiguity in identifying positive samples and formulating contrastive loss functions to construct the representation space. To address these challenges, we: (i) first define five distinct multi-label relations in MSCL to systematically identify positive samples, (ii) introduce a novel Similarity-Dissimilarity Loss that dynamically re-weights samples through computing the similarity and dissimilarity factors between positive samples and given anchors based on multi-label relations, and (iii) further provide theoretical grounded proofs for our method through rigorous mathematical analysis that supports the formulation and effectiveness of the proposed loss function. We conduct the experiments across both image and text modalities, and extend the evaluation to medical domain. The results demonstrate that our method consistently outperforms baselines in a comprehensive evaluation, confirming its effectiveness and robustness. Code is available at: https://github.com/guangminghuang/similarity-dissimilarity-loss.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2410.21000</link>
<guid>https://arxiv.org/abs/2410.21000</guid>
<content:encoded><![CDATA[
arXiv:2410.21000v3 Announce Type: replace-cross 
Abstract: Medical Visual Question Answering (MedVQA) has attracted growing interest at the intersection of medical image understanding and natural language processing for clinical applications. By interpreting medical images and providing precise answers to relevant clinical inquiries, MedVQA has the potential to support diagnostic decision-making and reduce workload across various fields like radiology. While recent approaches rely heavily on unified large pre-trained Visual-Language Models, research on more efficient fusion mechanisms remains relatively limited in this domain. In this paper, we introduce a fusion model, OMniBAN, that integrates Orthogonality loss, Multi-head attention, and a Bilinear Attention Network to achieve high computational efficiency as well as solid performance. We conduct comprehensive experiments and demonstrate how bilinear attention fusion can approximate the performance of larger fusion models like cross-modal Transformer. Our results show that OMniBAN requires fewer parameters (approximately 2/3 of Transformer-based Co-Attention) and substantially lower FLOPs (approximately 1/4), while achieving comparable overall performance and even slight improvements on closed-ended questions on two key MedVQA benchmarks. This balance between efficiency and accuracy suggests that OMniBAN could be a viable option for real-world medical image question answering, where computational resources are often constrained.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relationships between the degrees of freedom in the affine Gaussian derivative model for visual receptive fields and 2-D affine image transformations, with application to covariance properties of simple cells in the primary visual cortex</title>
<link>https://arxiv.org/abs/2411.05673</link>
<guid>https://arxiv.org/abs/2411.05673</guid>
<content:encoded><![CDATA[
arXiv:2411.05673v3 Announce Type: replace-cross 
Abstract: When observing the surface patterns of objects delimited by smooth surfaces, the projections of the surface patterns to the image domain will be subject to substantial variabilities, as induced by variabilities in the geometric viewing conditions, and as generated by either monocular or binocular imaging conditions, or by relative motions between the object and the observer over time. To first order of approximation, the image deformations of such projected surface patterns can be modelled as local linearizations in terms of local 2-D spatial affine transformations.
  This paper presents a theoretical analysis of relationships between the degrees of freedom in 2-D spatial affine image transformations and the degrees of freedom in the affine Gaussian derivative model for visual receptive fields. For this purpose, we first describe a canonical decomposition of 2-D affine transformations on a product form, closely related to a singular value decomposition, while in closed form, and which reveals the degrees of freedom in terms of (i) uniform scaling transformations, (ii) an overall amount of global rotation, (iii) a complementary non-uniform scaling transformation and (iv) a relative normalization to a preferred symmetry orientation in the image domain. Then, we show how these degrees of freedom relate to the degrees of freedom in the affine Gaussian derivative model.
  Finally, we use these theoretical results to consider whether we could regard the biological receptive fields in the primary visual cortex of higher mammals as being able to span the degrees of freedom of 2-D spatial affine transformations, based on interpretations of existing neurophysiological experimental results.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering</title>
<link>https://arxiv.org/abs/2412.00259</link>
<guid>https://arxiv.org/abs/2412.00259</guid>
<content:encoded><![CDATA[
arXiv:2412.00259v4 Announce Type: replace-cross 
Abstract: Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable programming to identify world models are incapable of jointly optimizing the geometry, appearance, and physical properties of the scene. In this work, we introduce a novel rigid object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based geometry representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of world model identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence. The code and additional videos are available at our project website: https://tianyi20.github.io/rigid-world-model.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application</title>
<link>https://arxiv.org/abs/2412.03887</link>
<guid>https://arxiv.org/abs/2412.03887</guid>
<content:encoded><![CDATA[
arXiv:2412.03887v4 Announce Type: replace-cross 
Abstract: Maritime environmental sensing requires overcoming challenges from complex conditions such as harsh weather, platform perturbations, large dynamic objects, and the requirement for long detection ranges. While cameras and LiDAR are commonly used in ground vehicle navigation, their applicability in maritime settings is limited by range constraints and hardware maintenance issues. Radar sensors, however, offer robust long-range detection capabilities and resilience to physical contamination from weather and saline conditions, making it a powerful sensor for maritime navigation. Among various radar types, X-band radar is widely employed for maritime vessel navigation, providing effective long-range detection essential for situational awareness and collision avoidance. Nevertheless, it exhibits limitations during berthing operations where near-field detection is critical. To address this shortcoming, we incorporate W-band radar, which excels in detecting nearby objects with a higher update rate. We present a comprehensive maritime sensor dataset featuring multi-range detection capabilities. This dataset integrates short-range LiDAR data, medium-range W-band radar data, and long-range X-band radar data into a unified framework. Additionally, it includes object labels for oceanic object detection usage, derived from radar and stereo camera images. The dataset comprises seven sequences collected from diverse regions with varying levels of \bl{navigation algorithm} estimation difficulty, ranging from easy to challenging, and includes common locations suitable for global localization tasks. This dataset serves as a valuable resource for advancing research in place recognition, odometry estimation, SLAM, object detection, and dynamic object elimination within maritime environments. Dataset can be found at https://sites.google.com/view/rpmmoana.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereo Hand-Object Reconstruction for Human-to-Robot Handover</title>
<link>https://arxiv.org/abs/2412.07487</link>
<guid>https://arxiv.org/abs/2412.07487</guid>
<content:encoded><![CDATA[
arXiv:2412.07487v3 Announce Type: replace-cross 
Abstract: Jointly estimating hand and object shape facilitates the grasping task in human-to-robot handovers. However, relying on hand-crafted prior knowledge about the geometric structure of the object fails when generalising to unseen objects, and depth sensors fail to detect transparent objects such as drinking glasses. In this work, we propose a stereo-based method for hand-object reconstruction that combines single-view reconstructions probabilistically to form a coherent stereo reconstruction. We learn 3D shape priors from a large synthetic hand-object dataset to ensure that our method is generalisable, and use RGB inputs to better capture transparent objects. We show that our method reduces the object Chamfer distance compared to existing RGB based hand-object reconstruction methods on single view and stereo settings. We process the reconstructed hand-object shape with a projection-based outlier removal step and use the output to guide a human-to-robot handover pipeline with wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a robot to successfully receive a diverse range of household objects from the human.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
<link>https://arxiv.org/abs/2501.19047</link>
<guid>https://arxiv.org/abs/2501.19047</guid>
<content:encoded><![CDATA[
arXiv:2501.19047v4 Announce Type: replace-cross 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation</title>
<link>https://arxiv.org/abs/2502.05485</link>
<guid>https://arxiv.org/abs/2502.05485</guid>
<content:encoded><![CDATA[
arXiv:2502.05485v4 Announce Type: replace-cross 
Abstract: Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[
arXiv:2503.18938v2 Announce Type: replace-cross 
Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging State Space Models in Long Range Genomics</title>
<link>https://arxiv.org/abs/2504.06304</link>
<guid>https://arxiv.org/abs/2504.06304</guid>
<content:encoded><![CDATA[
arXiv:2504.06304v2 Announce Type: replace-cross 
Abstract: Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoLa: B-Rep Generation using a Holistic Latent Representation</title>
<link>https://arxiv.org/abs/2504.14257</link>
<guid>https://arxiv.org/abs/2504.14257</guid>
<content:encoded><![CDATA[
arXiv:2504.14257v3 Announce Type: replace-cross 
Abstract: We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of $\textit{boundary representations}$ (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a $\textit{holistic latent}$ (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. $\approx$50%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniAudio: Generating Spatial Audio from 360-Degree Video</title>
<link>https://arxiv.org/abs/2504.14906</link>
<guid>https://arxiv.org/abs/2504.14906</guid>
<content:encoded><![CDATA[
arXiv:2504.14906v2 Announce Type: replace-cross 
Abstract: Traditional video-to-audio generation techniques primarily focus on field-of-view (FoV) video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and FoV video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo page is available at https://OmniAudio-360V2SA.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography</title>
<link>https://arxiv.org/abs/2504.19200</link>
<guid>https://arxiv.org/abs/2504.19200</guid>
<content:encoded><![CDATA[
arXiv:2504.19200v2 Announce Type: replace-cross 
Abstract: In situ synchrotron X-ray computed tomography enables dynamic material studies, but automated segmentation remains challenging due to complex imaging artefacts and limited training data. We present a methodology for deep learning-based segmentation by transforming high-quality ex situ laboratory data to train models for binary segmentation of in situ synchrotron data, demonstrated through copper oxide dissolution studies. Using a modified SegFormer architecture, our approach achieves high segmentation performance on unseen data while reducing processing time from hours to seconds per 3D dataset. The method maintains consistent performance over significant morphological changes during experiments, despite training only on static specimens. This methodology can be readily applied to diverse materials systems, accelerating the analysis of time-resolved tomographic data across scientific disciplines.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving</title>
<link>https://arxiv.org/abs/2505.05487</link>
<guid>https://arxiv.org/abs/2505.05487</guid>
<content:encoded><![CDATA[
<div> intersection detection, head pose estimation, object detection, naturalistic driving studies, automated video processing

Summary:
Intersection detection and driver head pose estimation were successfully carried out using a custom-developed algorithm on data collected from in-car recording systems. The algorithm accurately detected intersection signage and driving maneuvers in the majority of instances. The detection of vehicle entry into intersections had a small error margin and the overlap between ground truth and estimated intersection bounds was high. Object detection using YOLO models successfully identified traffic lights, stop signs, pedestrians, and other vehicles on the road. Turning maneuvers were independently detected using vehicle self-motion patterns, and stop lines on the road surface were identified through changing intensity patterns over time. The algorithm correctly inferred intersection type, maneuver, and bounds using scene videos and speed data. Overall, the automated video processing algorithm demonstrated high accuracy and reliability in characterizing driver behavior at intersections in naturalistic driving studies.<br /><br />Summary: <div>
arXiv:2505.05487v1 Announce Type: new 
Abstract: Naturalistic driving studies use devices in participants' own vehicles to record daily driving over many months. Due to diverse and extensive amounts of data recorded, automated processing is necessary. This report describes methods to extract and characterize driver head scans at intersections from data collected from an in-car recording system that logged vehicle speed, GPS location, scene videos, and cabin videos. Custom tools were developed to mark the intersections, synchronize location and video data, and clip the cabin and scene videos for +/-100 meters from the intersection location. A custom-developed head pose detection AI model for wide angle head turns was run on the cabin videos to estimate the driver head pose, from which head scans >20 deg were computed in the horizontal direction. The scene videos were processed using a YOLO object detection model to detect traffic lights, stop signs, pedestrians, and other vehicles on the road. Turning maneuvers were independently detected using vehicle self-motion patterns. Stop lines on the road surface were detected using changing intensity patterns over time as the vehicle moved. The information obtained from processing the scene videos, along with the speed data was used in a rule-based algorithm to infer the intersection type, maneuver, and bounds. We processed 190 intersections from 3 vehicles driven in cities and suburban areas from Massachusetts and California. The automated video processing algorithm correctly detected intersection signage and maneuvers in 100% and 94% of instances, respectively. The median [IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9] meters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and estimated intersection bounds was 0.88[0.82-0.93].
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Events to Enhancement: A Survey on Event-Based Imaging Technologies</title>
<link>https://arxiv.org/abs/2505.05488</link>
<guid>https://arxiv.org/abs/2505.05488</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, imaging tasks, image/video enhancement, light field estimation, challenges

Summary: 
Event cameras, with their high dynamic range and low latency capabilities, have become a disruptive technology in the field of imaging. However, a comprehensive study of recent advances and challenges in leveraging these benefits for various imaging tasks is still lacking. This survey aims to address this gap by first introducing a physical model and characteristics of different event sensors. The survey then delves into the interaction of image/video enhancement tasks with event cameras, highlighting advancements in this area. Furthermore, it explores advanced tasks such as light field estimation, multi-view generation, and photometric, which enable the capture of richer light information using event cameras. The survey concludes by discussing new challenges and open questions, providing a perspective on the rapidly evolving field of event imaging.  <br /><br />Summary: <div>
arXiv:2505.05488v1 Announce Type: new 
Abstract: Event cameras offering high dynamic range and low latency have emerged as disruptive technologies in imaging. Despite growing research on leveraging these benefits for different imaging tasks, a comprehensive study of recently advances and challenges are still lacking. This limits the broader understanding of how to utilize events in universal imaging applications. In this survey, we first introduce a physical model and the characteristics of different event sensors as the foundation. Following this, we highlight the advancement and interaction of image/video enhancement tasks with events. Additionally, we explore advanced tasks, which capture richer light information with events, \eg~light field estimation, multi-view generation, and photometric. Finally, we discuss new challenges and open questions offering a perspective for this rapidly evolving field. More continuously updated resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection</title>
<link>https://arxiv.org/abs/2505.05491</link>
<guid>https://arxiv.org/abs/2505.05491</guid>
<content:encoded><![CDATA[
<div> dynamic dual fusion network, object detection, traffic sign, feature extraction, MDDFNet 

Summary:
The article introduces a novel object detection network called Mamba-based Dynamic Dual Fusion Network (MDDFNet) designed for traffic sign detection. The network addresses two main challenges in detecting small objects such as traffic signs: singular feature extraction and difficulties in handling objects of varying sizes. The MDDFNet integrates a dynamic dual fusion module that utilizes multiple branches to enhance feature diversity and a Mamba-based backbone that combines global feature fusion and local feature interactions. Extensive experiments on the TT100K dataset show that MDDFNet outperforms state-of-the-art detectors in terms of performance, while maintaining real-time processing capabilities typical of single-stage models. These results confirm the effectiveness of MDDFNet in detecting small traffic signs. 

<br /><br />Summary: <div>
arXiv:2505.05491v1 Announce Type: new 
Abstract: The Detection of small objects, especially traffic signs, is a critical sub-task in object detection and autonomous driving. Despite signficant progress in previous research, two main challenges remain. First, the issue of feature extraction being too singular. Second, the detection process struggles to efectively handle objects of varying sizes or scales. These problems are also prevalent in general object detection tasks. To address these challenges, we propose a novel object detection network, Mamba-based Dynamic Dual Fusion Network (MDDFNet), for traffic sign detection. The network integrates a dynamic dual fusion module and a Mamba-based backbone to simultaneously tackle the aforementioned issues. Specifically, the dynamic dual fusion module utilizes multiple branches to consolidate various spatial and semantic information, thus enhancing feature diversity. The Mamba-based backbone leverages global feature fusion and local feature interaction, combining features in an adaptive manner to generate unique classification characteristics. Extensive experiments conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that MDDFNet outperforms other state-of-the-art detectors, maintaining real-time processing capabilities of single-stage models while achieving superior performance. This confirms the efectiveness of MDDFNet in detecting small traffic signs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision</title>
<link>https://arxiv.org/abs/2505.05492</link>
<guid>https://arxiv.org/abs/2505.05492</guid>
<content:encoded><![CDATA[
<div> Python, fairness, deep learning, vision classifiers, DebioxAI

Summary:
DetoxAI is a new Python library designed to address fairness in deep learning vision classifiers. Existing solutions for fairness in machine learning often focus on tabular data, leaving vision-based classification tasks overlooked. DetoxAI bridges this gap by implementing state-of-the-art debiasing algorithms, fairness metrics, and visualization tools specifically tailored for deep learning vision classifiers. The library supports interventions in internal representations for debiasing and includes attribution-based visualization tools and quantitative algorithmic fairness metrics for demonstrating bias mitigation. With a focus on improving fairness in vision classifiers, DetoxAI offers engineers and researchers valuable tools for assessing and improving the equity of their deep learning models. <div>
arXiv:2505.05492v1 Announce Type: new 
Abstract: While machine learning fairness has made significant progress in recent years, most existing solutions focus on tabular data and are poorly suited for vision-based classification tasks, which rely heavily on deep learning. To bridge this gap, we introduce DetoxAI, an open-source Python library for improving fairness in deep learning vision classifiers through post-hoc debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness metrics, and visualization tools. It supports debiasing via interventions in internal representations and includes attribution-based visualization tools and quantitative algorithmic fairness metrics to show how bias is mitigated. This paper presents the motivation, design, and use cases of DetoxAI, demonstrating its tangible value to engineers and researchers.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D Persistent Embodied World Models</title>
<link>https://arxiv.org/abs/2505.05495</link>
<guid>https://arxiv.org/abs/2505.05495</guid>
<content:encoded><![CDATA[
<div> video models, world model, simulation, memory, planning

Summary:
- The article introduces a new persistent embodied world model that incorporates memory of previously generated content, allowing for more consistent long-term simulation of future actions.
- A video diffusion model is used to predict RGB-D video of future observations, which is then aggregated into a 3D map of the environment.
- By conditioning the video model on this 3D spatial map, the model can simulate both seen and unseen parts of the world, enabling more accurate prediction of future outcomes.
- The proposed world model demonstrates efficacy in downstream applications such as planning and policy learning, showcasing its potential for enhancing decision-making processes in intelligent agents.
- The integration of memory into the world model addresses the limitations of existing myopic models, providing a more comprehensive and reliable framework for long-horizon planning in complex environments. 

<br /><br />Summary: <div>
arXiv:2505.05495v1 Announce Type: new 
Abstract: The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preliminary Explorations with GPT-4o(mni) Native Image Generation</title>
<link>https://arxiv.org/abs/2505.05501</link>
<guid>https://arxiv.org/abs/2505.05501</guid>
<content:encoded><![CDATA[
<div> image generation, multimodal comprehension, task taxonomy, qualitative test, model capabilities

Summary:
GPT-4o(mni) by OpenAI showcases exceptional visual generation capabilities with strong multimodal understanding. The study evaluates the model across various task categories including traditional image generation, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. Results indicate GPT-4o excels in general-purpose synthesis tasks like text-to-image generation and visual stylization but struggles with precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. The model also faces challenges with knowledge-intensive or domain-specific tasks, displaying hallucinations and factual errors. While GPT-4o represents a significant advancement in unified multimodal generation, there are limitations that need to be addressed before its application in professional or safety-critical domains.<br /><br />Summary: <div>
arXiv:2505.05501v1 Announce Type: new 
Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by OpenAI. It demonstrates a very remarkable generation capability with excellent multimodal condition understanding and varied task instructions. In this paper, we aim to explore the capabilities of GPT-4o across various tasks. Inspired by previous study, we constructed a task taxonomy along with a carefully curated set of test samples to conduct a comprehensive qualitative test. Benefiting from GPT-4o's powerful multimodal comprehension, its image-generation process demonstrates abilities surpassing those of traditional image-generation tasks. Thus, regarding the dimensions of model capabilities, we evaluate its performance across six task categories: traditional image generation tasks, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. These tasks not only assess the quality and conditional alignment of the model's outputs but also probe deeper into GPT-4o's understanding of real-world concepts. Our results reveal that GPT-4o performs impressively well in general-purpose synthesis tasks, showing strong capabilities in text-to-image generation, visual stylization, and low-level image processing. However, significant limitations remain in its ability to perform precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. Furthermore, when faced with knowledge-intensive or domain-specific scenarios, such as scientific illustrations or mathematical plots, the model often exhibits hallucinations, factual errors, or structural inconsistencies. These findings suggest that while GPT-4o marks a substantial advancement in unified multimodal generation, there is still a long way to go before it can be reliably applied to professional or safety-critical domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2505.05505</link>
<guid>https://arxiv.org/abs/2505.05505</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-3D models, attribute binding, hierarchical chain of generation, occluded object parts, semantic labels<br />
Summary:<br />
Recent advances in text-to-3D models have improved the rendering of high-quality assets but struggle with objects containing complex attributes. Existing approaches face challenges due to limited comprehension of long descriptions by text encoders, leading to incorrect attribute binding in generated results. Addressing these issues, a new automated method called Hierarchical Chain of Generation (HCoG) decomposes descriptions into parts and orders them based on occlusions, ensuring a disciplined generation process. HCoG generates components within blocks and binds attributes using target-region localization and 3D Gaussian kernel optimization. It introduces novel techniques like Gaussian Extension and Label Elimination for seamless part generation, resulting in structurally coherent 3D objects with complex attributes. Experimental results validate the effectiveness of HCoG in producing attribute-faithful objects. The code for HCoG is available at https://github.com/Wakals/GASCOL. <br /><br />Summary: <div>
arXiv:2505.05505v1 Announce Type: new 
Abstract: Recent text-to-3D models can render high-quality assets, yet they still stumble on objects with complex attributes. The key obstacles are: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Occluded object parts demand a disciplined generation order and explicit part disentanglement. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to decompose the long description into blocks representing different object parts, and orders them from inside out according to occlusions, forming a hierarchical chain. Within each block we first coarsely create components, then precisely bind attributes via target-region localization and corresponding 3D Gaussian kernel optimization. Between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments confirm that HCoG yields structurally coherent, attribute-faithful 3D objects with complex attributes. The code is available at https://github.com/Wakals/GASCOL .
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occupancy World Model for Robots</title>
<link>https://arxiv.org/abs/2505.05512</link>
<guid>https://arxiv.org/abs/2505.05512</guid>
<content:encoded><![CDATA[
<div> Keywords: scene evolutions, embodied agents, occupancy world model, indoor robotics, 3D occupancy scene evolution prediction

Summary:
This work introduces a new framework, RoboOccWorld, for learning and forecasting the scene evolutions of fine-grained occupancy in indoor robotic scenarios. The proposed model, based on a combination of spatio-temporal receptive field and guided autoregressive transformer, utilizes Conditional Causal State Attention (CCSA) to incorporate camera poses and Hybrid Spatio-Temporal Aggregation (HSTA) for multi-scale cue exploitation. A restructuring of the OccWorld-ScanNet benchmark facilitates the evaluation of indoor 3D occupancy scene evolution prediction. Experimental results show that RoboOccWorld outperforms existing methods in this task. The release of code for this framework is expected soon. 

<br /><br />Summary: <div>
arXiv:2505.05512v1 Announce Type: new 
Abstract: Understanding and forecasting the scene evolutions deeply affect the exploration and decision of embodied agents. While traditional methods simulate scene evolutions through trajectory prediction of potential instances, current works use the occupancy world model as a generative framework for describing fine-grained overall scene dynamics. However, existing methods cluster on the outdoor structured road scenes, while ignoring the exploration of forecasting 3D occupancy scene evolutions for robots in indoor scenes. In this work, we explore a new framework for learning the scene evolutions of observed fine-grained occupancy and propose an occupancy world model based on the combined spatio-temporal receptive field and guided autoregressive transformer to forecast the scene evolutions, called RoboOccWorld. We propose the Conditional Causal State Attention (CCSA), which utilizes camera poses of next state as conditions to guide the autoregressive transformer to adapt and understand the indoor robotics scenarios. In order to effectively exploit the spatio-temporal cues from historical observations, Hybrid Spatio-Temporal Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive field based on multi-scale spatio-temporal windows. In addition, we restructure the OccWorld-ScanNet benchmark based on local annotations to facilitate the evaluation of the indoor 3D occupancy scene evolution prediction task. Experimental results demonstrate that our RoboOccWorld outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction task. The code will be released soon.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach</title>
<link>https://arxiv.org/abs/2505.05513</link>
<guid>https://arxiv.org/abs/2505.05513</guid>
<content:encoded><![CDATA[
<div> Asia, rice, classification, convolutional neural network, quality check  
Summary:  
- Rice cultivation and utilization are essential for international trade and nutrition, with Asian countries like China, India, and Thailand leading in production.  
- Different rice varieties, including basmati and jasmine, cater to diverse culinary preferences and cultural traditions.
- Manual rice grain quality check is laborious and error-prone, necessitating an automatic classification solution.
- A convolutional neural network (CNN) framework was developed for accurate rice grain variety classification, achieving high performance metrics and minimal misclassifications.
- Explainability techniques like LIME and SHAP offered valuable insights into the model's decision-making process and feature importance in classification outcomes. 

<br /><br />Summary: <div>
arXiv:2505.05513v1 Announce Type: new 
Abstract: Rice is an essential staple food worldwide that is important in promoting international trade, economic growth, and nutrition. Asian countries such as China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their significant contribution to the cultivation and utilization of rice. These nations are also known for cultivating different rice grains, including short and long grains. These sizes are further classified as basmati, jasmine, kainat saila, ipsala, arborio, etc., catering to diverse culinary preferences and cultural traditions. For both local and international trade, inspecting and maintaining the quality of rice grains to satisfy customers and preserve a country's reputation is necessary. Manual quality check and classification is quite a laborious and time-consuming process. It is also highly prone to mistakes. Therefore, an automatic solution must be proposed for the effective and efficient classification of different varieties of rice grains. This research paper presents an automatic framework based on a convolutional neural network (CNN) for classifying different varieties of rice grains. We evaluated the proposed model based on performance metrics such as accuracy, recall, precision, and F1-Score. The CNN model underwent rigorous training and validation, achieving a remarkable accuracy rate and a perfect area under each class's Receiver Operating Characteristic (ROC) curve. The confusion matrix analysis confirmed the model's effectiveness in distinguishing between the different rice varieties, indicating minimal misclassifications. Additionally, the integration of explainability techniques such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided valuable insights into the model's decision-making process, revealing how specific features of the rice grains influenced classification outcomes.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions</title>
<link>https://arxiv.org/abs/2505.05517</link>
<guid>https://arxiv.org/abs/2505.05517</guid>
<content:encoded><![CDATA[
<div> Keywords: functional grasp, human hand-object interaction (HOI), multi-finger robot hands, web images, simulator-augmented data

Summary: 
The study focuses on training a functional grasping model for multi-finger robot hands using human grasp information extracted from web images. This approach eliminates the need for costly teleoperated demonstrations and allows for the training of the model on a diverse range of objects. By reconstructing human hand-object interaction 3D meshes from RGB images and aligning object meshes with their accurate 3D shapes, the model is trained on 10 object categories and evaluated on 9 unseen objects. The model achieves a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with significant improvements in functionality ratings compared to baselines. Simulator-augmented data further enhances performance, boosting the success rate to 83.4%. The sim-to-real transfer to the LEAP Hand demonstrates an 85% success rate, showcasing the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2505.05517v1 Announce Type: new 
Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands to manipulate objects effectively. However, most prior work either focuses on power grasping, which simply involves holding an object still, or relies on costly teleoperated robot demonstrations to teach robots how to grasp each object functionally. Instead, we propose extracting human grasp information from web images since they depict natural and functional object interactions, thereby bypassing the need for curated demonstrations. We reconstruct human hand-object interaction (HOI) 3D meshes from RGB images, retarget the human hand to multi-finger robot hands, and align the noisy object mesh with its accurate 3D shape. We show that these relatively low-quality HOI data from inexpensive web sources can effectively train a functional grasping model. To further expand the grasp dataset for seen and unseen objects, we use the initially-trained grasping policy with web data in the IsaacGym simulator to generate physically feasible grasps while preserving functionality. We train the grasping model on 10 object categories and evaluate it on 9 unseen objects, including challenging items such as syringes, pens, spray bottles, and tongs, which are underrepresented in existing datasets. The model trained on the web HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with a 6.7% improvement in success rate and a 1.8x increase in functionality ratings over baselines. Simulator-augmented data further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the LEAP Hand achieves a 85% success rate. Project website is at: https://webgrasp.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Privacy Preservation for Robot Visual Perception</title>
<link>https://arxiv.org/abs/2505.05519</link>
<guid>https://arxiv.org/abs/2505.05519</guid>
<content:encoded><![CDATA[
<div> method, privacy-constrained, video streaming, deep learning, detection model

Summary:
A new method for privacy-constrained video streaming (PCVS) has been developed to conceal privacy-sensitive objects in real-time video streams. The approach uses a logical specification to determine which objects to blur out, ensuring the concealment of sensitive information such as faces. A detection model evaluates the presence of these objects in each frame, with a conformal prediction approach establishing a theoretical lower bound on the probability of object existence. PCVS demonstrates a high specification satisfaction rate of over 95% across multiple datasets, outperforming other methods and consistently exceeding theoretical bounds. The method has been successfully deployed on robots in real-time operation, preserving privacy without compromising functionality. <div>
arXiv:2505.05519v1 Announce Type: new 
Abstract: Many robots (e.g., iRobot's Roomba) operate based on visual observations from live video streams, and such observations may inadvertently include privacy-sensitive objects, such as personal identifiers. Existing approaches for preserving privacy rely on deep learning models, differential privacy, or cryptography. They lack guarantees for the complete concealment of all sensitive objects. Guaranteeing concealment requires post-processing techniques and thus is inadequate for real-time video streams. We develop a method for privacy-constrained video streaming, PCVS, that conceals sensitive objects within real-time video streams. PCVS takes a logical specification constraining the existence of privacy-sensitive objects, e.g., never show faces when a person exists. It uses a detection model to evaluate the existence of these objects in each incoming frame. Then, it blurs out a subset of objects such that the existence of the remaining objects satisfies the specification. We then propose a conformal prediction approach to (i) establish a theoretical lower bound on the probability of the existence of these objects in a sequence of frames satisfying the specification and (ii) update the bound with the arrival of each subsequent frame. Quantitative evaluations show that PCVS achieves over 95 percent specification satisfaction rate in multiple datasets, significantly outperforming other methods. The satisfaction rate is consistently above the theoretical bounds across all datasets, indicating that the established bounds hold. Additionally, we deploy PCVS on robots in real-time operation and show that the robots operate normally without being compromised when PCVS conceals objects.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation</title>
<link>https://arxiv.org/abs/2505.05520</link>
<guid>https://arxiv.org/abs/2505.05520</guid>
<content:encoded><![CDATA[
<div> Keywords: Gliomas, deep learning, GaMNet, lesion segmentation, interpretability <br />
Summary: GaMNet is a novel approach for glioma lesion segmentation using deep learning. The proposed method combines the NMamba module for global modeling with a multi-scale CNN for efficient local feature extraction. By incorporating Gabor filters at multiple scales, GaMNet improves interpretability and mimics the human visual system. This approach achieves high segmentation accuracy while using fewer parameters and offering faster computation compared to existing methods. Extensive experiments demonstrate that GaMNet outperforms other models by reducing false positives and negatives, ultimately enhancing the reliability of clinical diagnosis. <div>
arXiv:2505.05520v1 Announce Type: new 
Abstract: Gliomas are aggressive brain tumors that pose serious health risks. Deep learning aids in lesion segmentation, but CNN and Transformer-based models often lack context modeling or demand heavy computation, limiting real-time use on mobile medical devices. We propose GaMNet, integrating the NMamba module for global modeling and a multi-scale CNN for efficient local feature extraction. To improve interpretability and mimic the human visual system, we apply Gabor filters at multiple scales. Our method achieves high segmentation accuracy with fewer parameters and faster computation. Extensive experiments show GaMNet outperforms existing methods, notably reducing false positives and negatives, which enhances the reliability of clinical diagnosis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
<div> attack, CLIP, adversarial perturbation, transferability, X-Transfer

Summary:
X-Transfer introduces a novel attack method targeting CLIP models, showcasing universal adversarial vulnerability. This vulnerability, termed as super transferability, allows for deceptive perturbations to be successful across various CLIP encoders and downstream VLMs. The method utilizes surrogate scaling, dynamically selecting suitable surrogates for efficient scaling instead of relying on fixed models. Through extensive evaluation, X-Transfer demonstrates superior performance compared to existing UAP methods, setting a new benchmark for adversarial transferability in CLIP models. The code for X-Transfer is openly accessible on the GitHub repository provided by the authors. <div>
arXiv:2505.05528v1 Announce Type: new 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours</title>
<link>https://arxiv.org/abs/2505.05531</link>
<guid>https://arxiv.org/abs/2505.05531</guid>
<content:encoded><![CDATA[
<div> Keywords: lip segmentation, attention UNet, multidimensional input, facial anomalies, fetal alcohol syndrome

Summary:<br />
- The proposed method integrates attention UNet and multidimensional input to improve lip segmentation accuracy.
- Local binary patterns are used to unravel micro-patterns in facial images for building multidimensional inputs.
- A mask generation method utilizing anatomical landmarks helps estimate complete lip contour, enhancing segmentation accuracy.
- The method achieved a mean dice score of 84.75% and a mean pixel accuracy of 99.77% in upper lip segmentation.
- Using a generative adversarial network (GAN), the method attained 98.55% accuracy in identifying fetal alcohol syndrome (FAS) based on lip-related facial anomalies.

<br /><br />Summary: <div>
arXiv:2505.05531v1 Announce Type: new 
Abstract: Lip segmentation plays a crucial role in various domains, such as lip synchronization, lipreading, and diagnostics. However, the effectiveness of supervised lip segmentation is constrained by the availability of lip contour in the training phase. A further challenge with lip segmentation is its reliance on image quality , lighting, and skin tone, leading to inaccuracies in the detected boundaries. To address these challenges, we propose a sequential lip segmentation method that integrates attention UNet and multidimensional input. We unravel the micro-patterns in facial images using local binary patterns to build multidimensional inputs. Subsequently, the multidimensional inputs are fed into sequential attention UNets, where the lip contour is reconstructed. We introduce a mask generation method that uses a few anatomical landmarks and estimates the complete lip contour to improve segmentation accuracy. This mask has been utilized in the training phase for lip segmentation. To evaluate the proposed method, we use facial images to segment the upper lips and subsequently assess lip-related facial anomalies in subjects with fetal alcohol syndrome (FAS). Using the proposed lip segmentation method, we achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in upper lip segmentation. To further evaluate the method, we implemented classifiers to identify those with FAS. Using a generative adversarial network (GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study populations. This method could be used to improve lip segmentation accuracy, especially around Cupid's bow, and shed light on distinct lip-related characteristics of FAS.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open Ended Action Environments</title>
<link>https://arxiv.org/abs/2505.05540</link>
<guid>https://arxiv.org/abs/2505.05540</guid>
<content:encoded><![CDATA[
<div> benchmark, VLM, VLA, zero-shot generalization, Procgen

Summary:<br />
(1) State-of-the-art vision-language-action models, including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST, were evaluated on diverse procedural tasks from the Procgen benchmark. (2) The models showed limitations in zero-shot generalization to out-of-distribution tasks, with performance influenced by factors like action representation and task complexity. (3) Vision-language-action models generally outperformed others due to their robust architecture. (4) Variants of vision-language models improved significantly when appropriately constrained, highlighting the impact of precise prompt engineering on performance. (5) The study emphasizes the importance of systematic evaluation and analysis of VLM and VLA models in diverse environments to enhance their generalization capabilities for real-world applications. 

<br /><br />Summary: <div>
arXiv:2505.05540v1 Announce Type: new 
Abstract: Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLM and VLA models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexit; (2) VLAs generally outperform other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05573</link>
<guid>https://arxiv.org/abs/2505.05573</guid>
<content:encoded><![CDATA[
<div> fine-tuning, medical images, text-to-image synthesis, MSDM, healthcare AI<br />
Summary:<br />
This paper explores text-to-image synthesis in the medical domain, comparing fine-tuning large pre-trained latent diffusion models (FLUX, Kandinsky) with training compact domain-specific models (MSDM). The newly proposed MSDM model integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to align medical text prompts with generated images efficiently. Evaluation on colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets shows that while large models achieve higher fidelity, MSDM delivers comparable quality with lower computational costs. Quantitative metrics and feedback from medical experts highlight the strengths and limitations of each approach. This study highlights the potential of text-to-image synthesis in addressing data scarcity challenges in healthcare AI while maintaining patient privacy. <br />Summary: <div>
arXiv:2505.05573v1 Announce Type: new 
Abstract: The generation of realistic medical images from text descriptions has significant potential to address data scarcity challenges in healthcare AI while preserving patient privacy. This paper presents a comprehensive study of text-to-image synthesis in the medical domain, comparing two distinct approaches: (1) fine-tuning large pre-trained latent diffusion models and (2) training small, domain-specific models. We introduce a novel model named MSDM, an optimized architecture based on Stable Diffusion that integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to better align medical text prompts with generated images. Our study compares two approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus training compact domain-specific models (MSDM). Evaluation across colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models achieve higher fidelity, our optimized MSDM delivers comparable quality with lower computational costs. Quantitative metrics and qualitative evaluations by medical experts reveal strengths and limitations of each approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steepest Descent Density Control for Compact 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.05587</link>
<guid>https://arxiv.org/abs/2505.05587</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D, rendering, optimization, efficiency
Summary:
The paper introduces a theoretical framework for improving density control in 3D Gaussian Splatting (3DGS). It addresses the issue of redundant point clouds generated by the densification algorithm, which leads to excessive memory usage and slower performance. The analysis reveals the importance of splitting in escaping saddle points and establishes necessary conditions for densification. It determines the minimal number of offspring Gaussians, identifies the optimal parameter update direction, and provides an analytical solution for normalizing offspring opacity. The proposed SteepGS approach incorporates steepest density control, achieving a significant reduction in Gaussian points while maintaining rendering quality. This results in a 50% decrease in points, enhancing efficiency and scalability of 3DGS. <br /><br />Summary: <div>
arXiv:2505.05587v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation</title>
<link>https://arxiv.org/abs/2505.05589</link>
<guid>https://arxiv.org/abs/2505.05589</guid>
<content:encoded><![CDATA[
<div> Keywords: Reactive dance generation, diffusion-based framework, multi-scale controllability, interaction fidelity, temporal consistency

Summary:
ReactDance introduces a novel diffusion-based framework for reactive dance generation that addresses limitations of existing methods by capturing fine-grained spatial interactions and localized temporal context. The framework utilizes Group Residual Finite Scalar Quantization (GRFSQ) to disentangle motion representation across multiple scales, allowing for accurate capture of interaction semantics. Additionally, a Blockwise Local Context (BLC) sampling strategy is employed to eliminate error accumulation in long sequence generation. The model, implemented with Layer-Decoupled Classifier-free Guidance (LDCFG), enables granular control over motion semantics at varying scales. Through extensive experiments on standard benchmarks, ReactDance showcases superior performance compared to existing methods, achieving state-of-the-art results in terms of interaction fidelity, synchronization, and temporal consistency.<br /><br />Summary: ReactDance provides a cutting-edge solution for reactive dance generation with enhanced spatial coordination and temporal coherence, surpassing current methods by incorporating multi-scale controllability, disentangled motion representation, local context sampling, and granular motion guidance. <div>
arXiv:2505.05589v1 Announce Type: new 
Abstract: Reactive dance generation (RDG) produces follower movements conditioned on guiding dancer and music while ensuring spatial coordination and temporal coherence. However, existing methods overemphasize global constraints and optimization, overlooking local information, such as fine-grained spatial interactions and localized temporal context. Therefore, we present ReactDance, a novel diffusion-based framework for high-fidelity RDG with long-term coherence and multi-scale controllability. Unlike existing methods that struggle with interaction fidelity, synchronization, and temporal consistency in duet synthesis, our approach introduces two key innovations: 1)Group Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion representation that captures interaction semantics from coarse body rhythms to fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling strategy eliminating error accumulation in long sequence generation via local block causal masking and periodic positional encoding. Built on the decoupled multi-scale GRFSQ representation, we implement a diffusion model withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control over motion semantics across scales. Extensive experiments on standard benchmarks demonstrate that ReactDance surpasses existing methods, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization</title>
<link>https://arxiv.org/abs/2505.05591</link>
<guid>https://arxiv.org/abs/2505.05591</guid>
<content:encoded><![CDATA[
<div> Keywords: surface reconstruction, computer vision, gaussian splatting, indoor scenes, data-driven optimization<br />
Summary: <br />
Surface reconstruction is a crucial aspect of computer vision and graphics, with applications in various fields such as 3D modeling and robotics. Existing approaches to surface reconstruction based on volumetric rendering have limitations in modeling under-observed or textureless regions. The QuickSplat method introduces data-driven priors to generate dense initializations for optimizing large-scale indoor scenes using 2D gaussian splatting. This accelerates the optimization process and enhances the geometry of flat wall structures. The method also includes a densifier network that predicts new Gaussians based on rendering gradients, eliminating the need for heuristics in densification. Extensive experiments show that this data-driven optimization significantly improves runtime efficiency by 8x and reduces depth errors by up to 48% compared to current state-of-the-art methods. <br /><br />Summary: <div>
arXiv:2505.05591v1 Announce Type: new 
Abstract: Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling</title>
<link>https://arxiv.org/abs/2505.05599</link>
<guid>https://arxiv.org/abs/2505.05599</guid>
<content:encoded><![CDATA[
<div> Keywords: satellite imagery, object localization, YOLO-DCAP, multi-scale features, attention-aided spatial pooling <br />
Summary: <br />
Object localization in satellite imagery is complex due to variability in objects, low resolution, and interference. This research focuses on GW, Bore, and OE datasets, each with unique challenges. YOLO-DCAP, an enhanced YOLOv5 variant, addresses these challenges with a MDRC block for multi-scale feature capture and an AaSP module for global spatial focus. YOLO-DCAP outperforms base model and state-of-the-art methods, with 20.95% and 32.23% mAP50 and IoU improvements over base model, and 7.35% and 9.84% over alternatives. The approach is robust and generalizable across all three datasets. Open-source code is available at the provided GitHub link. 
<br /> <div>
arXiv:2505.05599v1 Announce Type: new 
Abstract: Object localization in satellite imagery is particularly challenging due to the high variability of objects, low spatial resolution, and interference from noise and dominant features such as clouds and city lights. In this research, we focus on three satellite datasets: upper atmospheric Gravity Waves (GW), mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique challenges. These challenges include the variability in the scale and appearance of the main object patterns, where the size, shape, and feature extent of objects of interest can differ significantly. To address these challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed to improve object localization in these complex scenarios. YOLO-DCAP incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture multi-scale features at scale with varying dilation rates, and an Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant spatial regions, enhancing feature selection. These structural improvements help to better localize objects in satellite imagery. Experimental results demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model and state-of-the-art approaches, achieving an average improvement of 20.95% in mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively over state-of-the-art alternatives, consistently across all three satellite datasets. These consistent gains across all three satellite datasets highlight the robustness and generalizability of the proposed approach. Our code is open sourced at https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Preliminary Study for GPT-4o on Image Restoration</title>
<link>https://arxiv.org/abs/2505.05621</link>
<guid>https://arxiv.org/abs/2505.05621</guid>
<content:encoded><![CDATA[
<div> image restoration, GPT-4o, multi-modal inputs, autoregressive architecture, image generation<br />
Summary:<br />
OpenAI's GPT-4o model, incorporating multi-modal inputs and outputs in an autoregressive framework, has shown remarkable image generation capabilities. However, a systematic evaluation reveals issues with pixel-level fidelity in image restoration tasks, such as variations in proportions and object positions. Despite these challenges, GPT-4o's outputs can serve as potent visual priors in tasks like dehazing and derainning, bolstering existing networks' performance. This study provides guidance for integrating GPT-4o into image restoration pipelines, potentially driving innovation in the field. Released images and datasets will support further research in the broader realm of image generation.<br /> <div>
arXiv:2505.05621v1 Announce Type: new 
Abstract: OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models</title>
<link>https://arxiv.org/abs/2505.05626</link>
<guid>https://arxiv.org/abs/2505.05626</guid>
<content:encoded><![CDATA[
<div> vision, language, Multimodal Large Language Models, visual understanding, language generation

Summary:
- Multimodal Large Language Models (MLLMs) struggle to effectively combine vision and language, often relying too heavily on language priors.
- This study delves into how MLLMs internally process visual information from image regions.
- The researchers introduce techniques to enhance the model's grasp of visual content and ensure that this understanding shapes language generation.
- The resulting model shows superior multimodal comprehension, as evidenced by its ability to predict visually-relevant tokens and outperform on visually challenging tasks.
- Through a rigorous analysis, the model demonstrates a 10-point improvement on difficult multimodal tasks. 

<br /><br />Summary: <div>
arXiv:2505.05626v1 Announce Type: new 
Abstract: Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models</title>
<link>https://arxiv.org/abs/2505.05635</link>
<guid>https://arxiv.org/abs/2505.05635</guid>
<content:encoded><![CDATA[
<div> Keywords: Open-vocabulary recognition, bird species, multimodal vision language encoders, retrieval-augmented generation, biodiversity monitoring<br />
Summary:<br />
- The article addresses the challenge of open-vocabulary bird species recognition, focusing on classifying species without predefined taxonomic categories.<br />
- Traditional benchmarks are limited in real-world scenarios where novel species emerge, showing reduced performance under open-vocabulary settings.<br />
- The proposed framework integrates structured textual knowledge from Wikipedia articles of 11,202 bird species to improve recognition capabilities.<br />
- The Visual Re-ranking Retrieval-Augmented Generation (VR-RAG) framework uses visual similarities to rerank top candidates, enabling recognition of unseen species.<br />
- Extensive experiments across five classification benchmarks demonstrate the effectiveness of the approach, improving performance and surpassing conventional VLM-based methods.<br />
Summary: <div>
arXiv:2505.05635v1 Announce Type: new 
Abstract: Open-vocabulary recognition remains a challenging problem in computer vision, as it requires identifying objects from an unbounded set of categories. This is particularly relevant in nature, where new species are discovered every year. In this work, we focus on open-vocabulary bird species recognition, where the goal is to classify species based on their descriptions without being constrained to a predefined set of taxonomic categories. Traditional benchmarks like CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary paradigm, limiting their applicability to real-world scenarios where novel species continually emerge. We show that the performance of current systems when evaluated under settings closely aligned with open-vocabulary drops by a huge margin. To address this gap, we propose a scalable framework integrating structured textual knowledge from Wikipedia articles of 11,202 bird species distilled via GPT-4o into concise, discriminative summaries. We propose Visual Re-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented generation framework that uses visual similarities to rerank the top m candidates retrieved by a set of multimodal vision language encoders. This allows for the recognition of unseen taxa. Extensive experiments across five established classification benchmarks show that our approach is highly effective. By integrating VR-RAG, we improve the average performance of state-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five benchmarks. Our approach outperforms conventional VLM-based approaches, which struggle with unseen species. By bridging the gap between encyclopedic knowledge and visual recognition, our work advances open-vocabulary recognition, offering a flexible, scalable solution for biodiversity monitoring and ecological research.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Style Transfer for Enhancing Animal Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2505.05640</link>
<guid>https://arxiv.org/abs/2505.05640</guid>
<content:encoded><![CDATA[
<div> Style Transfer, Neural Networks, Facial Landmark Detection, Data Augmentation, Animal Detection<br />
<br />
Summary:<br />
Neural Style Transfer (NST) is explored for enhancing animal facial landmark detectors training. By using cropped facial images rather than full-body images, structural consistency is improved, enhancing image quality. Challenges arose with annotation misalignment when training on style-transferred images, but Supervised Style Transfer (SST) helped maintain up to 98% of baseline accuracy. Augmenting the dataset with style-transferred images proved more effective than traditional methods, boosting robustness. The study focused on cat facial landmarks but suggests the method can be applied to other species and landmark detection models. <div>
arXiv:2505.05640v1 Announce Type: new 
Abstract: Neural Style Transfer (NST) is a technique for applying the visual characteristics of one image onto another while preserving structural content. Traditionally used for artistic transformations, NST has recently been adapted, e.g., for domain adaptation and data augmentation. This study investigates the use of this technique for enhancing animal facial landmark detectors training. As a case study, we use a recently introduced Ensemble Landmark Detector for 48 anatomical cat facial landmarks and the CatFLW dataset it was trained on, making three main contributions. First, we demonstrate that applying style transfer to cropped facial images rather than full-body images enhances structural consistency, improving the quality of generated images. Secondly, replacing training images with style-transferred versions raised challenges of annotation misalignment, but Supervised Style Transfer (SST) - which selects style sources based on landmark accuracy - retained up to 98% of baseline accuracy. Finally, augmenting the dataset with style-transferred images further improved robustness, outperforming traditional augmentation methods. These findings establish semantic style transfer as an effective augmentation strategy for enhancing the performance of facial landmark detection models for animals and beyond. While this study focuses on cat facial landmarks, the proposed method can be generalized to other species and landmark detection models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction</title>
<link>https://arxiv.org/abs/2505.05644</link>
<guid>https://arxiv.org/abs/2505.05644</guid>
<content:encoded><![CDATA[
<div> transformer architecture, multimodal learning, reflectance parameter estimation, image-based 3D reconstruction, lunar images <br />
<br />
Summary: 
Multimodal learning, relatively unexplored in planetary science, is applied to the tasks of reflectance parameter estimation and image-based 3D reconstruction of lunar images. A unified transformer architecture is proposed and trained to learn shared representations from grayscale images, digital elevation models, surface normals, and albedo maps. The model can translate between different input and target modalities, enabling the simultaneous prediction of DEMs and albedo maps from grayscale images. This approach solves the 3D reconstruction problem while separating photometric parameters and height information. Results show that the model can learn meaningful relationships across multiple modalities, with potential for future enhancements like photometric normalization and co-registration by incorporating additional input sources. <div>
arXiv:2505.05644v1 Announce Type: new 
Abstract: Multimodal learning is an emerging research topic across multiple disciplines but has rarely been applied to planetary science. In this contribution, we identify that reflectance parameter estimation and image-based 3D reconstruction of lunar images can be formulated as a multimodal learning problem. We propose a single, unified transformer architecture trained to learn shared representations between multiple sources like grayscale images, digital elevation models, surface normals, and albedo maps. The architecture supports flexible translation from any input modality to any target modality. Predicting DEMs and albedo maps from grayscale images simultaneously solves the task of 3D reconstruction of planetary surfaces and disentangles photometric parameters and height information. Our results demonstrate that our foundation model learns physically plausible relations across these four modalities. Adding more input modalities in the future will enable tasks such as photometric normalization and co-registration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval</title>
<link>https://arxiv.org/abs/2505.05666</link>
<guid>https://arxiv.org/abs/2505.05666</guid>
<content:encoded><![CDATA[
<div> Vision-based RAG, OCR-based RAG, document quality, semantic answer evaluation benchmark, question-answering performance<br />
<br />
Summary:<br />
The study compares a vision-based RAG system (ColPali) and traditional OCR-based pipelines with Llama 3.2 and Nougat OCR. Vision-based RAG performs well on fine-tuned documents but struggles with generalization to unseen documents of varying quality. In contrast, OCR-based RAG shows better generalization capabilities. The study introduces a semantic answer evaluation benchmark to assess question-answering performance. It highlights the trade-offs between computational efficiency and semantic accuracy, providing practical guidance for RAG practitioners on choosing between OCR-dependent and vision-based document retrieval systems in production environments.<br /> <div>
arXiv:2505.05666v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for enhancing the reliability and utility of Large Language Models (LLMs) by grounding responses in external documents. Traditional RAG systems rely on Optical Character Recognition (OCR) to first process scanned documents into text. However, even state-of-the-art OCRs can introduce errors, especially in degraded or complex documents. Recent vision-language approaches, such as ColPali, propose direct visual embedding of documents, eliminating the need for OCR. This study presents a systematic comparison between a vision-based RAG system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2 (90B) and Nougat OCR across varying document qualities. Beyond conventional retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark to assess end-to-end question-answering performance. Our findings indicate that while vision-based RAG performs well on documents it has been fine-tuned on, OCR-based RAG is better able to generalize to unseen documents of varying quality. We highlight the key trade-offs between computational efficiency and semantic accuracy, offering practical guidance for RAG practitioners in selecting between OCR-dependent and vision-based document retrieval systems in production environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</title>
<link>https://arxiv.org/abs/2505.05672</link>
<guid>https://arxiv.org/abs/2505.05672</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D head avatars, photorealism, 3D Gaussian splatting, facial motion estimation, high-detail model

Summary: 
This paper introduces a new high-detail 3D head avatar model that improves on existing models by increasing the number of 3D Gaussians used for rendering at 4K resolution. The model is reconstructed from multiview input video and utilizes a mesh-based 3D morphable model for coarse deformation. 3D Gaussians, embedded within the UVD tangent space of the mesh, provide photoreal appearance and allow for densification where needed. A novel deformable Gaussian encoding and fitting procedure preserves appearance detail while capturing facial motion and high-frequency features like skin wrinkling. This approach addresses challenges such as inaccurate motion estimation and memory limitations, enhancing the fidelity and quality of photoreal avatars for applications in telepresence, extended reality, and entertainment. <div>
arXiv:2505.05672v1 Announce Type: new 
Abstract: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstanceGen: Image Generation with Instance-level Instructions</title>
<link>https://arxiv.org/abs/2505.05678</link>
<guid>https://arxiv.org/abs/2505.05678</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, text-to-image, structural constraints, instance-level attributes, spatial relations <br />
Summary: 
In this new arXiv announcement, the focus is on improving pretrained text-to-image models' ability to understand complex prompts involving multiple objects and attributes. The approach involves integrating structural constraints in the form of fine-grained structural initialization from contemporary image generation models. By combining this structural guidance with instance-level instructions, the proposed technique aims to generate images that accurately represent all aspects of the text prompt, including object counts, attributes, and spatial relations between instances. This advancement addresses the limitations faced by current generative models in capturing the semantics of complex prompts, offering a promising solution for more accurate and detailed image generation. <br /><br />Summary: <div>
arXiv:2505.05678v1 Announce Type: new 
Abstract: Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, %leveraging additional structural inputs typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible \emph{fine-grained} structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos</title>
<link>https://arxiv.org/abs/2505.05681</link>
<guid>https://arxiv.org/abs/2505.05681</guid>
<content:encoded><![CDATA[
<div> Keywords: Nonhuman primates, Capuchin monkeys, Video-text models, Fine-tuning, Behavioral analysis

Summary: 
The study focuses on developing computational models to assist researchers in extracting useful clips from videos of capuchin monkeys in their natural habitat. By fine-tuning pre-trained video-text models, the researchers aim to address the challenge of training models solely based on raw, unlabeled video footage and weak audio descriptions. They propose a two-folded approach involving an agentic data treatment pipeline and a fine-tuning process using Microsoft's X-CLIP model with Low-Rank Adaptation (LoRA). The results show a significant improvement in retrieval performance, with a 167% uplift in Hits@5 for the 16 frames model and a 114% uplift for the 8 frame model on domain data. Additionally, the model effectively ranks various behaviors based on NDCG@K results, outperforming raw pre-trained models in behavioral analysis tasks.<br /><br />Summary: <div>
arXiv:2505.05681v1 Announce Type: new 
Abstract: Video recordings of nonhuman primates in their natural habitat are a common source for studying their behavior in the wild. We fine-tune pre-trained video-text foundational models for the specific domain of capuchin monkeys, with the goal of developing useful computational models to help researchers to retrieve useful clips from videos. We focus on the challenging problem of training a model based solely on raw, unlabeled video footage, using weak audio descriptions sometimes provided by field collaborators. We leverage recent advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs) to address the extremely noisy nature of both video and audio content. Specifically, we propose a two-folded approach: an agentic data treatment pipeline and a fine-tuning process. The data processing pipeline automatically extracts clean and semantically aligned video-text pairs from the raw videos, which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of $167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame model on our domain data. Moreover, based on $NDCG@K$ results, our model is able to rank well most of the considered behaviors, while the tested raw pre-trained models are not able to rank them at all. The code will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder</title>
<link>https://arxiv.org/abs/2505.05710</link>
<guid>https://arxiv.org/abs/2505.05710</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, hyperspectral imagery, dual masking, spectral-spatial representations, transfer learning <br />
<br />
Summary: HyperspectralMAE is a Transformer-based model for hyperspectral data that uses dual masking during pre-training to learn robust spatial and spectral representations. It incorporates learnable harmonic Fourier positional embeddings based on wavelength and balances pixel-level accuracy and spectral-shape fidelity in reconstruction using mean-squared error and spectral angle mapper. With a large parameter capacity and 768-dimensional embeddings, it excels in transfer learning when fine-tuned for land-cover classification. Pre-trained on NASA Hyperion and DLR EnMAP datasets, HyperspectralMAE achieves state-of-the-art accuracy on the Indian Pines benchmark, showcasing the efficacy of dual masking and wavelength-aware embeddings in advancing hyperspectral image analysis. <br /><br />Summary: <div>
arXiv:2505.05710v1 Announce Type: new 
Abstract: Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation model for hyperspectral data that employs a \textit{dual masking} strategy: during pre-training we randomly occlude 50\% of spatial patches and 50\% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity.
  The resulting model contains about $1.8\times10^{8}$ parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra) and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer</title>
<link>https://arxiv.org/abs/2505.05711</link>
<guid>https://arxiv.org/abs/2505.05711</guid>
<content:encoded><![CDATA[
<div> Proposed Keywords: temporal action detection, transformer, multi-dilated gated encoder, central-adjacent region integrated decoder, state-of-the-art performance

Summary:
In this paper, the authors address limitations in query-based detectors for temporal action detection (TAD) by proposing a new model called DiGIT. The model tackles challenges specific to TAD by introducing a multi-dilated gated encoder to reduce redundant information and capture fine-grained temporal details. Additionally, a central-adjacent region integrated decoder is implemented to improve the sampling strategy for deformable cross-attention. Through extensive experiments on benchmark datasets like THUMOS14, ActivityNet v1.3, and HACS-Segment, DiGIT achieves state-of-the-art performance. This innovative approach outperforms existing models by effectively capturing long-range temporal context and essential information for accurate action detection. The code for DiGIT is publicly available on GitHub, providing a valuable resource for further research in the field. 

<br /><br />Summary: <div>
arXiv:2505.05711v1 Announce Type: new 
Abstract: In this paper, we examine a key limitation in query-based detectors for temporal action detection (TAD), which arises from their direct adaptation of originally designed architectures for object detection. Despite the effectiveness of the existing models, they struggle to fully address the unique challenges of TAD, such as the redundancy in multi-scale features and the limited ability to capture sufficient temporal context. To address these issues, we propose a multi-dilated gated encoder and central-adjacent region integrated decoder for temporal action detection transformer (DiGIT). Our approach replaces the existing encoder that consists of multi-scale deformable attention and feedforward network with our multi-dilated gated encoder. Our proposed encoder reduces the redundant information caused by multi-level features while maintaining the ability to capture fine-grained and long-range temporal information. Furthermore, we introduce a central-adjacent region integrated decoder that leverages a more comprehensive sampling strategy for deformable cross-attention to capture the essential information. Extensive experiments demonstrate that DiGIT achieves state-of-the-art performance on THUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at: https://github.com/Dotori-HJ/DiGIT
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Space-Intervened Diffusive Alignment for Visual Classification</title>
<link>https://arxiv.org/abs/2505.05721</link>
<guid>https://arxiv.org/abs/2505.05721</guid>
<content:encoded><![CDATA[
<div> alignment, cross-modal, semantic space, diffusive, classification

Summary:
The paper introduces a novel approach called Semantic-Space-Intervened Diffusive Alignment (SeDA) for improving visual classification through cross-modal alignment. SeDA utilizes a semantic space as a bridge in the visual-to-textual projection process, leveraging the shared class-level information between visual and textual features. A bi-stage diffusion framework is implemented in SeDA, with a Diffusion-Controlled Semantic Learner modeling the semantic feature space of visual features and a Diffusion-Controlled Semantic Translator focusing on learning the distribution of textual features from the semantic space. The Progressive Feature Interaction Network facilitates stepwise feature interactions to integrate textual information into mapped features. Experimental results demonstrate that SeDA outperforms existing methods in cross-modal feature alignment, leading to enhanced performance across various scenarios. <br /><br />Summary: <div>
arXiv:2505.05721v1 Announce Type: new 
Abstract: Cross-modal alignment is an effective approach to improving visual classification. Existing studies typically enforce a one-step mapping that uses deep neural networks to project the visual features to mimic the distribution of textual features. However, they typically face difficulties in finding such a projection due to the two modalities in both the distribution of class-wise samples and the range of their feature values. To address this issue, this paper proposes a novel Semantic-Space-Intervened Diffusive Alignment method, termed SeDA, models a semantic space as a bridge in the visual-to-textual projection, considering both types of features share the same class-level information in classification. More importantly, a bi-stage diffusion framework is developed to enable the progressive alignment between the two modalities. Specifically, SeDA first employs a Diffusion-Controlled Semantic Learner to model the semantic features space of visual features by constraining the interactive features of the diffusion model and the category centers of visual features. In the later stage of SeDA, the Diffusion-Controlled Semantic Translator focuses on learning the distribution of textual features from the semantic space. Meanwhile, the Progressive Feature Interaction Network introduces stepwise feature interactions at each alignment step, progressively integrating textual information into mapped features. Experimental results show that SeDA achieves stronger cross-modal feature alignment, leading to superior performance over existing methods across multiple scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation</title>
<link>https://arxiv.org/abs/2505.05722</link>
<guid>https://arxiv.org/abs/2505.05722</guid>
<content:encoded><![CDATA[
<div> Synthetic datasets, point tracking, surgical videos, domain shift, semi-supervised learning,<br />
<br />
Summary: SurgTracker is a framework designed to adapt synthetic-trained point trackers to surgical video environments. It utilizes filtered self-distillation to generate pseudo-labels and enforce geometric consistency during training. The approach improves tracking performance in high-shift domains such as surgery by utilizing only 80 unlabeled videos. SurgTracker addresses the challenges of domain shift and lack of labeled data in surgical videos, where scenes feature complex tissue deformation, occlusion, and lighting variation. This adaptation method provides stable supervision throughout training without the need for multiple teachers, making it computationally efficient. The results on the STIR benchmark demonstrate the potential of SurgTracker for robust tracking performance in data-scarce domains. <br /><br /> <div>
arXiv:2505.05722v1 Announce Type: new 
Abstract: Synthetic datasets have enabled significant progress in point tracking by providing large-scale, densely annotated supervision. However, deploying these models in real-world domains remains challenging due to domain shift and lack of labeled data-issues that are especially severe in surgical videos, where scenes exhibit complex tissue deformation, occlusion, and lighting variation. While recent approaches adapt synthetic-trained trackers to natural videos using teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their effectiveness in high-shift domains like surgery remains unexplored. This work presents SurgTracker, a semi-supervised framework for adapting synthetic-trained point trackers to surgical video using filtered self-distillation. Pseudo-labels are generated online by a fixed teacher-identical in architecture and initialization to the student-and are filtered using a cycle consistency constraint to discard temporally inconsistent trajectories. This simple yet effective design enforces geometric consistency and provides stable supervision throughout training, without the computational overhead of maintaining multiple teachers. Experiments on the STIR benchmark show that SurgTracker improves tracking performance using only 80 unlabeled videos, demonstrating its potential for robust adaptation in high-shift, data-scarce domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection</title>
<link>https://arxiv.org/abs/2505.05741</link>
<guid>https://arxiv.org/abs/2505.05741</guid>
<content:encoded><![CDATA[
<div> Keywords: Tiny object detection, Dome-DETR, DeFE, MWAS, PAQI

Summary:
Dome-DETR is a new framework designed for efficient tiny object detection in various applications such as drone surveillance and autonomous systems. It addresses challenges in feature redundancy and high computational costs by introducing innovative techniques. The Density-Focal Extractor (DeFE) generates compact foreground masks to reduce redundancies, while Masked Window Attention Sparsification (MWAS) prioritizes informative regions using sparse attention. Progressive Adaptive Query Initialization (PAQI) dynamically adjusts query density for improved allocation. Dome-DETR achieves state-of-the-art performance on AI-TOD-V2 and VisDrone datasets with a compact model size and low computational complexity. The proposed framework demonstrates superior results (+3.3 AP on AI-TOD-V2 and +2.5 AP on VisDrone) compared to existing methods. The code for Dome-DETR will be made available upon acceptance. 

<br /><br />Summary: Dome-DETR introduces novel techniques for efficient tiny object detection, including DeFE for generating compact foreground masks, MWAS for focusing on informative regions, and PAQI for adaptive query initialization. It achieves state-of-the-art performance on AI-TOD-V2 and VisDrone datasets while maintaining low computational complexity and a compact model size. The proposed framework addresses the limitations of existing methods and demonstrates significant improvements in accuracy and efficiency. <div>
arXiv:2505.05741v1 Announce Type: new 
Abstract: Tiny object detection plays a vital role in drone surveillance, remote sensing, and autonomous systems, enabling the identification of small targets across vast landscapes. However, existing methods suffer from inefficient feature leverage and high computational costs due to redundant feature processing and rigid query allocation. To address these challenges, we propose Dome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection. To reduce feature redundancies, we introduce a lightweight Density-Focal Extractor (DeFE) to produce clustered compact foreground masks. Leveraging these masks, we incorporate Masked Window Attention Sparsification (MWAS) to focus computational resources on the most informative regions via sparse attention. Besides, we propose Progressive Adaptive Query Initialization (PAQI), which adaptively modulates query density across spatial areas for better query allocation. Extensive experiments demonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on AI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational complexity and a compact model size. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>kFuse: A novel density based agglomerative clustering</title>
<link>https://arxiv.org/abs/2505.05748</link>
<guid>https://arxiv.org/abs/2505.05748</guid>
<content:encoded><![CDATA[
<div> agglomerative clustering, kFuse, sub-cluster partitioning, boundary connectivity, density similarity
Summary:<br />
The paper introduces a novel density-based agglomerative clustering method called kFuse. It addresses issues in existing clustering methods by utilizing natural neighbors for sub-cluster partitioning, determining boundary connectivity between sub-clusters based on adjacent samples and shortest distances, assessing density similarity through mean density and variance calculations, and establishing merging rules based on connectivity and density. kFuse only requires the number of clusters to be specified at the final merging stage, enhancing accuracy by considering adjacent samples, distances, and densities during merging. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of kFuse in improving clustering accuracy and identification capability. <br /> <div>
arXiv:2505.05748v1 Announce Type: new 
Abstract: Agglomerative clustering has emerged as a vital tool in data analysis due to its intuitive and flexible characteristics. However, existing agglomerative clustering methods often involve additional parameters for sub-cluster partitioning and inter-cluster similarity assessment. This necessitates different parameter settings across various datasets, which is undoubtedly challenging in the absence of prior knowledge. Moreover, existing agglomerative clustering techniques are constrained by the calculation method of connection distance, leading to unstable clustering results. To address these issues, this paper introduces a novel density-based agglomerative clustering method, termed kFuse. kFuse comprises four key components: (1) sub-cluster partitioning based on natural neighbors; (2) determination of boundary connectivity between sub-clusters through the computation of adjacent samples and shortest distances; (3) assessment of density similarity between sub-clusters via the calculation of mean density and variance; and (4) establishment of merging rules between sub-clusters based on boundary connectivity and density similarity. kFuse requires the specification of the number of clusters only at the final merging stage. Additionally, by comprehensively considering adjacent samples, distances, and densities among different sub-clusters, kFuse significantly enhances accuracy during the merging phase, thereby greatly improving its identification capability. Experimental results on both synthetic and real-world datasets validate the effectiveness of kFuse.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
<link>https://arxiv.org/abs/2505.05752</link>
<guid>https://arxiv.org/abs/2505.05752</guid>
<content:encoded><![CDATA[
<div> Keywords: automation, point cloud data, ADA compliance, deep learning, infrastructure surveying  

<br /><br />Summary: This paper discusses a framework for automating geometric measurements and compliance assessments in infrastructure surveying using point cloud data. The approach combines deep learning-based detection and segmentation with geometric and signal processing techniques to streamline surveying tasks. As a proof of concept, the authors apply this framework to assess curb ramps' compliance with the Americans with Disabilities Act (ADA), showcasing the effectiveness of point cloud data in automating surveys. A significant aspect of this work is the creation of a large annotated dataset of curb ramps, which is publicly accessible, aiding in robust model training and evaluation. The authors present experimental results comparing the proposed method with manual field measurements, demonstrating its accuracy and reliability. This method not only aims to reduce manual labor but also to enhance consistency in infrastructure assessments. Additionally, the framework lays the foundation for broader applications in infrastructure surveying and automated construction evaluation, encouraging further use of point cloud data in these fields. The associated annotated database, manual ramp survey data, and algorithms developed during this project are available on GitHub. <div>
arXiv:2505.05752v1 Announce Type: new 
Abstract: Automation can play a prominent role in improving efficiency, accuracy, and scalability in infrastructure surveying and assessing construction and compliance standards. This paper presents a framework for automation of geometric measurements and compliance assessment using point cloud data. The proposed approach integrates deep learning-based detection and segmentation, in conjunction with geometric and signal processing techniques, to automate surveying tasks. As a proof of concept, we apply this framework to automatically evaluate the compliance of curb ramps with the Americans with Disabilities Act (ADA), demonstrating the utility of point cloud data in survey automation. The method leverages a newly collected, large annotated dataset of curb ramps, made publicly available as part of this work, to facilitate robust model training and evaluation. Experimental results, including comparison with manual field measurements of several ramps, validate the accuracy and reliability of the proposed method, highlighting its potential to significantly reduce manual effort and improve consistency in infrastructure assessment. Beyond ADA compliance, the proposed framework lays the groundwork for broader applications in infrastructure surveying and automated construction evaluation, promoting wider adoption of point cloud data in these domains. The annotated database, manual ramp survey data, and developed algorithms are publicly available on the project's GitHub page: https://github.com/Soltanilara/SurveyAutomation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A review of advancements in low-light image enhancement using deep learning</title>
<link>https://arxiv.org/abs/2505.05759</link>
<guid>https://arxiv.org/abs/2505.05759</guid>
<content:encoded><![CDATA[
<div> Keywords: low-light, computer vision, deep learning, image enhancement, vision tasks 

Summary:
This review focuses on the challenges faced by computer vision algorithms in low-light environments and the advancements made in using deep learning for low-light image processing. It examines recent deep-learning-based methods for enhancing low-light images, discussing their operation, enhancement mechanisms, and impact on downstream vision tasks such as segmentation, detection, and classification. The review provides a comprehensive analysis of the strengths and limitations of different enhancement techniques, offering insights for optimizing vision task performance in low-light conditions. Through clear illustrations and detailed explanations, the review serves as a valuable reference for researchers and practitioners seeking to improve the performance of computer vision algorithms in low-light scenarios. Future research directions in the field of low-light image enhancement are also proposed. 

<br /><br />Summary: <div>
arXiv:2505.05759v1 Announce Type: new 
Abstract: In low-light environments, the performance of computer vision algorithms often deteriorates significantly, adversely affecting key vision tasks such as segmentation, detection, and classification. With the rapid advancement of deep learning, its application to low-light image processing has attracted widespread attention and seen significant progress in recent years. However, there remains a lack of comprehensive surveys that systematically examine how recent deep-learning-based low-light image enhancement methods function and evaluate their effectiveness in enhancing downstream vison tasks. To address this gap, this review provides a detailed elaboration on how various recent approaches (from 2020) operate and their enhancement mechanisms, supplemented with clear illustrations. It also investigates the impact of different enhancement techniques on subsequent vision tasks, critically analyzing their strengths and limitations. Additionally, it proposes future research directions. This review serves as a useful reference for determining low-light image enhancement techniques and optimizing vision task performance in low-light conditions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Describe Anything in Medical Images</title>
<link>https://arxiv.org/abs/2505.05804</link>
<guid>https://arxiv.org/abs/2505.05804</guid>
<content:encoded><![CDATA[
<div> Keywords: localized image captioning, medical imaging, vision-language models, region-specific captioning, clinical factuality<br />
Summary:<br />
Localized image captioning has seen advancements with models like DAM, but has not been extensively applied to medical imaging. To address this gap, MedDAM is proposed as the first framework utilizing large vision-language models for region-specific captioning in medical images. It incorporates medical expert-designed prompts and establishes a benchmark for evaluation focusing on clinical factuality through attribute-level verification tasks. MedDAM outperforms various leading models in the task, emphasizing the importance of region-level semantic alignment in medical image understanding. The framework shows promise for clinical vision-language integration. <div>
arXiv:2505.05804v1 Announce Type: new 
Abstract: Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAM's superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework</title>
<link>https://arxiv.org/abs/2505.05806</link>
<guid>https://arxiv.org/abs/2505.05806</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, variational models, deep learning, UNet, boundary preservation<br />
<br />
Summary: <br />
Traditional image segmentation methods, such as variational models based on PDEs, can struggle with parameter sensitivity and high computational costs. On the other hand, deep learning models like UNet are excellent at automatic feature extraction but lack interpretability and require extensive labeled data. To combine the strengths of both approaches, VM_TUNet integrates the fourth-order modified Cahn-Hilliard equation with UNet, offering both interpretability and adaptive feature learning. By introducing a data-driven operator and incorporating TFPM for boundary preservation, VM_TUNet outperforms existing methods in segmentation performance, particularly for precise boundary delineation. The experimental results on benchmark datasets validate the effectiveness of the proposed hybrid framework. <br /> <div>
arXiv:2505.05806v1 Announce Type: new 
Abstract: Traditional image segmentation methods, such as variational models based on partial differential equations (PDEs), offer strong mathematical interpretability and precise boundary modeling, but often suffer from sensitivity to parameter settings and high computational costs. In contrast, deep learning models such as UNet, which are relatively lightweight in parameters, excel in automatic feature extraction but lack theoretical interpretability and require extensive labeled data. To harness the complementary strengths of both paradigms, we propose Variational Model Based Tailored UNet (VM_TUNet), a novel hybrid framework that integrates the fourth-order modified Cahn-Hilliard equation with the deep learning backbone of UNet, which combines the interpretability and edge-preserving properties of variational methods with the adaptive feature learning of neural networks. Specifically, a data-driven operator is introduced to replace manual parameter tuning, and we incorporate the tailored finite point method (TFPM) to enforce high-precision boundary preservation. Experimental results on benchmark datasets demonstrate that VM_TUNet achieves superior segmentation performance compared to existing approaches, especially for fine boundary delineation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2505.05829</link>
<guid>https://arxiv.org/abs/2505.05829</guid>
<content:encoded><![CDATA[
<div> Diffusion transformer, image generation, caching, acceleration, calibration<br />
<br />
Summary: 
The paper introduces an increment-calibrated caching method for accelerating diffusion transformer (DiT) models in image generation. The method utilizes pre-trained models for calibration parameters and employs channel-aware Singular Value Decomposition (SVD) to enhance the calibration effect. Experimental results demonstrate superior performance compared to existing cache-based methods, reducing computation by over 45% while improving Inception Score (IS) by 12 with minimal increase in Frchet Inception Distance (FID). The proposed method eliminates the need for training and achieves efficient acceleration of DiT models, making it a promising approach for improving the scalability and generative capabilities of diffusion models. The code for implementing the method is available on GitHub for further exploration. 

Summary:<br />
Diffusion transformer, image generation, caching, acceleration, calibration. The paper proposes increment-calibrated caching for diffusion transformer (DiT) models, using pre-trained models for calibration and channel-aware SVD for correction. Experimental results show superior performance in computation reduction and Inception Score improvement with minimal FID increase. The method offers training-free acceleration of DiT models, aiding scalability and generative capabilities. Code is available on GitHub for implementation. <div>
arXiv:2505.05829v1 Announce Type: new 
Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression</title>
<link>https://arxiv.org/abs/2505.05834</link>
<guid>https://arxiv.org/abs/2505.05834</guid>
<content:encoded><![CDATA[
<div> Keywords: ordinal regression, patch-level features, fuzzy learning, image classification, deep learning

Summary:
The paper introduces a novel framework called Dual-level Fuzzy Learning with Patch Guidance (DFPG) for image ordinal regression. DFPG aims to learn precise grading boundaries using patch-level supervision, despite having only image-level ordinal labels. It utilizes patch-labeling and filtering strategies to concentrate on patch-level features, and incorporates a dual-level fuzzy learning module that handles label ambiguity effectively. Through extensive experiments on various datasets, DFPG outperforms existing methods and excels in categorizing challenging samples. The proposed framework demonstrates the ability to discern samples from difficult-to-classify categories with accuracy, showcasing its potential in advancing image classification tasks. The code for DFPG is publicly available for further exploration and implementation. 

<br /><br />
Summary: <div>
arXiv:2505.05834v1 Announce Type: new 
Abstract: Ordinal regression bridges regression and classification by assigning objects to ordered classes. While human experts rely on discriminative patch-level features for decisions, current approaches are limited by the availability of only image-level ordinal labels, overlooking fine-grained patch-level characteristics. In this paper, we propose a Dual-level Fuzzy Learning with Patch Guidance framework, named DFPG that learns precise feature-based grading boundaries from ambiguous ordinal labels, with patch-level supervision. Specifically, we propose patch-labeling and filtering strategies to enable the model to focus on patch-level features exclusively with only image-level ordinal labels available. We further design a dual-level fuzzy learning module, which leverages fuzzy logic to quantitatively capture and handle label ambiguity from both patch-wise and channel-wise perspectives. Extensive experiments on various image ordinal regression datasets demonstrate the superiority of our proposed method, further confirming its ability in distinguishing samples from difficult-to-classify categories. The code is available at https://github.com/ZJUMAI/DFPG-ord.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
<link>https://arxiv.org/abs/2505.05845</link>
<guid>https://arxiv.org/abs/2505.05845</guid>
<content:encoded><![CDATA[
<div> keyword: knots, wood, detection, machine learning, automation
Summary: 
The paper introduces a novel automated pipeline for knot detection and pairing in wood, addressing the labor-intensive and inefficient manual annotation process. Utilizing high-resolution surface images of wooden boards and machine learning techniques, the pipeline achieves a high detection accuracy using YOLOv8l with an mAP@0.5 of 0.887. In the pairing stage, knots are analyzed and paired based on multidimensional feature extraction and a triplet neural network to map features into a latent space, enabling clustering algorithms to identify corresponding knots with a pairing accuracy of 0.85. The experiments highlight the importance of distances from knot start and end points to the bottom of the wooden board, as well as longitudinal coordinates, in achieving high pairing accuracy. The study showcases the potential of AI in advancing wood science and industry.<br /><br />Summary: <div>
arXiv:2505.05845v1 Announce Type: new 
Abstract: Knots in wood are critical to both aesthetics and structural integrity, making their detection and pairing essential in timber processing. However, traditional manual annotation was labor-intensive and inefficient, necessitating automation. This paper proposes a lightweight and fully automated pipeline for knot detection and pairing based on machine learning techniques. In the detection stage, high-resolution surface images of wooden boards were collected using industrial-grade cameras, and a large-scale dataset was manually annotated and preprocessed. After the transfer learning, the YOLOv8l achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were analyzed and paired based on multidimensional feature extraction. A triplet neural network was used to map the features into a latent space, enabling clustering algorithms to identify and pair corresponding knots. The triplet network with learnable weights achieved a pairing accuracy of 0.85. Further analysis revealed that he distances from the knot's start and end points to the bottom of the wooden board, and the longitudinal coordinates play crucial roles in achieving high pairing accuracy. Our experiments validate the effectiveness of the proposed solution, demonstrating the potential of AI in advancing wood science and industry.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects</title>
<link>https://arxiv.org/abs/2505.05848</link>
<guid>https://arxiv.org/abs/2505.05848</guid>
<content:encoded><![CDATA[
<div> dataset, refractive, reflective, 3D reconstruction, neural rendering

Summary:
The article introduces a new synthetic dataset called RefRef for reconstructing scenes with refractive and reflective objects from posed images. The dataset includes 50 objects of varying complexity placed in different background types, resulting in 150 scenes. An oracle method is proposed to calculate accurate light paths for neural rendering based on object geometry and refractive indices. The performance of several state-of-the-art methods is benchmarked against the oracle, showing significant lag in performance. The challenges of handling refractive and reflective materials in 3D reconstruction and novel view synthesis are highlighted by the dataset and benchmark results. <div>
arXiv:2505.05848v1 Announce Type: new 
Abstract: Modern 3D reconstruction and novel view synthesis approaches have demonstrated strong performance on scenes with opaque Lambertian objects. However, most assume straight light paths and therefore cannot properly handle refractive and reflective materials. Moreover, datasets specialized for these effects are limited, stymieing efforts to evaluate performance and develop suitable techniques. In this work, we introduce a synthetic RefRef dataset and benchmark for reconstructing scenes with refractive and reflective objects from posed images. Our dataset has 50 such objects of varying complexity, from single-material convex shapes to multi-material non-convex shapes, each placed in three different background types, resulting in 150 scenes. We also propose an oracle method that, given the object geometry and refractive indices, calculates accurate light paths for neural rendering, and an approach based on this that avoids these assumptions. We benchmark these against several state-of-the-art methods and show that all methods lag significantly behind the oracle, highlighting the challenges of the task and dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PICD: Versatile Perceptual Image Compression with Diffusion Rendering</title>
<link>https://arxiv.org/abs/2505.05853</link>
<guid>https://arxiv.org/abs/2505.05853</guid>
<content:encoded><![CDATA[
<div> compression, image, text, diffusion, rendering

Summary:
- The article introduces a new perceptual image compression method called PICD that works effectively for both screen and natural images.
- PICD utilizes a diffusion rendering approach where text and image are encoded separately and combined using a diffusion model.
- Three levels of conditional information integration are incorporated into the diffusion models: domain level, adaptor level, and instance level.
- By fine-tuning the base diffusion model with text content prompts and efficiently controlling the model using compressed image and text input, PICD achieves high text accuracy and perceptual quality.
- Additionally, the PICD codec can serve as a high-quality perceptual codec for natural images without the need for text conditions. 

<br /><br />Summary: <div>
arXiv:2505.05853v1 Announce Type: new 
Abstract: Recently, perceptual image compression has achieved significant advancements, delivering high visual quality at low bitrates for natural images. However, for screen content, existing methods often produce noticeable artifacts when compressing text. To tackle this challenge, we propose versatile perceptual screen image compression with diffusion rendering (PICD), a codec that works well for both screen and natural images. More specifically, we propose a compression framework that encodes the text and image separately, and renders them into one image using diffusion model. For this diffusion rendering, we integrate conditional information into diffusion models at three distinct levels: 1). Domain level: We fine-tune the base diffusion model using text content prompts with screen content. 2). Adaptor level: We develop an efficient adaptor to control the diffusion model using compressed image and text as input. 3). Instance level: We apply instance-wise guidance to further enhance the decoding process. Empirically, our PICD surpasses existing perceptual codecs in terms of both text accuracy and perceptual quality. Additionally, without text conditions, our approach serves effectively as a perceptual codec for natural images.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations</title>
<link>https://arxiv.org/abs/2505.05855</link>
<guid>https://arxiv.org/abs/2505.05855</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, multi-contrast, super-resolution, unpaired, neural representations

Summary:
The article introduces a Modular Multi-Contrast Super-Resolution (MCSR) framework for enhancing MRI quality. It addresses the challenge of cross-modal enhancement by decoupling the MCSR task into two stages: Unpaired Cross-Modal Synthesis (U-CMS) and Unsupervised Super-Resolution (U-SR). This framework eliminates the need for paired training data and supports arbitrary upscaling. The U-CMS stage translates high-resolution reference modalities into synthesized versions of the target contrast, while the U-SR stage reconstructs the final output using implicit neural representations conditioned on spatial coordinates. The proposed method achieves superior performance at 4x and 8x upscaling, with improved fidelity and anatomical consistency compared to existing methods. The framework shows promise for scalable, subject-specific, and data-efficient MCSR in clinical settings. 

<br /><br />Summary: <div>
arXiv:2505.05855v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is often limited by long acquisition times and low signal-to-noise ratios, especially in modalities like diffusion and functional MRI. The multi-contrast nature of MRI presents a valuable opportunity for cross-modal enhancement, where high-resolution (HR) modalities can serve as references to boost the quality of their low-resolution (LR) counterparts-motivating the development of Multi-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that leveraging complementary contrasts can improve SR performance; however, effective feature extraction and fusion across modalities with varying resolutions remains a major challenge. Moreover, existing MCSR methods often assume fixed resolution settings and all require large, perfectly paired training datasets-conditions rarely met in real-world clinical environments. To address these challenges, we propose a novel Modular Multi-Contrast Super-Resolution (MCSR) framework that eliminates the need for paired training data and supports arbitrary upscaling. Our method decouples the MCSR task into two stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a high-resolution reference modality into a synthesized version of the target contrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the final output using implicit neural representations (INRs) conditioned on spatial coordinates. This design enables scale-agnostic and anatomically faithful reconstruction by bridging un-paired cross-modal synthesis with unsupervised resolution enhancement. Experiments show that our method achieves superior performance at 4x and 8x upscaling, with improved fidelity and anatomical consistency over existing baselines. Our framework demonstrates strong potential for scalable, subject-specific, and data-efficient MCSR in real-world clinical settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Facial Image Compression with Consistency Preserving Diffusion Prior</title>
<link>https://arxiv.org/abs/2505.05870</link>
<guid>https://arxiv.org/abs/2505.05870</guid>
<content:encoded><![CDATA[
<div> Facial Image Compression, Diffusion Prior, High-Frequency Information, Visual Quality, Machine Vision Accuracy
Summary:<br /><br />Facial Image Compression with a Stable Diffusion Prior (FaSDiff) addresses the issue of unsatisfactory reconstructed image quality in existing learned face image compression methods at low bit rates. By using a high-frequency-sensitive compressor and a hybrid low-frequency enhancement module, FaSDiff captures fine image details and disentangles low-frequency facial semantics to improve visual quality and machine vision accuracy. The method preserves consistency through frequency enhancement, leveraging diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments demonstrate that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code for FaSDiff will be released after the paper is accepted. <div>
arXiv:2505.05870v1 Announce Type: new 
Abstract: With the widespread application of facial image data across various domains, the efficient storage and transmission of facial images has garnered significant attention. However, the existing learned face image compression methods often produce unsatisfactory reconstructed image quality at low bit rates. Simply adapting diffusion-based compression methods to facial compression tasks results in reconstructed images that perform poorly in downstream applications due to insufficient preservation of high-frequency information. To further explore the diffusion prior in facial image compression, we propose Facial Image Compression with a Stable Diffusion Prior (FaSDiff), a method that preserves consistency through frequency enhancement. FaSDiff employs a high-frequency-sensitive compressor in an end-to-end framework to capture fine image details and produce robust visual prompts. Additionally, we introduce a hybrid low-frequency enhancement module that disentangles low-frequency facial semantics and stably modulates the diffusion prior alongside visual prompts. The proposed modules allow FaSDiff to leverage diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments show that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code will be released after the paper is accepted.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Register and CLS tokens yield a decoupling of local and global features in large ViTs</title>
<link>https://arxiv.org/abs/2505.05892</link>
<guid>https://arxiv.org/abs/2505.05892</guid>
<content:encoded><![CDATA[
<div> Keywords: DINOv2 model, attention maps, register tokens, global image information, interpretability<br />
<br />
Summary: <br />
Recent research has uncovered issues with attention maps in the DINOv2 model, where artifacts hinder interpretability and performance. The problem arises from the model using patch tokens for global image information, resulting in inaccuracies. To rectify this, register tokens are introduced to store global information separately. However, it was found that this leads to a dominance of global information from register tokens, causing a disconnect between local and global features. Surprisingly, the CLS token also exhibits similar behavior in models lacking explicit register tokens. This study highlights the importance of careful attention map interpretation in large vision models and suggests a pathway towards more interpretable models by addressing the issues related to register and CLS tokens. <div>
arXiv:2505.05892v1 Announce Type: new 
Abstract: Recent work has shown that the attention maps of the widely popular DINOv2 model exhibit artifacts, which hurt both model interpretability and performance on dense image tasks. These artifacts emerge due to the model repurposing patch tokens with redundant local information for the storage of global image information. To address this problem, additional register tokens have been incorporated in which the model can store such information instead. We carefully examine the influence of these register tokens on the relationship between global and local image features, showing that while register tokens yield cleaner attention maps, these maps do not accurately reflect the integration of local image information in large models. Instead, global information is dominated by information extracted from register tokens, leading to a disconnect between local and global features. Inspired by these findings, we show that the CLS token itself, which can be interpreted as a register, leads to a very similar phenomenon in models without explicit register tokens. Our work shows that care must be taken when interpreting attention maps of large ViTs. Further, by clearly attributing the faulty behaviour to register and CLS tokens, we show a path towards more interpretable vision models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI</title>
<link>https://arxiv.org/abs/2505.05895</link>
<guid>https://arxiv.org/abs/2505.05895</guid>
<content:encoded><![CDATA[
<div> Dataset, vision-language framework, automotive infotainment systems, adaptation, Molmo-7B model

Summary:
Automotive infotainment systems require intelligent solutions for handling UI updates and design variations. A vision-language framework is introduced to understand and interact with these systems, along with the release of an open-source dataset called AutomotiveUI-Bench-4K. A synthetic data pipeline is presented for generating training data, and a Molmo-7B model is fine-tuned using Low-Rank Adaptation (LoRa). The resulting Evaluative Large Action Model (ELAM) achieves strong performance on the dataset, with a +5.2% improvement on ScreenSpot compared to the baseline model. Despite being trained for infotainment, ELAM achieves high accuracy on ScreenSpot, matching specialized models for other UI domains. This research showcases how data collection and fine-tuning can drive AI progress in automotive UI understanding, with a cost-efficient approach that allows deployment on consumer-grade GPUs.

Summary: <div>
arXiv:2505.05895v1 Announce Type: new 
Abstract: Modern automotive infotainment systems require intelligent and adaptive solutions to handle frequent User Interface (UI) updates and diverse design variations. We introduce a vision-language framework for understanding and interacting with automotive infotainment systems, enabling seamless adaptation across different UI designs. To further support research in this field, we release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208 annotations. Additionally, we present a synthetic data pipeline to generate training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation (LoRa) and incorporating reasoning generated by our pipeline, along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face) and demonstrating strong cross-domain generalization, including a +5.2% improvement on ScreenSpot over the baseline model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot, closely matching or even surpassing specialized models for desktop, mobile, and web, such as ShowUI, despite being trained for the infotainment domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven progress within automotive UI understanding and interaction. The applied method is cost-efficient and fine-tuned models can be deployed on consumer-grade GPUs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.05901</link>
<guid>https://arxiv.org/abs/2505.05901</guid>
<content:encoded><![CDATA[
<div> Anomaly detection, Mechanics Complementary framework, Corrective forces, Diverse Anomaly-Generation module, Corrective Force Prediction Network, hierarchical quality control strategy<br />
<br />
Summary: 
This paper introduces a new approach to anomaly detection that considers the underlying causes of anomalies and generates corrective forces for each point. The Mechanics Complementary framework for 3D anomaly detection (MC4AD) is proposed, along with a Diverse Anomaly-Generation module to simulate various anomalies. A Corrective Force Prediction Network (CFP-Net) is developed to predict internal and external corrective forces for each point. A combined loss function is introduced to properly constrain the corrective forces. The study also presents a hierarchical quality control strategy based on a three-way decision and introduces a new dataset, Anomaly-IntraVariance, for model evaluation. Nine state-of-the-art performers were achieved on the proposed and existing datasets with minimal parameters and fast inference speed. <div>
arXiv:2505.05901v1 Announce Type: new 
Abstract: In this paper, we go beyond identifying anomalies only in structural terms and think about better anomaly detection motivated by anomaly causes. Most anomalies are regarded as the result of unpredictable defective forces from internal and external sources, and their opposite forces are sought to correct the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly detection (MC4AD) to generate internal and external Corrective forces for each point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to simulate various anomalies. Then, we present a Corrective Force Prediction Network (CFP-Net) with complementary representations for point-level representation to simulate the different contributions of internal and external corrective forces. A combined loss was proposed, including a new symmetric loss and an overall loss, to constrain the corrective forces properly. As a highlight, we consider 3D anomaly detection in industry more comprehensively, creating a hierarchical quality control strategy based on a three-way decision and contributing a dataset named Anomaly-IntraVariance with intraclass variance to evaluate the model. On the proposed and existing five datasets, we obtained nine state-of-the-art performers with the minimum parameters and the fastest inference speed. The source is available at https://github.com/hzzzzzhappy/MC4AD
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFEN: Dual Feature Equalization Network for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.05913</link>
<guid>https://arxiv.org/abs/2505.05913</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, dual feature equalization network, Swin Transformer, convolutional neural network, state-of-the-art performance

Summary:
The paper introduces a novel method for medical image segmentation, the dual feature equalization network, which combines the advantages of Swin Transformer and Convolutional Neural Network. It addresses the issue of unequal contextual feature information at image boundaries and low-class pixel regions by proposing image-level and class-level feature equalization modules. These modules enhance pixel feature representations by equalizing contextual information within the image and aggregating regions of the same class. By utilizing Swin Transformer for encoding and decoding, the model effectively captures long-range dependencies and spatial correlations. Experimental results on various datasets show superior performance, achieving state-of-the-art results. The code for the proposed method is publicly available, showcasing the reproducibility and potential for further research and development in medical image segmentation. 
<br /><br />Summary: <div>
arXiv:2505.05913v1 Announce Type: new 
Abstract: Current methods for medical image segmentation primarily focus on extracting contextual feature information from the perspective of the whole image. While these methods have shown effective performance, none of them take into account the fact that pixels at the boundary and regions with a low number of class pixels capture more contextual feature information from other classes, leading to misclassification of pixels by unequal contextual feature information. In this paper, we propose a dual feature equalization network based on the hybrid architecture of Swin Transformer and Convolutional Neural Network, aiming to augment the pixel feature representations by image-level equalization feature information and class-level equalization feature information. Firstly, the image-level feature equalization module is designed to equalize the contextual information of pixels within the image. Secondly, we aggregate regions of the same class to equalize the pixel feature representations of the corresponding class by class-level feature equalization module. Finally, the pixel feature representations are enhanced by learning weights for image-level equalization feature information and class-level equalization feature information. In addition, Swin Transformer is utilized as both the encoder and decoder, thereby bolstering the ability of the model to capture long-range dependencies and spatial correlations. We conducted extensive experiments on Breast Ultrasound Images (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated Cardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental results demonstrate that our method have achieved state-of-the-art performance. Our code is publicly available at https://github.com/JianJianYin/DFEN.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking</title>
<link>https://arxiv.org/abs/2505.05936</link>
<guid>https://arxiv.org/abs/2505.05936</guid>
<content:encoded><![CDATA[
arXiv:2505.05936v1 Announce Type: new 
Abstract: Recent advancements in visual object tracking have markedly improved the capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical component in real-world robotics applications. While the integration of hierarchical lightweight networks has become a prevalent strategy for enhancing efficiency in UAV tracking, it often results in a significant drop in network capacity, which further exacerbates challenges in UAV scenarios, such as frequent occlusions and extreme changes in viewing angles. To address these issues, we introduce a novel family of UAV trackers, termed CGTrack, which combines explicit and implicit techniques to expand network capacity within a coarse-to-fine framework. Specifically, we first introduce a Hierarchical Feature Cascade (HFC) module that leverages the spirit of feature reuse to increase network capacity by integrating the deep semantic cues with the rich spatial information, incurring minimal computational costs while enhancing feature representation. Based on this, we design a novel Lightweight Gated Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented coordinates from previously expanded features, which contain dense local discriminative information. Extensive experiments on three challenging UAV tracking benchmarks demonstrate that CGTrack achieves state-of-the-art performance while running fast. Code will be available at https://github.com/Nightwatch-Fox11/CGTrack.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving 3D Attention via Triplet Squeeze and Excitation Block</title>
<link>https://arxiv.org/abs/2505.05943</link>
<guid>https://arxiv.org/abs/2505.05943</guid>
<content:encoded><![CDATA[
arXiv:2505.05943v1 Announce Type: new 
Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and structural suitability of CNN-based models for vision tasks, re-establishing them as key players in image classification in general, and in facial expression recognition (FER) in particular. In this paper, we propose a new set of models that build on these advancements by incorporating a new set of attention mechanisms that combines Triplet attention with Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the effectiveness of these variants by applying them to the ResNet18, DenseNet and ConvNext architectures to validate their versatility and impact. Our study shows that incorporating a TripSE block in these CNN models boosts their performances, particularly for the ConvNeXt architecture, indicating its utility. We evaluate the proposed mechanisms and associated models across four datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where ConvNext with TripSE achieves state-of-the-art results with an accuracy of \textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition</title>
<link>https://arxiv.org/abs/2505.06002</link>
<guid>https://arxiv.org/abs/2505.06002</guid>
<content:encoded><![CDATA[
arXiv:2505.06002v1 Announce Type: new 
Abstract: Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained image models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on 5 benchmarks consistently. The code is open-sourced at https://github.com/Jaulin-Bage/Task-Adapter-pp.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection</title>
<link>https://arxiv.org/abs/2505.06003</link>
<guid>https://arxiv.org/abs/2505.06003</guid>
<content:encoded><![CDATA[
arXiv:2505.06003v1 Announce Type: new 
Abstract: Understanding the decision-making process of machine learning models provides valuable insights into the task, the data, and the reasons behind a model's failures. In this work, we propose a method that performs inherently interpretable predictions through the instance-wise sparsification of input images. To align the sparsification with human perception, we learn the masking in the space of semantically meaningful pixel regions rather than on pixel-level. Additionally, we introduce an explicit way to dynamically determine the required level of sparsity for each instance. We show empirically on semi-synthetic and natural image datasets that our inherently interpretable classifier produces more meaningful, human-understandable predictions than state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Image Rectification Bases on Self-Adaptive Multitask Fusion</title>
<link>https://arxiv.org/abs/2505.06038</link>
<guid>https://arxiv.org/abs/2505.06038</guid>
<content:encoded><![CDATA[
arXiv:2505.06038v1 Announce Type: new 
Abstract: Deformed document image rectification is essential for real-world document understanding tasks, such as layout analysis and text recognition. However, current multi-task methods -- such as background removal, 3D coordinate prediction, and text line segmentation -- often overlook the complementary features between tasks and their interactions. To address this gap, we propose a self-adaptive learnable multi-task fusion rectification network named SalmRec. This network incorporates an inter-task feature aggregation module that adaptively improves the perception of geometric distortions, enhances feature complementarity, and reduces negative interference. We also introduce a gating mechanism to balance features both within global tasks and between local tasks effectively. Experimental results on two English benchmarks (DIR300 and DocUNet) and one Chinese benchmark (DocReal) demonstrate that our method significantly improves rectification performance. Ablation studies further highlight the positive impact of different tasks on dewarping and the effectiveness of our proposed module.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Cephalometric Landmark Detection with Diffusion Data Generation</title>
<link>https://arxiv.org/abs/2505.06055</link>
<guid>https://arxiv.org/abs/2505.06055</guid>
<content:encoded><![CDATA[
arXiv:2505.06055v1 Announce Type: new 
Abstract: Cephalometric landmark detection is essential for orthodontic diagnostics and treatment planning. Nevertheless, the scarcity of samples in data collection and the extensive effort required for manual annotation have significantly impeded the availability of diverse datasets. This limitation has restricted the effectiveness of deep learning-based detection methods, particularly those based on large-scale vision models. To address these challenges, we have developed an innovative data generation method capable of producing diverse cephalometric X-ray images along with corresponding annotations without human intervention. To achieve this, our approach initiates by constructing new cephalometric landmark annotations using anatomical priors. Then, we employ a diffusion-based generator to create realistic X-ray images that correspond closely with these annotations. To achieve precise control in producing samples with different attributes, we introduce a novel prompt cephalometric X-ray image dataset. This dataset includes real cephalometric X-ray images and detailed medical text prompts describing the images. By leveraging these detailed prompts, our method improves the generation process to control different styles and attributes. Facilitated by the large, diverse generated data, we introduce large-scale vision detection models into the cephalometric landmark detection task to improve accuracy. Experimental results demonstrate that training with the generated data substantially enhances the performance. Compared to methods without using the generated data, our approach improves the Success Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and data are available at: https://um-lab.github.io/cepha-generation
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation</title>
<link>https://arxiv.org/abs/2505.06068</link>
<guid>https://arxiv.org/abs/2505.06068</guid>
<content:encoded><![CDATA[
arXiv:2505.06068v1 Announce Type: new 
Abstract: Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at GitHub.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.06113</link>
<guid>https://arxiv.org/abs/2505.06113</guid>
<content:encoded><![CDATA[
arXiv:2505.06113v1 Announce Type: new 
Abstract: Autonomous vehicle perception systems have traditionally relied on costly LiDAR sensors to generate precise environmental representations. In this paper, we propose a camera-only perception framework that produces Bird's Eye View (BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines YOLOv11-based object detection with DepthAnythingV2 monocular depth estimation across multi-camera inputs to achieve comprehensive 360-degree scene understanding. We evaluate our approach on the OpenLane-V2 and NuScenes datasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle detection rates when compared against LiDAR ground truth, with average positional errors limited to 1.2 meters. These results highlight the potential of deep learning to extract rich spatial information using only camera inputs, enabling cost-efficient autonomous navigation without sacrificing accuracy.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation</title>
<link>https://arxiv.org/abs/2505.06117</link>
<guid>https://arxiv.org/abs/2505.06117</guid>
<content:encoded><![CDATA[
arXiv:2505.06117v1 Announce Type: new 
Abstract: Accurate defect detection of photovoltaic (PV) cells is critical for ensuring quality and efficiency in intelligent PV manufacturing systems. However, the scarcity of rich defect data poses substantial challenges for effective model training. While existing methods have explored generative models to augment datasets, they often suffer from instability, limited diversity, and domain shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image Generator based on Stable Diffusion (SD). PDIG leverages the strong priors learned from large-scale datasets to enhance generation quality under limited data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that incorporates text-conditioned priors to capture the relational concepts between defect types and their appearances. To further enrich the domain distribution, we design a Lightweight Industrial Style Adaptor (LISA), which injects industrial defect characteristics into the SD model through cross-disentangled attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC) module, enforcing the quality of generated images via positional consistency and spatial smoothing alignment. Extensive experiments demonstrate that PDIG achieves superior realism and diversity compared to state-of-the-art methods. Specifically, our approach improves Frechet Inception Distance (FID) by 19.16 points over the second-best method and significantly enhances the performance of downstream defect detection tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation</title>
<link>https://arxiv.org/abs/2505.06133</link>
<guid>https://arxiv.org/abs/2505.06133</guid>
<content:encoded><![CDATA[
arXiv:2505.06133v1 Announce Type: new 
Abstract: The segmentation of substantial brain lesions is a significant and challenging task in the field of medical image segmentation. Substantial brain lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries between lesion regions and normal brain tissue. Small lesions in single slices are difficult to identify, making the accurate and reproducible segmentation of abnormal regions, as well as their feature description, highly complex. Existing methods have the following limitations: 1) They rely solely on single-modal information for learning, neglecting the multi-modal information commonly used in diagnosis. This hampers the ability to comprehensively acquire brain lesion information from multiple perspectives and prevents the effective integration and utilization of multi-modal data inputs, thereby limiting a holistic understanding of lesions. 2) They are constrained by the amount of data available, leading to low sensitivity to small lesions and difficulty in detecting subtle pathological changes. 3) Current SAM-based models rely on external prompts, which cannot achieve automatic segmentation and, to some extent, affect diagnostic efficiency.To address these issues, we have developed a large-scale fully automated segmentation model specifically designed for brain lesion segmentation, named BrainSegDMLF. This model has the following features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and integrates multi-modal data during the encoding process, providing the SAM encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling Decoder, enabling the model to extract rich low-level and high-level features even with limited data, thereby detecting the presence of small lesions. 3) Automatic segmentation masks, allowing the model to generate lesion masks automatically without requiring manual prompts.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks</title>
<link>https://arxiv.org/abs/2505.06152</link>
<guid>https://arxiv.org/abs/2505.06152</guid>
<content:encoded><![CDATA[
arXiv:2505.06152v1 Announce Type: new 
Abstract: Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped, primarily due to less specialized text descriptions in current dermatology multimodal datasets. To address this issue, we propose MM-Skin, the first large-scale multimodal dermatology dataset that encompasses 3 imaging modalities, including clinical, dermoscopic, and pathological and nearly 10k high-quality image-text pairs collected from professional textbooks. In addition, we generate over 27k diverse, instruction-following vision question answering (VQA) samples (9 times the size of current largest dermatology VQA dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a dermatology-specific VLM designed for precise and nuanced skin disease interpretation. Comprehensive benchmark evaluations of SkinVL on VQA, supervised fine-tuning (SFT) and zero-shot classification tasks across 8 datasets, reveal its exceptional performance for skin diseases in comparison to both general and medical VLM models. The introduction of MM-Skin and SkinVL offers a meaningful contribution to advancing the development of clinical dermatology VLM assistants. MM-Skin is available at https://github.com/ZwQ803/MM-Skin
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06166</link>
<guid>https://arxiv.org/abs/2505.06166</guid>
<content:encoded><![CDATA[
arXiv:2505.06166v1 Announce Type: new 
Abstract: We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at https://radualexandru.github.io/difflocks/
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting a Segmentation Foundation Model for Medical Image Classification</title>
<link>https://arxiv.org/abs/2505.06217</link>
<guid>https://arxiv.org/abs/2505.06217</guid>
<content:encoded><![CDATA[
arXiv:2505.06217v1 Announce Type: new 
Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have shown strong performance in various vision tasks, particularly image segmentation, due to their impressive zero-shot segmentation capabilities. However, effectively adapting such models for medical image classification is still a less explored topic. In this paper, we introduce a new framework to adapt SAM for medical image classification. First, we utilize the SAM image encoder as a feature extractor to capture segmentation-based features that convey important spatial and contextual details of the image, while freezing its weights to avoid unnecessary overhead during training. Next, we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to compute spatially localized attention weights for the feature maps. The features extracted from SAM's image encoder are processed through SLCA to compute attention weights, which are then integrated into deep learning classification models to enhance their focus on spatially relevant or meaningful regions of the image, thus improving classification performance. Experimental results on three public medical image classification datasets demonstrate the effectiveness and data-efficiency of our approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction</title>
<link>https://arxiv.org/abs/2505.06219</link>
<guid>https://arxiv.org/abs/2505.06219</guid>
<content:encoded><![CDATA[
arXiv:2505.06219v1 Announce Type: new 
Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using minimal resources, time, or number of captures to enable efficient 3D reconstruction of a scene. Existing approaches often rely on prior scene knowledge or additional image captures and often develop policies that maximize coverage. Yet, for many real scenes with complex geometry and self-occlusions, coverage maximization does not lead to better reconstruction quality directly. In this paper, we propose the View Introspection Network (VIN), which is trained to predict the reconstruction quality improvement of views directly, and the VIN-NBV policy. A greedy sequential sampling-based policy, where at each acquisition step, we sample multiple query views and choose the one with the highest VIN predicted improvement score. We design the VIN to perform 3D-aware featurization of the reconstruction built from prior acquisitions, and for each query view create a feature that can be decoded into an improvement score. We then train the VIN using imitation learning to predict the reconstruction improvement score. We show that VIN-NBV improves reconstruction quality by ~30% over a coverage maximization baseline when operating with constraints on the number of acquisitions or the time in motion.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network</title>
<link>https://arxiv.org/abs/2505.05477</link>
<guid>https://arxiv.org/abs/2505.05477</guid>
<content:encoded><![CDATA[
arXiv:2505.05477v1 Announce Type: cross 
Abstract: Electrocardiogram (ECG) signals are frequently corrupted by noise, such as baseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which significantly degrade their diagnostic utility. To address this issue, we propose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a Double Recurrent Dense Network architecture. In contrast to traditional approaches, we introduce a double recurrent scheme to enhance information reuse from both ECG waveforms and the estimated clean image. For ECG waveform processing, our basic model employs LSTM layers cascaded with DenseNet blocks. The estimated clean ECG image, obtained by subtracting predicted noise components from the noisy input, is iteratively fed back into the model. This dual recurrent architecture enables comprehensive utilization of both temporal waveform features and spatial image details, leading to more effective noise suppression. Experimental results on the MIT-BIH dataset demonstrate that our method achieves superior performance compared to conventional image denoising methods in terms of PSNR and SSIM while also surpassing classical ECG denoising techniques in both SNR and RMSE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Restoration via Multi-domain Learning</title>
<link>https://arxiv.org/abs/2505.05504</link>
<guid>https://arxiv.org/abs/2505.05504</guid>
<content:encoded><![CDATA[
arXiv:2505.05504v1 Announce Type: cross 
Abstract: Due to adverse atmospheric and imaging conditions, natural images suffer from various degradation phenomena. Consequently, image restoration has emerged as a key solution and garnered substantial attention. Although recent Transformer architectures have demonstrated impressive success across various restoration tasks, their considerable model complexity poses significant challenges for both training and real-time deployment. Furthermore, instead of investigating the commonalities among different degradations, most existing restoration methods focus on modifying Transformer under limited restoration priors. In this work, we first review various degradation phenomena under multi-domain perspective, identifying common priors. Then, we introduce a novel restoration framework, which integrates multi-domain learning into Transformer. Specifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain structure that facilitates local-region-global multi-receptive field modeling to replace vanilla self-attention. Additionally, in Feed-Forward Network, we incorporate multi-scale learning to fuse multi-domain features at different resolutions. Comprehensive experimental results across ten restoration tasks, such as dehazing, desnowing, motion deblurring, defocus deblurring, rain streak/raindrop removal, cloud removal, shadow removal, underwater enhancement and low-light enhancement, demonstrate that our proposed model outperforms state-of-the-art methods and achieves a favorable trade-off among restoration performance, parameter size, computational cost and inference latency. The code is available at: https://github.com/deng-ai-lab/SWFormer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2505.05509</link>
<guid>https://arxiv.org/abs/2505.05509</guid>
<content:encoded><![CDATA[
arXiv:2505.05509v1 Announce Type: cross 
Abstract: Stereo image super-resolution (SSR) aims to enhance high-resolution details by leveraging information from stereo image pairs. However, existing stereo super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook cross-view geometric consistency and are limited to fixed-scale upsampling. The key issue is that previous upsampling methods use convolution to independently process deep features of different views, lacking cross-view and non-local information perception, making it difficult to select beneficial information from multi-view scenes adaptively. In this work, we propose Stereo Implicit Neural Representation (StereoINR), which innovatively models stereo image pairs as continuous implicit representations. This continuous representation breaks through the scale limitations, providing a unified solution for arbitrary-scale stereo super-resolution reconstruction of left-right views. Furthermore, by incorporating spatial warping and cross-attention mechanisms, StereoINR enables effective cross-view information fusion and achieves significant improvements in pixel-level geometric consistency. Extensive experiments across multiple datasets show that StereoINR outperforms out-of-training-distribution scale upsampling and matches state-of-the-art SSR methods within training-distribution scales.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train Your Metamorphic Deep Neural Network</title>
<link>https://arxiv.org/abs/2505.05510</link>
<guid>https://arxiv.org/abs/2505.05510</guid>
<content:encoded><![CDATA[
arXiv:2505.05510v1 Announce Type: cross 
Abstract: Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural networks of varying width and depth. Based on Implicit Neural Representation (INR), NeuMeta learns a continuous weight manifold, enabling the direct generation of compressed models, including those with configurations not seen during training. While promising, the original formulation of NeuMeta proves effective only for the final layers of the undelying model, limiting its broader applicability. In this work, we propose a training algorithm that extends the capabilities of NeuMeta to enable full-network metamorphosis with minimal accuracy degradation. Our approach follows a structured recipe comprising block-wise incremental training, INR initialization, and strategies for replacing batch normalization. The resulting metamorphic networks maintain competitive accuracy across a wide range of compression ratios, offering a scalable solution for adaptable and efficient deployment of deep models. The code is available at: https://github.com/TSommariva/HTTY_NeuMeta.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility</title>
<link>https://arxiv.org/abs/2505.05518</link>
<guid>https://arxiv.org/abs/2505.05518</guid>
<content:encoded><![CDATA[
arXiv:2505.05518v1 Announce Type: cross 
Abstract: Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control.
  A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage.
  Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy.
  Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive Anywhere with Model-Based Reannotation11</title>
<link>https://arxiv.org/abs/2505.05592</link>
<guid>https://arxiv.org/abs/2505.05592</guid>
<content:encoded><![CDATA[
arXiv:2505.05592v1 Announce Type: cross 
Abstract: Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Self-supervised MRI Denoising</title>
<link>https://arxiv.org/abs/2505.05631</link>
<guid>https://arxiv.org/abs/2505.05631</guid>
<content:encoded><![CDATA[
arXiv:2505.05631v1 Announce Type: cross 
Abstract: Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized denoising score matching (GDSM) loss, which extends denoising score matching to work directly with noisy observations by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes</title>
<link>https://arxiv.org/abs/2505.05643</link>
<guid>https://arxiv.org/abs/2505.05643</guid>
<content:encoded><![CDATA[
arXiv:2505.05643v1 Announce Type: cross 
Abstract: Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New k-Space Model for Non-Cartesian Fourier Imaging</title>
<link>https://arxiv.org/abs/2505.05647</link>
<guid>https://arxiv.org/abs/2505.05647</guid>
<content:encoded><![CDATA[
arXiv:2505.05647v1 Announce Type: cross 
Abstract: For the past several decades, it has been popular to reconstruct Fourier imaging data using model-based approaches that can easily incorporate physical constraints and advanced regularization/machine learning priors. The most common modeling approach is to represent the continuous image as a linear combination of shifted "voxel" basis functions. Although well-studied and widely-deployed, this voxel-based model is associated with longstanding limitations, including high computational costs, slow convergence, and a propensity for artifacts. In this work, we reexamine this model from a fresh perspective, identifying new issues that may have been previously overlooked (including undesirable approximation, periodicity, and nullspace characteristics). Our insights motivate us to propose a new model that is more resilient to the limitations (old and new) of the previous approach. Specifically, the new model is based on a Fourier-domain basis expansion rather than the standard image-domain voxel-based approach. Illustrative results, which are presented in the context of non-Cartesian MRI reconstruction, demonstrate that the new model enables improved image quality (reduced artifacts) and/or reduced computational complexity (faster computations and improved convergence).
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models</title>
<link>https://arxiv.org/abs/2505.05659</link>
<guid>https://arxiv.org/abs/2505.05659</guid>
<content:encoded><![CDATA[
arXiv:2505.05659v1 Announce Type: cross 
Abstract: EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for granted the inter-channel relationships. This paper introduces vector-valued EfficientNets (V-EfficientNets), a novel extension of EfficientNet designed to process arbitrary vector-valued data. The proposed models are evaluated on a medical image classification task, achieving an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency, significantly reducing parameters while outperforming state-of-the-art models, including the original EfficientNet. The source code is available at https://github.com/mevalle/v-nets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology</title>
<link>https://arxiv.org/abs/2505.05689</link>
<guid>https://arxiv.org/abs/2505.05689</guid>
<content:encoded><![CDATA[
arXiv:2505.05689v1 Announce Type: cross 
Abstract: Histopathology evaluation of tissue specimens through microscopic examination is essential for accurate disease diagnosis and prognosis. However, traditional manual analysis by specially trained pathologists is time-consuming, labor-intensive, cost-inefficient, and prone to inter-rater variability, potentially affecting diagnostic consistency and accuracy. As digital pathology images continue to proliferate, there is a pressing need for automated analysis to address these challenges. Recent advancements in artificial intelligence-based tools such as machine learning (ML) models, have significantly enhanced the precision and efficiency of analyzing histopathological slides. However, despite their impressive performance, ML models are invariant only to translation, lacking invariance to rotation and reflection. This limitation restricts their ability to generalize effectively, particularly in histopathology, where images intrinsically lack meaningful orientation. In this study, we develop robust, equivariant histopathological biomarkers through a novel symmetric convolutional kernel via unsupervised segmentation. The approach is validated using prostate tissue micro-array (TMA) images from 50 patients in the Gleason 2019 Challenge public dataset. The biomarkers extracted through this approach demonstrate enhanced robustness and generalizability against rotation compared to models using standard convolution kernels, holding promise for enhancing the accuracy, consistency, and robustness of ML models in digital pathology. Ultimately, this work aims to improve diagnostic and prognostic capabilities of histopathology beyond prostate cancer through equivariant imaging.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference</title>
<link>https://arxiv.org/abs/2505.05703</link>
<guid>https://arxiv.org/abs/2505.05703</guid>
<content:encoded><![CDATA[
arXiv:2505.05703v1 Announce Type: cross 
Abstract: Purpose: Deep learning has demonstrated strong potential for MRI reconstruction, but conventional supervised learning methods require high-quality reference images, which are often unavailable in practice. Self-supervised learning offers an alternative, yet its performance degrades at high acceleration rates. To overcome these limitations, we propose hybrid learning, a novel two-stage training framework that combines self-supervised and supervised learning for robust image reconstruction.
  Methods: Hybrid learning is implemented in two sequential stages. In the first stage, self-supervised learning is employed to generate improved images from noisy or undersampled reference data. These enhanced images then serve as pseudo-ground truths for the second stage, which uses supervised learning to refine reconstruction performance and support higher acceleration rates. We evaluated hybrid learning in two representative applications: (1) accelerated 0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of the brain without access to fully sampled ground truth.
  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image quality over both self-supervised and conventional supervised methods across different acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping, hybrid learning achieved superior T1 quantification accuracy across a wide dynamic range, outperforming self-supervised learning in all tested conditions.
  Conclusions: Hybrid learning provides a practical and effective solution for training deep MRI reconstruction networks when only low-quality or incomplete reference data are available. It enables improved image quality and accurate quantitative mapping across different applications and field strengths, representing a promising technique toward broader clinical deployment of deep learning-based MRI.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05732</link>
<guid>https://arxiv.org/abs/2505.05732</guid>
<content:encoded><![CDATA[
arXiv:2505.05732v1 Announce Type: cross 
Abstract: Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
<link>https://arxiv.org/abs/2505.05736</link>
<guid>https://arxiv.org/abs/2505.05736</guid>
<content:encoded><![CDATA[
arXiv:2505.05736v1 Announce Type: cross 
Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition</title>
<link>https://arxiv.org/abs/2505.05768</link>
<guid>https://arxiv.org/abs/2505.05768</guid>
<content:encoded><![CDATA[
arXiv:2505.05768v1 Announce Type: cross 
Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment in diabetic patients. Treatment responses to intravitreal therapies vary, highlighting the need for patient stratification to predict therapeutic benefits and enable personalized strategies. To our knowledge, this study is the first to explore pre-treatment stratification for predicting DME treatment responses. To advance this research, we organized the 2nd Asia-Pacific Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The competition focused on improving predictive accuracy for anti-VEGF therapy responses using ophthalmic OCT images. We provided a dataset containing tens of thousands of OCT images from 2,000 patients with labels across four sub-tasks. This paper details the competition's structure, dataset, leading methods, and evaluation metrics. The competition attracted strong scientific community participation, with 170 teams initially registering and 41 reaching the final round. The top-performing team achieved an AUC of 80.06%, highlighting the potential of AI in personalized DME treatment and clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes</title>
<link>https://arxiv.org/abs/2505.05798</link>
<guid>https://arxiv.org/abs/2505.05798</guid>
<content:encoded><![CDATA[
arXiv:2505.05798v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using univariate spline compositions without nonlinear activations. In this work, we integrate Error-Correcting Output Codes (ECOC) into the KAN framework to transform multi-class classification into multiple binary tasks, improving robustness via Hamming-distance decoding. Our proposed KAN with ECOC method outperforms vanilla KAN on a challenging blood cell classification dataset, achieving higher accuracy under diverse hyperparameter settings. Ablation studies further confirm that ECOC consistently enhances performance across FastKAN and FasterKAN variants. These results demonstrate that ECOC integration significantly boosts KAN generalizability in critical healthcare AI applications. To the best of our knowledge, this is the first integration of ECOC with KAN for enhancing multi-class medical image classification performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks</title>
<link>https://arxiv.org/abs/2505.05800</link>
<guid>https://arxiv.org/abs/2505.05800</guid>
<content:encoded><![CDATA[
arXiv:2505.05800v1 Announce Type: cross 
Abstract: Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1$\%$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising</title>
<link>https://arxiv.org/abs/2505.05812</link>
<guid>https://arxiv.org/abs/2505.05812</guid>
<content:encoded><![CDATA[
arXiv:2505.05812v1 Announce Type: cross 
Abstract: Breast cancer is the most frequently diagnosed human cancer in the United States at present. Early detection is crucial for its successful treatment. X-ray mammography and digital breast tomosynthesis are currently the main methods for breast cancer screening. However, both have known limitations in terms of their sensitivity and specificity to breast cancers, while also frequently causing patient discomfort due to the requirement for breast compression. Breast computed tomography is a promising alternative, however, to obtain high-quality images, the X-ray dose needs to be sufficiently high. As the breast is highly radiosensitive, dose reduction is particularly important. Phase-contrast computed tomography (PCT) has been shown to produce higher-quality images at lower doses and has no need for breast compression. It is demonstrated in the present study that, when imaging full fresh mastectomy samples with PCT, deep learning-based image denoising can further reduce the radiation dose by a factor of 16 or more, without any loss of image quality. The image quality has been assessed both in terms of objective metrics, such as spatial resolution and contrast-to-noise ratio, as well as in an observer study by experienced medical imaging specialists and radiologists. This work was carried out in preparation for live patient PCT breast cancer imaging, initially at specialized synchrotron facilities.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints</title>
<link>https://arxiv.org/abs/2505.05957</link>
<guid>https://arxiv.org/abs/2505.05957</guid>
<content:encoded><![CDATA[
arXiv:2505.05957v1 Announce Type: cross 
Abstract: While classical convolutional neural networks (CNNs) have revolutionized image classification, the emergence of quantum computing presents new opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs) leverage quantum mechanical properties and hold potential to outperform classical approaches. However, their implementation on current noisy intermediate-scale quantum (NISQ) devices remains challenging due to hardware limitations. In our research, we address this challenge by introducing an encoding scheme that significantly reduces the input dimensionality. We demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to directly process $28\times 28$ pixel MNIST images, eliminating the need for classical dimensionality reduction pre-processing. Additionally, we propose an automated framework based on expressibility, entanglement, and complexity characteristics to identify the building blocks of QCNNs, parameterized quantum circuits (PQCs). Our approach demonstrates advantages in accuracy and convergence speed with a similar parameter count compared to both hybrid QCNNs and classical CNNs. We validated our experiments on IBM's Heron r2 quantum processor, achieving $96.08\%$ classification accuracy, surpassing the $71.74\%$ benchmark of traditional approaches under identical training conditions. These results represent one of the first implementations of image classifications on real quantum hardware and validate the potential of quantum computing in this area.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[
arXiv:2505.06020v1 Announce Type: cross 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects</title>
<link>https://arxiv.org/abs/2505.06030</link>
<guid>https://arxiv.org/abs/2505.06030</guid>
<content:encoded><![CDATA[
arXiv:2505.06030v1 Announce Type: cross 
Abstract: Combining natural language and geometric shapes is an emerging research area with multiple applications in robotics and language-assisted design. A crucial task in this domain is object referent identification, which involves selecting a 3D object given a textual description of the target. Variability in language descriptions and spatial relationships of 3D objects makes this a complex task, increasing the need to better understand the behavior of neural network models in this domain. However, limited research has been conducted in this area. Specifically, when a model makes an incorrect prediction despite being provided with a seemingly correct object description, practitioners are left wondering: "Why is the model wrong?". In this work, we present a method answering this question by generating counterfactual examples. Our method takes a misclassified sample, which includes two objects and a text description, and generates an alternative yet similar formulation that would have resulted in a correct prediction by the model. We have evaluated our approach with data from the ShapeTalk dataset along with three distinct models. Our counterfactual examples maintain the structure of the original description, are semantically similar and meaningful. They reveal weaknesses in the description, model bias and enhance the understanding of the models behavior. Theses insights help practitioners to better interact with systems as well as engineers to improve models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations</title>
<link>https://arxiv.org/abs/2505.06079</link>
<guid>https://arxiv.org/abs/2505.06079</guid>
<content:encoded><![CDATA[
arXiv:2505.06079v1 Announce Type: cross 
Abstract: Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: https://shuaiyihuang.github.io/publications/TREND.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram</title>
<link>https://arxiv.org/abs/2505.06105</link>
<guid>https://arxiv.org/abs/2505.06105</guid>
<content:encoded><![CDATA[
arXiv:2505.06105v1 Announce Type: cross 
Abstract: Echocardiogram is the most commonly used imaging modality in cardiac assessment duo to its non-invasive nature, real-time capability, and cost-effectiveness. Despite its advantages, most clinical echocardiograms provide only two-dimensional views, limiting the ability to fully assess cardiac anatomy and function in three dimensions. While three-dimensional echocardiography exists, it often suffers from reduced resolution, limited availability, and higher acquisition costs. To overcome these challenges, we propose a deep learning framework S2MNet that reconstructs continuous and high-fidelity 3D heart models by integrating six slices of routinely acquired 2D echocardiogram views. Our method has three advantages. First, our method avoid the difficulties on training data acquasition by simulate six of 2D echocardiogram images from corresponding slices of a given 3D heart mesh. Second, we introduce a deformation field-based method, which avoid spatial discontinuities or structural artifacts in 3D echocardiogram reconstructions. We validate our method using clinically collected echocardiogram and demonstrate that our estimated left ventricular volume, a key clinical indicator of cardiac function, is strongly correlated with the doctor measured GLPS, a clinical measurement that should demonstrate a negative correlation with LVE in medical theory. This association confirms the reliability of our proposed 3D construction method.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review</title>
<link>https://arxiv.org/abs/2505.06118</link>
<guid>https://arxiv.org/abs/2505.06118</guid>
<content:encoded><![CDATA[
arXiv:2505.06118v1 Announce Type: cross 
Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena</title>
<link>https://arxiv.org/abs/2505.06123</link>
<guid>https://arxiv.org/abs/2505.06123</guid>
<content:encoded><![CDATA[
arXiv:2505.06123v1 Announce Type: cross 
Abstract: Wasserstein distances provide a powerful framework for comparing data distributions. They can be used to analyze processes over time or to detect inhomogeneities within data. However, simply calculating the Wasserstein distance or analyzing the corresponding transport map (or coupling) may not be sufficient for understanding what factors contribute to a high or low Wasserstein distance. In this work, we propose a novel solution based on Explainable AI that allows us to efficiently and accurately attribute Wasserstein distances to various data components, including data subgroups, input features, or interpretable subspaces. Our method achieves high accuracy across diverse datasets and Wasserstein distance specifications, and its practical utility is demonstrated in two use cases.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills</title>
<link>https://arxiv.org/abs/2505.06176</link>
<guid>https://arxiv.org/abs/2505.06176</guid>
<content:encoded><![CDATA[
arXiv:2505.06176v1 Announce Type: cross 
Abstract: Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet</title>
<link>https://arxiv.org/abs/2505.06185</link>
<guid>https://arxiv.org/abs/2505.06185</guid>
<content:encoded><![CDATA[
arXiv:2505.06185v1 Announce Type: cross 
Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using transformers for classification and semantic segmentation. For spurious-correlation problems, this method allows us to enhance the image representation with two other image representations: representation obtained by semantic segmentation and representation obtained by image reconstruction. In our experiments, the proposed method outperformed in F-value measure than other classifiers when the test data included slices from the same patient (no covariate shift). Similarly, when the test data did not include slices from the same patient (covariate shift setting), the proposed method outperformed in AUC measure.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Concepts</title>
<link>https://arxiv.org/abs/2505.06191</link>
<guid>https://arxiv.org/abs/2505.06191</guid>
<content:encoded><![CDATA[
arXiv:2505.06191v1 Announce Type: cross 
Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation</title>
<link>https://arxiv.org/abs/2505.06210</link>
<guid>https://arxiv.org/abs/2505.06210</guid>
<content:encoded><![CDATA[
arXiv:2505.06210v1 Announce Type: cross 
Abstract: Convolutional neural network (CNN) and Transformer-based architectures are two dominant deep learning models for polyp segmentation. However, CNNs have limited capability for modeling long-range dependencies, while Transformers incur quadratic computational complexity. Recently, State Space Models such as Mamba have been recognized as a promising approach for polyp segmentation because they not only model long-range interactions effectively but also maintain linear computational complexity. However, Mamba-based architectures still struggle to capture topological features (e.g., connected components, loops, voids), leading to inaccurate boundary delineation and polyp segmentation. To address these limitations, we propose a new approach called Topo-VM-UNetV2, which encodes topological features into the Mamba-based state-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of two stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for the training and test images, which are then used to compute topology attention maps. Specifically, we first compute persistence diagrams of the PMs, then we generate persistence score maps by assigning persistence values (i.e., the difference between death and birth times) of each topological feature to its birth location, finally we transform persistence scores into attention weights using the sigmoid function. Stage 2: These topology attention maps are integrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to form a topology-guided semantics and detail infusion (Topo-SDI) module for enhancing the segmentation results. Extensive experiments on five public polyp segmentation datasets demonstrate the effectiveness of our proposed method. The code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Humanoids Hike! Integrative Skill Development on Complex Trails</title>
<link>https://arxiv.org/abs/2505.06218</link>
<guid>https://arxiv.org/abs/2505.06218</guid>
<content:encoded><![CDATA[
arXiv:2505.06218v1 Announce Type: cross 
Abstract: Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anymate: A Dataset and Baselines for Learning 3D Object Rigging</title>
<link>https://arxiv.org/abs/2505.06227</link>
<guid>https://arxiv.org/abs/2505.06227</guid>
<content:encoded><![CDATA[
arXiv:2505.06227v1 Announce Type: cross 
Abstract: Rigging and skinning are essential steps to create realistic 3D animations, often requiring significant expertise and manual effort. Traditional attempts at automating these processes rely heavily on geometric heuristics and often struggle with objects of complex geometry. Recent data-driven approaches show potential for better generality, but are often constrained by limited training data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information -- 70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction. We systematically design and experiment with various architectures as baselines for each module and conduct comprehensive evaluations on our dataset to compare their performance. Our models significantly outperform existing methods, providing a foundation for comparing future methods in automated rigging and skinning. Code and dataset can be found at https://anymate3d.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation</title>
<link>https://arxiv.org/abs/2303.17051</link>
<guid>https://arxiv.org/abs/2303.17051</guid>
<content:encoded><![CDATA[
arXiv:2303.17051v4 Announce Type: replace 
Abstract: The recent popularity of foundation models and the pre-train-and-adapt paradigm, where a large-scale model is transferred to downstream tasks, is gaining attention for volumetric medical image segmentation. However, current transfer learning strategies devoted to full fine-tuning for transfer learning may require significant resources and yield sub-optimal results when the labeled data of the target task is scarce. This makes its applicability in real clinical settings challenging since these institutions are usually constrained on data and computational resources to develop proprietary solutions. To address this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a novel and realistic scenario for adapting medical image segmentation foundation models. This setting considers the key role of both data- and parameter-efficiency during adaptation. Building on a foundation model pre-trained on open-access CT organ segmentation sources, we propose leveraging Parameter-Efficient Fine-Tuning and black-box Adapters to address such challenges. Furthermore, novel efficient adaptation methodologies are introduced in this work, which include Spatial black-box Adapters that are more appropriate for dense prediction tasks and constrained transductive inference, leveraging task-specific prior knowledge. Our comprehensive transfer learning experiments confirm the suitability of foundation models in medical image segmentation and unveil the limitations of popular fine-tuning strategies in few-shot scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2306.14070</link>
<guid>https://arxiv.org/abs/2306.14070</guid>
<content:encoded><![CDATA[
arXiv:2306.14070v2 Announce Type: replace 
Abstract: Super-resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench, the first benchmark dataset featuring high-resolution datasets, including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will help to advance SR methods for science.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image space formalism of convolutional neural networks for k-space interpolation</title>
<link>https://arxiv.org/abs/2402.17410</link>
<guid>https://arxiv.org/abs/2402.17410</guid>
<content:encoded><![CDATA[
arXiv:2402.17410v2 Announce Type: replace 
Abstract: Purpose: Noise resilience in image reconstructions by scan-specific robust artificial neural networks for k-space interpolation (RAKI) is linked to nonlinear activations in k-space. To gain a deeper understanding of this relationship, an image space formalism of RAKI is introduced for analyzing noise propagation analytically, identifying and characterizing image reconstruction features and to describe the role of nonlinear activations in a human readable manner. Methods: The image space formalism for RAKI inference is employed by expressing nonlinear activations in k-space as element-wise multiplications with activation masks, which transform into convolutions in image space. Jacobians of the de-aliased, coil-combined image relative to the aliased coil images can be expressed algebraically, and thus, the noise amplification is quantified analytically (g-factor maps). We analyze the role of nonlinearity for noise resilience by controlling the degree of nonlinearity in the reconstruction model via the negative slope parameter in leaky ReLU. Results: The analytical g-factor maps correspond with those obtained from Monte Carlo simulations and from an auto differentiation approach for in vivo brain images. Apparent blurring and contrast loss artifacts are identified as implications of enhanced noise resilience. These residual artifacts can be traded against noise resilience by adjusting the degree of nonlinearity in the model (Tikhonov-like regularization) in case of limited training data. The inspection of image space activations reveals an autocorrelation pattern leading to a potential center artifact. Conclusion: The image space formalism of RAKI provides the means for analytical quantitative noisepropagation analysis and human-readable visualization of the effects of the nonlinear activation functions in k-space.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations</title>
<link>https://arxiv.org/abs/2403.07887</link>
<guid>https://arxiv.org/abs/2403.07887</guid>
<content:encoded><![CDATA[
arXiv:2403.07887v4 Announce Type: replace 
Abstract: Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable concepts, ground them in our environment, and reason over these grounded entities. This trifecta of human thought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations extracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the Neural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is a nested schema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. Then, the NSI metric learns to ground primitives into slots through a structured contrastive learning objective that reasons over the intermodal alignment. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and interpretability of correspondences learned by NSI. From a scene representation standpoint, we find that emergent NSI slots that move beyond the image grid by binding to spatial objects facilitate improved visual grounding compared to conventional bounding-box-based approaches. From a data efficiency standpoint, we empirically validate that NSI learns more generalizable representations from a fixed amount of annotation data than the traditional approach. We also show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene complexity. Finally, we investigate the downstream efficacy of the grounded slots. Vision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on challenging few-shot classification tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model</title>
<link>https://arxiv.org/abs/2404.09957</link>
<guid>https://arxiv.org/abs/2404.09957</guid>
<content:encoded><![CDATA[
arXiv:2404.09957v3 Announce Type: replace 
Abstract: Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or "best-practice" guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer</title>
<link>https://arxiv.org/abs/2404.12734</link>
<guid>https://arxiv.org/abs/2404.12734</guid>
<content:encoded><![CDATA[
arXiv:2404.12734v4 Announce Type: replace 
Abstract: With the rapid development of OCR technology, mixed-scene text recognition has become a key technical challenge. Although deep learning models have achieved significant results in specific scenarios, their generality and stability still need improvement, and the high demand for computing resources affects flexibility. To address these issues, this paper proposes DLoRA-TrOCR, a parameter-efficient hybrid text spotting method based on a pre-trained OCR Transformer. By embedding a weight-decomposed DoRA module in the image encoder and a LoRA module in the text decoder, this method can be efficiently fine-tuned on various downstream tasks. Our method requires no more than 0.7\% trainable parameters, not only accelerating the training efficiency but also significantly improving the recognition accuracy and cross-dataset generalization performance of the OCR system in mixed text scenes. Experiments show that our proposed DLoRA-TrOCR outperforms other parameter-efficient fine-tuning methods in recognizing complex scenes with mixed handwritten, printed, and street text, achieving a CER of 4.02 on the IAM dataset, a F1 score of 94.29 on the SROIE dataset, and a WAR of 86.70 on the STR Benchmark, reaching state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</title>
<link>https://arxiv.org/abs/2407.06188</link>
<guid>https://arxiv.org/abs/2407.06188</guid>
<content:encoded><![CDATA[
arXiv:2407.06188v2 Announce Type: replace 
Abstract: While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning and Identity Adversarial Training for Facial Behavior Understanding</title>
<link>https://arxiv.org/abs/2407.11243</link>
<guid>https://arxiv.org/abs/2407.11243</guid>
<content:encoded><![CDATA[
arXiv:2407.11243v2 Announce Type: replace 
Abstract: Facial Action Unit (AU) detection has gained significant attention as it enables the breakdown of complex facial expressions into individual muscle movements. In this paper, we revisit two fundamental factors in AU detection: diverse and large-scale data and subject identity regularization. Motivated by recent advances in foundation models, we highlight the importance of data and introduce Face9M, a diverse dataset comprising 9 million facial images from multiple public sources. Pretraining a masked autoencoder on Face9M yields strong performance in AU detection and facial expression tasks. More importantly, we emphasize that the Identity Adversarial Training (IAT) has not been well explored in AU tasks. To fill this gap, we first show that subject identity in AU datasets creates shortcut learning for the model and leads to sub-optimal solutions to AU predictions. Secondly, we demonstrate that strong IAT regularization is necessary to learn identity-invariant features. Finally, we elucidate the design space of IAT and empirically show that IAT circumvents the identity-based shortcut learning and results in a better solution. Our proposed methods, Facial Masked Autoencoder (FMAE) and IAT, are simple, generic and effective. Remarkably, the proposed FMAE-IAT approach achieves new state-of-the-art F1 scores on BP4D (67.1\%), BP4D+ (66.8\%), and DISFA (70.1\%) databases, significantly outperforming previous work. We release the code and model at https://github.com/forever208/FMAE-IAT.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAPTRv2: Attention-based Position Update Improves Tracking Any Point</title>
<link>https://arxiv.org/abs/2407.16291</link>
<guid>https://arxiv.org/abs/2407.16291</guid>
<content:encoded><![CDATA[
arXiv:2407.16291v2 Announce Type: replace 
Abstract: In this paper, we present TAPTRv2, a Transformer-based approach built upon TAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from DEtection TRansformer (DETR) and formulates each tracking point as a point query, making it possible to leverage well-studied operations in DETR-like algorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its reliance on cost-volume,which contaminates the point query\'s content feature and negatively impacts both visibility prediction and cost-volume computation. In TAPTRv2, we propose a novel attention-based position update (APU) operation and use key-aware deformable attention to realize. For each query, this operation uses key-aware attention weights to combine their corresponding deformable sampling positions to predict a new query position. This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. By introducing this new operation, TAPTRv2 not only removes the extra burden of cost-volume computation, but also leads to a substantial performance improvement. TAPTRv2 surpasses TAPTR and achieves state-of-the-art performance on many challenging datasets, demonstrating the superiority
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention</title>
<link>https://arxiv.org/abs/2409.08840</link>
<guid>https://arxiv.org/abs/2409.08840</guid>
<content:encoded><![CDATA[
arXiv:2409.08840v3 Announce Type: replace 
Abstract: Collaborative perception (CP) leverages visual data from connected and autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV). Despite recent progress, current CP methods expand the ego vehicle's 360-degree perceptual range almost equally, which faces two key challenges. Firstly, in areas with uneven traffic distribution, focusing on directions with little traffic offers limited benefits. Secondly, under limited communication budgets, allocating excessive bandwidth to less critical directions lowers the perception accuracy in more vital areas. To address these issues, we propose Direct-CP, a proactive and direction-aware CP system aiming at improving CP in specific directions. Our key idea is to enable an ego vehicle to proactively signal its interested directions and readjust its attention to enhance local directional CP performance. To achieve this, we first propose an RSU-aided direction masking mechanism that assists an ego vehicle in identifying vital directions. Additionally, we design a direction-aware selective attention module to wisely aggregate pertinent features based on ego vehicle's directional priorities, communication budget, and the positional data of CAVs. Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture the divergence between directional CP outcomes and the ground truth, facilitating effective model training. Extensive experiments on the V2X-Sim 2.0 dataset demonstrate that our approach achieves 19.8\% higher local perception accuracy in interested directions and 2.5\% higher overall perception accuracy than the state-of-the-art methods in collaborative 3D object detection tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker</title>
<link>https://arxiv.org/abs/2410.01966</link>
<guid>https://arxiv.org/abs/2410.01966</guid>
<content:encoded><![CDATA[
arXiv:2410.01966v3 Announce Type: replace 
Abstract: Being able to accurately monitor the screen exposure of young children is important for research on phenomena linked to screen use such as childhood obesity, physical activity, and social interaction. Most existing studies rely upon self-report or manual measures from bulky wearable sensors, thus lacking efficiency and accuracy in capturing quantitative screen exposure data. In this work, we developed a novel sensor informatics framework that utilizes egocentric images from a wearable sensor, termed the screen time tracker (STT), and a vision language model (VLM). In particular, we devised a multi-view VLM that takes multiple views from egocentric image sequences and interprets screen exposure dynamically. We validated our approach by using a dataset of children's free-living activities, demonstrating significant improvement over existing methods in plain vision language models and object detection models. Results supported the promise of this monitoring approach, which could optimize behavioral research on screen exposure in children's naturalistic settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Egocentric and Exocentric Methods: A Short Survey</title>
<link>https://arxiv.org/abs/2410.20621</link>
<guid>https://arxiv.org/abs/2410.20621</guid>
<content:encoded><![CDATA[
arXiv:2410.20621v2 Announce Type: replace 
Abstract: Egocentric vision captures the scene from the point of view of the camera wearer, while exocentric vision captures the overall scene context. Jointly modeling ego and exo views is crucial to developing next-generation AI agents. The community has regained interest in the field of egocentric vision. While the third-person view and first-person have been thoroughly investigated, very few works aim to study both synchronously. Exocentric videos contain many relevant signals that are transferrable to egocentric videos. This paper provides a timely overview of works combining egocentric and exocentric visions, a very new but promising research topic. We describe in detail the datasets and present a survey of the key applications of ego-exo joint learning, where we identify the most recent advances. With the presentation of the current status of the progress, we believe this short but timely survey will be valuable to the broad video-understanding community, particularly when multi-view modeling is critical.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VladVA: Discriminative Fine-tuning of LVLMs</title>
<link>https://arxiv.org/abs/2412.04378</link>
<guid>https://arxiv.org/abs/2412.04378</guid>
<content:encoded><![CDATA[
arXiv:2412.04378v3 Announce Type: replace 
Abstract: Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown to be capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.
  In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.
  Our contributions include (1) a carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components; (2) a parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters; (3) significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</title>
<link>https://arxiv.org/abs/2412.14123</link>
<guid>https://arxiv.org/abs/2412.14123</guid>
<content:encoded><![CDATA[
arXiv:2412.14123v3 Announce Type: replace 
Abstract: Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and $11$ distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned or probed, we reach state-of-the-art results on the test sets of GeoPlex and for 6 external datasets across various environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, climate type classification, and segmentation of flood, burn scar, and deforestation. The code and models are available at https://github.com/gastruc/AnySat.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking</title>
<link>https://arxiv.org/abs/2412.20002</link>
<guid>https://arxiv.org/abs/2412.20002</guid>
<content:encoded><![CDATA[
arXiv:2412.20002v2 Announce Type: replace 
Abstract: Visual tracking has made significant strides due to the adoption of transformer-based models. Most state-of-the-art trackers struggle to meet real-time processing demands on mobile platforms with constrained computing resources, particularly for real-time unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we introduce AVTrack, an adaptive computation framework designed to selectively activate transformer blocks for real-time UAV tracking. The proposed Activation Module (AM) dynamically optimizes the ViT architecture by selectively engaging relevant components, thereby enhancing inference efficiency without significant compromise to tracking performance. Furthermore, to tackle the challenges posed by extreme changes in viewing angles often encountered in UAV tracking, the proposed method enhances ViTs' effectiveness by learning view-invariant representations through mutual information (MI) maximization. Two effective design principles are proposed in the AVTrack. Building on it, we propose an improved tracker, dubbed AVTrack-MD, which introduces the novel MI maximization-based multi-teacher knowledge distillation (MD) framework. It harnesses the benefits of multiple teachers, specifically the off-the-shelf tracking models from the AVTrack, by integrating and refining their outputs, thereby guiding the learning process of the compact student network. Specifically, we maximize the MI between the softened feature representations from the multi-teacher models and the student model, leading to improved generalization and performance of the student model, particularly in noisy conditions. Extensive experiments on multiple UAV tracking benchmarks demonstrate that AVTrack-MD not only achieves performance comparable to the AVTrack baseline but also reduces model complexity, resulting in a significant 17\% increase in average tracking speed.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Class Discovery in Instance Segmentation</title>
<link>https://arxiv.org/abs/2502.08149</link>
<guid>https://arxiv.org/abs/2502.08149</guid>
<content:encoded><![CDATA[
arXiv:2502.08149v2 Announce Type: replace 
Abstract: This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Pretraining for Fine-Grained Plankton Recognition</title>
<link>https://arxiv.org/abs/2503.11341</link>
<guid>https://arxiv.org/abs/2503.11341</guid>
<content:encoded><![CDATA[
arXiv:2503.11341v2 Announce Type: replace 
Abstract: Plankton recognition is an important computer vision problem due to plankton's essential role in ocean food webs and carbon capture, highlighting the need for species-level monitoring. However, this task is challenging due to its fine-grained nature and dataset shifts caused by different imaging instruments and varying species distributions. As new plankton image datasets are collected at an increasing pace, there is a need for general plankton recognition models that require minimal expert effort for data labeling. In this work, we study large-scale self-supervised pretraining for fine-grained plankton recognition. We first employ masked autoencoding and a large volume of diverse plankton image data to pretrain a general-purpose plankton image encoder. Then we utilize fine-tuning to obtain accurate plankton recognition models for new datasets with a very limited number of labeled training images. Our experiments show that self-supervised pretraining with diverse plankton data clearly increases plankton recognition accuracy compared to standard ImageNet pretraining when the amount of training data is limited. Moreover, the accuracy can be further improved when unlabeled target data is available and utilized during the pretraining.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion</title>
<link>https://arxiv.org/abs/2503.20698</link>
<guid>https://arxiv.org/abs/2503.20698</guid>
<content:encoded><![CDATA[
arXiv:2503.20698v4 Announce Type: replace 
Abstract: Videos inherently contain multiple modalities, including visual events, text overlays, sounds, and speech, all of which are important for retrieval. However, state-of-the-art multimodal language models like VAST and LanguageBind are built on vision-language models (VLMs), and thus overly prioritize visual signals. Retrieval benchmarks further reinforce this bias by focusing on visual queries and neglecting other modalities. We create a search system MMMORRF that extracts text and features from both visual and audio modalities and integrates them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is both effective and efficient, demonstrating practicality in searching videos based on users' information needs instead of visual descriptive queries. We evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed for more targeted information needs, and find that it improves nDCG@20 by 81% over leading multimodal encoders and 37% over single-modality retrieval, demonstrating the value of integrating diverse modalities.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</title>
<link>https://arxiv.org/abs/2503.22976</link>
<guid>https://arxiv.org/abs/2503.22976</guid>
<content:encoded><![CDATA[
arXiv:2503.22976v3 Announce Type: replace 
Abstract: Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models For Seismic Data Processing: An Extensive Review</title>
<link>https://arxiv.org/abs/2503.24166</link>
<guid>https://arxiv.org/abs/2503.24166</guid>
<content:encoded><![CDATA[
arXiv:2503.24166v2 Announce Type: replace 
Abstract: Seismic processing plays a crucial role in transforming raw data into high-quality subsurface images, pivotal for various geoscience applications. Despite its importance, traditional seismic processing techniques face challenges such as noisy and damaged data and the reliance on manual, time-consuming workflows. The emergence of deep learning approaches has introduced effective and user-friendly alternatives, yet many of these deep learning approaches rely on synthetic datasets and specialized neural networks. Recently, foundation models have gained traction in the seismic domain, due to their success in the natural image domain. Therefore, we investigate the application of natural image foundation models on the three seismic processing tasks: demultiple, interpolation, and denoising. We evaluate the impact of different model characteristics, such as pre-training technique and neural network architecture, on performance and efficiency. Rather than proposing a single seismic foundation model, we critically examine various natural image foundation models and suggest some promising candidates for future exploration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualization of a multidimensional point cloud as a 3D swarm of avatars</title>
<link>https://arxiv.org/abs/2504.06751</link>
<guid>https://arxiv.org/abs/2504.06751</guid>
<content:encoded><![CDATA[
arXiv:2504.06751v2 Announce Type: replace 
Abstract: The article presents an innovative approach to the visualization of multidimensional data, using icons inspired by Chernoff faces. The approach merges classical projection techniques with the assignment of particular data dimensions to mimic features, capitalizing on the natural ability of the human brain to interpret facial expressions. We introduce a semantic division of data dimensions into intuitive and technical categories, assigning the former to avatar features and projecting the latter into a hyperspace of four, or potentially more dimensions. The technique is implemented as a plugin to the dpVision open-source image handling platform. The plugin allows the data to be interactively explored in the form of a swarm of avatars whose position in hyperspace as well as facial features represent various aspects of the data. Sample visualizations, based on synthetic test data as well as the 12-dimensional database on Portuguese Vinho Verde wines, confirm the usefulness of our approach to the analysis of complex data structures.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery</title>
<link>https://arxiv.org/abs/2504.08049</link>
<guid>https://arxiv.org/abs/2504.08049</guid>
<content:encoded><![CDATA[
arXiv:2504.08049v2 Announce Type: replace 
Abstract: This work presents a new approach to anomaly detection and localization in synthetic aperture radar imagery (SAR), expanding upon the existing patch distribution modeling framework (PaDiM). We introduce the adaptive cosine estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at inference, an unbounded metric. ACE instead uses the cosine similarity metric, providing bounded anomaly detection scores. The proposed method is evaluated across multiple SAR datasets, with performance metrics including the area under the receiver operating curve (AUROC) at the image and pixel level, aiming for increased performance in anomaly detection and localization of SAR imagery. The code is publicly available: https://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.13580</link>
<guid>https://arxiv.org/abs/2504.13580</guid>
<content:encoded><![CDATA[
arXiv:2504.13580v2 Announce Type: replace 
Abstract: High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Perception-Inspired Grain Segmentation Refinement Using Conditional Random Fields</title>
<link>https://arxiv.org/abs/2312.09968</link>
<guid>https://arxiv.org/abs/2312.09968</guid>
<content:encoded><![CDATA[
arXiv:2312.09968v2 Announce Type: replace-cross 
Abstract: Automated detection of grain boundaries in electron microscope images of polycrystalline materials could help accelerate the nanoscale characterization of myriad engineering materials and novel materials under scientific research. Accurate segmentation of interconnected line networks, such as grain boundaries in polycrystalline material microstructures, poses a significant challenge due to the fragmented masks produced by conventional computer vision algorithms, including convolutional neural networks. These algorithms struggle with thin masks, often necessitating post-processing for effective contour closure and continuity. Previous approaches in this domain have typically relied on custom post-processing techniques that are problem-specific and heavily dependent on the quality of the mask obtained from a computer vision algorithm. Addressing this issue, this paper introduces a fast, high-fidelity post-processing technique that is universally applicable to segmentation masks of interconnected line networks. Leveraging domain knowledge about grain boundary connectivity, this method employs conditional random fields and perceptual grouping rules to refine segmentation masks of any image with a discernible grain structure. This approach significantly enhances segmentation mask accuracy, achieving a 79% segment identification accuracy in validation with a U-Net model on electron microscopy images of a polycrystalline oxide. Additionally, a novel grain alignment metric is introduced, showing a 51% improvement in grain alignment. This method not only enables rapid and accurate segmentation but also facilitates an unprecedented level of data analysis, significantly improving the statistical representation of grain boundary networks, making it suitable for a range of disciplines where precise segmentation of interconnected line networks is essential.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer</title>
<link>https://arxiv.org/abs/2408.08456</link>
<guid>https://arxiv.org/abs/2408.08456</guid>
<content:encoded><![CDATA[
arXiv:2408.08456v2 Announce Type: replace-cross 
Abstract: Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect the prediction results of machine learning models. However, current methods have limitations in detecting drift, for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a pre-trained Vision Transformer model to extract relevant features, using mammography as a case study, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 99.1%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction</title>
<link>https://arxiv.org/abs/2502.04521</link>
<guid>https://arxiv.org/abs/2502.04521</guid>
<content:encoded><![CDATA[
arXiv:2502.04521v2 Announce Type: replace-cross 
Abstract: Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL aggregates locally trained model weights into a global model, inherently constraining all sites to use a homogeneous model architecture. This rigidity forces sites to compromise on architectures tailored to their compute resources and application-specific needs, making conventional FL unsuitable for model-heterogeneous settings where each site may prefer a distinct architecture. To overcome this limitation, we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that learns the distribution of multi-site MR images. For high-fidelity synthesis, we propose a novel site-prompted GAT prior that controllably synthesizes realistic MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its own reconstruction model -- using an architecture of its choice -- on a hybrid dataset augmenting its local MRI dataset with GAT-generated synthetic MR images emulating datasets from other sites. This hybrid training strategy enables site-specific reconstruction models to generalize more effectively across diverse data distributions while preserving data privacy. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT enables flexible, model-heterogeneous collaborations and achieves superior within-site and cross-site reconstruction performance compared to state-of-the-art FL baselines.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-preserving contrastive learning for spatial time series</title>
<link>https://arxiv.org/abs/2502.06380</link>
<guid>https://arxiv.org/abs/2502.06380</guid>
<content:encoded><![CDATA[
arXiv:2502.06380v3 Announce Type: replace-cross 
Abstract: The effectiveness of neural network models largely relies on learning meaningful latent patterns from data, where self-supervised learning of informative representations can enhance model performance and generalisability. However, self-supervised representation learning for spatially characterised time series, which are ubiquitous in transportation domain, poses unique challenges due to the necessity of maintaining fine-grained spatio-temporal similarities in the latent space. In this study, we introduce two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance the contrastive learning objective and the need for structure preservation, we propose a dynamic weighting mechanism that adaptively manages this trade-off and stabilises training. We validate the proposed method through extensive experiments, including multivariate time series classification to demonstrate its general applicability, as well as macroscopic and microscopic traffic prediction to highlight its particular usefulness in encoding traffic interactions. Across all tasks, our method preserves the similarity structures more effectively and improves state-of-the-art task performances. This method can be integrated with an arbitrary neural network model and is particularly beneficial for time series data with spatial or geographical features. Furthermore, our findings suggest that well-preserved similarity structures in the latent space indicate more informative and useful representations. This provides insights to design more effective neural networks for data-driven transportation research. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RS2AD: End-to-End Autonomous Driving Data Generation from Roadside Sensor Observations</title>
<link>https://arxiv.org/abs/2503.07085</link>
<guid>https://arxiv.org/abs/2503.07085</guid>
<content:encoded><![CDATA[
arXiv:2503.07085v3 Announce Type: replace-cross 
Abstract: End-to-end autonomous driving solutions, which process multi-modal sensory data to directly generate refined control commands, have become a dominant paradigm in autonomous driving research. However, these approaches predominantly depend on single-vehicle data collection for model training and optimization, resulting in significant challenges such as high data acquisition and annotation costs, the scarcity of critical driving scenarios, and fragmented datasets that impede model generalization. To mitigate these limitations, we introduce RS2AD, a novel framework for reconstructing and synthesizing vehicle-mounted LiDAR data from roadside sensor observations. Specifically, our method transforms roadside LiDAR point clouds into the vehicle-mounted LiDAR coordinate system by leveraging the target vehicle's relative pose. Subsequently, high-fidelity vehicle-mounted LiDAR data is synthesized through virtual LiDAR modeling, point cloud classification, and resampling techniques. To the best of our knowledge, this is the first approach to reconstruct vehicle-mounted LiDAR data from roadside sensor inputs. Extensive experimental evaluations demonstrate that incorporating the data generated by the RS2AD method (the RS2V-L dataset) into model training as a supplement to the KITTI dataset can significantly enhance the accuracy of 3D object detection and greatly improve the efficiency of end-to-end autonomous driving data generation. These findings strongly validate the effectiveness of the proposed method and underscore its potential in reducing dependence on costly vehicle-mounted data collection while improving the robustness of autonomous driving models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage</title>
<link>https://arxiv.org/abs/2504.20007</link>
<guid>https://arxiv.org/abs/2504.20007</guid>
<content:encoded><![CDATA[
arXiv:2504.20007v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating video, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. We present our methodology, computational techniques, and findings, outlining a practical approach for law enforcement while advancing the frontiers of knowledge discovery from police BWC data.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&amp;E Whole Slide Images of Cutaneous Squamous Cell Carcinoma</title>
<link>https://arxiv.org/abs/2505.04672</link>
<guid>https://arxiv.org/abs/2505.04672</guid>
<content:encoded><![CDATA[
<div> Keywords: digital pathology, Whole-Slide Images, cutaneous squamous cell carcinoma, Histo-Miner, tumor segmentation  

<br /><br />Summary: Recent advancements in digital pathology facilitate the comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, but there is a scarcity of labeled datasets and open-source pipelines specific to skin tissue analysis. To address this gap, we introduce Histo-Miner, a deep learning-based pipeline specifically designed for analyzing skin WSIs, particularly focusing on patients with cutaneous squamous cell carcinoma (cSCC), a prevalent form of non-melanoma skin cancer. We created two datasets featuring 47,392 annotated cell nuclei and 144 tumor-segmented WSIs from cSCC patients. Histo-Miner utilizes convolutional neural networks and vision transformers for nucleus segmentation, classification, and tumor region segmentation, achieving competitive performance metrics. The results include a multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, a macro-averaged F1 score of 0.832 for nucleus classification, and a mean Intersection over Union (mIoU) of 0.884 for tumor segmentation. Furthermore, Histo-Miner forecasts patient responses to immunotherapy using WSI analysis, identifying key immune features that predict therapy outcomes, thus demonstrating its clinical relevance and interpretative power for underlying biological insights. <div>
arXiv:2505.04672v1 Announce Type: new 
Abstract: Recent advancements in digital pathology have enabled comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution microscopy and computational capabilities. Despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based pipeline for analysis of skin WSIs and generate two datasets with labeled nuclei and tumor regions. We develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients, Histo-Miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. Performance of trained models positively compares to state of the art with multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, macro-averaged F1 of 0.832 for nucleus classification and mean Intersection over Union (mIoU) of 0.884 for tumor region segmentation. From these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. Here, we use Histo-Miner to predict cSCC patient response to immunotherapy based on pre-treatment WSIs from 45 patients. Histo-Miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. This highlights the applicability of Histo-Miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Visual Trackers for Biomechanical Analysis of Running</title>
<link>https://arxiv.org/abs/2505.04713</link>
<guid>https://arxiv.org/abs/2505.04713</guid>
<content:encoded><![CDATA[
<div> Keywords: human pose estimation, deep learning, biomechanical analysis, joint trackers, root mean squared errors 

<br /><br />Summary: This work explores the advancements in human pose estimation, particularly in the context of biomechanics for sprinting. It evaluates the performance of six different trackers, including two point trackers and four joint trackers, by comparing their outputs against manual annotations from biomechanical experts across 5870 frames. The study focuses on forty sprints from five professional runners and emphasizes three crucial angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. To enhance accuracy, a post-processing module is introduced for outlier detection and fusion prediction of joint angles. The results indicate that the joint-based models achieved root mean squared errors ranging from 11.41 to 4.37. When utilizing the post-processing modules, accuracy improves further, with errors reducing to 6.99 and 3.88, respectively. The findings suggest that human pose tracking is a promising tool for biomechanical analysis in running. Still, the study identifies opportunities for improvement, particularly for applications demanding high precision. This research highlights the potential of integrating advanced pose estimation techniques into sports analysis and performance evaluation. <div>
arXiv:2505.04713v1 Announce Type: new 
Abstract: Human pose estimation has witnessed significant advancements in recent years, mainly due to the integration of deep learning models, the availability of a vast amount of data, and large computational resources. These developments have led to highly accurate body tracking systems, which have direct applications in sports analysis and performance evaluation.
  This work analyzes the performance of six trackers: two point trackers and four joint trackers for biomechanical analysis in sprints. The proposed framework compares the results obtained from these pose trackers with the manual annotations of biomechanical experts for more than 5870 frames. The experimental framework employs forty sprints from five professional runners, focusing on three key angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. We propose a post-processing module for outlier detection and fusion prediction in the joint angles.
  The experimental results demonstrate that using joint-based models yields root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. When integrated with the post-processing modules, these errors can be reduced to 6.99{\deg} and 3.88{\deg}, respectively. The experimental findings suggest that human pose tracking approaches can be valuable resources for the biomechanical analysis of running. However, there is still room for improvement in applications where high accuracy is required.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.04718</link>
<guid>https://arxiv.org/abs/2505.04718</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-layout, open-vocabulary, diffusion Transformer, scene generation, image editing  

<br /><br />Summary: Lay-Your-Scene (LayouSyn) is a novel pipeline designed for generating layouts of natural scenes from text prompts. The primary limitation of previous scene layout generation methods is their reliance on closed-vocabulary approaches or proprietary large language models, which hampers their flexibility and applicability in generating controllable images. LayouSyn addresses this by utilizing lightweight, open-source language models to extract scene elements from the prompts and implementing a new aspect-aware diffusion Transformer architecture for layout generation in an open-vocabulary format. Extensive experiments have shown that LayouSyn outperforms existing methodologies, achieving state-of-the-art results on complex spatial and numerical reasoning benchmarks. The article also presents two key applications for LayouSyn: Firstly, the method can integrate coarse initializations from larger language models, leading to improved outcomes. Secondly, it introduces a pipeline for seamlessly adding objects to existing images, underscoring LayouSyn's potential in image editing tasks. This combination of open-vocabulary capabilities and innovative architecture positions LayouSyn as a versatile tool in text-to-layout generation and editing applications. <div>
arXiv:2505.04718v1 Announce Type: new 
Abstract: We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims</title>
<link>https://arxiv.org/abs/2505.04720</link>
<guid>https://arxiv.org/abs/2505.04720</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, AI, performance comparisons, false claims, Bayesian approach

<br /><br />Summary: This paper investigates the reliability of performance claims in medical imaging AI research, emphasizing that many studies assert superiority based on empirical mean performance without adequate validation. Analyzing a representative cohort of medical imaging papers, the authors apply a Bayesian framework to assess the probability of false claims regarding outperformance. They find that over 80% of studies claim to outperform existing methods, with a particularly high likelihood of false claimsover 5%in 86% of classification papers and 53% of segmentation papers. This suggests that the majority of claims about new methods exceeding the state of the art may be unwarranted and often result purely from chance rather than genuine improvements. This investigation sheds light on a significant flaw in current benchmarking practices within the field, indicating that many assertions of advancement could mislead researchers and misdirect future investigations. Overall, the results call for more rigorous validation and transparent reporting to enhance the credibility of performance comparisons in medical imaging AI. <div>
arXiv:2505.04720v1 Announce Type: new 
Abstract: Performance comparisons are fundamental in medical imaging Artificial Intelligence (AI) research, often driving claims of superiority based on relative improvements in common performance metrics. However, such claims frequently rely solely on empirical mean performance. In this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. We quantify the probability of false claims based on a Bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. According to our results, the majority (>80%) of papers claims outperformance when introducing a new method. Our analysis further revealed a high probability (>5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. These findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging AI are frequently unsubstantiated, posing a risk of misdirecting future research efforts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer</title>
<link>https://arxiv.org/abs/2505.04740</link>
<guid>https://arxiv.org/abs/2505.04740</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Layer Perceptrons, Vision Transformers, Wavelet Functions, Hybrid Kolmogorov-Arnold Network, ImageNet-1K

<br /><br />Summary: This article presents a new framework called Hybrid Kolmogorov-Arnold Network-Vision Transformer (Hyb-KAN ViT) aimed at addressing the limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs). It highlights the importance of leveraging the prebuilt modularity of the ViT architecture and integrating edge detection capabilities through wavelet functions. The framework proposes two innovative modules: Efficient-KAN (Eff-KAN), which substitutes MLP layers with spline functions, and Wavelet-KAN (Wav-KAN), which utilizes orthogonal wavelet transforms for multi-resolution feature extraction. These modules are effectively incorporated into ViT encoder layers and classification heads to improve spatial-frequency modeling while reducing computational constraints. Experimental evaluations on datasets such as ImageNet-1K, COCO, and ADE20K indicate that Hyb-KAN ViT achieves state-of-the-art performance across various tasks, including image recognition, object detection, and semantic segmentation. Additionally, ablation studies confirm the effectiveness of wavelet-driven spectral priors in enhancing segmentation and the efficiency of spline functions for detection tasks. Overall, the proposed framework establishes a new standard for optimizing parameter efficiency while facilitating multi-scale representation in vision architectures. <div>
arXiv:2505.04740v1 Announce Type: new 
Abstract: This study addresses the inherent limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold Network (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates wavelet-based spectral decomposition and spline-optimized activation functions, prior work has failed to focus on the prebuilt modularity of the ViT architecture and integration of edge detection capabilities of Wavelet functions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces MLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging orthogonal wavelet transforms for multi-resolution feature extraction. These modules are systematically integrated in ViT encoder layers and classification heads to enhance spatial-frequency modeling while mitigating computational bottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object Detection and Instance Segmentation), and ADE20K (Semantic Segmentation) demonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies validate the efficacy of wavelet-driven spectral priors in segmentation and spline-based efficiency in detection tasks. The framework establishes a new paradigm for balancing parameter efficiency and multi-scale representation in vision architectures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective</title>
<link>https://arxiv.org/abs/2505.04758</link>
<guid>https://arxiv.org/abs/2505.04758</guid>
<content:encoded><![CDATA[
<div> Keywords: RGB-D, SATNet, lightweight, depth quality, feature representation

<br /><br />Summary: The authors propose a novel Speed-Accuracy Tradeoff Network (SATNet) specifically designed for Lightweight RGB-D Salient Object Detection (SOD). Recognizing the typical trade-offs between accuracy and efficiency in existing models, SATNet addresses three core areas: depth quality, modality fusion, and feature representation. First, to improve depth quality, a Depth Anything Model is introduced to generate high-quality depth maps, which helps bridge the gaps present in current datasets. Regarding modality fusion, the Decoupled Attention Module (DAM) is developed to enhance the consistency of features within and between different modalities, effectively separating them into dual-view feature vectors to boost discriminative capabilities. For feature representation, a Dual Information Representation Module (DIRM) employs a bi-directional inverted framework to expand the feature space generated by lightweight backbones, capturing both texture and saliency features. Additionally, the model incorporates two-way prediction heads to optimize parameters through bi-directional backpropagation. Finally, a Dual Feature Aggregation Module (DFAM) is designed for effective integration of texture and saliency features in the decoder. Experimental results demonstrate that SATNet outperforms state-of-the-art heavyweight models while maintaining a lightweight structure with only 5.2 million parameters and 415 frames per second (FPS). <div>
arXiv:2505.04758v1 Announce Type: new 
Abstract: Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. For modality fusion, we propose a Decoupled Attention Module (DAM) to explore the consistency within and between modalities. Here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. For feature representation, we develop a Dual Information Representation Module (DIRM) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. DIRM models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. Finally, we design a Dual Feature Aggregation Module (DFAM) in the decoder to aggregate texture and saliency features. Extensive experiments on five public RGB-D SOD datasets indicate that the proposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and achieves a lightweight framework with 5.2 M parameters and 415 FPS.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</title>
<link>https://arxiv.org/abs/2505.04769</link>
<guid>https://arxiv.org/abs/2505.04769</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, Agentic AI, AI Agents, Vision-language Models, robotics  

<br /><br />Summary: This foundational review synthesizes recent advancements in Vision-Language-Action (VLA) models, emphasizing their role in unifying perception, natural language understanding, and embodied actions. It begins by establishing the conceptual foundations of VLA systems, tracing their development from cross-modal learning to integrated systems combining vision-language models, action planners, and hierarchical controllers. The methodology employs a robust literature review of over 80 VLA models published in the last three years. Key advancements are highlighted in architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. Various application domains are discussed, including humanoid robotics, autonomous vehicles, medical robotics, precision agriculture, and augmented reality navigation. The review also confronts major challenges such as real-time control, multimodal action representation, and generalization to new tasks, while assessing ethical deployment risks. Proposed solutions involve agentic AI adaptation and cross-embodiment generalization. Finally, the article outlines a forward-looking roadmap for the convergence of VLA models and agentic AI, aiming to create socially aligned, adaptive, and general-purpose embodied agents. This work serves as a crucial reference for advancing intelligent robotics and artificial general intelligence. <div>
arXiv:2505.04769v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay</title>
<link>https://arxiv.org/abs/2505.04787</link>
<guid>https://arxiv.org/abs/2505.04787</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Learning, Catastrophic Forgetting, generative replay, uncertainty-driven, synthetic labeled data

<br /><br />Summary: The article introduces a new framework for Continual Learning, termed "Replay to Remember (R2R)," aimed at addressing the challenge of Catastrophic Forgetting in neural networks. R2R utilizes a novel uncertainty-driven approach combined with Generative Replay to effectively assimilate new knowledge while preserving previous learning. The framework distinguishes itself by operating without the need for pre-trained models and pseudo-labels, relying instead on visual features extracted from unlabeled data. It employs a cluster-level uncertainty feedback mechanism, enhanced by dynamic thresholding, to adapt and improve continuously. Additionally, the architecture incorporates a generative replay mechanism, powered by a DeepSeek-R1 and CLIP VLM combination, to synthesize labeled data that reflects past experiences. This method mimics biological visual cognition, facilitating memory replay for enhanced performance in novel tasks. Extensive experiments conducted across benchmarks such as CIFAR-10, CIFAR-100, CINIC-10, SVHN, and TinyImageNet demonstrate the effectiveness of R2R, achieving unprecedented knowledge retention rates of 98.13%, 73.06%, 93.41%, 95.18%, and 59.74%, respectively, surpassing prior state-of-the-art results by over 4.36%. <div>
arXiv:2505.04787v1 Announce Type: new 
Abstract: Continual Learning entails progressively acquiring knowledge from new data while retaining previously acquired knowledge, thereby mitigating ``Catastrophic Forgetting'' in neural networks. Our work presents a novel uncertainty-driven Unsupervised Continual Learning framework using Generative Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture efficiently uses unlabelled and synthetic labelled data in a balanced proportion using a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module. Unlike traditional memory-buffer methods that depend on pretrained models and pseudo-labels, our R2R framework operates without any prior training. It leverages visual features from unlabeled data and adapts continuously using clustering-based uncertainty estimation coupled with dynamic thresholding. Concurrently, a generative replay mechanism along with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data representative of past experiences, resembling biological visual thinking that replays memory to remember and act in new, unseen tasks. Extensive experimental analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and TinyImageNet datasets. Our proposed R2R approach improves knowledge retention, achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%, 59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World</title>
<link>https://arxiv.org/abs/2505.04788</link>
<guid>https://arxiv.org/abs/2505.04788</guid>
<content:encoded><![CDATA[
<div> Keywords: vanishing points, Manhattan world, convex relaxation, GlobustVP, semidefinite programming  

<br /><br />Summary: The paper addresses the task of determining vanishing points (VPs) in a Manhattan world, which is crucial for various 3D vision applications. It highlights the limitations of existing methods, which are either sub-optimal or computationally intensive in pursuit of global optimality. To overcome these drawbacks, the authors introduce convex relaxation techniques to jointly infer line-VP associations and VP locations. They propose a "soft" association scheme using a truncated multi-selection error, leading to a primal problem reformulated into a quadratically constrained quadratic programming (QCQP) problem. This problem is further relaxed into a convex semidefinite programming (SDP) problem. The key innovation is the development of a globally optimal outlier-robust iterative solver called GlobustVP, which updates one VP at a time while treating others as outliers. This iterative process is complemented by a local refinement that enforces the mutual orthogonality of the three VPs inherent in a Manhattan world. Comprehensive experiments on synthetic and real-world data validate that GlobustVP strikes a commendable balance between efficiency, robustness, and global optimality in comparison to existing methods. The code for this approach is publicly available. <div>
arXiv:2505.04788v1 Announce Type: new 
Abstract: Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a ``soft'' association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs' locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called \textbf{GlobustVP}), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that \textbf{GlobustVP} achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. The code is publicly available at https://github.com/WU-CVGL/GlobustVP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition</title>
<link>https://arxiv.org/abs/2505.04793</link>
<guid>https://arxiv.org/abs/2505.04793</guid>
<content:encoded><![CDATA[
<div> Keywords: Person Reidentification, DetReIDX, dataset, aerial-ground, real-world conditions

<br /><br />Summary: Person reidentification (ReID) technology has shown promise in controlled, ground-level environments but struggles in real-world scenarios due to variability in data such as resolution, viewpoint changes, and occlusions. Existing public datasets do not adequately represent these conditions, hindering progress in the field. In response, this paper introduces DetReIDX, a large-scale aerial-ground person dataset designed to stress-test ReID under realistic circumstances. Comprising over 13 million bounding boxes from 509 individuals, the dataset was collected across seven university campuses spanning three continents, at drone altitudes between 5.8 and 120 meters. A key feature of DetReIDX is that subjects were recorded over multiple sessions on different days, capturing variations in clothing, daylight, and locations, making it suitable for evaluating long-term ReID. Additionally, the dataset includes annotations for 16 soft biometric attributes and multitask labels, encompassing human detection, tracking, ReID, and action recognition. Empirical evidence demonstrates that state-of-the-art methods experience significant performance degradationup to 80% in detection accuracy and over 70% in Rank-1 ReIDwhen tested with DetReIDX, underscoring the dataset's relevance. The dataset and evaluation protocols are available at https://www.it.ubi.pt/DetReIDX/. <div>
arXiv:2505.04793v1 Announce Type: new 
Abstract: Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?</title>
<link>https://arxiv.org/abs/2505.04835</link>
<guid>https://arxiv.org/abs/2505.04835</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, robustness, synthetic corruptions, semantic segmentation, benchmarking

<br /><br />Summary: Deep learning (DL) models are widely utilized in practical applications but show susceptibility to distribution shifts, particularly due to variations in weather and lighting. Collecting diverse real-world data for testing DL models' robustness can be resource-intensive. Therefore, synthetic corruptions serve as an appealing alternative for evaluating robustness. This study aims to determine whether synthetic corruptions are reliable proxies for real-world corruptions. We conduct the largest benchmarking study focused on semantic segmentation models, comparing their performance on datasets with real-world and synthetic corruptions. Our findings demonstrate a strong correlation in mean performance across both types of corruptions, suggesting that synthetic corruptions can be effectively used for robustness evaluation. Additionally, we examine corruption-specific correlations, which provide important insights into the circumstances under which synthetic corruptions accurately represent real-world scenarios. Overall, our research supports the viability of synthetic corruptions for testing the resilience of DL models against various adverse conditions, paving the way for enhanced model evaluation techniques. The study includes open-source code to facilitate further exploration in this area, available at the provided GitHub link. <div>
arXiv:2505.04835v1 Announce Type: new 
Abstract: Deep learning (DL) models are widely used in real-world applications but remain vulnerable to distribution shifts, especially due to weather and lighting changes. Collecting diverse real-world data for testing the robustness of DL models is resource-intensive, making synthetic corruptions an attractive alternative for robustness testing. However, are synthetic corruptions a reliable proxy for real-world corruptions? To answer this, we conduct the largest benchmarking study on semantic segmentation models, comparing performance on real-world corruptions and synthetic corruptions datasets. Our results reveal a strong correlation in mean performance, supporting the use of synthetic corruptions for robustness evaluation. We further analyze corruption-specific correlations, providing key insights to understand when synthetic corruptions succeed in representing real-world corruptions. Open-source Code: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Cells Clearly: Evaluating Machine Vision Strategies for Microglia Centroid Detection in 3D Images</title>
<link>https://arxiv.org/abs/2505.04838</link>
<guid>https://arxiv.org/abs/2505.04838</guid>
<content:encoded><![CDATA[
<div> Keywords: microglia, 3D images, ilastik, 3D Morph, Omnipose<br /><br />Summary: Microglia are crucial cells in the brain, and their morphology can provide insights into brain health. In this study, the author evaluates three different tools designed to identify the center points of microglia in 3D microscope images. The tools assessed include ilastik, 3D Morph, and Omnipose, each with unique methodologies for cell detection. The project aims to determine the effectiveness of each tool in accurately locating microglial cells and to compare the outcomes produced by these different software. Through the analysis, it becomes evident that each tool perceives the cells in distinct ways, leading to variations in the information extracted from the images. This highlights the importance of choosing the appropriate tool for specific research needs in neuroimaging, as the results can significantly influence our understanding of microglial function and overall brain health. Ultimately, this project contributes to the ongoing discourse on the best practices for analyzing microglia morphology in neurobiology research. <div>
arXiv:2505.04838v1 Announce Type: new 
Abstract: Microglia are important cells in the brain, and their shape can tell us a lot about brain health. In this project, I test three different tools for finding the center points of microglia in 3D microscope images. The tools include ilastik, 3D Morph, and Omnipose. I look at how well each one finds the cells and how their results compare. My findings show that each tool sees the cells in its own way, and this can affect the kind of information we get from the images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORXE: Orchestrating Experts for Dynamically Configurable Efficiency</title>
<link>https://arxiv.org/abs/2505.04850</link>
<guid>https://arxiv.org/abs/2505.04850</guid>
<content:encoded><![CDATA[
<div> Keywords: ORXE, AI models, adaptability, inference pathways, image classification

<br /><br />Summary: This paper introduces ORXE, a modular and adaptable framework designed for real-time configurable efficiency in AI models. ORXE utilizes a collection of pre-trained experts, each with varying computational costs and performance, allowing it to dynamically adjust inference pathways based on the complexity of input samples. The system stands out from conventional approaches due to its avoidance of complex metamodel training, thus simplifying the development process while achieving high efficiency and flexibility. A confidence-based gating mechanism is employed to optimize the allocation of computational resources for each input, providing a balance between inference cost and prediction performance, which can be adjusted during runtime. The authors implemented a training-free ORXE system specifically for image classification tasks, rigorously evaluating its efficiency and accuracy across different devices. Results indicate that ORXE consistently outperforms individual experts and other dynamic models in most scenarios. The proposed framework not only excels in image classification but also has the potential to be expanded to various applications, making it a scalable solution suitable for diverse real-world deployment contexts. <div>
arXiv:2505.04850v1 Announce Type: new 
Abstract: This paper presents ORXE, a modular and adaptable framework for achieving real-time configurable efficiency in AI models. By leveraging a collection of pre-trained experts with diverse computational costs and performance levels, ORXE dynamically adjusts inference pathways based on the complexity of input samples. Unlike conventional approaches that require complex metamodel training, ORXE achieves high efficiency and flexibility without complicating the development process. The proposed system utilizes a confidence-based gating mechanism to allocate appropriate computational resources for each input. ORXE also supports adjustments to the preference between inference cost and prediction performance across a wide range during runtime. We implemented a training-free ORXE system for image classification tasks, evaluating its efficiency and accuracy across various devices. The results demonstrate that ORXE achieves superior performance compared to individual experts and other dynamic models in most cases. This approach can be extended to other applications, providing a scalable solution for diverse real-world deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model</title>
<link>https://arxiv.org/abs/2505.04861</link>
<guid>https://arxiv.org/abs/2505.04861</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, Post-Training Quantization, mixed-precision, Integer Quadratic Programming, bit-width allocation  

<br /><br />Summary: The Segment Anything Model (SAM) faces challenges in deployment on resource-constrained devices due to its high computational and memory requirements. Post-Training Quantization (PTQ) is a common solution for alleviating these demands, but traditional methods typically use fixed bit-width quantization, which can compromise accuracy and efficiency. The proposed solution, Mix-QSAM, introduces a mixed-precision PTQ framework tailored for SAM. It first establishes a layer-wise importance score using Kullback-Leibler (KL) divergence to measure each layer's output contribution. A novel metric, cross-layer synergy, is introduced based on causal mutual information to track dependencies between adjacent layers, ensuring consistent bit-width allocations and enhancing numerical stability. An Integer Quadratic Programming (IQP) formulation is employed to optimize bit-width assignments while adhering to model size and bit-operation constraints, allocating higher precision to essential layers and reducing bit-width for less critical ones. Experimental findings reveal that Mix-QSAM outperforms existing PTQ methods across instance segmentation and object detection tasks, achieving up to 20% higher average precision in mixed-precision settings of 6-bit and 4-bit, all while maintaining computational efficiency. <div>
arXiv:2505.04861v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) is a popular vision foundation model; however, its high computational and memory demands make deployment on resource-constrained devices challenging. While Post-Training Quantization (PTQ) is a practical approach for reducing computational overhead, existing PTQ methods rely on fixed bit-width quantization, leading to suboptimal accuracy and efficiency. To address this limitation, we propose Mix-QSAM, a mixed-precision PTQ framework for SAM. First, we introduce a layer-wise importance score, derived using Kullback-Leibler (KL) divergence, to quantify each layer's contribution to the model's output. Second, we introduce cross-layer synergy, a novel metric based on causal mutual information, to capture dependencies between adjacent layers. This ensures that highly interdependent layers maintain similar bit-widths, preventing abrupt precision mismatches that degrade feature propagation and numerical stability. Using these metrics, we formulate an Integer Quadratic Programming (IQP) problem to determine optimal bit-width allocation under model size and bit-operation constraints, assigning higher precision to critical layers while minimizing bit-width in less influential layers. Experimental results demonstrate that Mix-QSAM consistently outperforms existing PTQ methods on instance segmentation and object detection tasks, achieving up to 20% higher average precision under 6-bit and 4-bit mixed-precision settings, while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-regressive transformation for image alignment</title>
<link>https://arxiv.org/abs/2505.04864</link>
<guid>https://arxiv.org/abs/2505.04864</guid>
<content:encoded><![CDATA[
<div> Keywords: image alignment, transformation field, multi-scale features, cross-attention, Auto-Regressive Transformation (ART)  

<br /><br />Summary: Existing image alignment methods face challenges in feature-sparse regions, large deformation, and extreme scale differences, leading to suboptimal accuracy. The iterative refinement of transformation fields is crucial for enhancing robustness against these issues. To address this, the authors propose a new method called Auto-Regressive Transformation (ART), which estimates transformations from coarse to fine within an auto-regressive framework. ART utilizes hierarchical multi-scale features and refines transformations by randomly sampling points at different scales. Additionally, the incorporation of a cross-attention layer allows the model to emphasize critical regions, promoting accurate alignment even under difficult conditions with limited features. Extensive experiments on various datasets indicate that ART significantly outperforms current state-of-the-art image alignment methods. As a result, ART is established as a powerful and innovative approach for achieving precise image alignment, demonstrating its broad applicability in diverse scenarios. This approach not only enhances accuracy but also addresses the inherent challenges faced in the field, making it a significant advancement in image processing techniques. <div>
arXiv:2505.04864v1 Announce Type: new 
Abstract: Existing methods for image alignment struggle in cases involving feature-sparse regions, extreme scale and field-of-view differences, and large deformations, often resulting in suboptimal accuracy. Robustness to these challenges improves through iterative refinement of the transformation field while focusing on critical regions in multi-scale image representations. We thus propose Auto-Regressive Transformation (ART), a novel method that iteratively estimates the coarse-to-fine transformations within an auto-regressive framework. Leveraging hierarchical multi-scale features, our network refines the transformations using randomly sampled points at each scale. By incorporating guidance from the cross-attention layer, the model focuses on critical regions, ensuring accurate alignment even in challenging, feature-limited conditions. Extensive experiments across diverse datasets demonstrate that ART significantly outperforms state-of-the-art methods, establishing it as a powerful new method for precise image alignment with broad applicability.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning</title>
<link>https://arxiv.org/abs/2505.04877</link>
<guid>https://arxiv.org/abs/2505.04877</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed Precision Quantization, optimization, quantization policies, CIFAR10, ImageNet

<br /><br />Summary: Mixed Precision Quantization (MPQ) is a critical strategy for optimizing neural networks by finding the ideal bitwidth for each layer. Traditional MPQ methods are hindered by their reliance on costly searches for quantization policies using large-scale datasets. To address this challenge, the proposed method first explores quantization strategies on smaller datasets and subsequently generalizes these findings to larger datasets, thus streamlining the process. The technique reduces the need for exhaustive fine-tuning on large-scale data by allowing for adjustments merely to the model weights. Key components of this approach include: employing sharpness-aware minimization to improve generalization in quantization, using implicit gradient direction alignment to manage conflicts in gradient objectives, and implementing an adaptive perturbation radius to speed up optimization. Theoretical evaluations and practical experiments support the effectiveness of the approach. When utilizing the CIFAR10 dataset, which is significantly smaller than ImageNet's training data, the method achieved comparable accuracy to models trained directly on ImageNet while demonstrating a computational cost reduction and improving efficiency by up to 150% compared to existing baselines. <div>
arXiv:2505.04877v1 Announce Type: new 
Abstract: Mixed Precision Quantization (MPQ) has become an essential technique for optimizing neural network by determining the optimal bitwidth per layer. Existing MPQ methods, however, face a major hurdle: they require a computationally expensive search for quantization policies on large-scale datasets. To resolve this issue, we introduce a novel approach that first searches for quantization policies on small datasets and then generalizes them to large-scale datasets. This approach simplifies the process, eliminating the need for large-scale quantization fine-tuning and only necessitating model weight adjustment. Our method is characterized by three key techniques: sharpness-aware minimization for enhanced quantization generalization, implicit gradient direction alignment to handle gradient conflicts among different optimization objectives, and an adaptive perturbation radius to accelerate optimization. Both theoretical analysis and experimental results validate our approach. Using the CIFAR10 dataset (just 0.5\% the size of ImageNet training data) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a significantly lower computational cost, while improving efficiency by up to 150% over the baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.04888</link>
<guid>https://arxiv.org/abs/2505.04888</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake, generative AI, detection, feature disentanglement, multimedia content

<br /><br />Summary: Remarkable advancements in generative AI have led to new deepfake categories that exhibit unprecedented realism, posing challenges to law enforcement and public trust. These face deepfakes have created significant confusion and deception, undermining societal faith in multimedia content. Existing deepfake detection methods are struggling to keep pace with the rapid evolution of deepfake technologies, primarily due to their reliance on specific forgery artifacts that limit generalization capabilities. To address the issue of malicious face deepfakes, this paper introduces a novel strategy that leverages coarse-to-fine spatial and semantic information, ensuring feature distinctiveness and reducing redundancy. A key innovation is the implementation of a feature orthogonality-based disentanglement strategy that facilitates branch-level and cross-branch feature disentanglement. This approach enables the integration of various feature vectors while maintaining simplicity and enhancing generalization. Experimental results on three public benchmarksFaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC)demonstrate that this method outperforms current state-of-the-art techniques, achieving improvements of 5% on Celeb-DF and 7% on DFDC in cross-dataset evaluations. <div>
arXiv:2505.04888v1 Announce Type: new 
Abstract: Remarkable advancements in generative AI technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. In particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. This is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. To combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. A novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. Comprehensive experiments on three public benchmarks: FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a cross-dataset evaluation setting.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
<link>https://arxiv.org/abs/2505.04899</link>
<guid>https://arxiv.org/abs/2505.04899</guid>
<content:encoded><![CDATA[
<div> Keywords: representation learning, organ-wise tokenization, interpretability, medical imaging, segmentation

<br /><br />Summary: Recent advancements in representation learning have highlighted the limitations of holistic embeddings, which entangle multiple semantic components, specifically in the realm of medical imaging. To overcome these challenges, the authors propose an Organ-Wise Tokenization (OWT) framework, complemented by a Token Group-based Reconstruction (TGR) training paradigm. OWT distinctively separates an image into token groups, each corresponding to specific organs or semantic entities, thereby ensuring that individual token groups capture organ-specific information. This structural separation enhances interpretability, generalization, and efficiency while allowing for precise control in various downstream tasks. Experimental results utilizing CT and MRI datasets indicate that OWT significantly improves image reconstruction and segmentation performance. Furthermore, OWT facilitates innovative semantic-level generation and retrieval applications that traditional holistic embedding methods cannot achieve. These insights emphasize the promise of OWT as a foundational framework for semantically disentangled representation learning. This approach not only showcases broad scalability but also substantial applicability to practical medical imaging scenarios and beyond, paving the way for enhanced methodologies in the field. <div>
arXiv:2505.04899v1 Announce Type: new 
Abstract: Recent advances in representation learning often rely on holistic, black-box embeddings that entangle multiple semantic components, limiting interpretability and generalization. These issues are especially critical in medical imaging. To address these limitations, we propose an Organ-Wise Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR) training paradigm. Unlike conventional approaches that produce holistic features, OWT explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. Our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while allowing fine-grained control in downstream tasks. Experiments on CT and MRI datasets demonstrate the effectiveness of OWT in not only achieving strong image reconstruction and segmentation performance, but also enabling novel semantic-level generation and retrieval applications that are out of reach for standard holistic embedding methods. These findings underscore the potential of OWT as a foundational framework for semantically disentangled representation learning, offering broad scalability and applicability to real-world medical imaging scenarios and beyond.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization</title>
<link>https://arxiv.org/abs/2505.04905</link>
<guid>https://arxiv.org/abs/2505.04905</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly Supervised Object Localization, Class Activation Map, Segment Anything Model, fine-grained segmentation, mask prompt  

<br /><br />Summary: Weakly Supervised Object Localization (WSOL) seeks to identify objects using only image-level labels, minimizing annotation costs. Traditional methods like Class Activation Maps (CAM) and self-attention maps struggle to capture fine-grained pixel-level information, limiting WSOL's effectiveness. To overcome this challenge, the authors leverage the zero-shot generalization and fine-grained segmentation capabilities of the Segment Anything Model (SAM) to enhance activation of complete object regions. They introduce a novel mask prompt for the SAM, called the Pro2SAM network, which utilizes grid points instead of relying solely on single point prompts to address semantic ambiguity. A Global Token Transformer (GTFormer) generates a coarse-grained foreground map that acts as a flexible mask prompt, incorporating patch tokens and global tokens to capture foreground semantics. The authors then input dense grid points into SAM to maximize the likelihood of accurate foreground masks. They also propose a pixel-level similarity metric to match mask prompts with SAM, selecting the mask with the highest score as the final localization output. Experimental results demonstrate that Pro2SAM achieves state-of-the-art performance on the CUB-200-2011 and ILSVRC datasets, achieving Top-1 Localization rates of 84.03% and 66.85%, respectively. <div>
arXiv:2505.04905v1 Announce Type: new 
Abstract: Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\% and 66.85\% Top-1 Loc, respectively.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.04911</link>
<guid>https://arxiv.org/abs/2505.04911</guid>
<content:encoded><![CDATA[
<div> Keywords: SpatialPrompting, zero-shot spatial reasoning, multimodal, keyframe-driven, benchmark datasets<br /><br />Summary: This study presents SpatialPrompting, a new framework that leverages the reasoning capabilities of multimodal large language models to facilitate zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike traditional approaches that require costly 3D-specific fine-tuning and specialized inputs such as point clouds or voxel representations, SpatialPrompting employs a keyframe-driven prompt generation strategy. The method selects a diverse and informative set of keyframes from image sequences using metrics including vision-language similarity, Mahalanobis distance, field of view, and image sharpness. These keyframes are integrated with camera pose data to effectively abstract spatial relationships and infer complex 3D structures. This innovative framework establishes a new flexible paradigm for spatial reasoning by utilizing intuitive visual and positional cues. Moreover, it achieves state-of-the-art zero-shot performance on benchmark datasets like ScanQA and SQA3D across multiple metrics. Ultimately, SpatialPrompting eliminates the need for specialized 3D inputs and fine-tuning, providing a simpler and more scalable alternative to conventional methods, thus paving the way for advancements in spatial reasoning tasks. <div>
arXiv:2505.04911v1 Announce Type: new 
Abstract: This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing</title>
<link>https://arxiv.org/abs/2505.04915</link>
<guid>https://arxiv.org/abs/2505.04915</guid>
<content:encoded><![CDATA[
<div> Keywords: scene text editing, diffusion-based methods, glyph encoder, stroke-level precision, multi-scale features

<br /><br />Summary: Scene text editing involves modifying text in images while maintaining style and visual coherence. Diffusion-based methods have potential in text generation; however, they often generate distorted characters, particularly complex ones like Chinese. Characters consist of intricate stroke patterns that require precise maintenance in these systems. To address this issue, the authors introduce GlyphMastero, a glyph encoder designed to enhance the capability of latent diffusion models for generating text with stroke-level accuracy. Existing methods often overlook the hierarchical nature of text structures, leading to inefficiencies. GlyphMastero addresses this gap by capturing cross-level interactions between individual characters and text lines using a novel glyph attention module. Additionally, the model employs a feature pyramid network to integrate multi-scale features from an OCR backbone. This combination allows for detailed glyph-aware guidance, resulting in improved control over scene text generation. The proposed method demonstrates a significant 18.02% increase in sentence accuracy compared to the leading multi-lingual scene text editing methods and also reduces the text-region Frchet inception distance by 53.28%, highlighting its effectiveness in generating high-quality scene text. <div>
arXiv:2505.04915v1 Announce Type: new 
Abstract: Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region Fr\'echet inception distance by 53.28\%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Detector with Frame Dynamics is a Strong Tracker</title>
<link>https://arxiv.org/abs/2505.04917</link>
<guid>https://arxiv.org/abs/2505.04917</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared tracking, Anti-UAV, motion-aware learning, frame dynamics, trajectory constraint filtering  

<br /><br />Summary:  
Infrared object tracking is essential for Anti-Unmanned Aerial Vehicle (Anti-UAV) applications, yet existing trackers often face challenges with tiny targets due to reliance on cropped templates and limited motion capabilities. This paper proposes a novel infrared tiny-object tracker that significantly improves tracking performance by integrating global detection with motion-aware learning and temporal priors. The method introduces two key innovations to enhance effectiveness. First, it utilizes frame dynamics, employing frame differences and optical flow to better encode prior target features and motion characteristics, thereby improving target-background distinction. Second, it implements a trajectory constraint filtering strategy in the post-processing phase, which capitalizes on spatio-temporal priors to reduce false positives and bolster tracking robustness. Comprehensive experiments demonstrate that this approach consistently outperforms existing methods across various metrics in challenging infrared UAV tracking scenarios. The method has achieved notable success, earning state-of-the-art performance during the 4th Anti-UAV Challenge, where it secured 1st place in Track 1 and 2nd place in Track 2. <div>
arXiv:2505.04917v1 Announce Type: new 
Abstract: Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Reasoning, Large Multimodal Reasoning Models, Chain-of-Thought, Omni-modal Generalization, Agentic Behavior

<br /><br />Summary: The article discusses the importance of reasoning in artificial intelligence, particularly within Large Multimodal Reasoning Models (LMRMs) that integrate various modalities such as text, images, audio, and video. It highlights the evolution of multimodal reasoning from early, modular approaches to unified frameworks that enhance cross-modal understanding. Specifically, the authors review initial efforts where reasoning was implicitly included in task-specific modules, transitioning to contemporary models that incorporate Multimodal Chain-of-Thought (MCoT) and reinforcement learning to facilitate more structured reasoning processes. The survey addresses ongoing challenges, including omni-modal generalization, the depth of reasoning, and the need for agentic behavior in these systems. Furthermore, it presents a developmental roadmap to guide future research toward the conceptualization of native large multimodal reasoning models (N-LMRMs), which are designed to support scalable, adaptive reasoning and planning capabilities in complex, real-world environments. The authors aim to spark discussions on how to effectively advance multimodal reasoning research to create more robust AI systems capable of effective decision-making and problem-solving in diverse settings. <div>
arXiv:2505.04921v1 Announce Type: new 
Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training</title>
<link>https://arxiv.org/abs/2505.04922</link>
<guid>https://arxiv.org/abs/2505.04922</guid>
<content:encoded><![CDATA[
<div> Keywords: palmprint recognition, Canny2Palm, synthetic data, Pix2Pix, large-scale pre-training

<br /><br />Summary: Palmprint recognition offers a secure and privacy-friendly biometric identification method, but it faces challenges due to limited palmprint data. Recent research focuses on synthesizing virtual palmprints for extensive pre-training. This paper presents a novel synthesis method called Canny2Palm, which utilizes the Canny edge detector to extract palm textures, conditioning a Pix2Pix network for realistic palmprint generation. By reassembling textures from diverse identities, new identities can be created by introducing new assemblies to the generator. The Canny2Palm method not only generates realistic data that aligns with the distribution of actual palmprints but also facilitates controllable diversity, allowing for the creation of a large-scale variety of new identities. In open-set palmprint recognition benchmarks, models pre-trained with data from Canny2Palm demonstrate improved performance, achieving up to 7.2% higher identification accuracy compared to state-of-the-art approaches. Furthermore, models pre-trained with Canny2Palm continue to show improvement as synthetic IDs increase to 10,000, while performances of those using existing methods tend to plateau, highlighting the effectiveness and potential of Canny2Palm for large-scale pre-training. <div>
arXiv:2505.04922v1 Announce Type: new 
Abstract: Palmprint recognition is a secure and privacy-friendly method of biometric identification. One of the major challenges to improve palmprint recognition accuracy is the scarcity of palmprint data. Recently, a popular line of research revolves around the synthesis of virtual palmprints for large-scale pre-training purposes. In this paper, we propose a novel synthesis method named Canny2Palm that extracts palm textures with Canny edge detector and uses them to condition a Pix2Pix network for realistic palmprint generation. By re-assembling palmprint textures from different identities, we are able to create new identities by seeding the generator with new assemblies. Canny2Palm not only synthesizes realistic data following the distribution of real palmprints but also enables controllable diversity to generate large-scale new identities. On open-set palmprint recognition benchmarks, models pre-trained with Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2% higher identification accuracy. Moreover, the performance of models pre-trained with Canny2Palm continues to improve given 10,000 synthetic IDs while those with existing methods already saturate, demonstrating the potential of our method for large-scale pre-training.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration</title>
<link>https://arxiv.org/abs/2505.04938</link>
<guid>https://arxiv.org/abs/2505.04938</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, pyramid registration network, Residual Feature Fusion Module, Residual Deformation Field Fusion Module, registration accuracy  

<br /><br />Summary: In recent years, deformable medical image registration techniques have significantly advanced, yet existing models still face challenges in efficiently extracting both coarse and fine-grained features in parallel. To address these limitations, a new pyramid registration network has been constructed, named FF-PNet, which leverages two innovative modules. For coarse-grained feature extraction, a Residual Feature Fusion Module (RFFM) is designed, while a Residual Deformation Field Fusion Module (RDFFM) is introduced for capturing fine-grained image deformations. The parallel operation of these two modules allows the model to effectively manage complex image deformations. Notably, the encoding stage of FF-PNet relies solely on traditional convolutional neural networks, eschewing attention mechanisms or multilayer perceptrons, yet still achieves significant improvements in registration accuracy. This emphasizes the enhanced feature decoding capabilities provided by RFFM and RDFFM. Comprehensive experiments conducted on the LPBA and OASIS datasets demonstrate that the proposed network consistently outperforms established methods in key evaluation metrics, such as the Dice Similarity Coefficient, confirming its efficacy in the domain of medical image registration. <div>
arXiv:2505.04938v1 Announce Type: new 
Abstract: In recent years, deformable medical image registration techniques have made significant progress. However, existing models still lack efficiency in parallel extraction of coarse and fine-grained features. To address this, we construct a new pyramid registration network based on feature and deformation field (FF-PNet). For coarse-grained feature extraction, we design a Residual Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a Residual Deformation Field Fusion Module (RDFFM). Through the parallel operation of these two modules, the model can effectively handle complex image deformations. It is worth emphasizing that the encoding stage of FF-PNet only employs traditional convolutional neural networks without any attention mechanisms or multilayer perceptrons, yet it still achieves remarkable improvements in registration accuracy, fully demonstrating the superior feature decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on the LPBA and OASIS datasets. The results show our network consistently outperforms popular methods in metrics like the Dice Similarity Coefficient.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping</title>
<link>https://arxiv.org/abs/2505.04941</link>
<guid>https://arxiv.org/abs/2505.04941</guid>
<content:encoded><![CDATA[
<div> Keywords: building damage assessment, remote sensing images, pseudo-label learning, multi-model fusion, change detection  

<br /><br />Summary: This study focuses on accurately assessing building damage using bi-temporal multi-modal remote sensing images, which is vital for disaster response and recovery. It introduces a Building-Guided Pseudo-Label Learning Framework to tackle the challenges posed by the mapping of building damage from pre-disaster optical and post-disaster SAR images. Initially, the research trains multiple building extraction models using pre-disaster optical images coupled with building labels. To improve building segmentation, strategies like multi-model fusion and test-time augmentation are applied to create pseudo-probabilities, culminating in a low-uncertainty pseudo-label training method for refinement. Subsequently, a change detection model is developed, using bi-temporal cross-modal images and damaged building labels. To enhance damage classification accuracy, a building-guided low-uncertainty pseudo-label refinement approach is introduced, utilizing prior knowledge of buildings to inform pseudo-label generation for damaged structures. The efficacy of this framework is evidenced by experimental results from the 2025 IEEE GRSS Data Fusion Contest dataset, where it achieved the highest mean Intersection over Union (mIoU) score of 54.28% and secured the top position in the competition. <div>
arXiv:2505.04941v1 Announce Type: new 
Abstract: Accurate building damage assessment using bi-temporal multi-modal remote sensing images is essential for effective disaster response and recovery planning. This study proposes a novel Building-Guided Pseudo-Label Learning Framework to address the challenges of mapping building damage from pre-disaster optical and post-disaster SAR images. First, we train a series of building extraction models using pre-disaster optical images and building labels. To enhance building segmentation, we employ multi-model fusion and test-time augmentation strategies to generate pseudo-probabilities, followed by a low-uncertainty pseudo-label training method for further refinement. Next, a change detection model is trained on bi-temporal cross-modal images and damaged building labels. To improve damage classification accuracy, we introduce a building-guided low-uncertainty pseudo-label refinement strategy, which leverages building priors from the previous step to guide pseudo-label generation for damaged buildings, reducing uncertainty and enhancing reliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest dataset demonstrate the effectiveness of our approach, which achieved the highest mIoU score (54.28%) and secured first place in the competition.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models</title>
<link>https://arxiv.org/abs/2505.04946</link>
<guid>https://arxiv.org/abs/2505.04946</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, benchmark, fidelity, evaluation, synthesis  

<br /><br />Summary: Recent advancements in scalable deep architectures and large-scale pretraining have significantly improved text-to-video generation, enabling high-fidelity content across diverse styles, with applications in advertising, entertainment, and education. However, these models face challenges in rendering precise on-screen text, such as captions and mathematical formulas, which is crucial for applications requiring textual accuracy. To address this gap, the authors introduce T2VTextBench, the first human-evaluation benchmark specifically designed to assess on-screen text fidelity and temporal consistency in text-to-video models. The benchmark includes a suite of prompts with complex text strings and dynamic scene changes, testing each model's ability to maintain detailed instructions throughout the video frames. The researchers evaluate ten state-of-the-art text-to-video systems, spanning both open-source and commercial offerings. Their findings reveal that most of the models struggle to generate legible and consistent text, highlighting a significant shortcoming in current video generation technology. This research underscores the necessity for further advancements in textual manipulation within video synthesis to improve the overall quality and utility of text-to-video applications. <div>
arXiv:2505.04946v1 Announce Type: new 
Abstract: Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects</title>
<link>https://arxiv.org/abs/2505.04962</link>
<guid>https://arxiv.org/abs/2505.04962</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous picking, pose estimation, cuboidal objects, local registration, linear time approach

<br /><br />Summary: This paper introduces a solution for the autonomous picking of cuboidal objects from both organized and unorganized piles, emphasizing high precision. The focus is on an efficient method for accurate pose estimation of cuboid-shaped objects to minimize errors in target pose while being time-efficient. Traditional pose estimation techniques, particularly global point cloud registrations, often encounter minor pose errors, necessitating the use of local registration algorithms to enhance accuracy. However, these local methods introduce execution time overhead and uncertainty related to the final pose accuracy. To address these concerns, the authors propose an alternative approach that operates in linear time for pose error estimation and correction. The paper provides an overview of the entire solution and subsequently details the individual modules that constitute the proposed algorithm, presenting a comprehensive framework designed to improve the reliability and efficiency of pose estimation in robotic applications for picking tasks. The proposed method aims to achieve both precision and speed, which are critical for effective automation in various industrial settings. <div>
arXiv:2505.04962v1 Announce Type: new 
Abstract: The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis</title>
<link>https://arxiv.org/abs/2505.04963</link>
<guid>https://arxiv.org/abs/2505.04963</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, ViCTr, liver cirrhosis, pathologies, image synthesis  

<br /><br />Summary: Synthesizing medical images, particularly for conditions like liver cirrhosis, poses significant challenges due to limited annotated data and domain gaps. Existing methodologies often struggle with maintaining anatomical fidelity and addressing diffuse pathologies. To tackle these issues, this work introduces ViCTr (Vital Consistency Transfer), a two-stage framework that integrates a rectified flow trajectory with a Tweedie-corrected diffusion process. The first stage involves pretraining ViCTr on the ATLAS-8k dataset utilizing Elastic Weight Consolidation (EWC) to preserve anatomy. In the second stage, the model undergoes adversarial fine-tuning with Low-Rank Adaptation (LoRA) to enable precise control over the severity of pathologies. Notably, ViCTr reformulates Tweedie's formula, allowing for one-step sampling that trims inference time from 50 to just 4 steps while maintaining anatomical realism. Evaluation on BTCV, AMOS, and CirrMRI600+ datasets reveals state-of-the-art performance, with a Medical Frechet Inception Distance (MFID) score that is 28% lower than previous techniques. Additionally, the model improves nnUNet segmentation by 3.8% mDSC when used for data augmentation. Radiologist reviews affirm the clinical indistinguishability of ViCTr-generated MRIs from actual scans, marking a significant advancement in AI-driven medical imaging research. <div>
arXiv:2505.04963v1 Announce Type: new 
Abstract: Synthesizing medical images remains challenging due to limited annotated pathological data, modality domain gaps, and the complexity of representing diffuse pathologies such as liver cirrhosis. Existing methods often struggle to maintain anatomical fidelity while accurately modeling pathological features, frequently relying on priors derived from natural images or inefficient multi-step sampling. In this work, we introduce ViCTr (Vital Consistency Transfer), a novel two-stage framework that combines a rectified flow trajectory with a Tweedie-corrected diffusion process to achieve high-fidelity, pathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k dataset using Elastic Weight Consolidation (EWC) to preserve critical anatomical structures. We then fine-tune the model adversarially with Low-Rank Adaptation (LoRA) modules for precise control over pathology severity. By reformulating Tweedie's formula within a linear trajectory framework, ViCTr supports one-step sampling, reducing inference from 50 steps to just 4, without sacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and CirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art performance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for cirrhosis synthesis 28% lower than existing approaches and improving nnUNet segmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews indicate that ViCTr-generated liver cirrhosis MRIs are clinically indistinguishable from real scans. To our knowledge, ViCTr is the first method to provide fine-grained, pathology-aware MRI synthesis with graded severity control, closing a critical gap in AI-driven medical imaging research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems</title>
<link>https://arxiv.org/abs/2505.04964</link>
<guid>https://arxiv.org/abs/2505.04964</guid>
<content:encoded><![CDATA[
<div> Keywords: coronary angiography, AI, VLMs, decision support, key-frame detection  

<br /><br />Summary: Coronary angiography (CAG) is recognized as the gold-standard imaging technique for assessing coronary artery disease, but its interpretation is primarily dependent on expert cardiologists. To enhance decision-making with AI, the authors present a two-stage, physician-curated workflow alongside a bilingual CAG image-report dataset (Japanese/English). In the first stage, they sampled 14,686 frames from 539 exams and annotated them for key-frame detection and laterality classification. A ConvNeXt-Base CNN trained on this dataset demonstrated an impressive 0.96 F1 score for laterality classification, even with low-contrast frames. In the second stage, this CNN was applied to 243 independent exams to extract 1,114 key frames, which were paired with their associated pre-procedure reports and validated diagnostic and treatment summaries, forming a parallel corpus. The authors subsequently fine-tuned three open-source Vision-Language Models (VLMs)PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3utilizing Low-Rank Adaptation (LoRA), evaluating them against VLScore and cardiologist assessments. Although PaliGemma2 with LoRA received the highest VLScore, Gemma3 with LoRA garnered the best clinician rating, thus designating it as the optimal model, CAG-VLM, indicating its potential to aid cardiologists in generating clinical reports and treatment recommendations from CAG images. <div>
arXiv:2505.04964v1 Announce Type: new 
Abstract: Coronary angiography (CAG) is the gold-standard imaging modality for evaluating coronary artery disease, but its interpretation and subsequent treatment planning rely heavily on expert cardiologists. To enable AI-based decision support, we introduce a two-stage, physician-curated pipeline and a bilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686 frames from 539 exams and annotate them for key-frame detection and left/right laterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on laterality classification, even on low-contrast frames. Second, we apply the CNN to 243 independent exams, extract 1,114 key frames, and pair each with its pre-procedure report and expert-validated diagnostic and treatment summary, yielding a parallel corpus. We then fine-tune three open-source VLMs (PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate them using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains the highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean 7.20/10); we designate this best-performing model as CAG-VLM. These results demonstrate that specialized, fine-tuned VLMs can effectively assist cardiologists in generating clinical reports and treatment recommendations from CAG images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2505.04965</link>
<guid>https://arxiv.org/abs/2505.04965</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D visual grounding, DenseGrounding, ego-centric, language models, semantic enhancement  

<br /><br />Summary: Enabling intelligent agents to effectively comprehend and interact with 3D environments using natural language is essential for advancements in robotics and human-computer interaction. A primary challenge in this domain is ego-centric 3D visual grounding, where agents must identify target objects based on verbal descriptions. This task faces two significant hurdles: the loss of fine-grained visual semantics during the fusion of point clouds and ego-centric multi-view images, and the constraints imposed by arbitrary language descriptions on textual context. To tackle these challenges, we introduce DenseGrounding, which enhances both visual and textual semantics. Our approach includes a Hierarchical Scene Semantic Enhancer that captures global scene features while maintaining fine-grained details, thereby facilitating better cross-modal alignment. Additionally, we employ a Language Semantic Enhancer that utilizes large language models to generate enriched and contextually diverse textual descriptions during training. Extensive experiments demonstrate that DenseGrounding outperforms existing methods by 5.81% and 7.56% in accuracy on comprehensive and smaller datasets, respectively, setting a new state-of-the-art in ego-centric 3D visual grounding. Furthermore, our method won 1st place and received the Innovation Award at the CVPR 2024 Autonomous Grand Challenge, underscoring its effectiveness and robustness. <div>
arXiv:2505.04965v1 Announce Type: new 
Abstract: Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding. Our method also achieves 1st place and receives the Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</title>
<link>https://arxiv.org/abs/2505.04974</link>
<guid>https://arxiv.org/abs/2505.04974</guid>
<content:encoded><![CDATA[
<div> Keywords: Bilingual, Motion Generation, Dataset, Diffusion Model, Alignment

<br /><br />Summary: Bilingual text-to-motion generation synthesizes 3D human motions from bilingual text inputs and has significant potential in fields like gaming, film, and robotics. However, it faces challenges including the lack of bilingual motion-language datasets and alignment issues in diffusion models, which can result in semantically inconsistent or low-quality motions. To tackle these problems, the authors introduce BiHumanML3D, a novel bilingual human motion dataset, establishing an important benchmark for bilingual text-to-motion generation models. Additionally, they present the Bilingual Motion Diffusion model (BiMD), which utilizes cross-lingual aligned representations to capture semantic meanings, facilitating a unified bilingual approach. The study further introduces the Reward-guided sampling Alignment (ReAlign) method, featuring a step-aware reward model that evaluates alignment quality during the sampling process. This method employs a reward-guided strategy to steer the diffusion process toward better alignment. The reward model incorporates step-aware tokens along with a text-aligned module for semantic consistency and a motion-aligned module for realism, refining motions at each timestep. Experimental results indicate that the proposed approach considerably enhances text-motion alignment and motion quality when compared to existing state-of-the-art methods. <div>
arXiv:2505.04974v1 Announce Type: new 
Abstract: Bilingual text-to-motion generation, which synthesizes 3D human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. However, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. To address these challenges, we propose BiHumanML3D, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. Building upon this, we propose Reward-guided sampling Alignment (ReAlign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. Project page: https://wengwanjiang.github.io/ReAlign-page/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization</title>
<link>https://arxiv.org/abs/2505.04979</link>
<guid>https://arxiv.org/abs/2505.04979</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, attribute bias, deconfounding, debiasing, causal graph

<br /><br />Summary:  
Attribute bias in federated learning (FL) can degrade model performance by promoting non-causal associations. Current solutions either use data augmentation to enhance sample diversity or knowledge distillation for invariant representations, but they fall short of comprehensively analyzing inference paths and suffer from confounding factors. To overcome these challenges, the authors introduce the FedDDL method, which constructs a structured causal graph to analyze the inference process and utilizes backdoor adjustment to remove confounding paths. It includes an intra-client deconfounding learning module tailored for computer vision tasks, effectively decoupling backgrounds from objects and generating counterfactual samples that prevent the model from incorrectly using background information to infer labels. Additionally, it features an inter-client debiasing learning module that forms causal prototypes tailored to minimize background presence in prototypes. This innovative approach enhances the alignment of heterogeneous representations through causal prototypical regularization. Extensive experiments conducted on two benchmark datasets reveal that FedDDL significantly improves the models ability to focus on primary objects in unseen data, yielding an average increase of 4.5% in Top-1 Accuracy compared to nine existing state-of-the-art methods. <div>
arXiv:2505.04979v1 Announce Type: new 
Abstract: Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \underline{Fed}erated \underline{D}econfounding and \underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</title>
<link>https://arxiv.org/abs/2505.05001</link>
<guid>https://arxiv.org/abs/2505.05001</guid>
<content:encoded><![CDATA[
<div> Keywords: video stitching, warping shake, temporal stabilization, unsupervised learning, StabStitch++

<br /><br />Summary: The article addresses a new issue in video stitching known as warping shake, which results in undesirable temporal shakes due to sequentially unsmooth warps, even when input videos are stable. To tackle this issue, the authors propose a novel framework called StabStitch++, which integrates spatial stitching and temporal stabilization using unsupervised learning. Instead of the traditional approach that warps one image to align with another, StabStitch++ introduces a virtual midplane for projection, utilizing a differentiable bidirectional decomposition module to disentangle homography transformations. This evenly distributes alignment burdens and distortions across image views. The framework derives stitching trajectories by combining spatial and temporal warps inspired by video stabilization techniques. A warp smoothing model is introduced to create stable stitched videos, supported by a hybrid loss function that promotes content alignment, trajectory smoothness, and collaborative online processing. StabStitch++ improves upon its predecessor, StabStitch, by optimizing both alignment and stabilization simultaneously, particularly in real-time applications. The authors also establish a benchmarking dataset with diverse camera motions and scenes to evaluate their framework, demonstrating superior performance and efficiency in video stitching. <div>
arXiv:2505.05001v1 Announce Type: new 
Abstract: We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort</title>
<link>https://arxiv.org/abs/2505.05004</link>
<guid>https://arxiv.org/abs/2505.05004</guid>
<content:encoded><![CDATA[
<div> Keywords: thoracolumbar, stump ribs, deep-learning, segmentation, morphology

<br /><br />Summary: This study focuses on thoracolumbar stump ribs, which are critical indicators of thoracolumbar transitional vertebrae or enumeration anomalies. Unlike previous research that qualitatively describes these anomalies through manual assessment, this work automates the detection of stump ribs and quantitatively analyzes their morphology. A high-resolution deep-learning model for rib segmentation was successfully trained, achieving a significant improvement in accuracy (Dice score of 0.997 compared to 0.779, p-value < 0.01). Additionally, an iterative algorithm and piece-wise linear interpolation were employed to assess rib length, resulting in a success rate of 98.2%. Morphological analysis revealed that stump ribs articulate more posteriorly at the vertebrae (-19.2  3.8 vs. -13.8  2.5, p-value < 0.01), are thinner (260.6  103.4 vs. 563.6  127.1, p-value < 0.01), and demonstrate a more downward and sideways orientation in the initial centimeters compared to full-length ribs. The study also achieved an F1-score of 0.84 in distinguishing stump ribs from regular ribs, even with partial visibility. The model weights and masks developed are made publicly available for further research. <div>
arXiv:2505.05004v1 Announce Type: new 
Abstract: Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. While some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. To this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. When analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value < 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. We show that with partially visible ribs, these features can achieve an F1-score of 0.84 in differentiating stump ribs from regular ones. We publish the model weights and masks for public use.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition</title>
<link>https://arxiv.org/abs/2505.05007</link>
<guid>https://arxiv.org/abs/2505.05007</guid>
<content:encoded><![CDATA[
<div> Keywords: map matching, Hidden Markov Model, lane markings, scenario recognition, multilevel road

<br /><br />Summary: Accurate online map matching is vital for vehicle navigation and intelligent driving features. Existing methods struggle in complex road networks, particularly in multilevel areas. This study introduces an online Standard Definition (SD) map matching technique that employs a Hidden Markov Model (HMM) enhanced by multiple probability factors. The method accurately matches maps by utilizing lane markings and scenario recognition. Initially, lane markings are generated using a multi-lane tracking approach and linked to the SD map through HMM, creating an enriched SD map. Vehicles can then re-localize by applying Iterative Closest Point (ICP) registration based on these lane markings. The probability factor for lane marking detection is determined through association probabilities between adjacent lanes and roads. Additionally, a driving scenario recognition model yields an emission probability factor, significantly enhancing map matching on elevated roads and the urban roads below them. Extensive road tests in Europe and China validate the method, revealing superior accuracy compared to existing techniques, notably in multilevel road regions. The proposed method achieves F1 scores of 98.04% on the Zenseact Open Dataset and 94.60% on Shanghai multilevel area data, outperforming benchmark methods. Implementation details are available at https://github.com/TRV-Lab/LMSR-OMM. <div>
arXiv:2505.05007v1 Announce Type: new 
Abstract: Accurate online map matching is fundamental to vehicle navigation and the activation of intelligent driving functions. Current online map matching methods are prone to errors in complex road networks, especially in multilevel road area. To address this challenge, we propose an online Standard Definition (SD) map matching method by constructing a Hidden Markov Model (HMM) with multiple probability factors. Our proposed method can achieve accurate map matching even in complex road networks by carefully leveraging lane markings and scenario recognition in the designing of the probability factors. First, the lane markings are generated by a multi-lane tracking method and associated with the SD map using HMM to build an enriched SD map. In areas covered by the enriched SD map, the vehicle can re-localize itself by performing Iterative Closest Point (ICP) registration for the lane markings. Then, the probability factor accounting for the lane marking detection can be obtained using the association probability between adjacent lanes and roads. Second, the driving scenario recognition model is applied to generate the emission probability factor of scenario recognition, which improves the performance of map matching on elevated roads and ordinary urban roads underneath them. We validate our method through extensive road tests in Europe and China, and the experimental results show that our proposed method effectively improves the online map matching accuracy as compared to other existing methods, especially in multilevel road area. Specifically, the experiments show that our proposed method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset and test data of multilevel road areas in Shanghai respectively, significantly outperforming benchmark methods. The implementation is available at https://github.com/TRV-Lab/LMSR-OMM.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Contextual Embedding for Robust Far-View Borehole Detection</title>
<link>https://arxiv.org/abs/2505.05008</link>
<guid>https://arxiv.org/abs/2505.05008</guid>
<content:encoded><![CDATA[
<div> Keywords: controlled blasting, detection method, adaptive augmentation, embedding stabilization, contextual refinement  

<br /><br />Summary:  
In controlled blasting operations, the ability to detect densely packed small boreholes from far-view imagery is vital for ensuring safety and operational efficiency. Existing methods struggle due to the minute scale, dense arrangements, and the lack of distinctive visual features of these boreholes. To overcome these limitations, the authors propose a novel adaptive detection approach that enhances existing architectures like YOLO by employing consistent embedding representations through exponential moving average (EMA)-based statistical updates. The method introduces three key components: first, adaptive augmentation that utilizes dynamically updated image statistics to effectively manage variations in illumination and texture; second, embedding stabilization to guarantee consistent and reliable feature extraction; and third, contextual refinement that uses spatial context to enhance detection accuracy. The use of EMA is particularly beneficial given the challenges of limited visual complexity and the small scale of boreholes, leading to stable and robust representation learning. Experimental results on a challenging proprietary quarry-site dataset demonstrate significant improvements over baseline YOLO-based architectures, underscoring the proposed method's effectiveness under realistic and complex industrial conditions. <div>
arXiv:2505.05008v1 Announce Type: new 
Abstract: In controlled blasting operations, accurately detecting densely distributed tiny boreholes from far-view imagery is critical for operational safety and efficiency. However, existing detection methods often struggle due to small object scales, highly dense arrangements, and limited distinctive visual features of boreholes. To address these challenges, we propose an adaptive detection approach that builds upon existing architectures (e.g., YOLO) by explicitly leveraging consistent embedding representations derived through exponential moving average (EMA)-based statistical updates.
  Our method introduces three synergistic components: (1) adaptive augmentation utilizing dynamically updated image statistics to robustly handle illumination and texture variations; (2) embedding stabilization to ensure consistent and reliable feature extraction; and (3) contextual refinement leveraging spatial context for improved detection accuracy. The pervasive use of EMA in our method is particularly advantageous given the limited visual complexity and small scale of boreholes, allowing stable and robust representation learning even under challenging visual conditions. Experiments on a challenging proprietary quarry-site dataset demonstrate substantial improvements over baseline YOLO-based architectures, highlighting our method's effectiveness in realistic and complex industrial scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOAP: Style-Omniscient Animatable Portraits</title>
<link>https://arxiv.org/abs/2505.05022</link>
<guid>https://arxiv.org/abs/2505.05022</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D avatars, diffusion models, topology-consistent, animation controls, soap

<br /><br />Summary: Creating animatable 3D avatars from a single image is a complex task, particularly due to style limitations such as realistic, cartoon, or anime representations, as well as challenges in handling accessories and hairstyles. Traditional 3D diffusion models have made strides in reconstructing single-view images but often lack necessary animation controls and can produce artifacts due to the domain gap. To address these issues, we introduce SOAP, a style-omniscient framework designed to generate rigged, topology-consistent avatars from any portrait. Our approach utilizes a multiview diffusion model trained on a dataset of 24,000 3D heads across various styles and employs an adaptive optimization pipeline to deform the FLAME mesh while preserving its topology and rigging through differentiable rendering techniques. The outcome is textured avatars that support FACS-based animation and include realistic features such as eyeballs, teeth, and intricate hairstyles or accessories. Extensive experiments validate the effectiveness of our method, demonstrating its superiority over existing state-of-the-art techniques in both single-view head modeling and diffusion-based 3D generation from images. Our implementation and dataset are publicly accessible for research at https://github.com/TingtingLiao/soap. <div>
arXiv:2505.05022v1 Announce Type: new 
Abstract: Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at https://github.com/TingtingLiao/soap.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Split Matching for Inductive Zero-shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.05023</link>
<guid>https://arxiv.org/abs/2505.05023</guid>
<content:encoded><![CDATA[
<div> Keywords: Zero-shot Semantic Segmentation, query-based segmentation, Split Matching, Hungarian matching, Multi-scale Feature Enhancement

<br /><br />Summary: 
This article focuses on Zero-shot Semantic Segmentation (ZSS), which involves segmenting categories not annotated during training. Traditional fine-tuning of vision-language models faces challenges, primarily overfitting to seen categories due to the absence of supervision for unseen classes. It highlights query-based segmentation as a promising alternative that excels in object localization without explicit labels. A major issue with conventional Hungarian matching in this context is its reliance on full supervision, often misclassifying unseen categories as background. To tackle this, the authors propose a novel Split Matching (SM) strategy that separates Hungarian matching into two components: one for annotated regions (seen classes) and another for unannotated regions (unseen candidates). They cluster CLIP dense features to create pseudo masks and derive region-level embeddings. Matching occurs independently for these two groups, focusing on class-level similarity and mask-level consistency. Additionally, they introduce a Multi-scale Feature Enhancement (MFE) module, which enhances decoder features through multi-scale aggregation for improved spatial detail capture. The work presents SM as the first decoupled Hungarian matching approach under the inductive ZSS setting, achieving state-of-the-art results on two standard benchmarks. <div>
arXiv:2505.05023v1 Announce Type: new 
Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not annotated during training. While fine-tuning vision-language models has achieved promising results, these models often overfit to seen categories due to the lack of supervision for unseen classes. As an alternative to fully supervised approaches, query-based segmentation has shown great latent in ZSS, as it enables object localization without relying on explicit labels. However, conventional Hungarian matching, a core component in query-based frameworks, needs full supervision and often misclassifies unseen categories as background in the setting of ZSS. To address this issue, we propose Split Matching (SM), a novel assignment strategy that decouples Hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). Specifically, we partition the queries into seen and candidate groups, enabling each to be optimized independently according to its available supervision. To discover unseen candidates, we cluster CLIP dense features to generate pseudo masks and extract region-level embeddings using CLS tokens. Matching is then conducted separately for the two groups based on both class-level similarity and mask-level consistency. Additionally, we introduce a Multi-scale Feature Enhancement (MFE) module that refines decoder features through residual multi-scale aggregation, improving the model's ability to capture spatial details across resolutions. SM is the first to introduce decoupled Hungarian matching under the inductive ZSS setting, and achieves state-of-the-art performance on two standard benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition</title>
<link>https://arxiv.org/abs/2505.05043</link>
<guid>https://arxiv.org/abs/2505.05043</guid>
<content:encoded><![CDATA[
<div> Keywords: facial affect, emotion recognition, xTrace, video datasets, dimensional emotions

<br /><br />Summary: The paper addresses the challenges of recognizing expressive behaviors in face videos, focusing on two primary issues: the lack of large-scale labeled datasets and the difficulty of extracting meaningful facial features. To tackle these, the authors introduce xTrace, a tool designed for analyzing facial expressive behavior and predicting continuous values of emotions, specifically valence and arousal. xTrace is trained on an extensive dataset comprising approximately 450,000 videos that encapsulate a wide range of emotional expressions, enhancing its versatility in real-world applications. It employs facial affect descriptors that ensure explainability while maintaining high accuracy and low computational demands. The performance of xTrace is benchmarked against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox. In validation using a set of 50,000 videos, xTrace achieved a mean Concordance Correlation Coefficient (CCC) of 0.86 and a mean absolute error of 0.13. A thorough error analysis reveals its high accuracy in recognizing emotions, robustness to non-frontal head poses, and a strong relationship between uncertainty estimates and accuracy, validating its effectiveness in the domain of Affective Computing. <div>
arXiv:2505.05043v1 Announce Type: new 
Abstract: Recognising expressive behaviours in face videos is a long-standing challenge in Affective Computing. Despite significant advancements in recent years, it still remains a challenge to build a robust and reliable system for naturalistic and in-the-wild facial expressive behaviour analysis in real time. This paper addresses two key challenges in building such a system: (1). The paucity of large-scale labelled facial affect video datasets with extensive coverage of the 2D emotion space, and (2). The difficulty of extracting facial video features that are discriminative, interpretable, robust, and computationally efficient. Toward addressing these challenges, we introduce xTrace, a robust tool for facial expressive behaviour analysis and predicting continuous values of dimensional emotions, namely valence and arousal, from in-the-wild face videos.
  To address challenge (1), our affect recognition model is trained on the largest facial affect video data set, containing ~450k videos that cover most emotion zones in the dimensional emotion space, making xTrace highly versatile in analysing a wide spectrum of naturalistic expressive behaviours. To address challenge (2), xTrace uses facial affect descriptors that are not only explainable, but can also achieve a high degree of accuracy and robustness with low computational complexity. The key components of xTrace are benchmarked against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox. On an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86 mean CCC and 0.13 mean absolute error values. We present a detailed error analysis of affect predictions from xTrace, illustrating (a). its ability to recognise emotions with high accuracy across most bins in the 2D emotion space, (b). its robustness to non-frontal head pose angles, and (c). a strong correlation between its uncertainty estimates and its accuracy.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
<link>https://arxiv.org/abs/2505.05049</link>
<guid>https://arxiv.org/abs/2505.05049</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, uncertainty quantification, Bayesian entropy, semi-supervised, predictive capabilities  

<br /><br />Summary:  
The introduction of the Segment Anything Model (SAM) has significantly impacted semantic segmentation applications, raising the importance of quantifying uncertainty in its outputs. The class-agnostic nature of SAM presents challenges for existing uncertainty quantification (UQ) methods. In response, this paper introduces a novel UQ model grounded in a Bayesian entropy framework that incorporates aleatoric, epistemic, and a newly defined task uncertainty. This theoretical foundation leads to the creation of USAM, a lightweight post-hoc method for uncertainty quantification. The model identifies the sources of uncertainty as stemming from under-parameterised models, poor prompts, or ambiguities within images. Demonstrating remarkable predictive capabilities, USAM is applied across several datasets, including SA-V, MOSE, ADE20k, DAVIS, and COCO, outperforming other methods. Furthermore, it offers a computationally efficient and user-friendly UQ solution capable of supporting user-prompting and enhancing semi-supervised learning pipelines. USAM allows users to effectively manage the tradeoff between accuracy and cost efficiency in their applications, making it a valuable tool in the realm of semantic segmentation. <div>
arXiv:2505.05049v1 Announce Type: new 
Abstract: The introduction of the Segment Anything Model (SAM) has paved the way for numerous semantic segmentation applications. For several tasks, quantifying the uncertainty of SAM is of particular interest. However, the ambiguous nature of the class-agnostic foundation model SAM challenges current uncertainty quantification (UQ) approaches. This paper presents a theoretically motivated uncertainty quantification model based on a Bayesian entropy formulation jointly respecting aleatoric, epistemic, and the newly introduced task uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ method. Our model traces the root of uncertainty back to under-parameterised models, insufficient prompts or image ambiguities. Our proposed deterministic USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ alternative that can support user-prompting, enhance semi-supervised pipelines, or balance the tradeoff between accuracy and cost efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.05062</link>
<guid>https://arxiv.org/abs/2505.05062</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-Tailed Semi-Supervised Learning, large-scale visual foundation models, Unbiased Lightweight Fine-tuning, pseudo-labels, model performance  

<br /><br />Summary: This paper investigates the effects of large-scale visual foundation models, such as CLIP, on Long-Tailed Semi-Supervised Learning (LTSSL) using three strategies: Linear Probing (LP), Lightweight Fine-Tuning (LFT), and Full Fine-Tuning (FFT). The research reveals key insights: i) FFT leads to decreased model performance compared to LTSSL algorithms trained from scratch. Although LP and LFT enhance overall performance, they provide minimal benefits for tail classes. ii) LP generates numerous false pseudo-labels due to underlearned data. In contrast, while LFT reduces these false labels, it becomes overconfident, leading to biased fitting and exacerbating inherent pseudo-labeled and classifier biases in LTSSL, ultimately hindering performance in tail classes. To address these issues, the authors propose ULFine, an Unbiased Lightweight Fine-tuning strategy. ULFine mitigates overconfidence through confidence-aware adaptive fitting of textual prototypes and counters biases with complementary fusion of dual logits. The extensive experiments demonstrate that ULFine significantly decreases training costs by over ten times while substantially improving prediction accuracies compared to state-of-the-art methods. <div>
arXiv:2505.05062v1 Announce Type: new 
Abstract: Based on the success of large-scale visual foundation models like CLIP in various downstream tasks, this paper initially attempts to explore their impact on Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation model with three strategies: Linear Probing (LP), Lightweight Fine-Tuning (LFT), and Full Fine-Tuning (FFT). Our analysis presents the following insights: i) Compared to LTSSL algorithms trained from scratch, FFT results in a decline in model performance, whereas LP and LFT, although boosting overall model performance, exhibit negligible benefits to tail classes. ii) LP produces numerous false pseudo-labels due to \textit{underlearned} training data, while LFT can reduce the number of these false labels but becomes overconfident about them owing to \textit{biased fitting} training data. This exacerbates the pseudo-labeled and classifier biases inherent in LTSSL, limiting performance improvement in the tail classes. With these insights, we propose a Unbiased Lightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates the overconfidence via confidence-aware adaptive fitting of textual prototypes and counteracts the pseudo-labeled and classifier biases via complementary fusion of dual logits. Extensive experiments demonstrate that ULFine markedly decreases training costs by over ten times and substantially increases prediction accuracies compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FG-CLIP: Fine-Grained Visual and Textual Alignment</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
<div> Keywords: FG-CLIP, fine-grained understanding, multimodal tasks, long captions, negative samples  

<br /><br />Summary:  
Contrastive Language-Image Pre-training (CLIP) has shown impressive performance in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its reliance on coarse-grained short captions. To improve this, the authors introduce Fine-Grained CLIP (FG-CLIP) featuring three key innovations. First, FG-CLIP utilizes large multimodal models to generate 1.6 billion long caption-image pairs, enhancing the capture of global semantic details. Second, a high-quality dataset is constructed containing 12 million images with 40 million region-specific bounding boxes aligned with detailed captions, promoting precise and context-rich representations. Third, the inclusion of 10 million hard fine-grained negative samples helps the model to better distinguish subtle semantic differences. Corresponding training methodologies are carefully designed to leverage this data. Extensive experiments confirm that FG-CLIP exceeds the performance of the original CLIP and other leading methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These findings underscore FG-CLIP's efficacy in capturing intricate image details and enhancing overall model performance. The related data, code, and models are publicly available at https://github.com/360CVGroup/FG-CLIP. <div>
arXiv:2505.05071v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Affordances: Enabling Robots to Understand Object Functionality</title>
<link>https://arxiv.org/abs/2505.05074</link>
<guid>https://arxiv.org/abs/2505.05074</guid>
<content:encoded><![CDATA[
<div> Keywords: human-robot interaction, affordance prediction, reproducibility, Affordance Sheet, visual perception  

<br /><br />Summary: This work addresses the critical issue of reproducibility in human-robot interaction for assistive technologies, specifically focusing on the prediction of affordances, or the actions a robot can perform on objects. It critiques the varying formulations used for tasks like grasping detection and affordance classification, which can lead to unfair and unreliable benchmarks. To tackle this, a unified approach to visual affordance prediction is proposed. The authors conduct a comprehensive review of previous research, outlining strengths and weaknesses of existing methods and datasets, and identifying factors that hinder reproducibility. In pursuit of enhancing transparency, they introduce the Affordance Sheet, a documentation tool detailing solutions, datasets, and validation processes. Furthermore, they present a framework that links visual affordance prediction to the physical properties of objects, emphasizing that these properties influence robot interactions. By exploring the example of estimating object mass, the paper illustrates how this factor can significantly affect affordance predictions. Ultimately, this approach aims to connect affordance perception and robot actuation, synthesizing all relevant information about objects and their interactions to improve task accomplishment. <div>
arXiv:2505.05074v1 Announce Type: new 
Abstract: Human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. Predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. In this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. To address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. To favour transparency, we introduce the Affordance Sheet, a document to detail the proposed solution, the datasets, and the validation. As the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. Using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. Our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIDiff: Image Customization for Personalized Identities with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05081</link>
<guid>https://arxiv.org/abs/2505.05081</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generation, PIDiff, identity information, diffusion model, StyleGAN

<br /><br />Summary: This paper introduces PIDiff, a novel fine-tuning-based diffusion model for generating personalized identities from text prompts and identity images. It addresses limitations in previous works that struggle with disentangling identity and background information, which often leads to diminished identity characteristics and reduced image diversity. Prior attempts combined the W+ space from StyleGAN with diffusion models to enhance identity representation but suffered from semantic interference during training due to entanglement. PIDiff overcomes these challenges by employing a fine-tuning strategy that leverages the W+ space for accurate feature extraction and localization, thereby avoiding semantic entanglement. Furthermore, PIDiff enables style editing by preserving the identity features across varying levels of detail. It also integrates a cross-attention block and an optimized parameter strategy to maintain both identity preservation and generative capabilities of pre-trained models during inference for in-the-wild images. Experimental results demonstrate the effectiveness of PIDiff in enhancing personalized identity image generation, showing improved accuracy and diversity while accurately reflecting identity characteristics in generated images. <div>
arXiv:2505.05081v1 Announce Type: new 
Abstract: Text-to-image generation for personalized identities aims at incorporating the specific identity into images using a text prompt and an identity image. Based on the powerful generative capabilities of DDPMs, many previous works adopt additional prompts, such as text embeddings and CLIP image embeddings, to represent the identity information, while they fail to disentangle the identity information and background information. As a result, the generated images not only lose key identity characteristics but also suffer from significantly reduced diversity. To address this issue, previous works have combined the W+ space from StyleGAN with diffusion models, leveraging this space to provide a more accurate and comprehensive representation of identity features through multi-level feature extraction. However, the entanglement of identity and background information in in-the-wild images during training prevents accurate identity localization, resulting in severe semantic interference between identity and background. In this paper, we propose a novel fine-tuning-based diffusion model for personalized identities text-to-image generation, named PIDiff, which leverages the W+ space and an identity-tailored fine-tuning strategy to avoid semantic entanglement and achieves accurate feature extraction and localization. Style editing can also be achieved by PIDiff through preserving the characteristics of identity features in the W+ space, which vary from coarse to fine. Through the combination of the proposed cross-attention block and parameter optimization strategy, PIDiff preserves the identity information and maintains the generation capability for in-the-wild images of the pre-trained model during inference. Our experimental results validate the effectiveness of our method in this task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow</title>
<link>https://arxiv.org/abs/2505.05089</link>
<guid>https://arxiv.org/abs/2505.05089</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, optical flow, spatio-temporal, nonlinear motion, unsupervised learning  

<br /><br />Summary: Event cameras capture continuous motion information, making them ideal for optical flow estimation. Existing learning-based methods often employ frame-based techniques, overlooking the spatio-temporal characteristics of events. These methods typically assume linear motion between consecutive events, leading to increased errors in long-time sequences. This study highlights the importance of spatio-temporal information and the need for accurate nonlinear motion modeling for effective optical flow estimation. To address these challenges, we introduce E-NMSTFlow, an unsupervised event-based optical flow network designed for long sequences. Key components of our approach include the Spatio-Temporal Motion Feature Aware (STMFA) module and the Adaptive Motion Feature Enhancement (AMFE) module, both of which leverage rich spatio-temporal data for better associations. Furthermore, we present a nonlinear motion compensation loss that enhances unsupervised learning by accounting for the nonlinear motion among events. Our extensive experiments underscore the effectiveness of E-NMSTFlow, demonstrating its superiority over existing methods. Notably, it achieves the top ranking among unsupervised learning approaches on the MVSEC and DSEC-Flow datasets. For further details, visit our project page at https://wynelio.github.io/E-NMSTFlow. <div>
arXiv:2505.05089v1 Announce Type: new 
Abstract: Event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. However, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. Additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. In this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel unsupervised event-based optical flow network focusing on long-time sequences. We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. Meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. Extensive experiments demonstrate the effectiveness and superiority of our method. Remarkably, our method ranks first among unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project page is available at https://wynelio.github.io/E-NMSTFlow.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions</title>
<link>https://arxiv.org/abs/2505.05091</link>
<guid>https://arxiv.org/abs/2505.05091</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Disparity Estimation, Robustness, Benchmarking, Adversarial Attacks<br /><br />Summary: Deep learning (DL) has outperformed human capabilities in various benchmarks, particularly in disparity estimationan essential task for applications like medical surgeries and autonomous navigation. However, reliance on DL methods for disparity estimation raises concerns due to their vulnerability to distribution shifts and adversarial attacks, which could undermine their reliability and generalization. Despite these challenges, a standardized benchmark to evaluate the robustness of disparity estimation techniques is currently lacking, impeding advancements in this area. To fill this void, we present DispBench, a comprehensive benchmarking tool designed to systematically assess the reliability of disparity estimation methods. DispBench tests robustness against synthetic image corruptions, including adversarial attacks and out-of-distribution shifts stemming from 2D Common Corruptions. This initiates the most extensive performance and robustness analysis of disparity estimation methods conducted to date, revealing significant correlations among accuracy, reliability, and generalization. The open-source code for DispBench is available at https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation. <div>
arXiv:2505.05091v1 Announce Type: new 
Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. One such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. However, DL-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. Despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.
  To address this gap, we introduce DispBench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. DispBench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2D Common Corruptions across multiple datasets and diverse corruption scenarios. We conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. Open-source code for DispBench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05101</link>
<guid>https://arxiv.org/abs/2505.05101</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-object editing, localization, attribute-object mismatch, MDE-Edit, optimization

<br /><br />Summary: The paper addresses the challenges of multi-object editing in complex scenes, particularly when objects overlap or interact. Two main issues are presented: inaccurate localization of target objects leading to incomplete edits and attribute-object mismatches causing semantic conflicts, such as color bleeding. Current methods struggle with these due to attention alignment problems and feature entanglement. To improve upon existing techniques, the authors propose MDE-Edit, a training-free optimization method that operates at the inference stage. This approach focuses on precise localized image manipulation and introduces two key components: Object Alignment Loss (OAL), which aligns multi-layer cross-attention with segmentation masks for better object positioning, and Color Consistency Loss (CCL), which enhances attribute attention for target regions while minimizing leakage to adjacent areas. This dual-loss framework ensures coherent and accurate edits across multiple objects. Experiments showcase that MDE-Edit achieves superior editing accuracy and visual quality compared to state-of-the-art methods, establishing it as an effective solution for complex multi-object image manipulation tasks. <div>
arXiv:2505.05101v1 Announce Type: new 
Abstract: Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation</title>
<link>https://arxiv.org/abs/2505.05136</link>
<guid>https://arxiv.org/abs/2505.05136</guid>
<content:encoded><![CDATA[
<div> Keywords: subglottic stenosis, bronchoscopy, automated evaluation, 3D model, dataset  

<br /><br />Summary: Subglottic stenosis involves the narrowing of the airway between the vocal cords and the trachea, typically assessed by estimating the percentage of obstruction through CT data or expert visual inspections, which can be subjective. This study proposes a novel automated pipeline for estimating subglottic stenosis severity during bronchoscopy without requiring the physician to navigate through the stenosed region. The method utilizes the physical effect of illumination decline in endoscopy to segment and track the airway lumen, generating a 3D model from a single frame to measure airway narrowing. Notably, this pipeline is the first of its kind for automated evaluation of subglottic stenosis severity using bronchoscopy images. Results demonstrate high consistency with ground-truth estimations from CT scans and expert assessments while showing reliable repeatability in multiple estimations for the same patient. Evaluation is performed using a newly created Subglottic Stenosis Dataset comprising real bronchoscopy procedures. This approach aims to facilitate quicker diagnoses and monitoring, minimizing radiation exposure for patients by eliminating the need for CT scans, and introduces the first public benchmark for assessing subglottic stenosis severity. <div>
arXiv:2505.05136v1 Announce Type: new 
Abstract: Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the airway between the vocal cords and the trachea. Its severity is typically evaluated by estimating the percentage of obstructed airway. This estimation can be obtained from CT data or through visual inspection by experts exploring the region. However, visual inspections are inherently subjective, leading to less consistent and robust diagnoses. No public methods or datasets are currently available for automated evaluation of this condition from bronchoscopy video.
  Methods: We propose a pipeline for automated subglottic stenosis severity estimation during the bronchoscopy exploration, without requiring the physician to traverse the stenosed region. Our approach exploits the physical effect of illumination decline in endoscopy to segment and track the lumen and obtain a 3D model of the airway. This 3D model is obtained from a single frame and is used to measure the airway narrowing.
  Results: Our pipeline is the first to enable automated and robust subglottic stenosis severity measurement using bronchoscopy images. The results show consistency with ground-truth estimations from CT scans and expert estimations, and reliable repeatability across multiple estimations on the same patient. Our evaluation is performed on our new Subglottic Stenosis Dataset of real bronchoscopy procedures data.
  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis severity using only bronchoscopy. Our approach can assist with and shorten diagnosis and monitoring procedures, with automated and repeatable estimations and less exploration time, and save radiation exposure to patients as no CT is required. Additionally, we release the first public benchmark for subglottic stenosis severity assessment.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models</title>
<link>https://arxiv.org/abs/2505.05163</link>
<guid>https://arxiv.org/abs/2505.05163</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, probabilistic embeddings, Gaussian Process, uncertainty calibration, cross-modal retrieval

<br /><br />Summary: Vision-Language Models (VLMs) aim to create joint representations by linking images and text within a shared latent space. However, traditional VLMs often face challenges in capturing the uncertainties related to ambiguities in visual and textual descriptions. These challenges arise from the multiple possible connections between images and texts. Current methods that address this issue typically require extensive datasets and do not fully utilize the robust representations learned by existing large-scale VLMs like CLIP. To overcome these limitations, this paper introduces GroVE, a novel post-hoc method for deriving probabilistic embeddings from fixed VLMs. GroVE employs a Gaussian Process Latent Variable Model (GPLVM) to construct a coherent low-dimensional latent space, effectively mapping image and text inputs into a unified representation. The optimization is based on objectives focused on single-modal embedding reconstruction and cross-modal alignment. After training, the resulting Gaussian Process model is capable of generating uncertainty-aware probabilistic embeddings. Evaluation results demonstrate that GroVE achieves state-of-the-art uncertainty calibration performance across various downstream tasks, such as cross-modal retrieval, visual question answering, and active learning. <div>
arXiv:2505.05163v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) learn joint representations by mapping images and text into a shared latent space. However, recent research highlights that deterministic embeddings from standard VLMs often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. Existing approaches tackle this by learning probabilistic embeddings during VLM training, which demands large datasets and does not leverage the powerful representations already learned by large-scale VLMs like CLIP. In this paper, we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model (GPLVM) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. Once trained, the Gaussian Process model generates uncertainty-aware probabilistic embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting</title>
<link>https://arxiv.org/abs/2505.05183</link>
<guid>https://arxiv.org/abs/2505.05183</guid>
<content:encoded><![CDATA[
arXiv:2505.05183v1 Announce Type: new 
Abstract: The safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving Teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). While previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. In this research, we unveil PaniCar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. This vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. In addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3, "manufacturer C", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors (YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. We also evaluate four SOTA flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. To mitigate this risk, we propose Caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster RCNN, Caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. In addition, Caracetamol is capable of processing frames at a rate of between 30-50 FPS, enabling real-time ADAS car detection.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.05189</link>
<guid>https://arxiv.org/abs/2505.05189</guid>
<content:encoded><![CDATA[
arXiv:2505.05189v1 Announce Type: new 
Abstract: Prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (VLMs) to the biomedical image classification tasks in few shot scenarios. However, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. In this work, we propose Biomed-DPT, a knowledge-enhanced dual modality prompt tuning technique. In designing the text prompt, Biomed-DPT constructs a dual prompt including the template-driven clinical prompts and the large language model (LLM)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. In designing the vision prompt, Biomed-DPT introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. Biomed-DPT achieves an average classification accuracy of 66.14\% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06\% in base classes and 75.97\% in novel classes, surpassing the Context Optimization (CoOp) method by 6.20\%, 3.78\%, and 8.04\%, respectively. Our code are available at \underline{https://github.com/Kanyooo/Biomed-DPT}.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution</title>
<link>https://arxiv.org/abs/2505.05209</link>
<guid>https://arxiv.org/abs/2505.05209</guid>
<content:encoded><![CDATA[
arXiv:2505.05209v1 Announce Type: new 
Abstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HQC-NBV: A Hybrid Quantum-Classical View Planning Approach</title>
<link>https://arxiv.org/abs/2505.05212</link>
<guid>https://arxiv.org/abs/2505.05212</guid>
<content:encoded><![CDATA[
arXiv:2505.05212v1 Announce Type: new 
Abstract: Efficient view planning is a fundamental challenge in computer vision and robotic perception, critical for tasks ranging from search and rescue operations to autonomous navigation. While classical approaches, including sampling-based and deterministic methods, have shown promise in planning camera viewpoints for scene exploration, they often struggle with computational scalability and solution optimality in complex settings. This study introduces HQC-NBV, a hybrid quantum-classical framework for view planning that leverages quantum properties to efficiently explore the parameter space while maintaining robustness and scalability. We propose a specific Hamiltonian formulation with multi-component cost terms and a parameter-centric variational ansatz with bidirectional alternating entanglement patterns that capture the hierarchical dependencies between viewpoint parameters. Comprehensive experiments demonstrate that quantum-specific components provide measurable performance advantages. Compared to the classical methods, our approach achieves up to 49.2% higher exploration efficiency across diverse environments. Our analysis of entanglement architecture and coherence-preserving terms provides insights into the mechanisms of quantum advantage in robotic exploration tasks. This work represents a significant advancement in integrating quantum computing into robotic perception systems, offering a paradigm-shifting solution for various robot vision tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model Quantization: A Review</title>
<link>https://arxiv.org/abs/2505.05215</link>
<guid>https://arxiv.org/abs/2505.05215</guid>
<content:encoded><![CDATA[
arXiv:2505.05215v1 Announce Type: new 
Abstract: Recent success of large text-to-image models has empirically underscored the exceptional performance of diffusion models in generative tasks. To facilitate their efficient deployment on resource-constrained edge devices, model quantization has emerged as a pivotal technique for both compression and acceleration. This survey offers a thorough review of the latest advancements in diffusion model quantization, encapsulating and analyzing the current state of the art in this rapidly advancing domain. First, we provide an overview of the key challenges encountered in the quantization of diffusion models, including those based on U-Net architectures and Diffusion Transformers (DiT). We then present a comprehensive taxonomy of prevalent quantization techniques, engaging in an in-depth discussion of their underlying principles. Subsequently, we perform a meticulous analysis of representative diffusion model quantization schemes from both qualitative and quantitative perspectives. From a quantitative standpoint, we rigorously benchmark a variety of methods using widely recognized datasets, delivering an extensive evaluation of the most recent and impactful research in the field. From a qualitative standpoint, we categorize and synthesize the effects of quantization errors, elucidating these impacts through both visual analysis and trajectory examination. In conclusion, we outline prospective avenues for future research, proposing novel directions for the quantization of generative models in practical applications. The list of related papers, corresponding codes, pre-trained models and comparison results are publicly available at the survey project homepage https://github.com/TaylorJocelyn/Diffusion-Model-Quantization.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does CLIP perceive art the same way we do?</title>
<link>https://arxiv.org/abs/2505.05229</link>
<guid>https://arxiv.org/abs/2505.05229</guid>
<content:encoded><![CDATA[
arXiv:2505.05229v1 Announce Type: new 
Abstract: CLIP has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it "see" the same way humans do - especially when interpreting artworks? In this paper, we investigate CLIP's ability to extract high-level semantic and stylistic information from paintings, including both human-created and AI-generated imagery. We evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. By designing targeted probing tasks and comparing CLIP's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. Our findings reveal both strengths and limitations in CLIP's visual representations, particularly in relation to aesthetic cues and artistic intent. We further discuss the implications of these insights for using CLIP as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. Our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADriver: Towards Personalized Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.05240</link>
<guid>https://arxiv.org/abs/2505.05240</guid>
<content:encoded><![CDATA[
arXiv:2505.05240v1 Announce Type: new 
Abstract: In this paper, we propose PADriver, a novel closed-loop framework for personalized autonomous driving (PAD). Built upon Multi-modal Large Language Model (MLLM), PADriver takes streaming frames and personalized textual prompts as inputs. It autoaggressively performs scene understanding, danger level estimation and action decision. The predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. Moreover, we construct a closed-loop benchmark named PAD-Highway based on Highway-Env simulator to comprehensively evaluate the decision performance under traffic rules. The dataset contains 250 hours videos with high-quality annotation to facilitate the development of PAD behavior analysis. Experimental results on the constructed benchmark show that PADriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes</title>
<link>https://arxiv.org/abs/2505.05288</link>
<guid>https://arxiv.org/abs/2505.05288</guid>
<content:encoded><![CDATA[
arXiv:2505.05288v1 Announce Type: new 
Abstract: We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining</title>
<link>https://arxiv.org/abs/2505.05307</link>
<guid>https://arxiv.org/abs/2505.05307</guid>
<content:encoded><![CDATA[
arXiv:2505.05307v1 Announce Type: new 
Abstract: Event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. Existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. In this paper, we propose PRE-Mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. Our framework introduces a 4D event cloud representation that integrates dual temporal scales to preserve high temporal precision, a Spatio-Temporal Decoupling and Fusion module (STDF) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a Multi-Scale State Space Model (MS3M) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. Enhanced by frequency-domain regularization, PRE-Mamba achieves superior performance (0.95 SR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a comprehensive dataset with labeled synthetic and real-world sequences. Moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects</title>
<link>https://arxiv.org/abs/2505.05318</link>
<guid>https://arxiv.org/abs/2505.05318</guid>
<content:encoded><![CDATA[
arXiv:2505.05318v1 Announce Type: new 
Abstract: The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery</title>
<link>https://arxiv.org/abs/2505.05321</link>
<guid>https://arxiv.org/abs/2505.05321</guid>
<content:encoded><![CDATA[
arXiv:2505.05321v1 Announce Type: new 
Abstract: Accurate building segmentation from high-resolution RGB imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. In this study, we present a comprehensive deep learning framework for multiscale building segmentation using RGB aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including Principal Component Analysis (PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index (MBI), and Sobel edge filters from RGB channels. These features guide a Res-U-Net architecture in learning complex spatial patterns more effectively. We also propose training policies incorporating layer freezing, cyclical learning rates, and SuperConvergence to reduce training time and resource usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of 0.80, outperforming existing RGB-based benchmarks. This study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aesthetics Without Semantics</title>
<link>https://arxiv.org/abs/2505.05331</link>
<guid>https://arxiv.org/abs/2505.05331</guid>
<content:encoded><![CDATA[
arXiv:2505.05331v1 Announce Type: new 
Abstract: While it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. Furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. We address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. The resulting Minimum Semantic Content (MSC) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. We next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. Taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>