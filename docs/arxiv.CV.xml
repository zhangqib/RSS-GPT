<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera</title>
<link>https://arxiv.org/abs/2505.03093</link>
<guid>https://arxiv.org/abs/2505.03093</guid>
<content:encoded><![CDATA[
<div> LiDAR, forest inventories, DBH measurement, 360 video camera, photogrammetry <br>
Summary: <br>
The study presents a low-cost alternative to LiDAR-based techniques for accurate DBH measurement in forest inventories using a consumer-grade 360 video camera. Their pipeline includes dense point cloud reconstruction via photogrammetry software, trunk segmentation using SAM masks, and DBH estimation using a RANSAC-based technique. Their method achieved a median absolute relative error of 5-9% compared to manual measurements, only slightly higher than LiDAR-based estimates. The use of a single 360 camera significantly reduces costs and operational complexity, making it a feasible option for ecological monitoring and resource management in forest environments. They also introduced an interactive visualization tool for inspecting segmented trees and their estimated DBH. <div>
arXiv:2505.03093v2 Announce Type: replace 
Abstract: Forest inventories rely on accurate measurements of the diameter at breast height (DBH) for ecological monitoring, resource management, and carbon accounting. While LiDAR-based techniques can achieve centimeter-level precision, they are cost-prohibitive and operationally complex. We present a low-cost alternative that only needs a consumer-grade 360 video camera. Our semi-automated pipeline comprises of (i) a dense point cloud reconstruction using Structure from Motion (SfM) photogrammetry software called Agisoft Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based technique to estimate cross section shape and DBH. We introduce an interactive visualization tool for inspecting segmented trees and their estimated DBH. On 61 acquisitions of 43 trees under a variety of conditions, our method attains median absolute relative errors of 5-9% with respect to "ground-truth" manual measurements. This is only 2-4% higher than LiDAR-based estimates, while employing a single 360 camera that costs orders of magnitude less, requires minimal setup, and is widely available.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium</title>
<link>https://arxiv.org/abs/2505.09746</link>
<guid>https://arxiv.org/abs/2505.09746</guid>
<content:encoded><![CDATA[
<div> 4D Flow MRI, left atrium, hemodynamics, computational framework, prognostic biomarkers
Summary:
The study introduces an open-source computational framework for analyzing 4D Flow MRI in the left atrium. The framework allows qualitative and quantitative analysis of hemodynamic parameters with high accuracy, even with data from different centers. It demonstrates robustness in automated segmentations and can analyze a spectrum of disorders. The research aims to investigate the potential of energy, vorticity, and pressure parameters in the left atrium as prognostic biomarkers. <div>
arXiv:2505.09746v1 Announce Type: new 
Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular filling, but our comprehension of its hemodynamics is significantly limited by the constraints of conventional ultrasound analysis. 4D flow magnetic resonance imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial hemodynamics. However, the low velocities within the LA and the limited spatial resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore, the absence of dedicated computational frameworks, combined with diverse acquisition protocols and vendors, complicates gathering large cohorts for studying the prognostic value of hemodynamic parameters provided by 4D Flow MRI. In this study, we introduce the first open-source computational framework tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive qualitative and quantitative analysis of advanced hemodynamic parameters. Our framework proves robust to data from different centers of varying quality, producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95 $<$ 3 mm), even with limited training data. Additionally, we conducted the first comprehensive assessment of energy, vorticity, and pressure parameters in the LA across a spectrum of disorders to investigate their potential as prognostic biomarkers.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyadic Mamba: Long-term Dyadic Human Motion Synthesis</title>
<link>https://arxiv.org/abs/2505.09827</link>
<guid>https://arxiv.org/abs/2505.09827</guid>
<content:encoded><![CDATA[
<div> Keywords: dyadic human motion, State-Space Models, transformer-based approaches, long-term motion synthesis, benchmark 

Summary:
Dyadic Mamba introduces a novel approach utilizing State-Space Models (SSMs) for generating realistic dyadic human motion from text descriptions. It addresses the limitations of transformer-based methods by facilitating information flow between individual motion sequences through concatenation, eliminating the need for complex cross-attention mechanisms. The method achieves competitive performance on short-term benchmarks and outperforms transformer-based approaches on longer sequences. It also proposes a new benchmark for evaluating long-term motion synthesis quality, providing a standardized framework for future research. Overall, Dyadic Mamba showcases the effectiveness of SSM-based architectures in tackling the challenging task of long-term dyadic human motion synthesis, showcasing promising results for generating high-quality motion sequences of arbitrary lengths. 

<br /><br />Summary: <div>
arXiv:2505.09827v1 Announce Type: new 
Abstract: Generating realistic dyadic human motion from text descriptions presents significant challenges, particularly for extended interactions that exceed typical training sequence lengths. While recent transformer-based approaches have shown promising results for short-term dyadic motion synthesis, they struggle with longer sequences due to inherent limitations in positional encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach that leverages State-Space Models (SSMs) to generate high-quality dyadic human motion of arbitrary length. Our method employs a simple yet effective architecture that facilitates information flow between individual motion sequences through concatenation, eliminating the need for complex cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves competitive performance on standard short-term benchmarks while significantly outperforming transformer-based approaches on longer sequences. Additionally, we propose a new benchmark for evaluating long-term motion synthesis quality, providing a standardized framework for future research. Our results demonstrate that SSM-based architectures offer a promising direction for addressing the challenging task of long-term dyadic human motion synthesis from text descriptions.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes</title>
<link>https://arxiv.org/abs/2505.09829</link>
<guid>https://arxiv.org/abs/2505.09829</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, semi-supervised methods, organ boundary prediction, annotation costs, computational efficiency

Summary:
BoundarySeg is a novel approach for medical image segmentation that improves accuracy without the need for unannotated data. This multi-task framework incorporates organ boundary prediction as an auxiliary task to full organ segmentation, utilizing the consistency between the two task predictions for additional supervision. Unlike traditional semi-supervised methods, BoundarySeg does not rely on scarce unannotated data, making it a cost-effective solution. The proposed approach achieves performance comparable to or exceeding state-of-the-art methods while maintaining computational efficiency. By leveraging existing annotations, BoundarySeg overcomes the limitations of low data regimes, offering a practical solution for large-scale medical data analysis. The code for BoundarySeg will be made available upon acceptance for further research and implementation. 

<br /><br />Summary: <div>
arXiv:2505.09829v1 Announce Type: new 
Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging due to stringent privacy regulations and data protection policies. In addition, annotating medical images requires that domain experts manually delineate anatomical structures, making the process both time-consuming and costly. As a result, semi-supervised methods have gained popularity for reducing annotation costs. However, the performance of semi-supervised methods is heavily dependent on the availability of unannotated data, and their effectiveness declines when such data are scarce or absent. To overcome this limitation, we propose a simple, yet effective and computationally efficient approach for medical image segmentation that leverages only existing annotations. We propose BoundarySeg , a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between the two task predictions to provide additional supervision. This strategy improves segmentation accuracy, especially in low data regimes, allowing our method to achieve performance comparable to or exceeding state-of-the-art semi supervised approaches all without relying on unannotated data or increasing computational demands. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.09858</link>
<guid>https://arxiv.org/abs/2505.09858</guid>
<content:encoded><![CDATA[
<div> Keywords: computer-assisted interventions, deep learning, data imbalance, surgical videos, synthetic generation

Summary: 
Computer-assisted interventions benefit from deep learning methods leveraging surgical video data. However, data imbalance within these datasets presents challenges for model development. To address this, a novel two-stage text-conditioned diffusion-based method is proposed to generate high-quality surgical videos for under-represented classes. The approach utilizes a 2D latent diffusion model for spatial content and integrates temporal attention layers for consistency. A rejection sampling strategy selects suitable synthetic samples, effectively augmenting datasets to mitigate class imbalance issues. Evaluation on surgical action recognition and intra-operative event prediction tasks demonstrates significant performance improvements with the incorporation of synthetic videos generated by the proposed method. The implementation is open-sourced for further research and application. 

Summary: <br /><br />Computer-assisted interventions can benefit from deep learning methods using surgical videos. However, data imbalance in these datasets poses challenges for model development. To address this issue, a novel two-stage text-conditioned diffusion-based method is proposed to generate high-quality surgical videos for under-represented classes. The approach uses a 2D latent diffusion model for spatial content and integrates temporal attention layers for consistency. A rejection sampling strategy selects suitable synthetic samples, effectively augmenting datasets to mitigate class imbalance issues. Evaluation on surgical action recognition and intra-operative event prediction tasks demonstrates significant performance improvements with the incorporation of synthetic videos generated by the proposed method. The implementation is open-sourced for further research and application. <div>
arXiv:2505.09858v1 Announce Type: new 
Abstract: Computer-assisted interventions can improve intra-operative guidance, particularly through deep learning methods that harness the spatiotemporal information in surgical videos. However, the severe data imbalance often found in surgical video datasets hinders the development of high-performing models. In this work, we aim to overcome the data imbalance by synthesizing surgical videos. We propose a unique two-stage, text-conditioned diffusion-based method to generate high-fidelity surgical videos for under-represented classes. Our approach conditions the generation process on text prompts and decouples spatial and temporal modeling by utilizing a 2D latent diffusion model to capture spatial content and then integrating temporal attention layers to ensure temporal consistency. Furthermore, we introduce a rejection sampling strategy to select the most suitable synthetic samples, effectively augmenting existing datasets to address class imbalance. We evaluate our method on two downstream tasks-surgical action recognition and intra-operative event prediction-demonstrating that incorporating synthetic videos from our approach substantially enhances model performance. We open-source our implementation at https://gitlab.com/nct_tso_public/surgvgen.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction</title>
<link>https://arxiv.org/abs/2505.09859</link>
<guid>https://arxiv.org/abs/2505.09859</guid>
<content:encoded><![CDATA[
<div> Keywords: compositional concept learning, structured representations, analogical mapping, schema, deep learning 

Summary: 
Probabilistic Schema Induction (PSI) is a new model that uses deep learning to learn compositional visual concepts from limited examples. It operates on structured representations of examples, employing analogical mapping to identify shared relational structures across examples and create schemas. By considering both object-level and relational similarity, PSI outperforms models using unstructured feature vectors. Its success lies in amplifying relations relevant to classification, similar to selective attention mechanisms in traditional models. The model's adaptive strategy prioritizes relational similarity over object-level similarity and gives weight to distinguishing relations. This demonstrates the importance of structured representations and analogical mapping in quickly learning complex concepts. Overall, PSI's human-like performance showcases the potential of leveraging deep learning to create psychologically plausible models. 

<br /><br />Summary: <div>
arXiv:2505.09859v1 Announce Type: new 
Abstract: The ability to learn new visual concepts from limited examples is a hallmark of human cognition. While traditional category learning models represent each example as an unstructured feature vector, compositional concept learning is thought to depend on (1) structured representations of examples (e.g., directed graphs consisting of objects and their relations) and (2) the identification of shared relational structure across examples through analogical mapping. Here, we introduce Probabilistic Schema Induction (PSI), a prototype model that employs deep learning to perform analogical mapping over structured representations of only a handful of examples, forming a compositional concept called a schema. In doing so, PSI relies on a novel conception of similarity that weighs object-level similarity and relational similarity, as well as a mechanism for amplifying relations relevant to classification, analogous to selective attention parameters in traditional models. We show that PSI produces human-like learning performance and outperforms two controls: a prototype model that uses unstructured feature vectors extracted from a deep learning model, and a variant of PSI with weaker structured representations. Notably, we find that PSI's human-like performance is driven by an adaptive strategy that increases relational similarity over object-level similarity and upweights the contribution of relations that distinguish classes. These findings suggest that structured representations and analogical mapping are critical to modeling rapid human-like learning of compositional visual concepts, and demonstrate how deep learning can be leveraged to create psychological models.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Gaussian Splatting SLAM</title>
<link>https://arxiv.org/abs/2505.09915</link>
<guid>https://arxiv.org/abs/2505.09915</guid>
<content:encoded><![CDATA[
<div> NeRF, 3DGS, visual SLAM, LSG-SLAM, large-scale<br />
Summary:<br />
The paper introduces LSG-SLAM, a large-scale 3DGS-based visual SLAM system using stereo cameras. It utilizes a multi-modality strategy for estimating poses under large view changes and feature-alignment warping constraints for tracking. LSG-SLAM addresses scalability with continuous Gaussian Splatting submaps for unbounded scenes. Loop closure is performed through place recognition and pose optimization. Global optimization and structure refinement enhance reconstruction quality. Evaluations on EuRoc and KITTI datasets show LSG-SLAM outperforms existing methods. The project page is available at https://lsg-slam.github.io.<br /> <div>
arXiv:2505.09915v1 Announce Type: new 
Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown encouraging and impressive results for visual SLAM. However, most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored. This paper introduces a large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. In tracking, we introduce feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. For the scalability of large-scale scenarios, we introduce continuous Gaussian Splatting submaps to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. After the global optimization of camera poses and Gaussian points, a structure refinement module enhances the reconstruction quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches. Project page: https://lsg-slam.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.09926</link>
<guid>https://arxiv.org/abs/2505.09926</guid>
<content:encoded><![CDATA[
<div> Keywords: Universal visual anomaly detection, AdaptCLIP, zero/few-shot generalization, comparative learning, industrial and medical domains

Summary: 
The article introduces AdaptCLIP, a novel method for universal visual anomaly detection that does not require additional fine-tuning in open scenarios. AdaptCLIP learns adaptive visual and textual representations alternately, incorporating both contextual and aligned residual features in comparative learning between query and normal image prompt. The method adds visual, textual, and prompt-query adapters to CLIP models, enabling zero/few-shot generalization across domains. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks in industrial and medical domains, surpassing existing methods. The approach is simple yet effective, supporting a training-free manner on target domains once trained on a base dataset. The code and model for AdaptCLIP will be made available on GitHub at https://github.com/gaobb/AdaptCLIP. 

Summary: <br /><br />Keywords: Universal visual anomaly detection, AdaptCLIP, zero/few-shot generalization, comparative learning, industrial and medical domains <div>
arXiv:2505.09926v1 Announce Type: new 
Abstract: Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.09927</link>
<guid>https://arxiv.org/abs/2505.09927</guid>
<content:encoded><![CDATA[
<div> domain adaptation, source-free domain adaptation, medical datasets, style translation, pseudo-labels

Summary:
- The paper introduces a novel framework for source-free domain adaptation (SFDA) in medical datasets to improve model performance in the presence of domain gaps.
- The proposed approach includes preadaptation to generate a preadapted model, providing an initialization for the target model and producing high-quality enhanced pseudo-labels without adding extra parameters.
- A data-dependent frequency prompt is used to translate target domain images into a source-like style more effectively.
- The framework employs a style-related layer fine-tuning strategy designed for SFDA to train the target model using prompted target domain images and pseudo-labels.
- Experimental results on cross-modality abdominal and cardiac SFDA segmentation tasks demonstrate that the proposed method surpasses existing state-of-the-art approaches. 

<br /><br />Summary: <div>
arXiv:2505.09927v1 Announce Type: new 
Abstract: Domain adaptation addresses the challenge of model performance degradation caused by domain gaps. In the typical setup for unsupervised domain adaptation, labeled data from a source domain and unlabeled data from a target domain are used to train a target model. However, access to labeled source domain data, particularly in medical datasets, can be restricted due to privacy policies. As a result, research has increasingly shifted to source-free domain adaptation (SFDA), which requires only a pretrained model from the source domain and unlabeled data from the target domain data for adaptation. Existing SFDA methods often rely on domain-specific image style translation and self-supervision techniques to bridge the domain gap and train the target domain model. However, the quality of domain-specific style-translated images and pseudo-labels produced by these methods still leaves room for improvement. Moreover, training the entire model during adaptation can be inefficient under limited supervision. In this paper, we propose a novel SFDA framework to address these challenges. Specifically, to effectively mitigate the impact of domain gap in the initial training phase, we introduce preadaptation to generate a preadapted model, which serves as an initialization of target model and allows for the generation of high-quality enhanced pseudo-labels without introducing extra parameters. Additionally, we propose a data-dependent frequency prompt to more effectively translate target domain images into a source-like style. To further enhance adaptation, we employ a style-related layer fine-tuning strategy, specifically designed for SFDA, to train the target model using the prompted target domain images and pseudo-labels. Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks demonstrate that our proposed method outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety</title>
<link>https://arxiv.org/abs/2505.09935</link>
<guid>https://arxiv.org/abs/2505.09935</guid>
<content:encoded><![CDATA[
<div> Intersection Safety, Vulnerable Road Users, Crossing Intentions, Predictive Model, Real-time Inference Speed
Summary: 
The VRU-CIPI framework utilizes a sequential attention-based model to predict Vulnerable Road Users' (VRUs) crossing intentions at urban intersections. By incorporating a Gated Recurrent Unit (GRU) and a multi-head Transformer self-attention mechanism, the model captures temporal dynamics and spatial dependencies crucial for accurate predictions. Tested on the UCF-VRU dataset, VRU-CIPI achieved a high accuracy of 96.45% and demonstrated real-time inference speeds of up to 33 frames per second. Additionally, by integrating with Infrastructure-to-Vehicles (I2V) communication, the framework can enhance intersection safety by activating crossing signals and providing early warnings to connected vehicles. This proactive approach ensures smoother and safer interactions for all road users. 
<br /><br />Summary: <div>
arXiv:2505.09935v1 Announce Type: new 
Abstract: Understanding and predicting human behavior in-thewild, particularly at urban intersections, remains crucial for enhancing interaction safety between road users. Among the most critical behaviors are crossing intentions of Vulnerable Road Users (VRUs), where misinterpretation may result in dangerous conflicts with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a sequential attention-based model designed to predict VRU crossing intentions at intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal dynamics in VRU movements, combined with a multi-head Transformer self-attention mechanism to encode contextual and spatial dependencies critical for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed achieves state-of-the-art performance with an accuracy of 96.45% and achieving real-time inference speed reaching 33 frames per second. Furthermore, by integrating with Infrastructure-to-Vehicles (I2V) communication, our approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles, ensuring smoother and safer interactions for all road users.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.09939</link>
<guid>https://arxiv.org/abs/2505.09939</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, change detection, non-registration, disasters, dataset

Summary: 
In this study, the authors introduce a new remote sensing change detection task called non-registration change detection, aimed at addressing the challenges posed by emergencies such as natural disasters, human-made accidents, and military actions. The study identifies eight scenarios that could lead to non-registration issues, highlighting the need for specialized approaches. The authors propose unique image transformation methods tailored to these scenarios to convert registration change detection datasets into non-registration versions. Through experiments, they show that non-registration change detection can significantly impact the performance of state-of-the-art methods. The code and dataset for the study are publicly available, allowing for further exploration and validation of the proposed techniques.

<br /><br />Summary: <div>
arXiv:2505.09939v1 Announce Type: new 
Abstract: In this study, we propose a novel remote sensing change detection task, non-registration change detection, to address the increasing number of emergencies such as natural disasters, anthropogenic accidents, and military strikes. First, in light of the limited discourse on the issue of non-registration change detection, we systematically propose eight scenarios that could arise in the real world and potentially contribute to the occurrence of non-registration problems. Second, we develop distinct image transformation schemes tailored to various scenarios to convert the available registration change detection dataset into a non-registration version. Finally, we demonstrate that non-registration change detection can cause catastrophic damage to the state-of-the-art methods. Our code and dataset are available at https://github.com/ShanZard/NRCD.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2505.09943</link>
<guid>https://arxiv.org/abs/2505.09943</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared small target detection, contour-aware, saliency priors, surround-convergent prior extraction, feature fusion 

Summary: 
The paper introduces a contour-aware and saliency priors embedding network (CSPENet) for improving infrared small target detection (ISTD) performance. The proposed network includes a surround-convergent prior extraction module (SCPEM) to capture target contour information effectively. This module extracts boosted saliency and multi-scale structural priors for accurate localization and detailed representation. A dual-branch priors embedding architecture (DBPEA) is employed to fuse these priors at optimal network positions. An attention-guided feature enhancement module (AGFEM) refines feature representations and enhances saliency estimation accuracy. Experimental results show that CSPENet outperforms existing methods in ISTD on public datasets. The code for CSPENet is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.09943v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range of civilian and military applications. Existing methods suffer from deficiencies in the localization of dim targets and the perception of contour information under dense clutter environments, severely limiting their detection performance. To tackle these issues, we propose a contour-aware and saliency priors embedding network (CSPENet) for ISTD. We first design a surround-convergent prior extraction module (SCPEM) that effectively captures the intrinsic characteristic of target contour pixel gradients converging toward their center. This module concurrently extracts two collaborative priors: a boosted saliency prior for accurate target localization and multi-scale structural priors for comprehensively enriching contour detail representation. Building upon this, we propose a dual-branch priors embedding architecture (DBPEA) that establishes differentiated feature fusion pathways, embedding these two priors at optimal network positions to achieve performance enhancement. Finally, we develop an attention-guided feature enhancement module (AGFEM) to refine feature representations and improve saliency estimation accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art methods in detection performance. The code is available at https://github.com/IDIP2025/CSPENet.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction</title>
<link>https://arxiv.org/abs/2505.09965</link>
<guid>https://arxiv.org/abs/2505.09965</guid>
<content:encoded><![CDATA[
<div> Keywords: precision medicine, disease progression, spatio-temporal dynamics, MambaControl, Alzheimer's disease prediction

Summary:
MambaControl is a new framework designed for modelling disease progression in precision medicine. It addresses the challenge of capturing complex spatio-temporal dynamics while maintaining anatomical integrity. The framework integrates selective state-space modelling with diffusion processes to predict medical image trajectories accurately. MambaControl combines Mamba-based long-range modelling with graph-guided anatomical control to represent anatomical correlations effectively. It also utilizes Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail. The framework achieves state-of-the-art performance in predicting Alzheimer's disease progression, demonstrating improved prediction quality and anatomical fidelity. MambaControl has the potential to provide personalised prognosis and support clinical decision-making in the context of progressive disorders. <br /><br />Summary: MambaControl is a novel framework that integrates selective state-space modelling and diffusion processes for accurate prediction of medical image trajectories. It effectively captures subtle structural changes over time while maintaining anatomical consistency. The framework combines long-range modelling with graph-guided anatomical control and Fourier-enhanced spectral graph representations to achieve superior performance in Alzheimer's disease prediction. Its advancements in progression prediction quality and anatomical fidelity highlight its potential for personalised prognosis and clinical decision support in precision medicine. <div>
arXiv:2505.09965v1 Announce Type: new 
Abstract: Modelling disease progression in precision medicine requires capturing complex spatio-temporal dynamics while preserving anatomical integrity. Existing methods often struggle with longitudinal dependencies and structural consistency in progressive disorders. To address these limitations, we introduce MambaControl, a novel framework that integrates selective state-space modelling with diffusion processes for high-fidelity prediction of medical image trajectories. To better capture subtle structural changes over time while maintaining anatomical consistency, MambaControl combines Mamba-based long-range modelling with graph-guided anatomical control to more effectively represent anatomical correlations. Furthermore, we introduce Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail, enabling MambaControl to achieve state-of-the-art performance in Alzheimer's disease prediction. Quantitative and regional evaluations demonstrate improved progression prediction quality and anatomical fidelity, highlighting its potential for personalised prognosis and clinical decision support.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2505.09967</link>
<guid>https://arxiv.org/abs/2505.09967</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial expression recognition, Texture Key Driver Factors, Texture-Aware Feature Extractor, Dual Contextual Information Filtering, State-of-the-art performance

Summary: 
Texture Key Driver Factors (TKDF) play a crucial role in facial expression recognition (FER) by capturing localized texture regions that exhibit strong discriminative power for different emotional categories. The proposed framework introduces a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF) to effectively extract and refine texture cues in facial images. TAFE uses a ResNet-based backbone with multi-branch attention to extract fine-grained texture representations, while DCIF refines these features with adaptive pooling and attention mechanisms. Experimental results on RAF-DB and KDEF datasets show that the method achieves state-of-the-art performance in FER. By incorporating TKDFs into FER pipelines, the framework demonstrates effectiveness and robustness in capturing subtle facial expression cues for accurate emotion recognition. 

Summary: <div>
arXiv:2505.09967v1 Announce Type: new 
Abstract: Facial expression recognition (FER) in the wild remains a challenging task due to the subtle and localized nature of expression-related features, as well as the complex variations in facial appearance. In this paper, we introduce a novel framework that explicitly focuses on Texture Key Driver Factors (TKDF), localized texture regions that exhibit strong discriminative power across emotional categories. By carefully observing facial image patterns, we identify that certain texture cues, such as micro-changes in skin around the brows, eyes, and mouth, serve as primary indicators of emotional dynamics. To effectively capture and leverage these cues, we propose a FER architecture comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced with multi-branch attention to extract fine-grained texture representations, while DCIF refines these features by filtering context through adaptive pooling and attention mechanisms. Experimental results on RAF-DB and KDEF datasets demonstrate that our method achieves state-of-the-art performance, verifying the effectiveness and robustness of incorporating TKDFs into FER pipelines.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2505.09971</link>
<guid>https://arxiv.org/abs/2505.09971</guid>
<content:encoded><![CDATA[
<div> Keywords: Airborne laser scanning, Point cloud segmentation, Continuous Test-Time Adaptation, Entropy-based consistency loss, Benchmarks 

Summary: 
The paper introduces APCoTTA, a novel method for Continuous Test-Time Adaptation tailored for ALS point cloud segmentation. It addresses challenges such as domain shifts and error accumulation during adaptation by incorporating a dynamic trainable layer selection module, an entropy-based consistency loss, and a random parameter interpolation mechanism. These techniques help improve model stability and balance target adaptation with source knowledge retention. The authors also introduce two benchmarks, ISPRSC and H3DC, to provide standardized evaluation metrics for CTTA in the context of ALS point cloud segmentation. Experimental results demonstrate that APCoTTA outperforms direct inference, achieving significant improvements in mean Intersection over Union (mIoU) on the benchmarks. The code and benchmarks are made available for further research and development. 

<br /><br />Summary: <div>
arXiv:2505.09971v1 Announce Type: new 
Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task for large-scale 3D scene understanding. In real-world applications, models are typically fixed after training. However, domain shifts caused by changes in the environment, sensor types, or sensor degradation often lead to a decline in model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by adapting a source-pretrained model to evolving, unlabeled target domains. Despite its potential, research on ALS point clouds remains limited, facing challenges such as the absence of standardized datasets and the risk of catastrophic forgetting and error accumulation during prolonged adaptation. To tackle these challenges, we propose APCoTTA, the first CTTA method tailored for ALS point cloud semantic segmentation. We propose a dynamic trainable layer selection module. This module utilizes gradient information to select low-confidence layers for training, and the remaining layers are kept frozen, mitigating catastrophic forgetting. To further reduce error accumulation, we propose an entropy-based consistency loss. By losing such samples based on entropy, we apply consistency loss only to the reliable samples, enhancing model stability. In addition, we propose a random parameter interpolation mechanism, which randomly blends parameters from the selected trainable layers with those of the source model. This approach helps balance target adaptation and source knowledge retention, further alleviating forgetting. Finally, we construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA benchmarks for ALS point cloud segmentation. Experimental results demonstrate that APCoTTA achieves the best performance on two benchmarks, with mIoU improvements of approximately 9% and 14% over direct inference. The new benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation</title>
<link>https://arxiv.org/abs/2505.09986</link>
<guid>https://arxiv.org/abs/2505.09986</guid>
<content:encoded><![CDATA[
<div> algorithm, underwater images, compression, HQUIC, evaluation

Summary:<br />
The article introduces HQUIC, a new underwater image compression algorithm that leverages the unique characteristics of underwater scenes for enhanced efficiency. HQUIC utilizes an ALTC module to adaptively predict attenuation coefficients and global light information, addressing issues caused by lighting and tone variations. It also incorporates a codebook to extract common objects in underwater images, enhancing performance. Additionally, HQUIC dynamically weights multi-scale frequency components to prioritize critical information for distortion quality. Evaluations on various underwater datasets show that HQUIC outperforms existing compression methods in terms of compression efficiency. <div>
arXiv:2505.09986v1 Announce Type: new 
Abstract: With the increasing exploration and exploitation of the underwater world, underwater images have become a critical medium for human interaction with marine environments, driving extensive research into their efficient transmission and storage. However, contemporary underwater image compression algorithms fail to fully leverage the unique characteristics distinguishing underwater scenes from terrestrial images, resulting in suboptimal performance. To address this limitation, we introduce HQUIC, designed to exploit underwater-image-specific features for enhanced compression efficiency. HQUIC employs an ALTC module to adaptively predict the attenuation coefficients and global light information of the images, which effectively mitigates the issues caused by the differences in lighting and tone existing in underwater images. Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the common objects within underwater images and enhances the performance of the main branch. Furthermore, HQUIC dynamically weights multi-scale frequency components, prioritizing information critical for distortion quality while discarding redundant details. Extensive evaluations on diverse underwater datasets demonstrate that HQUIC outperforms state-of-the-art compression methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointArena: Probing Multimodal Grounding Through Language-Guided Pointing</title>
<link>https://arxiv.org/abs/2505.09990</link>
<guid>https://arxiv.org/abs/2505.09990</guid>
<content:encoded><![CDATA[
<div> benchmarking, pointing, multimodal models, robotic manipulation, reasoning tasks 

Summary: 
The article introduces PointArena, a platform for evaluating multimodal pointing abilities in various reasoning scenarios. PointArena consists of three components: Point-Bench, a dataset with 1,000 pointing tasks across five reasoning categories; Point-Battle, an interactive arena for comparing models; and Point-Act, a robotic manipulation system for real-world evaluations. State-of-the-art models were tested, with Molmo-72B performing best. Supervised training for pointing tasks improved model performance significantly. The study also highlights the importance of precise pointing in enabling multimodal models to connect abstract reasoning with real-world actions. The results show that proprietary models are catching up to open-source models in performance. Overall, the evaluations emphasize the vital role of accurate pointing abilities in multimodal models across various applications. <br /><br /> <div>
arXiv:2505.09990v1 Announce Type: new 
Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. Project page: https://pointarena.github.io/
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Descriptive Image-Text Matching with Graded Contextual Similarity</title>
<link>https://arxiv.org/abs/2505.09997</link>
<guid>https://arxiv.org/abs/2505.09997</guid>
<content:encoded><![CDATA[
<div> Keywords: Image-text matching, Descriptive flexibility, TF-IDF, Graded contextual similarity, Hierarchical reasoning <br />
Summary: <br />
This article introduces a new approach called Descriptive Image-Text Matching (DITM) to improve the process of building correspondences between images and text by considering their many-to-many relationships. DITM leverages the descriptive flexibility of language to learn graded contextual similarity and balance pairwise similarity using cumulative TF-IDF scores. By refining false negative labeling and aligning relevant sentences in a generic-to-specific order, DITM enhances the discovery of optimal matches and potential positive pairs. The method outperforms existing approaches in representing complex image-text relationships and improves hierarchical reasoning ability, as demonstrated through experiments on multiple datasets. <div>
arXiv:2505.09997v1 Announce Type: new 
Abstract: Image-text matching aims to build correspondences between visual and textual data by learning their pairwise similarities. Most existing approaches have adopted sparse binary supervision, indicating whether a pair of images and sentences matches or not. However, such sparse supervision covers a limited subset of image-text relationships, neglecting their inherent many-to-many correspondences; an image can be described in numerous texts at different descriptive levels. Moreover, existing approaches overlook the implicit connections from general to specific descriptions, which form the underlying rationale for the many-to-many relationships between vision and language. In this work, we propose descriptive image-text matching, called DITM, to learn the graded contextual similarity between image and text by exploring the descriptive flexibility of language. We formulate the descriptiveness score of each sentence with cumulative term frequency-inverse document frequency (TF-IDF) to balance the pairwise similarity according to the keywords in the sentence. Our method leverages sentence descriptiveness to learn robust image-text matching in two key ways: (1) to refine the false negative labeling, dynamically relaxing the connectivity between positive and negative pairs, and (2) to build more precise matching, aligning a set of relevant sentences in a generic-to-specific order. By moving beyond rigid binary supervision, DITM enhances the discovery of both optimal matches and potential positive pairs. Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the effectiveness of our method in representing complex image-text relationships compared to state-of-the-art approaches. In addition, DITM enhances the hierarchical reasoning ability of the model, supported by the extensive analysis on HierarCaps benchmark.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching</title>
<link>https://arxiv.org/abs/2505.09998</link>
<guid>https://arxiv.org/abs/2505.09998</guid>
<content:encoded><![CDATA[
<div> sketch-driven, 3D garment generation, virtual fashion, AR/VR environments, democratized design  
<br />  
Summary:  
A new framework is introduced for 3D garment generation through 3D sketches in AR/VR, allowing ordinary users to create personalized digital clothing. The system combines a conditional diffusion model, sketch encoder, and adaptive learning strategy to interpret free-hand input and generate realistic garments. The KO3DClothes dataset is also introduced to address the lack of training data. Extensive experiments and user studies show superior performance in fidelity and usability compared to existing methods, indicating the potential for democratized fashion design on consumer platforms.  
<br /> <div>
arXiv:2505.09998v1 Announce Type: new 
Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and smart devices, people increasingly seek ways to express their identity through virtual fashion. However, existing 3D garment design tools remain inaccessible to everyday users due to steep technical barriers and limited data. In this work, we introduce a 3D sketch-driven 3D garment generation framework that empowers ordinary users - even those without design experience - to create high-quality digital clothing through simple 3D sketches in AR/VR environments. By combining a conditional diffusion model, a sketch encoder trained in a shared latent space, and an adaptive curriculum learning strategy, our system interprets imprecise, free-hand input and produces realistic, personalized garments. To address the scarcity of training data, we also introduce KO3DClothes, a new dataset of paired 3D garments and user-created sketches. Extensive experiments and user studies confirm that our method significantly outperforms existing baselines in both fidelity and usability, demonstrating its promise for democratized fashion design on next-generation consumer platforms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of YOLOv8 in monocular downward multiple Car Target detection</title>
<link>https://arxiv.org/abs/2505.10016</link>
<guid>https://arxiv.org/abs/2505.10016</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous driving, Object detection, YOLOv8, Bidirectional pyramid structure, Detection accuracy

Summary:
The article introduces an improved autonomous target detection network based on YOLOv8, addressing limitations in current object detection technologies. By incorporating structural reparameterization and a bidirectional pyramid structure network model, the proposed approach achieves precise detection of multi-scale, small, and remote objects. A novel detection pipeline enhances efficiency and accuracy, with experimental results showing a 65% detection accuracy for large and small objects. The model is highlighted for its potential in real-world applications and autonomous driving competitions, particularly excelling in single-target and small-object detection scenarios. This advancement showcases significant progress in autonomous driving technology, paving the way for enhanced safety, efficiency, and emergency responses in modern transportation systems.<br /><br />Summary: <div>
arXiv:2505.10016v1 Announce Type: new 
Abstract: Autonomous driving technology is progressively transforming traditional car driving methods, marking a significant milestone in modern transportation. Object detection serves as a cornerstone of autonomous systems, playing a vital role in enhancing driving safety, enabling autonomous functionality, improving traffic efficiency, and facilitating effective emergency responses. However, current technologies such as radar for environmental perception, cameras for road perception, and vehicle sensor networks face notable challenges, including high costs, vulnerability to weather and lighting conditions, and limited resolution.To address these limitations, this paper presents an improved autonomous target detection network based on YOLOv8. By integrating structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework, the proposed approach achieves highly efficient and precise detection of multi-scale, small, and remote objects. Experimental results demonstrate that the enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showcasing significant advancements over traditional methods.This improved model holds substantial potential for real-world applications and is well-suited for autonomous driving competitions, such as the Formula Student Autonomous China (FSAC), particularly excelling in scenarios involving single-target and small-object detection.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction</title>
<link>https://arxiv.org/abs/2505.10027</link>
<guid>https://arxiv.org/abs/2505.10027</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, latent diffusion model, remote sensing, super-resolution, image reconstruction
Summary:
This paper introduces a reinforcement learning-based latent diffusion model (LDM) fine-tuning approach for remote sensing image super-resolution. By optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model, the method significantly enhances super-resolution quality in structured and complex natural scenes. Experiments on the RESISC45 dataset demonstrate improvements over the baseline model, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10. The proposed method effectively preserves image details, handles complex scenes, and enhances adaptability across a variety of scenes.<br /><br />Summary: <div>
arXiv:2505.10027v1 Announce Type: new 
Abstract: With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the method's effectiveness in enhancing super-resolution quality and adaptability across scenes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera</title>
<link>https://arxiv.org/abs/2505.10030</link>
<guid>https://arxiv.org/abs/2505.10030</guid>
<content:encoded><![CDATA[
<div> deep learning, disease identification, coconut tree, precision agriculture, AI-based

Summary:
DeepSeqCoco is a deep learning model designed for automatic disease identification in coconut trees. It outperforms existing models with up to 99.5% accuracy and reduces training and prediction times significantly. The model was tested with different optimizers, showing that the hybrid SGD-Adam configuration achieved the lowest validation loss of 2.81%. This suggests a promising future for AI-based disease monitoring systems in agriculture, particularly in developing countries where early diagnosis is crucial. By providing accurate and scalable disease identification, DeepSeqCoco has the potential to improve agricultural yield and efficiency. <div>
arXiv:2505.10030v1 Announce Type: new 
Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly in developing countries where conventional farming practices restrict early diagnosis and intervention. Current disease identification methods are manual, labor-intensive, and non-scalable. In response to these limitations, we come up with DeepSeqCoco, a deep learning based model for accurate and automatic disease identification from coconut tree images. The model was tested under various optimizer settings, such as SGD, Adam, and hybrid configurations, to identify the optimal balance between accuracy, minimization of loss, and computational cost. Results from experiments indicate that DeepSeqCoco can achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than existing models) with the hybrid SGD-Adam showing the lowest validation loss of 2.81%. It also shows a drop of up to 18% in training time and up to 85% in prediction time for input images. The results point out the promise of the model to improve precision agriculture through an AI-based, scalable, and efficient disease monitoring system.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis</title>
<link>https://arxiv.org/abs/2505.10046</link>
<guid>https://arxiv.org/abs/2505.10046</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image synthesis, large language models, diffusion transformers, multi-modal generation, empirical study

Summary: 
This paper delves into the design space of text-to-image synthesis by examining the fusion of large language models (LLMs) and diffusion transformers (DiTs). Unlike previous studies that focused on overall system performance without detailed comparisons or disclosed training methods, this work provides empirical insights through controlled comparisons with established baselines. It analyzes crucial design choices and offers a reproducible training recipe for achieving large-scale results. By addressing gaps in existing research and offering practical guidelines, this study aims to provide valuable data points and insights for future research in multi-modal generation.<br /><br />Summary: <div>
arXiv:2505.10046v1 Announce Type: new 
Abstract: This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field</title>
<link>https://arxiv.org/abs/2505.10049</link>
<guid>https://arxiv.org/abs/2505.10049</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic scene representation, neural radiance fields, 3D Gaussian splatting, differentiable volumetric rendering, motion representation paradigms

Summary: 
Dynamic scene representation and reconstruction have made significant advancements in recent years, driven by developments in neural radiance fields and 3D Gaussian splatting techniques. These methods have been adapted to address the complexities of 4D dynamic scenes, improving motion representation quality and scene reconstruction. The survey reviews over 200 papers on dynamic scene representation using radiance fields, categorizing works based on motion representation paradigms, reconstruction techniques, auxiliary information integration, and regularization approaches. By analyzing different methodologies, the survey aims to provide a comprehensive overview for researchers entering the field and offer experienced practitioners a systematic understanding of the principles and frontiers in dynamic scene reconstruction. Persistent challenges and future research directions are also highlighted in the survey. <br /><br />Summary: <div>
arXiv:2505.10049v1 Announce Type: new 
Abstract: Dynamic scene representation and reconstruction have undergone transformative advances in recent years, catalyzed by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques. While initially developed for static environments, these methodologies have rapidly evolved to address the complexities inherent in 4D dynamic scenes through an expansive body of research. Coupled with innovations in differentiable volumetric rendering, these approaches have significantly enhanced the quality of motion representation and dynamic scene reconstruction, thereby garnering substantial attention from the computer vision and graphics communities. This survey presents a systematic analysis of over 200 papers focused on dynamic scene representation using radiance field, spanning the spectrum from implicit neural representations to explicit Gaussian primitives. We categorize and evaluate these works through multiple critical lenses: motion representation paradigms, reconstruction techniques for varied scene dynamics, auxiliary information integration strategies, and regularization approaches that ensure temporal consistency and physical plausibility. We organize diverse methodological approaches under a unified representational framework, concluding with a critical examination of persistent challenges and promising research directions. By providing this comprehensive overview, we aim to establish a definitive reference for researchers entering this rapidly evolving field while offering experienced practitioners a systematic understanding of both conceptual principles and practical frontiers in dynamic scene reconstruction.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language</title>
<link>https://arxiv.org/abs/2505.10055</link>
<guid>https://arxiv.org/abs/2505.10055</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large Multimodal Models, Optical Character Recognition, Pashto language, PsOCR <br />
<br />
Summary: This paper evaluates the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. The study addresses challenges in Pashto NLP, such as the cursive script and limited datasets, by introducing the PsOCR synthetic dataset. With annotated images across various font families, colors, and layouts, PsOCR facilitates training and evaluating models like CNNs and Transformers. Testing seven open-source and four closed-source LMMs on a benchmark subset showed Gemini and Qwen-7B as top performers. The research emphasizes the assessment of LMM capabilities and limitations for Pashto OCR, laying groundwork for future studies in related scripts like Arabic and Urdu. PsOCR is accessible for further exploration. <br /> <div>
arXiv:2505.10055v1 Announce Type: new 
Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. Natural Language Processing (NLP) in Pashto faces several challenges due to the cursive nature of its script and a scarcity of structured datasets. To address this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one million images annotated with bounding boxes at word, line, and document levels, suitable for training and evaluating models based on different architectures, including Convolutional Neural Networks (CNNs) and Transformers. PsOCR covers variations across 1,000 unique font families, colors, image sizes, and layouts. A benchmark subset of 10K images was selected to evaluate the performance of several LMMs, including seven open-source models: DeepSeek's Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results demonstrate that Gemini achieves the best performance among all models, whereas among open-source models, Qwen-7B stands out. This work provides an insightful assessment of the capabilities and limitations of current LMMs for OCR tasks in Pashto and establishes a foundation for further research not only in Pashto OCR but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is available at https://github.com/zirak-ai/PashtoOCR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars</title>
<link>https://arxiv.org/abs/2505.10072</link>
<guid>https://arxiv.org/abs/2505.10072</guid>
<content:encoded><![CDATA[
<div> StyleGAN, 3D Gaussian blendshapes, facial image stylization, Toonify, animatable head avatars <br />
<br />Summary:
The article introduces a new framework called ToonifyGB that extends the use of Toonify for creating stylized 3D head avatars using Gaussian blendshapes. The framework consists of two stages: Stage 1 focuses on generating stylized videos from input frames using an improved StyleGAN, allowing for a more stable video output that captures high-frequency details. In Stage 2, a stylized neutral head model and expression blendshapes are learned from the generated video to efficiently render stylized avatars with various expressions. The effectiveness of ToonifyGB is validated on a benchmark dataset using two distinct styles – Arcane and Pixar. Overall, ToonifyGB enhances the capabilities of facial image stylization and animatable head avatar reconstruction using 3D Gaussian blendshapes. <div>
arXiv:2505.10072v1 Announce Type: new 
Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based framework, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1 (stylized video generation), we employ an improved StyleGAN to generate the stylized video from the input video frames, which addresses the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN. This process provides a more stable video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, and efficiently generate high-quality animation in the next stage. In Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head model and a set of expression blendshapes from the generated video. By combining the neutral head model with expression blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary expressions. We validate the effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane and Pixar.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10088</link>
<guid>https://arxiv.org/abs/2505.10088</guid>
<content:encoded><![CDATA[
<div> propose, Multi-Modal Representation Learning, adaptation, generalization, transfer learning
Summary:
Multi-Modal Representation Learning (MMRL) addresses the overfitting of large-scale pre-trained Vision-Language Models (VLMs) when adapting to limited few-shot data by introducing a shared representation space for text and image modalities. MMRL optimizes representation tokens in higher encoder layers for task-specific features, while preserving general knowledge in lower layers. Training involves joint optimization of class and representation features with a regularized alignment to the frozen VLM's zero-shot features. At inference, a decoupling strategy is used for base and novel tasks, leveraging class and representation features accordingly. MMRL++ builds on this by reducing trainable parameters and enhancing intra-modal interactions, leading to improved performance across datasets. Extensive experiments demonstrate the effectiveness of both MMRL and MMRL++ in achieving a balance between task-specific adaptation and generalization.<br /><br />Summary: <div>
arXiv:2505.10088v1 Announce Type: new 
Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this, we propose Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, modality-agnostic representation space. MMRL generates space tokens projected into both text and image encoders as representation tokens, enabling more effective cross-modal interactions. Unlike prior methods that mainly optimize class token features, MMRL inserts representation tokens into higher encoder layers--where task-specific features are more prominent--while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term aligning class and text features with the frozen VLM's zero-shot features. At inference, a decoupling strategy uses both class and representation features for base tasks, but only class features for novel tasks due to their stronger generalization. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces trainable parameters and enhances intra-modal interactions--particularly across the layers of representation tokens--allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering</title>
<link>https://arxiv.org/abs/2505.10118</link>
<guid>https://arxiv.org/abs/2505.10118</guid>
<content:encoded><![CDATA[
<div> visual token pruning, Hausdorff distance, multi-objective optimization, covering problem, scalability

Summary: 
This study introduces a novel approach, Multi-Objective Balanced Covering (MoB), to address the varying importance of prompt alignment and visual preservation in visual token pruning. By deriving a closed-form error bound based on the Hausdorff distance and leveraging epsilon-covering theory, the trade-off between these objectives is quantified and optimal levels are identified under a fixed budget. MoB reformulates token pruning as a bi-objective covering problem, allowing for a flexible budget allocation through greedy radius trading. The approach offers a provable performance bound and linear scalability, enabling efficient adaptation to challenging pruning scenarios. Experimental results demonstrate that MoB achieves a high preservation rate of performance for various tasks while significantly reducing the number of visual tokens. Integration with advanced models and diverse vision-language tasks further confirms the effectiveness and versatility of the proposed approach in enhancing model efficiency and performance. 

<br /><br />Summary: <div>
arXiv:2505.10118v1 Announce Type: new 
Abstract: Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance. To address this, we derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging $\epsilon$-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading. MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMITATE: Image Registration with Context for unknown time frame recovery</title>
<link>https://arxiv.org/abs/2505.10124</link>
<guid>https://arxiv.org/abs/2505.10124</guid>
<content:encoded><![CDATA[
<div> conditional U-Net architecture, image registration, 4D-CT scans, radiotherapy treatment, movement artefacts <br />
Summary: <br />
This paper presents a novel image registration formalism for estimating unknown condition-related images using known images and associated conditions. The formalism is implemented through a conditional U-Net architecture that incorporates conditional information without requiring fixed images. The formalism is applied to the challenging task of registering 4D-CT scans of moving tumors during radiotherapy treatment, particularly in thoracoabdominal regions. The approach minimizes reconstruction artefacts arising from irregular patient breathing and poor correlation of breathing signal to internal motion. Experimental results on clinical data demonstrate artifact-free volume reconstructions with real-time latencies. The publicly available code repository enables easy replication and further exploration of the proposed method. <div>
arXiv:2505.10124v1 Announce Type: new 
Abstract: In this paper, we formulate a novel image registration formalism dedicated to the estimation of unknown condition-related images, based on two or more known images and their associated conditions. We show how to practically model this formalism by using a new conditional U-Net architecture, which fully takes into account the conditional information and does not need any fixed image. Our formalism is then applied to image moving tumors for radiotherapy treatment at different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal regions. This driving application is particularly complex as it requires to stitch a collection of sequential 2D slices into several 3D volumes at different organ positions. Movement interpolation with standard methods then generates well known reconstruction artefacts in the assembled volumes due to irregular patient breathing, hysteresis and poor correlation of breathing signal to internal motion. Results obtained on 4D-CT clinical data showcase artefact-free volumes achieved through real-time latencies. The code is publicly available at https://github.com/Kheil-Z/IMITATE .
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2505.10152</link>
<guid>https://arxiv.org/abs/2505.10152</guid>
<content:encoded><![CDATA[
<div> style augmentation, federated domain generalization, multi-source collaborative, domain-invariant learning, data decentralization<br />
<br />
Summary:<br />
The article introduces a novel approach called Multi-source Collaborative Style Augmentation and Domain-invariant learning (MCSAD) for federated domain generalization. This method addresses the limitations of existing style augmentation techniques by incorporating a multi-source collaborative style augmentation module to explore a broader style space. Additionally, domain-invariant learning is conducted through cross-domain feature alignment within the same class and classes relation ensemble distillation between different classes. By iteratively enhancing data styles and ensuring domain invariance, the model can effectively generalize to unseen target domains. Experimental results demonstrate that MCSAD outperforms state-of-the-art federated domain generalization methods, showcasing its efficacy in improving generalization performance across multiple datasets. <br /> <div>
arXiv:2505.10152v1 Announce Type: new 
Abstract: Federated domain generalization aims to learn a generalizable model from multiple decentralized source domains for deploying on the unseen target domain. The style augmentation methods have achieved great progress on domain generalization. However, the existing style augmentation methods either explore the data styles within isolated source domain or interpolate the style information across existing source domains under the data decentralization scenario, which leads to limited style space. To address this issue, we propose a Multi-source Collaborative Style Augmentation and Domain-invariant learning method (MCSAD) for federated domain generalization. Specifically, we propose a multi-source collaborative style augmentation module to generate data in the broader style space. Furthermore, we conduct domain-invariant learning between the original data and augmented data by cross-domain feature alignment within the same class and classes relation ensemble distillation between different classes to learn a domain-invariant model. By alternatively conducting collaborative style augmentation and domain-invariant learning, the model can generalize well on unseen target domain. Extensive experiments on multiple domain generalization datasets indicate that our method significantly outperforms the state-of-the-art federated domain generalization methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Saliency Dataset Bias</title>
<link>https://arxiv.org/abs/2505.10169</link>
<guid>https://arxiv.org/abs/2505.10169</guid>
<content:encoded><![CDATA[
<div> Architecture, Saliency prediction, Dataset bias, Generalization gap, Multi-scale structure <br />
Summary:
Recent advances in image-based saliency prediction have achieved high performance levels on various benchmarks. However, predicting fixations across multiple saliency datasets remains challenging due to dataset bias, leading to a significant drop in performance when models are applied to different datasets. Increasing dataset diversity does not fully address this gap, with a majority of the difference attributed to dataset-specific biases. A novel architecture with a limited number of dataset-specific parameters has been proposed to address the generalization gap, showing significant improvement in performance with as few as 50 samples. The model sets a new state-of-the-art on multiple datasets, providing insights into spatial saliency properties and revealing complex multi-scale effects that combine absolute and relative sizes.<br /><br /> <div>
arXiv:2505.10169v1 Announce Type: new 
Abstract: Recent advances in image-based saliency prediction are approaching gold standard performance levels on existing benchmarks. Despite this success, we show that predicting fixations across multiple saliency datasets remains challenging due to dataset bias. We find a significant performance drop (around 40%) when models trained on one dataset are applied to another. Surprisingly, increasing dataset diversity does not resolve this inter-dataset gap, with close to 60% attributed to dataset-specific biases. To address this remaining generalization gap, we propose a novel architecture extending a mostly dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters that govern interpretable mechanisms such as multi-scale structure, center bias, and fixation spread. Adapting only these parameters to new data accounts for more than 75% of the generalization gap, with a large fraction of the improvement achieved with as few as 50 samples. Our model sets a new state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark (MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from unrelated datasets, but with a substantial boost when adapting to the respective training datasets. The model also provides valuable insights into spatial saliency properties, revealing complex multi-scale effects that combine both absolute and relative sizes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</title>
<link>https://arxiv.org/abs/2505.10205</link>
<guid>https://arxiv.org/abs/2505.10205</guid>
<content:encoded><![CDATA[
<div> framework, food volume estimation, mobile device-driven 3D reconstruction, AR-capable mobile devices, food video segmentation<br />
Summary:<br />
The article introduces VolE, a framework for accurate food volume estimation using mobile device-driven 3D reconstruction. Unlike existing methods that rely on single-purpose hardware or sensor-oriented information, VolE captures images and camera locations in free motion to generate precise 3D models. It is a reference- and depth-free framework that leverages food video segmentation to generate food masks for real-world measurement. Additionally, a new food dataset with challenging scenarios is introduced. Experimental results show that VolE outperforms existing techniques with a 2.22% Mean Absolute Percentage Error (MAPE), demonstrating its superior performance in food volume estimation. <div>
arXiv:2505.10205v1 Announce Type: new 
Abstract: Accurate food volume estimation is crucial for medical nutrition management and health monitoring applications, but current food volume estimation methods are often limited by mononuclear data, leveraging single-purpose hardware such as 3D scanners, gathering sensor-oriented information such as depth information, or relying on camera calibration using a reference object. In this paper, we present VolE, a novel framework that leverages mobile device-driven 3D reconstruction to estimate food volume. VolE captures images and camera locations in free motion to generate precise 3D models, thanks to AR-capable mobile devices. To achieve real-world measurement, VolE is a reference- and depth-free framework that leverages food video segmentation for food mask generation. We also introduce a new food dataset encompassing the challenging scenarios absent in the previous benchmarks. Our experiments demonstrate that VolE outperforms the existing volume estimation techniques across multiple datasets by achieving 2.22 % MAPE, highlighting its superior performance in food volume estimation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation</title>
<link>https://arxiv.org/abs/2505.10223</link>
<guid>https://arxiv.org/abs/2505.10223</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, data augmentation, MixUp, Auxiliary Fourier Augmentation, nnU-Net<br />
Summary:<br />
- Medical image segmentation models trained on curated datasets often perform poorly in real clinical settings due to distribution mismatches.
- Traditional data augmentation techniques may not be robust enough for diverse real-world scenarios.
- MixUp and Auxiliary Fourier Augmentation are evaluated as alternative augmentation strategies to improve generalization and robustness in medical image segmentation.
- These methods effectively enhance feature representations by promoting separability and compactness.
- Integration of these techniques into nnU-Net training pipelines offers an easy-to-implement solution for improving the reliability of medical segmentation models in real-world applications.<br /> 

Summary: <div>
arXiv:2505.10223v1 Announce Type: new 
Abstract: Medical image segmentation models are often trained on curated datasets, leading to performance degradation when deployed in real-world clinical settings due to mismatches between training and test distributions. While data augmentation techniques are widely used to address these challenges, traditional visually consistent augmentation strategies lack the robustness needed for diverse real-world scenarios. In this work, we systematically evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary Fourier Augmentation. These methods mitigate the effects of multiple variations without explicitly targeting specific sources of distribution shifts. We demonstrate how these techniques significantly improve out-of-distribution generalization and robustness to imaging variations across a wide range of transformations in cardiac cine MRI and prostate MRI segmentation. We quantitatively find that these augmentation methods enhance learned feature representations by promoting separability and compactness. Additionally, we highlight how their integration into nnU-Net training pipelines provides an easy-to-implement, effective solution for enhancing the reliability of medical segmentation models in real-world applications.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging</title>
<link>https://arxiv.org/abs/2505.10231</link>
<guid>https://arxiv.org/abs/2505.10231</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, medical imaging, fairness, human-AI alignment, generalization 

Summary:
Deep neural networks are highly successful in medical imaging, but they can exhibit biases that lead to fairness disparities among different demographic groups. This study delves into the concept of Human-AI alignment in the medical imaging domain, uncovering that integrating human insights can mitigate fairness gaps and improve generalization outside the training dataset. However, over-alignment can potentially result in performance trade-offs, underlining the importance of implementing balanced strategies. These findings emphasize the significance of Human-AI alignment in developing equitable, resilient, and versatile medical AI systems that strike a harmonious balance between human expertise and machine efficiency. The code for this study is openly accessible on the GitHub platform at https://github.com/Roypic/Aligner. 

<br /><br />Summary: 
- Deep neural networks are effective in medical imaging but can exhibit biases leading to fairness disparities.
- Incorporating human insights reduces fairness gaps and improves generalization in medical AI systems.
- Excessive alignment may result in performance trade-offs, emphasizing the need for calibrated strategies.
- Human-AI alignment is crucial for developing fair, robust, and generalizable medical AI systems.
- The study code is available for public access on GitHub. <div>
arXiv:2505.10231v1 Announce Type: new 
Abstract: Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</title>
<link>https://arxiv.org/abs/2505.10238</link>
<guid>https://arxiv.org/abs/2505.10238</guid>
<content:encoded><![CDATA[
<div> 3D motion sequences, human image animation, 4DMoT, MV-DiT, MTVCrafter <br />
<br />
Summary: <br />
The article presents MTVCrafter, a novel framework for human image animation that directly models raw 3D motion sequences. By using 4D motion tokens generated by 4DMoT, MTVCrafter offers robust spatio-temporal cues and flexible control, surpassing existing methods that rely on 2D-rendered pose images. The framework also introduces MV-DiT, which leverages motion tokens as context for human image animation in a 3D world. MTVCrafter achieves state-of-the-art results in pose-guided human video generation, with an FID-VID of 6.98, showing superior performance and generalization across diverse open-world characters and scenarios. This advancement marks a significant step forward in the field, opening up new possibilities for human image animation research and development. The code and video demos are available for further exploration and experimentation. <br /> <div>
arXiv:2505.10238v1 Announce Type: new 
Abstract: Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are provided in the supplementary material and at this anonymous GitHub link: https://anonymous.4open.science/r/MTVCrafter-1B13.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2505.10250</link>
<guid>https://arxiv.org/abs/2505.10250</guid>
<content:encoded><![CDATA[
<div> Keywords: Human mesh recovery, ADHMR, Preference optimization, HMR-Scorer, Probabilistic methods

Summary: 
ADHMR proposes a novel framework for human mesh recovery (HMR) from a single image. The framework utilizes a preference optimization approach to align a Diffusion-based HMR model, addressing depth ambiguity and occlusion issues. By training a human mesh prediction assessment model, HMR-Scorer, images can be evaluated without 3D annotations, enabling data cleaning and improving existing HMR models. The preference dataset created using HMR-Scorer facilitates the finetuning of the base model through direct preference optimization. ADHMR surpasses current state-of-the-art methods and demonstrates superior performance, particularly in handling in-the-wild images. The framework's code is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2505.10250v1 Announce Type: new 
Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to depth ambiguity and occlusions. Probabilistic methods have tried to solve this by generating numerous plausible 3D human mesh predictions, but they often exhibit misalignment with 2D image observations and weak robustness to in-the-wild images. To address these issues, we propose ADHMR, a framework that Aligns a Diffusion-based HMR model in a preference optimization manner. First, we train a human mesh prediction assessment model, HMR-Scorer, capable of evaluating predictions even for in-the-wild images without 3D annotations. We then use HMR-Scorer to create a preference dataset, where each input image has a pair of winner and loser mesh predictions. This dataset is used to finetune the base model using direct preference optimization. Moreover, HMR-Scorer also helps improve existing HMR models by data cleaning, even with fewer training samples. Extensive experiments show that ADHMR outperforms current state-of-the-art methods. Code is available at: https://github.com/shenwenhao01/ADHMR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot</title>
<link>https://arxiv.org/abs/2505.10257</link>
<guid>https://arxiv.org/abs/2505.10257</guid>
<content:encoded><![CDATA[
<div> alignment, generalist, driving agent, SAGE Deer, benchmark 
Summary:
Sage Deer is a new intelligent driving agent designed to cater to different users' comfort, interaction, and safety needs. It stands out in three key areas: super alignment, catering to different preferences and biases; being a generalist, understanding various inputs to reason out user behavior; and self-eliciting, being able to elicit implicit thought chains. The agent can analyze physiological indicators, facial emotions, hand and body movements, driving scenarios, and behavioral decisions. A large-scale benchmark was created to measure the agent's perceptual decision-making ability and accuracy in super alignment. Sage Deer aims to enhance the driving experience by providing personalized and safe interactions for users. 
<br /><br />Summary: <div>
arXiv:2505.10257v1 Announce Type: new 
Abstract: The intelligent driving cockpit, an important part of intelligent driving, needs to match different users' comfort, interaction, and safety needs. This paper aims to build a  Super-Aligned and GEneralist DRiving agent, SAGE DeeR. Sage Deer achieves three highlights: (1) Super alignment: It achieves different reactions according to different people's preferences and biases. (2) Generalist: It can understand the multi-view and multi-mode inputs to reason the user's physiological indicators, facial emotions, hand movements, body movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It can elicit implicit thought chains in the language space to further increase generalist and super-aligned abilities. Besides, we collected multiple data sets and built a large-scale benchmark. This benchmark measures the deer's perceptual decision-making ability and the super alignment's accuracy.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Driving Maps by Deep Learning-based Trail Map Extraction</title>
<link>https://arxiv.org/abs/2505.10258</link>
<guid>https://arxiv.org/abs/2505.10258</guid>
<content:encoded><![CDATA[
<div> Mapping, HD maps, autonomous driving systems, offline mapping, deep learning

Summary: 
- High-definition (HD) maps are essential for autonomous driving systems for accurate planning.
- Automated map creation methods have been developed to reduce manual labeling efforts.
- Online mapping has become popular for up-to-date map availability.
- Challenges such as temporal consistency and sensor occlusion persist in online mapping.
- A novel offline mapping approach integrating trail data has been proposed.
- The method uses transformer-based deep learning models for global map construction.
- Continuous updates are enabled while remaining sensor-agnostic, improving efficiency.
- Superior performance compared to online mapping approaches is demonstrated.
- Improved generalization to new environments and sensor configurations is achieved.
- Validation on benchmark datasets highlights the robustness and applicability of the approach. 

<br /><br />Summary: <div>
arXiv:2505.10258v1 Announce Type: new 
Abstract: High-definition (HD) maps offer extensive and accurate environmental information about the driving scene, making them a crucial and essential element for planning within autonomous driving systems. To avoid extensive efforts from manual labeling, methods for automating the map creation have emerged. Recent trends have moved from offline mapping to online mapping, ensuring availability and actuality of the utilized maps. While the performance has increased in recent years, online mapping still faces challenges regarding temporal consistency, sensor occlusion, runtime, and generalization. We propose a novel offline mapping approach that integrates trails - informal routes used by drivers - into the map creation process. Our method aggregates trail data from the ego vehicle and other traffic participants to construct a comprehensive global map using transformer-based deep learning models. Unlike traditional offline mapping, our approach enables continuous updates while remaining sensor-agnostic, facilitating efficient data transfer. Our method demonstrates superior performance compared to state-of-the-art online mapping approaches, achieving improved generalization to previously unseen environments and sensor configurations. We validate our approach on two benchmark datasets, highlighting its robustness and applicability in autonomous driving systems.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HandReader: Advanced Techniques for Efficient Fingerspelling Recognition</title>
<link>https://arxiv.org/abs/2505.10267</link>
<guid>https://arxiv.org/abs/2505.10267</guid>
<content:encoded><![CDATA[
<div> Keywords: fingerspelling recognition, sign language, HandReader, Temporal Shift-Adaptive Module, Temporal Pose Encoder

Summary:
HandReader introduces three architectures, including HandReader_RGB, HandReader_KP, and HandReader_RGB+KP, to improve fingerspelling recognition in sign language. HandReader_RGB utilizes a Temporal Shift-Adaptive Module to process RGB features effectively. HandReader_KP incorporates a Temporal Pose Encoder to operate on keypoints as tensors, capturing temporal and spatial information. HandReader_RGB+KP combines RGB and keypoint modalities for enhanced performance. The models achieve state-of-the-art results on the ChicagoFSWild and ChicagoFSWild+ datasets and demonstrate high accuracy on the Znaki dataset, the first open dataset for Russian fingerspelling. The Znaki dataset and pre-trained HandReader models are publicly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.10267v1 Announce Type: new 
Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the interpretation of proper names, characterized by fast hand movements during signing. Although previous works on fingerspelling recognition have focused on processing the temporal dimension of videos, there remains room for improving the accuracy of these approaches. This paper introduces HandReader, a group of three architectures designed to address the fingerspelling recognition task. HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to process RGB features from videos of varying lengths while preserving important sequential information. HandReader$_{KP}$ is built on the proposed Temporal Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition in a batch allows the encoder to pass them through 2D and 3D convolution layers, utilizing temporal and spatial information and accumulating keypoints coordinates. We also introduce HandReader_RGB+KP - architecture with a joint encoder to benefit from RGB and keypoint modalities. Each HandReader model possesses distinct advantages and achieves state-of-the-art results on the ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate high performance on the first open dataset for Russian fingerspelling, Znaki, presented in this paper. The Znaki dataset and HandReader pre-trained models are publicly available.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting</title>
<link>https://arxiv.org/abs/2505.10281</link>
<guid>https://arxiv.org/abs/2505.10281</guid>
<content:encoded><![CDATA[
<div> dataset, marine fog, deep learning, multi-regional, geostationary satellites

Summary:
The article introduces MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting. With annotations from 15 coastal fog-prone regions and six geostationary satellites, MFogHub includes over 68,000 high-resolution samples. By incorporating diverse regions and satellite perspectives, the dataset enables evaluation of detection and forecasting methods under varying conditions. Experiments with 16 baseline models illustrate MFogHub's ability to reveal generalization fluctuations due to regional and satellite differences, while also supporting the development of targeted fog prediction techniques. The dataset aims to advance practical monitoring and scientific understanding of marine fog dynamics globally. The dataset and code can be accessed at https://github.com/kaka0910/MFogHub. 

<br /><br />Summary: <div>
arXiv:2505.10281v1 Announce Type: new 
Abstract: Deep learning approaches for marine fog detection and forecasting have outperformed traditional methods, demonstrating significant scientific and practical importance. However, the limited availability of open-source datasets remains a major challenge. Existing datasets, often focused on a single region or satellite, restrict the ability to evaluate model performance across diverse conditions and hinder the exploration of intrinsic marine fog characteristics. To address these limitations, we introduce \textbf{MFogHub}, the first multi-regional and multi-satellite dataset to integrate annotated marine fog observations from 15 coastal fog-prone regions and six geostationary satellites, comprising over 68,000 high-resolution samples. By encompassing diverse regions and satellite perspectives, MFogHub facilitates rigorous evaluation of both detection and forecasting methods under varying conditions. Extensive experiments with 16 baseline models demonstrate that MFogHub can reveal generalization fluctuations due to regional and satellite discrepancy, while also serving as a valuable resource for the development of targeted and scalable fog prediction techniques. Through MFogHub, we aim to advance both the practical monitoring and scientific understanding of marine fog dynamics on a global scale. The dataset and code are at \href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2505.10289</link>
<guid>https://arxiv.org/abs/2505.10289</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositional Zero-Shot Learning, Multi-Stage Cross-modal Interaction, CLIP, fine-grained local features, attention weights 

Summary: 
The study introduces a Multi-Stage Cross-modal Interaction (MSCI) model to enhance Compositional Zero-Shot Learning (CZSL) by addressing the limitations of CLIP in capturing fine-grained local features. The MSCI model utilizes two self-adaptive aggregators to extract and integrate local and global visual information from CLIP's visual encoder. This allows for the progressive incorporation of key information into textual representations, improving the model's perception of fine-grained details. The MSCI model also dynamically adjusts attention weights between global and local visual information based on different combinations and elements, providing flexibility in adapting to diverse scenarios. Experimental results on three datasets demonstrate the effectiveness and superiority of the proposed model. The data and code for the MSCI model are available at the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2505.10289v1 Announce Type: new 
Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object combinations by leveraging known combinations. Existing studies basically rely on the cross-modal alignment capabilities of CLIP but tend to overlook its limitations in capturing fine-grained local features, which arise from its architectural and training paradigm. To address this issue, we propose a Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and utilizes intermediate-layer information from CLIP's visual encoder. Specifically, we design two self-adaptive aggregators to extract local information from low-level visual features and integrate global information from high-level visual features, respectively. These key information are progressively incorporated into textual representations through a stage-by-stage interaction mechanism, significantly enhancing the model's perception capability for fine-grained local visual information. Additionally, MSCI dynamically adjusts the attention weights between global and local visual information based on different combinations, as well as different elements within the same combination, allowing it to flexibly adapt to diverse scenarios. Experiments on three widely used datasets fully validate the effectiveness and superiority of the proposed model. Data and code are available at https://github.com/ltpwy/MSCI.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation</title>
<link>https://arxiv.org/abs/2505.10292</link>
<guid>https://arxiv.org/abs/2505.10292</guid>
<content:encoded><![CDATA[
<div> Dataset, visual storytelling, character identity, grounded stories, object consistency <br />
Summary: 
The article introduces StoryReasoning, a dataset containing structured scene analyses and grounded stories to address issues related to maintaining character identity and linking actions in visual storytelling systems. The dataset consists of 4,178 stories derived from 52,016 movie images and features explicit modeling of multi-frame relationships through structured tabular representations. The proposed approach includes cross-frame object re-identification, chain-of-thought reasoning for narrative modeling, and a grounding scheme linking textual elements to visual entities. By fine-tuning the Qwen2.5-VL 7B model to create Qwen Storyteller, end-to-end object detection, re-identification, and landmark detection are achieved while ensuring consistent object references throughout the story. Evaluation results show a significant reduction in hallucinations per story when compared to a non-fine-tuned model. <br /><br />Summary: <div>
arXiv:2505.10292v1 Announce Type: new 
Abstract: Visual storytelling systems struggle to maintain character identity across frames and link actions to appropriate subjects, frequently leading to referential hallucinations. These issues can be addressed through grounding of characters, objects, and other entities on the visual elements. We propose StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie images, with both structured scene analyses and grounded stories. Each story maintains character and object consistency across frames while explicitly modeling multi-frame relationships through structured tabular representations. Our approach features cross-frame object re-identification using visual similarity and face recognition, chain-of-thought reasoning for explicit narrative modeling, and a grounding scheme that links textual elements to visual entities across multiple frames. We establish baseline performance by fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end object detection, re-identification, and landmark detection while maintaining consistent object references throughout the story. Evaluation demonstrates a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when compared to a non-fine-tuned model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&amp;E Images using ViT Foundation Models</title>
<link>https://arxiv.org/abs/2505.10294</link>
<guid>https://arxiv.org/abs/2505.10294</guid>
<content:encoded><![CDATA[
<div> H&amp;E staining, multiplex immunofluorescence, cancer diagnosis, MIPHEI, ViT foundation models <br />
Summary: <br />
Histopathological analysis plays a crucial role in cancer diagnosis, with H&amp;E staining providing valuable information on cell morphology. However, multiplex immunofluorescence (mIF) offers more precise cell type identification but faces challenges in clinical adoption. To address this, MIPHEI integrates ViT foundation models to predict mIF signals from H&amp;E images, achieving accurate cell-type classification for various markers. The model outperforms baseline and random classifiers, demonstrating its ability to capture complex relationships between nuclear morphologies and molecular markers in H&amp;E images. MIPHEI shows promise in enabling cell-type-aware analysis of large-scale H&amp;E datasets, potentially leading to insights into spatial cellular organization and patient outcomes. <div>
arXiv:2505.10294v1 Announce Type: new 
Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with Hematoxylin and Eosin (H&amp;E) staining routinely acquired for every patient to visualize cell morphology and tissue architecture. On the other hand, multiplex immunofluorescence (mIF) enables more precise cell type identification via proteomic markers, but has yet to achieve widespread clinical adoption due to cost and logistical constraints. To bridge this gap, we introduce MIPHEI (Multiplex Immunofluorescence Prediction from H&amp;E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&amp;E images. MIPHEI targets a comprehensive panel of markers spanning nuclear content, immune lineages (T cells, B cells, myeloid), epithelium, stroma, vasculature, and proliferation. We train our model using the publicly available ORION dataset of restained H&amp;E and mIF images from colorectal cancer tissue, and validate it on two independent datasets. MIPHEI achieves accurate cell-type classification from H&amp;E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers. Our results indicate that our model effectively captures the complex relationships between nuclear morphologies in their tissue context, as visible in H&amp;E images and molecular markers defining specific cell types. MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&amp;E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability</title>
<link>https://arxiv.org/abs/2505.10351</link>
<guid>https://arxiv.org/abs/2505.10351</guid>
<content:encoded><![CDATA[
<div> membership inference, self-supervised learning, privacy concerns, PartCrop, defense methods <br />
Summary: <br />
This paper introduces a new method called PartCrop for performing membership inference on visual self-supervised models in a black-box setting. The method leverages the shared part-aware capability among models and stronger part response on the training data. Extensive attacks on self-supervised models with different training protocols and structures validate the effectiveness of PartCrop. Defense methods such as early stop, differential privacy, and shrinking crop scale range are evaluated and found to be effective against PartCrop attacks. Additionally, the impacts of scaling from both data and model aspects are quantitatively studied, leading to the proposal of a scalable PartCrop-v2 with structural improvements. Overall, the study addresses privacy concerns in self-supervised learning and provides practical solutions for defending against membership inference attacks. <div>
arXiv:2505.10351v1 Announce Type: new 
Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled data, but it also confronts significant privacy concerns, especially in vision. In this paper, we perform membership inference on visual self-supervised models in a more realistic setting: self-supervised training method and details are unknown for an adversary when attacking as he usually faces a black-box system in practice. In this setting, considering that self-supervised model could be trained by completely different self-supervised paradigms, e.g., masked image modeling and contrastive learning, with complex training details, we propose a unified membership inference method called PartCrop. It is motivated by the shared part-aware capability among models and stronger part response on the training data. Specifically, PartCrop crops parts of objects in an image to query responses within the image in representation space. We conduct extensive attacks on self-supervised models with different training protocols and structures using three widely used image datasets. The results verify the effectiveness and generalization of PartCrop. Moreover, to defend against PartCrop, we evaluate two common approaches, i.e., early stop and differential privacy, and propose a tailored method called shrinking crop scale range. The defense experiments indicate that all of them are effective. Finally, besides prototype testing on toy visual encoders and small-scale image datasets, we quantitatively study the impacts of scaling from both data and model aspects in a realistic scenario and propose a scalable PartCrop-v2 by introducing two structural improvements to PartCrop. Our code is at https://github.com/JiePKU/PartCrop.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity</title>
<link>https://arxiv.org/abs/2505.10352</link>
<guid>https://arxiv.org/abs/2505.10352</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, SpikeVideoFormer, spike-driven attention, video tasks, efficiency gains

Summary:
SpikeVideoFormer is introduced as an efficient spike-driven video Transformer with linear temporal complexity. It features a spike-driven Hamming attention (SDHA) that adapts traditional real-valued attention to spike-driven attention. Various spike-driven space-time attention designs are analyzed, identifying an optimal scheme for video tasks with linear temporal complexity. The model demonstrates strong generalization ability and efficiency in diverse video tasks such as classification, human pose tracking, and semantic segmentation. Empirical results show that SpikeVideoFormer achieves state-of-the-art performance compared to existing SNN approaches, with significant improvements in human pose tracking and semantic segmentation tasks. It matches the performance of recent ANN-based methods while offering significant efficiency gains, with up to 16x, 10x, and 5x improvements in the three tasks.

<br /><br />Summary: 
SpikeVideoFormer is an efficient spike-driven video Transformer with linear temporal complexity, featuring spike-driven Hamming attention for video tasks. It outperforms existing SNN approaches, achieving state-of-the-art results in various tasks like human pose tracking and semantic segmentation. The model matches recent ANN-based methods while significantly enhancing efficiency, with substantial improvements in performance across different video tasks. <div>
arXiv:2505.10352v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to Artificial Neural Networks (ANNs) in various vision tasks, while offering superior energy efficiency. However, existing SNN-based Transformers primarily focus on single-image tasks, emphasizing spatial features while not effectively leveraging SNNs' efficiency in video-based vision tasks. In this paper, we introduce SpikeVideoFormer, an efficient spike-driven video Transformer, featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design a spike-driven Hamming attention (SDHA) which provides a theoretically guided adaptation from traditional real-valued attention to spike-driven attention. Building on SDHA, we further analyze various spike-driven space-time attention designs and identify an optimal scheme that delivers appealing performance for video tasks, while maintaining only linear temporal complexity. The generalization ability and efficiency of our model are demonstrated across diverse downstream video tasks, including classification, human pose tracking, and semantic segmentation. Empirical results show our method achieves state-of-the-art (SOTA) performance compared to existing SNN approaches, with over 15\% improvement on the latter two tasks. Additionally, it matches the performance of recent ANN-based methods while offering significant efficiency gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the three tasks. https://github.com/JimmyZou/SpikeVideoFormer
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Lightweight Smartphone ISP with Unpaired Data</title>
<link>https://arxiv.org/abs/2505.10420</link>
<guid>https://arxiv.org/abs/2505.10420</guid>
<content:encoded><![CDATA[
<div> Keywords: Image Signal Processor, deep learning, smartphone camera, unpaired data, lightweight neural network

Summary:
This article introduces a novel training method for a learnable Image Signal Processor (ISP) in smartphone cameras, utilizing deep learning techniques. The traditional method of acquiring paired data for training is costly and challenging. The proposed approach eliminates the need for direct correspondences between raw images and ground-truth data by using unpaired data. The method utilizes a multi-term loss function and adversarial training with multiple discriminators to maintain content structure while learning color and texture characteristics. Lightweight neural network architectures suitable for mobile devices are used as backbones in the training process. The method is evaluated on Zurich RAW to RGB and Fujifilm UltraISP datasets and shows strong potential, achieving high fidelity across multiple evaluation metrics. The code and pre-trained models are made available for further research and development. <br /><br />Summary: <div>
arXiv:2505.10420v1 Announce Type: new 
Abstract: The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision language models have difficulty recognizing virtual objects</title>
<link>https://arxiv.org/abs/2505.10453</link>
<guid>https://arxiv.org/abs/2505.10453</guid>
<content:encoded><![CDATA[
<div> virtual objects, scene comprehension, vision language models, AI systems, multimodal input
Summary:<br /><br />Vision language models (VLMs) combine language and vision encoders to process multimodal input. While capable of tasks like automatic captioning, their comprehension of scene visuospatial properties remains uncertain. The study proposes using virtual objects in descriptions to test scene comprehension. By introducing virtual objects not visually depicted, such as a kite stuck in a tree in an image showing a person under a tree, VLMs should update their representations and reason spatial relations. The research systematically evaluates leading VLMs and finds their processing of virtual objects lacking. This highlights a gap in VLMs' scene comprehension abilities, indicating a need for further development to enhance their understanding of complex semantic tasks. <br /><br />Summary: <div>
arXiv:2505.10453v1 Announce Type: new 
Abstract: Vision language models (VLMs) are AI systems paired with both language and vision encoders to process multimodal input. They are capable of performing complex semantic tasks such as automatic captioning, but it remains an open question about how well they comprehend the visuospatial properties of scenes depicted in the images they process. We argue that descriptions of virtual objects -- objects that are not visually represented in an image -- can help test scene comprehension in these AI systems. For example, an image that depicts a person standing under a tree can be paired with the following prompt: imagine that a kite is stuck in the tree. VLMs that comprehend the scene should update their representations and reason sensibly about the spatial relations between all three objects. We describe systematic evaluations of state-of-the-art VLMs and show that their ability to process virtual objects is inadequate.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.10473</link>
<guid>https://arxiv.org/abs/2505.10473</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian splatting, rendering quality, quantity-quality trade-off, optimization, ControlGS

Summary: ControlGS is a new method for 3D Gaussian splatting that allows users to adjust the trade-off between the number of Gaussians used and rendering quality. It achieves consistent quantity-quality control across different scenes, from compact objects to large outdoor scenes, by automatically finding desirable trade-off points based on user-specified preferences. ControlGS outperforms existing methods by producing higher rendering quality with fewer Gaussians, making it more efficient in terms of storage and computational costs. It also offers stepless control over the trade-off, allowing users to fine-tune the balance according to their specific needs. Overall, ControlGS provides a flexible and intuitive way to optimize 3D Gaussian splatting for diverse practical requirements. 

<br /><br />Summary: ControlGS is a new method for 3D Gaussian splatting that enables users to adjust the trade-off between the number of Gaussians used and rendering quality. It achieves consistent quantity-quality control across different scenes, outperforming existing methods by producing higher rendering quality with fewer Gaussians. ControlGS offers stepless control over the trade-off and provides a flexible and intuitive way to optimize 3D Gaussian splatting for diverse practical requirements. <div>
arXiv:2505.10473v1 Announce Type: new 
Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks to minimize the number of Gaussians used while preserving high rendering quality, introducing an inherent trade-off between Gaussian quantity and rendering quality. Existing methods strive for better quantity-quality performance, but lack the ability for users to intuitively adjust this trade-off to suit practical needs such as model deployment under diverse hardware and communication constraints. Here, we present ControlGS, a 3DGS optimization method that achieves semantically meaningful and cross-scene consistent quantity-quality control while maintaining strong quantity-quality performance. Through a single training run using a fixed setup and a user-specified hyperparameter reflecting quantity-quality preference, ControlGS can automatically find desirable quantity-quality trade-off points across diverse scenes, from compact objects to large outdoor scenes. It also outperforms baselines by achieving higher rendering quality with fewer Gaussians, and supports a broad adjustment range with stepless control over the trade-off.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logos as a Well-Tempered Pre-train for Sign Language Recognition</title>
<link>https://arxiv.org/abs/2505.10481</link>
<guid>https://arxiv.org/abs/2505.10481</guid>
<content:encoded><![CDATA[
<div> Keywords: isolated sign language recognition, cross-language transfer learning, Logos dataset, visually similar sign groups, pre-trained models

Summary:
The paper addresses challenges in isolated sign language recognition, particularly the lack of data for individual sign languages and ambiguity in labeling similar signs with different meanings. To tackle these issues, the authors introduce the Logos dataset for Russian Sign Language, which is extensive and includes visually similar sign groups. They demonstrate that a model pretrained on Logos can serve as a universal encoder for various sign language recognition tasks, including few-shot learning. By exploring cross-language transfer learning, they show that joint training with multiple classification heads enhances accuracy on low-resource target datasets. Additionally, explicitly labeling visually similar signs improves model quality as a visual encoder for downstream tasks. The study surpasses current state-of-the-art results on the WLASL dataset and achieves competitive performance on the AUTSL dataset using a single-stream model processing RGB video. The source code, dataset, and pretrained models are publicly available. 

<br /><br />Summary: <div>
arXiv:2505.10481v1 Announce Type: new 
Abstract: This paper examines two aspects of the isolated sign language recognition (ISLR) task. First, despite the availability of a number of datasets, the amount of data for most individual sign languages is limited. It poses the challenge of cross-language ISLR model training, including transfer learning. Second, similar signs can have different semantic meanings. It leads to ambiguity in dataset labeling and raises the question of the best policy for annotating such signs. To address these issues, this study presents Logos, a novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by the number of signers and one of the largest available datasets while also the largest RSL dataset in size and vocabulary. It is shown that a model, pre-trained on the Logos dataset can be used as a universal encoder for other language SLR tasks, including few-shot learning. We explore cross-language transfer learning approaches and find that joint training using multiple classification heads benefits accuracy for the target lowresource datasets the most. The key feature of the Logos dataset is explicitly annotated visually similar sign groups. We show that explicitly labeling visually similar signs improves trained model quality as a visual encoder for downstream tasks. Based on the proposed contributions, we outperform current state-of-the-art results for the WLASL dataset and get competitive results for the AUTSL dataset, with a single stream model processing solely RGB video. The source code, dataset, and pre-trained models are publicly available.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2505.10483</link>
<guid>https://arxiv.org/abs/2505.10483</guid>
<content:encoded><![CDATA[
<div> evaluation framework, unified multimodal models, instruction-following capabilities, benchmark, UniBench

Summary:
The article introduces UniEval, an evaluation framework for unified multimodal models that aims to streamline the evaluation process by eliminating the need for extra models, images, or annotations. UniEval includes UniBench, a comprehensive benchmark with 81 diverse tags to assess model performance. The framework also introduces UniScore, a new metric that aligns closely with human evaluations and surpasses current metrics in evaluating instruction-following capabilities. Experimental results demonstrate that UniBench is more challenging than existing benchmarks and provides valuable insights into the strengths of state-of-the-art unified and visual generation models. Overall, UniEval offers a simplified and unified approach to evaluating multimodal models, enhancing their capabilities in instruction-following tasks. 

<br /><br />Summary: <div>
arXiv:2505.10483v1 Announce Type: new 
Abstract: The emergence of unified multimodal understanding and generation models is rapidly attracting attention because of their ability to enhance instruction-following capabilities while minimizing model redundancy. However, there is a lack of a unified evaluation framework for these models, which would enable an elegant, simplified, and overall evaluation. Current models conduct evaluations on multiple task-specific benchmarks, but there are significant limitations, such as the lack of overall results, errors from extra evaluation models, reliance on extensive labeled images, benchmarks that lack diversity, and metrics with limited capacity for instruction-following evaluation. To tackle these challenges, we introduce UniEval, the first evaluation framework designed for unified multimodal models without extra models, images, or annotations. This facilitates a simplified and unified evaluation process. The UniEval framework contains a holistic benchmark, UniBench (supports both unified and visual generation models), along with the corresponding UniScore metric. UniBench includes 81 fine-grained tags contributing to high diversity. Experimental results indicate that UniBench is more challenging than existing benchmarks, and UniScore aligns closely with human evaluations, surpassing current metrics. Moreover, we extensively evaluated SoTA unified and visual generation models, uncovering new insights into Univeral's unique values.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs</title>
<link>https://arxiv.org/abs/2505.10496</link>
<guid>https://arxiv.org/abs/2505.10496</guid>
<content:encoded><![CDATA[
<div> evaluation framework, synthetic chest radiograph generation, generative AI, medical domain, text-to-image architectures

Summary: 
The article introduces CheXGenBench, a comprehensive evaluation framework for synthetic chest radiograph generation that assesses fidelity, privacy risks, and clinical utility of leading text-to-image generative models in the medical domain. The framework addresses methodological inconsistencies, outdated comparisons, and disconnected assessment criteria that hinder evaluations in the medical AI field. It includes standardized data partitioning and a unified protocol with over 20 quantitative metrics to analyze generation quality, privacy vulnerabilities, and clinical applicability. The results highlight inefficiencies in current evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent comparisons. CheXGenBench establishes a benchmark for the medical AI community, enabling objective comparisons and facilitating integration of existing and future generative models. Additionally, a synthetic dataset, SynthCheX-75K, generated by the top-performing model (Sana 0.6B) is released to support further research in this area. The framework, models, and dataset are available at https://raman1121.github.io/CheXGenBench/.<br /><br />Summary: <div>
arXiv:2505.10496v1 Announce Type: new 
Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks</title>
<link>https://arxiv.org/abs/2505.10497</link>
<guid>https://arxiv.org/abs/2505.10497</guid>
<content:encoded><![CDATA[
arXiv:2505.10497v1 Announce Type: new 
Abstract: Face recognition has evolved significantly with the advancement of deep learning techniques, enabling its widespread adoption in various applications requiring secure authentication. However, this progress has also increased its exposure to presentation attacks, including face morphing, which poses a serious security threat by allowing one identity to impersonate another. Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face recognition with enhanced robustness to face morphing attacks. Our method modifies the classification task by introducing a dual-branch classification strategy that effectively handles the ambiguity in the labeling of face morphs. This adaptation allows the model to incorporate morph images into the training process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its effectiveness in enhancing robustness against face morphing attacks. Furthermore, our approach is universally applicable and can be integrated into existing face recognition training pipelines to improve classification-based recognition methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multi-Image Question Answering via Submodular Subset Selection</title>
<link>https://arxiv.org/abs/2505.10533</link>
<guid>https://arxiv.org/abs/2505.10533</guid>
<content:encoded><![CDATA[
arXiv:2505.10533v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have achieved high performance in vision-language tasks involving single image but they struggle when presented with a collection of multiple images (Multiple Image Question Answering scenario). These tasks, which involve reasoning over large number of images, present issues in scalability (with increasing number of images) and retrieval performance. In this work, we propose an enhancement for retriever framework introduced in MIRAGE model using submodular subset selection techniques. Our method leverages query-aware submodular functions, such as GraphCut, to pre-select a subset of semantically relevant images before main retrieval component. We demonstrate that using anchor-based queries and augmenting the data improves submodular-retriever pipeline effectiveness, particularly in large haystack sizes.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis</title>
<link>https://arxiv.org/abs/2505.10541</link>
<guid>https://arxiv.org/abs/2505.10541</guid>
<content:encoded><![CDATA[
arXiv:2505.10541v1 Announce Type: new 
Abstract: Recent advancements have enhanced the capability of Multimodal Large Language Models (MLLMs) to comprehend multi-image information. However, existing benchmarks primarily evaluate answer correctness, overlooking whether models genuinely comprehend the visual input. To address this, we define implicit visual misunderstanding (IVM), where MLLMs provide correct answers without fully comprehending the visual input. Through our analysis, we decouple the visual and textual modalities within the causal attention module, revealing that attention distribution increasingly converges on the image associated with the correct answer as the network layers deepen. This insight leads to the introduction of a scale-agnostic metric, \textit{attention accuracy}, and a novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the model's visual understanding via internal mechanisms, remaining robust to positional biases for more reliable assessments. Furthermore, we extend our approach to finer granularities and demonstrate its effectiveness in unimodal scenarios, underscoring its versatility and generalizability.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data</title>
<link>https://arxiv.org/abs/2505.10551</link>
<guid>https://arxiv.org/abs/2505.10551</guid>
<content:encoded><![CDATA[
arXiv:2505.10551v1 Announce Type: new 
Abstract: With the development of photorealistic diffusion models, models trained in part or fully on synthetic data achieve progressively better results. However, diffusion models still routinely generate images that would not exist in reality, such as a dog floating above the ground or with unrealistic texture artifacts. We define the concept of feasibility as whether attributes in a synthetic image could realistically exist in the real-world domain; synthetic images containing attributes that violate this criterion are considered infeasible. Intuitively, infeasible images are typically considered out-of-distribution; thus, training on such images is expected to hinder a model's ability to generalize to real-world data, and they should therefore be excluded from the training set whenever possible. However, does feasibility really matter? In this paper, we investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture. We introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model. Our experiments show that feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Also, the attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Finally, mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.10557</link>
<guid>https://arxiv.org/abs/2505.10557</guid>
<content:encoded><![CDATA[
arXiv:2505.10557v1 Announce Type: new 
Abstract: Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at https://github.com/mathllm/MathCoder.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Vision Tokenizer Tuning</title>
<link>https://arxiv.org/abs/2505.10562</link>
<guid>https://arxiv.org/abs/2505.10562</guid>
<content:encoded><![CDATA[
arXiv:2505.10562v1 Announce Type: new 
Abstract: Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is agnostic to downstream tasks requiring varied representations and semantics. This decoupled paradigm introduces a critical misalignment: The loss of the vision tokenization can be the representation bottleneck for target tasks. For example, errors in tokenizing text in a given image lead to poor results when recognizing or generating them. To address this, we propose ETT, an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth Anything with Any Prior</title>
<link>https://arxiv.org/abs/2505.10565</link>
<guid>https://arxiv.org/abs/2505.10565</guid>
<content:encoded><![CDATA[
arXiv:2505.10565v1 Announce Type: new 
Abstract: This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-Fixup: Advancing Photo Editing with 3D Priors</title>
<link>https://arxiv.org/abs/2505.10566</link>
<guid>https://arxiv.org/abs/2505.10566</guid>
<content:encoded><![CDATA[
arXiv:2505.10566v1 Announce Type: new 
Abstract: Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative diffusion model surrogates for mechanistic agent-based biological models</title>
<link>https://arxiv.org/abs/2505.09630</link>
<guid>https://arxiv.org/abs/2505.09630</guid>
<content:encoded><![CDATA[
arXiv:2505.09630v1 Announce Type: cross 
Abstract: Mechanistic, multicellular, agent-based models are commonly used to investigate tissue, organ, and organism-scale biology at single-cell resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework for developing and interrogating these models. CPMs become computationally expensive at large space- and time- scales making application and investigation of developed models difficult. Surrogate models may allow for the accelerated evaluation of CPMs of complex biological systems. However, the stochastic nature of these models means each set of parameters may give rise to different model configurations, complicating surrogate model development. In this work, we leverage denoising diffusion probabilistic models to train a generative AI surrogate of a CPM used to investigate \textit{in vitro} vasculogenesis. We describe the use of an image classifier to learn the characteristics that define unique areas of a 2-dimensional parameter space. We then apply this classifier to aid in surrogate model selection and verification. Our CPM model surrogate generates model configurations 20,000 timesteps ahead of a reference configuration and demonstrates approximately a 22x reduction in computational time as compared to native code execution. Our work represents a step towards the implementation of DDPMs to develop digital twins of stochastic biological systems.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses</title>
<link>https://arxiv.org/abs/2505.09819</link>
<guid>https://arxiv.org/abs/2505.09819</guid>
<content:encoded><![CDATA[
arXiv:2505.09819v1 Announce Type: cross 
Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern recognition (PR) control systems that translate electromyography (EMG) signals into desired movements. As prosthesis movement complexity increases, users often struggle to produce sufficiently distinct EMG patterns for reliable classification. Existing training typically involves heuristic, trial-and-error user adjustments to static decoder boundaries. Goal: We introduce the Reviewer, a 3D visual interface projecting EMG signals directly into the decoder's classification space, providing intuitive, real-time insight into PR algorithm behavior. This structured feedback reduces cognitive load and fosters mutual, data-driven adaptation between user-generated EMG patterns and decoder boundaries. Methods: A 10-session study with 12 able-bodied participants compared PR performance after motor-based training and updating using the Reviewer versus conventional virtual arm visualization. Performance was assessed using a Fitts law task that involved the aperture of the cursor and the control of orientation. Results: Participants trained with the Reviewer achieved higher completion rates, reduced overshoot, and improved path efficiency and throughput compared to the standard visualization group. Significance: The Reviewer introduces decoder-informed motor training, facilitating immediate and consistent PR-based myoelectric control improvements. By iteratively refining control through real-time feedback, this approach reduces reliance on trial-and-error recalibration, enabling a more adaptive, self-correcting training framework. Conclusion: The 3D visual feedback significantly improves PR control in novice operators through structured training, enabling feedback-driven adaptation and reducing reliance on extensive heuristic adjustments.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions</title>
<link>https://arxiv.org/abs/2505.09831</link>
<guid>https://arxiv.org/abs/2505.09831</guid>
<content:encoded><![CDATA[
arXiv:2505.09831v1 Announce Type: cross 
Abstract: Hematoxylin and eosin (H&amp;E) staining is a gold standard for microscopic diagnosis in pathology. However, H&amp;E staining does not capture all the diagnostic information that may be needed. To obtain additional molecular information, immunohistochemical (IHC) stains highlight proteins that mark specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells. While IHC stains are vital for prognosis and treatment guidance, they are typically only available at specialized centers and time consuming to acquire, leading to treatment delays for patients. Virtual staining, enabled by deep learning-based image translation models, provides a promising alternative by computationally generating IHC stains from H&amp;E stained images. Although many GAN and diffusion based image to image (I2I) translation methods have been used for virtual staining, these models treat image patches as independent data points, which results in increased and more diverse data requirements for effective generation. We present ImplicitStainer, a novel approach that leverages local implicit functions to improve image translation, specifically virtual staining performance, by focusing on pixel-level predictions. This method enhances robustness to variations in dataset sizes, delivering high-quality results even with limited data. We validate our approach on two datasets using a comprehensive set of metrics and benchmark it against over fifteen state-of-the-art GAN- and diffusion based models. Full Code and models trained will be released publicly via Github upon acceptance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction</title>
<link>https://arxiv.org/abs/2505.09985</link>
<guid>https://arxiv.org/abs/2505.09985</guid>
<content:encoded><![CDATA[
arXiv:2505.09985v1 Announce Type: cross 
Abstract: Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT projection data into equal subsets and employs multi-subsets diffusion model (MSDM) to learn from each subset independently. This targeted learning approach reduces complexity and enhances the reconstruction of fine details. Furthermore, the integration of one-whole diffusion model (OWDM) with complete sinogram data acts as a global information constraint, which can reduce the possibility of generating erroneous or inconsistent sinogram information. Moreover, the OSMM's unsupervised learning framework provides strong robustness and generalizability, adapting seamlessly to varying sparsity levels of CT sinograms. This ensures consistent and reliable performance across different clinical scenarios. Experimental results demonstrate that OSMM outperforms traditional diffusion models in terms of image quality and noise resilience, offering a powerful and versatile solution for advanced CT imaging in sparse-view scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation</title>
<link>https://arxiv.org/abs/2505.10075</link>
<guid>https://arxiv.org/abs/2505.10075</guid>
<content:encoded><![CDATA[
arXiv:2505.10075v1 Announce Type: cross 
Abstract: This paper investigates training better visual world models for robot manipulation, i.e., models that can predict future visual observations by conditioning on past frames and robot actions. Specifically, we consider world models that operate on RGB-D frames (RGB-D world models). As opposed to canonical approaches that handle dynamics prediction mostly implicitly and reconcile it with visual rendering in a single model, we introduce FlowDreamer, which adopts 3D scene flow as explicit motion representations. FlowDreamer first predicts 3D scene flow from past frame and action conditions with a U-Net, and then a diffusion model will predict the future frame utilizing the scene flow. FlowDreamer is trained end-to-end despite its modularized nature. We conduct experiments on 4 different benchmarks, covering both video prediction and visual planning tasks. The results demonstrate that FlowDreamer achieves better performance compared to other baseline RGB-D world models by 7% on semantic similarity, 11% on pixel quality, and 6% on success rate in various robot manipulation domains.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality</title>
<link>https://arxiv.org/abs/2505.10144</link>
<guid>https://arxiv.org/abs/2505.10144</guid>
<content:encoded><![CDATA[
arXiv:2505.10144v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours</title>
<link>https://arxiv.org/abs/2505.10271</link>
<guid>https://arxiv.org/abs/2505.10271</guid>
<content:encoded><![CDATA[
arXiv:2505.10271v1 Announce Type: cross 
Abstract: We present a deep learning model for high-resolution probabilistic precipitation forecasting over an 8-hour horizon in Europe, overcoming the limitations of radar-only deep learning models with short forecast lead times. Our model efficiently integrates multiple data sources - including radar, satellite, and physics-based numerical weather prediction (NWP) - while capturing long-range interactions, resulting in accurate forecasts with robust uncertainty quantification through consistent probabilistic maps. Featuring a compact architecture, it enables more efficient training and faster inference than existing models. Extensive experiments demonstrate that our model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models, setting a new standard for high-resolution precipitation forecasting in Europe, ensuring a balance between accuracy, interpretability, and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.10312</link>
<guid>https://arxiv.org/abs/2505.10312</guid>
<content:encoded><![CDATA[
arXiv:2505.10312v1 Announce Type: cross 
Abstract: In the realm of Human Activity Recognition (HAR), obtaining high quality and variance data is still a persistent challenge due to high costs and the inherent variability of real-world activities. This study introduces a generation dataset by deep learning approaches (Attention Autoencoder and conditional Generative Adversarial Networks). Another problem that data heterogeneity is a critical challenge, one of the solutions is to shuffle the data to homogenize the distribution. Experimental results demonstrate that the random sequence strategy significantly improves classification performance, achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64 $\pm$ 0.01. For that, disrupting temporal dependencies through random sequence reordering compels the model to focus on instantaneous recognition, thereby improving robustness against activity transitions. This approach not only broadens the effective training dataset but also offers promising avenues for enhancing HAR systems in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding</title>
<link>https://arxiv.org/abs/2505.10405</link>
<guid>https://arxiv.org/abs/2505.10405</guid>
<content:encoded><![CDATA[
arXiv:2505.10405v1 Announce Type: cross 
Abstract: Generative semantic communication (Gen-SemCom) with large artificial intelligence (AI) model promises a transformative paradigm for 6G networks, which reduces communication costs by transmitting low-dimensional prompts rather than raw data. However, purely prompt-driven generation loses fine-grained visual details. Additionally, there is a lack of systematic metrics to evaluate the performance of Gen-SemCom systems. To address these issues, we develop a hybrid Gen-SemCom system with a critical information embedding (CIE) framework, where both text prompts and semantically critical features are extracted for transmissions. First, a novel approach of semantic filtering is proposed to select and transmit the semantically critical features of images relevant to semantic label. By integrating the text prompt and critical features, the receiver reconstructs high-fidelity images using a diffusion-based generative model. Next, we propose the generative visual information fidelity (GVIF) metric to evaluate the visual quality of the generated image. By characterizing the statistical models of image features, the GVIF metric quantifies the mutual information between the distorted features and their original counterparts. By maximizing the GVIF metric, we design a channel-adaptive Gen-SemCom system that adaptively control the volume of features and compression rate according to the channel state. Experimental results validate the GVIF metric's sensitivity to visual fidelity, correlating with both the PSNR and critical information volume. In addition, the optimized system achieves superior performance over benchmarking schemes in terms of higher PSNR and lower FID scores.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIF: Anomaly detection via preference embedding</title>
<link>https://arxiv.org/abs/2505.10441</link>
<guid>https://arxiv.org/abs/2505.10441</guid>
<content:encoded><![CDATA[
arXiv:2505.10441v1 Announce Type: cross 
Abstract: We address the problem of detecting anomalies with respect to structured patterns. To this end, we conceive a novel anomaly detection method called PIF, that combines the advantages of adaptive isolation methods with the flexibility of preference embedding. Specifically, we propose to embed the data in a high dimensional space where an efficient tree-based method, PI-Forest, is employed to compute an anomaly score. Experiments on synthetic and real datasets demonstrate that PIF favorably compares with state-of-the-art anomaly detection techniques, and confirm that PI-Forest is better at measuring arbitrary distances and isolate points in the preference space.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAL: Searching Expandable Architectures for Incremental Learning</title>
<link>https://arxiv.org/abs/2505.10457</link>
<guid>https://arxiv.org/abs/2505.10457</guid>
<content:encoded><![CDATA[
arXiv:2505.10457v1 Announce Type: cross 
Abstract: Incremental learning is a machine learning paradigm where a model learns from a sequential stream of tasks. This setting poses a key challenge: balancing plasticity (learning new tasks) and stability (preserving past knowledge). Neural Architecture Search (NAS), a branch of AutoML, automates the design of the architecture of Deep Neural Networks and has shown success in static settings. However, existing NAS-based approaches to incremental learning often rely on expanding the model at every task, making them impractical in resource-constrained environments. In this work, we introduce SEAL, a NAS-based framework tailored for data-incremental learning, a scenario where disjoint data samples arrive sequentially and are not stored for future access. SEAL adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric. Stability is preserved through cross-distillation training after each expansion step. The NAS component jointly searches for both the architecture and the optimal expansion policy. Experiments across multiple benchmarks demonstrate that SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods. These results highlight the promise of combining NAS and selective expansion for efficient, adaptive learning in incremental scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation</title>
<link>https://arxiv.org/abs/2505.10464</link>
<guid>https://arxiv.org/abs/2505.10464</guid>
<content:encoded><![CDATA[
arXiv:2505.10464v1 Announce Type: cross 
Abstract: Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities' anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68\% in the Dice score while maintaining solid robustness. The dataset and code are public via https://github.com/JeMing-creater/HWA-UNETR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-contrast laser endoscopy for in vivo gastrointestinal imaging</title>
<link>https://arxiv.org/abs/2505.10492</link>
<guid>https://arxiv.org/abs/2505.10492</guid>
<content:encoded><![CDATA[
arXiv:2505.10492v1 Announce Type: cross 
Abstract: White light endoscopy is the clinical gold standard for detecting diseases in the gastrointestinal tract. Most applications involve identifying visual abnormalities in tissue color, texture, and shape. Unfortunately, the contrast of these features is often subtle, causing many clinically relevant cases to go undetected. To overcome this challenge, we introduce Multi-contrast Laser Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable spectral, coherent, and directional illumination. We demonstrate three capabilities of MLE: enhancing tissue chromophore contrast with multispectral diffuse reflectance, quantifying blood flow using laser speckle contrast imaging, and characterizing mucosal topography using photometric stereo. We validate MLE with benchtop models, then demonstrate MLE in vivo during clinical colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold improvement in contrast and a five-fold improvement in color difference compared to white light and narrow band imaging. With the ability to reveal multiple complementary types of tissue contrast while seamlessly integrating into the clinical environment, MLE shows promise as an investigative tool to improve gastrointestinal imaging.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Token Prediction Needs Registers</title>
<link>https://arxiv.org/abs/2505.10518</link>
<guid>https://arxiv.org/abs/2505.10518</guid>
<content:encoded><![CDATA[
arXiv:2505.10518v1 Announce Type: cross 
Abstract: Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10526</link>
<guid>https://arxiv.org/abs/2505.10526</guid>
<content:encoded><![CDATA[
arXiv:2505.10526v1 Announce Type: cross 
Abstract: Speculative decoding significantly accelerates language model inference by enabling a lightweight draft model to propose multiple tokens that a larger target model verifies simultaneously. However, applying this technique to vision-language models (VLMs) presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context. We introduce Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models (MASSV), which transforms existing small language models into effective multimodal drafters through a two-phase approach. MASSV first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions. Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV provides a scalable, architecture-compatible method for accelerating both current and future VLMs.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Customization of Text-to-Vector Generation with Image Diffusion Priors</title>
<link>https://arxiv.org/abs/2505.10558</link>
<guid>https://arxiv.org/abs/2505.10558</guid>
<content:encoded><![CDATA[
arXiv:2505.10558v1 Announce Type: cross 
Abstract: Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Efficient 3D Human Pose Tracking from Events with Spiking Spatiotemporal Transformer</title>
<link>https://arxiv.org/abs/2303.09681</link>
<guid>https://arxiv.org/abs/2303.09681</guid>
<content:encoded><![CDATA[
arXiv:2303.09681v5 Announce Type: replace 
Abstract: Event camera, as an asynchronous vision sensor capturing scene dynamics, presents new opportunities for highly efficient 3D human pose tracking. Existing approaches typically adopt modern-day Artificial Neural Networks (ANNs), such as CNNs or Transformer, where sparse events are converted into dense images or paired with additional gray-scale images as input. Such practices, however, ignore the inherent sparsity of events, resulting in redundant computations, increased energy consumption, and potentially degraded performance. Motivated by these observations, we introduce the first sparse Spiking Neural Networks (SNNs) framework for 3D human pose tracking based solely on events. Our approach eliminates the need to convert sparse data to dense formats or incorporate additional images, thereby fully exploiting the innate sparsity of input events. Central to our framework is a novel Spiking Spatiotemporal Transformer, which enables bi-directional spatiotemporal fusion of spike pose features and provides a guaranteed similarity measurement between binary spike features in spiking attention. Moreover, we have constructed a large-scale synthetic dataset, SynEventHPD, that features a broad and diverse set of 3D human motions, as well as much longer hours of event streams. Empirical experiments demonstrate the superiority of our approach over existing state-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6% energy cost. Furthermore, our approach outperforms existing SNN-based benchmarks in this task, highlighting the effectiveness of our proposed SNN framework. The dataset will be released upon acceptance, and code can be found at https://github.com/JimmyZou/HumanPoseTracking_SNN.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Student Behavioral Engagement using Histogram of Actions</title>
<link>https://arxiv.org/abs/2307.09420</link>
<guid>https://arxiv.org/abs/2307.09420</guid>
<content:encoded><![CDATA[
arXiv:2307.09420v2 Announce Type: replace 
Abstract: In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition. The proposed approach recognizes student actions then predicts the student behavioral engagement level. For student action recognition, we use human skeletons to model student postures and upper body movements. To learn the dynamics of student upper body, a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies. This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged. To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with Multimodal Diffusion</title>
<link>https://arxiv.org/abs/2401.14066</link>
<guid>https://arxiv.org/abs/2401.14066</guid>
<content:encoded><![CDATA[
arXiv:2401.14066v3 Announce Type: replace 
Abstract: Although remarkable progress has been made in image style transfer, style is just one of the components of artistic paintings. Directly transferring extracted style features to natural images often results in outputs with obvious synthetic traces. This is because key painting attributes including layout, perspective, shape, and semantics often cannot be conveyed and expressed through style transfer. Large-scale pretrained text-to-image generation models have demonstrated their capability to synthesize a vast amount of high-quality images. However, even with extensive textual descriptions, it is challenging to fully express the unique visual properties and details of paintings. Moreover, generic models often disrupt the overall artistic effect when modifying specific areas, making it more complicated to achieve a unified aesthetic in artworks. Our main novel idea is to integrate multimodal semantic information as a synthesis guide into artworks, rather than transferring style to the real world. We also aim to reduce the disruption to the harmony of artworks while simplifying the guidance conditions. Specifically, we propose an innovative multi-task unified framework called CreativeSynth, based on the diffusion model with the ability to coordinate multimodal inputs. CreativeSynth combines multimodal features with customized attention mechanisms to seamlessly integrate real-world semantic content into the art domain through Cross-Art-Attention for aesthetic maintenance and semantic fusion. We demonstrate the results of our method across a wide range of different art categories, proving that CreativeSynth bridges the gap between generative models and artistic expression. Code and results are available at https://github.com/haha-lisa/CreativeSynth.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</title>
<link>https://arxiv.org/abs/2403.07547</link>
<guid>https://arxiv.org/abs/2403.07547</guid>
<content:encoded><![CDATA[
arXiv:2403.07547v2 Announce Type: replace 
Abstract: Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. To effectively handle this issue, we propose sequential motion understanding radiance fields (SMURF), a novel approach that models continuous camera motion and leverages the explicit volumetric representation method for robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a module designed to model a continuous camera movements for processing blurry inputs. Our model is evaluated against benchmark datasets and demonstrates state-of-the-art performance both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>https://arxiv.org/abs/2405.03689</link>
<guid>https://arxiv.org/abs/2405.03689</guid>
<content:encoded><![CDATA[
arXiv:2405.03689v2 Announce Type: replace 
Abstract: Language is often used to describe physical interaction, yet most 3D human pose estimation methods overlook this rich source of information. We bridge this gap by leveraging large multimodal models (LMMs) as priors for reconstructing contact poses, offering a scalable alternative to traditional methods that rely on human annotations or motion capture data. Our approach extracts contact-relevant descriptors from an LMM and translates them into tractable losses to constrain 3D human pose optimization. Despite its simplicity, our method produces compelling reconstructions for both two-person interactions and self-contact scenarios, accurately capturing the semantics of physical and social interactions. Our results demonstrate that LMMs can serve as powerful tools for contact prediction and pose estimation, offering an alternative to costly manual human annotations or motion capture data. Our code is publicly available at https://prosepose.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2406.02147</link>
<guid>https://arxiv.org/abs/2406.02147</guid>
<content:encoded><![CDATA[
arXiv:2406.02147v2 Announce Type: replace 
Abstract: 3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods are still in the early stages of development and lack systematic improvements, failing to track objects in certain complex scenarios, like occlusions and the small size of target object's situations. In this paper, we first summarize the current end-to-end 3D MOT framework by decomposing it into three constituent parts: query initialization, query propagation, and query matching. Then we propose corresponding improvements, which lead to a strong yet simple tracker: S2-Track. Specifically, for query initialization, we present 2D-Prompted Query Initialization, which leverages predicted 2D object and depth information to prompt an initial estimate of the object's 3D location. For query propagation, we introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty of complex environment in object prediction with probabilistic attention. For query matching, we propose a Hierarchical Query Denoising strategy to enhance training robustness and convergence. As a result, our S2-Track achieves state-of-the-art performance on nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st place on the nuScenes tracking task leaderboard.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Video Highlight Detection by Learning from Audio and Visual Recurrence</title>
<link>https://arxiv.org/abs/2407.13933</link>
<guid>https://arxiv.org/abs/2407.13933</guid>
<content:encoded><![CDATA[
arXiv:2407.13933v2 Announce Type: replace 
Abstract: With the exponential growth of video content, the need for automated video highlight detection to extract key moments or highlights from lengthy videos has become increasingly pressing. This technology has the potential to enhance user experiences by allowing quick access to relevant content across diverse domains. Existing methods typically rely either on expensive manually labeled frame-level annotations, or on a large external dataset of videos for weak supervision through category information. To overcome this, we focus on unsupervised video highlight detection, eliminating the need for manual annotations. We propose a novel unsupervised approach which capitalizes on the premise that significant moments tend to recur across multiple videos of the similar category in both audio and visual modalities. Surprisingly, audio remains under-explored, especially in unsupervised algorithms, despite its potential to detect key moments. Through a clustering technique, we identify pseudo-categories of videos and compute audio pseudo-highlight scores for each video by measuring the similarities of audio features among audio clips of all the videos within each pseudo-category. Similarly, we also compute visual pseudo-highlight scores for each video using visual features. Then, we combine audio and visual pseudo-highlights to create the audio-visual pseudo ground-truth highlight of each video for training an audio-visual highlight detection network. Extensive experiments and ablation studies on three benchmarks showcase the superior performance of our method over prior work.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models</title>
<link>https://arxiv.org/abs/2410.01262</link>
<guid>https://arxiv.org/abs/2410.01262</guid>
<content:encoded><![CDATA[
arXiv:2410.01262v3 Announce Type: replace 
Abstract: While many diffusion models perform well when controlling for particular aspect among style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper first introduces a novel training-free algorithm in fine-grained generation, Aggregation of Multiple Diffusion Models (AMDM), which integrates features from multiple diffusion models into a specified model to activate specific features and enable fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional control generation in diffusion models: We can fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: https://github.com/Hammour-steak/AMDM.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality</title>
<link>https://arxiv.org/abs/2410.16430</link>
<guid>https://arxiv.org/abs/2410.16430</guid>
<content:encoded><![CDATA[
arXiv:2410.16430v2 Announce Type: replace 
Abstract: Human hand and head movements are the most pervasive input modalities in extended reality (XR) and are significant for a wide range of applications. However, prior works on hand and head modelling in XR only explored a single modality or focused on specific applications. We present HaHeAE - a novel self-supervised method for learning generalisable joint representations of hand and head movements in XR. At the core of our method is an autoencoder (AE) that uses a graph convolutional network-based semantic encoder and a diffusion-based stochastic encoder to learn the joint semantic and stochastic representations of hand-head movements. It also features a diffusion-based decoder to reconstruct the original signals. Through extensive evaluations on three public XR datasets, we show that our method 1) significantly outperforms commonly used self-supervised methods by up to 74.0% in terms of reconstruction quality and is generalisable across users, activities, and XR environments, 2) enables new applications, including interpretable hand-head cluster identification and variable hand-head movement generation, and 3) can serve as an effective feature extractor for downstream tasks. Together, these results demonstrate the effectiveness of our method and underline the potential of self-supervised methods for jointly modelling hand-head behaviours in extended reality.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for View-Adaptive Rendering</title>
<link>https://arxiv.org/abs/2411.05731</link>
<guid>https://arxiv.org/abs/2411.05731</guid>
<content:encoded><![CDATA[
arXiv:2411.05731v3 Announce Type: replace 
Abstract: Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in real-time, high-quality 3D scene rendering. However, it faces several challenges, including Gaussian redundancy, limited ability to capture view-dependent effects, and difficulties in handling complex lighting and specular reflections. Additionally, methods that use spherical harmonics for color representation often struggle to effectively capture anisotropic components, especially when modeling view-dependent colors under complex lighting conditions, leading to insufficient contrast and unnatural color saturation. To address these limitations, we introduce PEP-GS, a perceptually-enhanced framework that dynamically predicts Gaussian attributes, including opacity, color, and covariance. We replace traditional spherical harmonics with a Hierarchical Granular-Structural Attention mechanism, which enables more accurate modeling of complex view-dependent color effects. By employing a stable and interpretable framework for opacity and covariance estimation, PEP-GS avoids the removal of essential Gaussians prematurely, ensuring a more accurate scene representation. Furthermore, perceptual optimization is applied to the final rendered images, enhancing perceptual consistency across different views and ensuring high-quality renderings with improved texture fidelity and fine-scale detail preservation. Experimental results demonstrate that PEP-GS outperforms state-of-the-art methods, particularly in challenging scenarios involving view-dependent effects and fine-scale details.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with Fused Geometric and Semantic Guidance</title>
<link>https://arxiv.org/abs/2411.08665</link>
<guid>https://arxiv.org/abs/2411.08665</guid>
<content:encoded><![CDATA[
arXiv:2411.08665v2 Announce Type: replace 
Abstract: OpenStreetMap (OSM), a rich and versatile source of volunteered geographic information (VGI), facilitates human self-localization and scene understanding by integrating nearby visual observations with vectorized map data. However, the disparity in modalities and perspectives poses a major challenge for effectively matching camera imagery with compact map representations, thereby limiting the full potential of VGI data in real-world localization applications.
  Inspired by the fact that the human brain relies on the fusion of geometric and semantic understanding for spatial localization tasks, we propose the OSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach based on first-person-view images against the OSM maps. It integrates semantic and geometric guidance to significantly improve accuracy, robustness, and generalization capability. First, we equip the OSMLoc with the visual foundational model to extract powerful image features. Second, a geometry-guided depth distribution adapter is proposed to bridge the monocular depth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings from the OSM data are utilized as auxiliary guidance for image-to-OSM feature matching. To validate the proposed OSMLoc, we collect a worldwide cross-area and cross-condition (CC) benchmark for extensive evaluation. Experiments on the MGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the superiority of our method. Code, pre-trained models, CC validation benchmark, and additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</title>
<link>https://arxiv.org/abs/2411.14347</link>
<guid>https://arxiv.org/abs/2411.14347</guid>
<content:encoded><![CDATA[
arXiv:2411.14347v3 Announce Type: replace 
Abstract: In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images</title>
<link>https://arxiv.org/abs/2412.16028</link>
<guid>https://arxiv.org/abs/2412.16028</guid>
<content:encoded><![CDATA[
arXiv:2412.16028v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has attracted significant attention for its high-quality novel view rendering, inspiring research to address real-world challenges. While conventional methods depend on sharp images for accurate scene reconstruction, real-world scenarios are often affected by defocus blur due to finite depth of field, making it essential to account for realistic 3D scene representation. In this study, we propose CoCoGaussian, a Circle of Confusion-aware Gaussian Splatting that enables precise 3D scene representation using only defocused images. CoCoGaussian addresses the challenge of defocus blur by modeling the Circle of Confusion (CoC) through a physically grounded approach based on the principles of photographic defocus. Exploiting 3D Gaussians, we compute the CoC diameter from depth and learnable aperture information, generating multiple Gaussians to precisely capture the CoC shape. Furthermore, we introduce a learnable scaling factor to enhance robustness and provide more flexibility in handling unreliable depth in scenes with reflective or refractive surfaces. Experiments on both synthetic and real-world datasets demonstrate that CoCoGaussian achieves state-of-the-art performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild</title>
<link>https://arxiv.org/abs/2412.16147</link>
<guid>https://arxiv.org/abs/2412.16147</guid>
<content:encoded><![CDATA[
arXiv:2412.16147v2 Announce Type: replace 
Abstract: Seagrass meadows play a crucial role in marine ecosystems, providing benefits such as carbon sequestration, water quality improvement, and habitat provision. Monitoring the distribution and abundance of seagrass is essential for environmental impact assessments and conservation efforts. However, the current manual methods of analyzing underwater video data to assess seagrass coverage are time-consuming and subjective. This work explores the use of deep learning models to automate the process of seagrass detection and coverage estimation from underwater video data. We create a new dataset of over 8,300 annotated underwater images, and subsequently evaluate several deep learning architectures, including ResNet, InceptionNetV3, DenseNet, and Vision Transformer for the task of binary classification on the presence and absence of seagrass by transfer learning. The results demonstrate that deep learning models, particularly Vision Transformers, can achieve high performance in predicting eelgrass presence, with AUROC scores exceeding 0.95 on the final test dataset. The application of underwater image enhancement further improved the models' prediction capabilities. Furthermore, we introduce a novel approach for estimating seagrass coverage from video data, showing promising preliminary results that align with expert manual labels, and indicating potential for consistent and scalable monitoring. The proposed methodology allows for the efficient processing of large volumes of video data, enabling the acquisition of much more detailed information on seagrass distributions in comparison to current manual methods. This information is crucial for environmental impact assessments and monitoring programs, as seagrasses are important indicators of coastal ecosystem health. This project demonstrates the value that deep learning can bring to the field of marine ecology and environmental monitoring.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illegal Waste Detection in Remote Sensing Images: A Case Study</title>
<link>https://arxiv.org/abs/2502.06607</link>
<guid>https://arxiv.org/abs/2502.06607</guid>
<content:encoded><![CDATA[
arXiv:2502.06607v3 Announce Type: replace 
Abstract: Environmental crime is the third largest criminal activity worldwide, with significant revenues coming from illegal management of solid waste. Thanks to the increasing availability and the decreasing cost of Very High Resolution Remote Sensing (VHR RS) images, the fight against environmental crime can nowadays rely on modern image-analysis tools to support photo-interpretation for scanning vast territories in search of illegal waste disposal sites. This paper illustrates a semi-automatic waste detection pipeline, developed in collaboration with a regional environmental protection agency, for detecting candidate illegal dumping sites in VHR RS images. To optimize the effectiveness of the waste detector, extensive experiments evaluate such design choices as the network architecture, the ground resolution and geographic span of the input images, as well as the pretraining procedures. The best model attains remarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A generalization study assesses the performance variation when the detector processes images from a territory substantially different from the one used during training, incurring only a moderate performance loss, i.e., 6.5% decrease in the F1-Score. Finally, an exercise in which photo interpreters compare the territory scanning effort with and without the support of the waste detector assesses the concrete benefit of using a computer-aided image analysis tool in a professional environment protection agency. Results show that a reduction up to 30% of the time spent for waste site detection can be attained.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoMamba: An Efficient Foundation Model for Endoscopic Videos via Hierarchical Pre-training</title>
<link>https://arxiv.org/abs/2502.19090</link>
<guid>https://arxiv.org/abs/2502.19090</guid>
<content:encoded><![CDATA[
arXiv:2502.19090v2 Announce Type: replace 
Abstract: Endoscopic video-based tasks, such as visual navigation and surgical phase recognition, play a crucial role in minimally invasive surgeries by providing real-time assistance. While recent video foundation models have shown promise, their applications are hindered by (1) computational inefficiencies and (2) suboptimal performance caused by limited data for pre-training in endoscopy. To address these issues, we present EndoMamba, a foundation model designed for real-time inference while learning generalized spatiotemporal representations. First, to mitigate computational inefficiencies, we propose the EndoMamba backbone, optimized for real-time inference. Inspired by recent advancements in state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial modeling within individual frames and vanilla Mamba blocks for past-to-present reasoning across the temporal domain. This design enables both strong spatiotemporal modeling and efficient inference in online video streams. Second, we propose a self-supervised hierarchical pre-training diagram to enhance EndoMamba's representation learning using endoscopic videos and incorporating general video domain knowledge. Specifically, our approach combines masked reconstruction with auxiliary supervision, leveraging low-level reconstruction to capture spatial-temporal structures and high-level alignment to transfer broader knowledge from a pretrained general-video domain foundation model. Extensive experiments on four downstream tasks--classification, segmentation, surgical phase recognition, and localization--demonstrate that EndoMamba outperforms existing foundation models and task-specific methods while maintaining real-time inference speed. The source code is available at https://github.com/TianCuteQY/EndoMamba.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model</title>
<link>https://arxiv.org/abs/2503.11792</link>
<guid>https://arxiv.org/abs/2503.11792</guid>
<content:encoded><![CDATA[
arXiv:2503.11792v2 Announce Type: replace 
Abstract: For 3D face modeling, the recently developed 3D-aware neural rendering methods are able to render photorealistic face images with arbitrary viewing directions. The training of the parametric controllable 3D-aware face models, however, still relies on a large-scale dataset that is lab-collected. To address this issue, this paper introduces "StyleMorpheus", the first style-based neural 3D Morphable Face Model (3DMM) that is trained on in-the-wild images. It inherits 3DMM's disentangled controllability (over face identity, expression, and appearance) but without the need for accurately reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder structure. The encoder aims at learning a representative disentangled parametric code space and the decoder improves the disentanglement using shape and appearance-related style codes in the different sub-modules of the network. Furthermore, we fine-tune the decoder through style-based generative adversarial learning to achieve photorealistic 3D rendering quality. The proposed style-based design enables StyleMorpheus to achieve state-of-the-art 3D-aware face reconstruction results, while also allowing disentangled control of the reconstructed face. Our model achieves real-time rendering speed, allowing its use in virtual reality applications. We also demonstrate the capability of the proposed style-based design in face editing applications such as style mixing and color editing. Project homepage: https://github.com/ubc-3d-vision-lab/StyleMorpheus.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets</title>
<link>https://arxiv.org/abs/2503.20291</link>
<guid>https://arxiv.org/abs/2503.20291</guid>
<content:encoded><![CDATA[
arXiv:2503.20291v2 Announce Type: replace 
Abstract: Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at intermediate resolution (4-8 {\AA}) is crucial in protein structure determination. Recent advances in deep learning have led to the development of automated approaches for enhancing experimental cryo-EM density maps. Yet, these methods are not optimized for intermediate-resolution maps and rely on map density features alone. To address this, we propose CryoSAMU, a novel method designed to enhance 3D cryo-EM density maps of protein structures using structure-aware multimodal U-Nets and trained on curated intermediate-resolution density maps. We comprehensively evaluate CryoSAMU across various metrics and demonstrate its competitive performance compared to state-of-the-art methods. Notably, CryoSAMU achieves significantly faster processing speed, showing promise for future practical applications. Our code is available at https://github.com/chenwei-zhang/CryoSAMU.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Image Compression with Dictionary-based Entropy Model</title>
<link>https://arxiv.org/abs/2504.00496</link>
<guid>https://arxiv.org/abs/2504.00496</guid>
<content:encoded><![CDATA[
arXiv:2504.00496v2 Announce Type: replace 
Abstract: Learned image compression methods have attracted great research interest and exhibited superior rate-distortion performance to the best classical image compression standards of the present. The entropy model plays a key role in learned image compression, which estimates the probability distribution of the latent representation for further entropy coding. Most existing methods employed hyper-prior and auto-regressive architectures to form their entropy models. However, they only aimed to explore the internal dependencies of latent representation while neglecting the importance of extracting prior from training data. In this work, we propose a novel entropy model named Dictionary-based Cross Attention Entropy model, which introduces a learnable dictionary to summarize the typical structures occurring in the training dataset to enhance the entropy model. Extensive experimental results have demonstrated that the proposed model strikes a better balance between performance and latency, achieving state-of-the-art results on various benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment</title>
<link>https://arxiv.org/abs/2504.02522</link>
<guid>https://arxiv.org/abs/2504.02522</guid>
<content:encoded><![CDATA[
arXiv:2504.02522v2 Announce Type: replace 
Abstract: The capacity of Vision transformers (ViTs) to handle variable-sized inputs is often constrained by computational complexity and batch processing limitations. Consequently, ViTs are typically trained on small, fixed-size images obtained through downscaling or cropping. While reducing computational burden, these methods result in significant information loss, negatively affecting tasks like image aesthetic assessment. We introduce Charm, a novel tokenization approach that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale information simultaneously. Charm prioritizes high-resolution details in specific regions while downscaling others, enabling shorter fixed-size input sequences for ViTs while incorporating essential information. Charm is designed to be compatible with pre-trained ViTs and their learned positional embeddings. By providing multiscale input and introducing variety to input tokens, Charm improves ViT performance and generalizability for image aesthetic assessment. We avoid cropping or changing the aspect ratio to further preserve information. Extensive experiments demonstrate significant performance improvements on various image aesthetic and quality assessment datasets (up to 8.1 %) using a lightweight ViT backbone. Code and pre-trained models are available at https://github.com/FBehrad/Charm.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment</title>
<link>https://arxiv.org/abs/2504.04722</link>
<guid>https://arxiv.org/abs/2504.04722</guid>
<content:encoded><![CDATA[
arXiv:2504.04722v2 Announce Type: replace 
Abstract: Tactile graphics are essential for providing access to visual information for the 43 million people globally living with vision loss. Traditional methods for creating these graphics are labor-intensive and cannot meet growing demand. We introduce TactileNet, the first comprehensive dataset and AI-driven framework for generating embossing-ready 2D tactile templates using text-to-image Stable Diffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes SD models to produce high-fidelity, guideline-compliant graphics while reducing computational costs. Quantitative evaluations with tactile experts show 92.86% adherence to accessibility standards. Structural fidelity analysis revealed near-human design similarity, with an SSIM of 0.538 between generated graphics and expert-designed tactile images. Notably, our method preserves object silhouettes better than human designs (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation of manual tactile abstraction. The framework scales to 32,000 images (7,050 high-quality) across 66 classes, with prompt editing enabling customizable outputs (e.g., adding or removing details). By automating the 2D template generation step-compatible with standard embossing workflows-TactileNet accelerates production while preserving design flexibility. This work demonstrates how AI can augment (not replace) human expertise to bridge the accessibility gap in education and beyond. Code, data, and models will be publicly released to foster further research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation</title>
<link>https://arxiv.org/abs/2504.05904</link>
<guid>https://arxiv.org/abs/2504.05904</guid>
<content:encoded><![CDATA[
arXiv:2504.05904v2 Announce Type: replace 
Abstract: Recent mainstream unsupervised video object segmentation (UVOS) motion-appearance approaches use either the bi-encoder structure to separately encode motion and appearance features, or the uni-encoder structure for joint encoding. However, these methods fail to properly balance the motion-appearance relationship. Consequently, even with complex fusion modules for motion-appearance integration, the extracted suboptimal features degrade the models' overall performance. Moreover, the quality of optical flow varies across scenarios, making it insufficient to rely solely on optical flow to achieve high-quality segmentation results. To address these challenges, we propose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which better balances the motion-appearance relationship and incorporates model's intrinsic saliency information to enhance segmentation performance. Specifically, considering that optical flow maps are derived from RGB images, they share both commonalities and differences. Accordingly, we propose a novel Trunk-Collateral structure for motion-appearance UVOS. The shared trunk backbone captures the motion-appearance commonality, while the collateral branch learns the uniqueness of motion features. Furthermore, an Intrinsic Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the model's intrinsic saliency information to refine high-level features, and provide pixel-level guidance for motion-appearance fusion, thereby enhancing performance without additional input. Experimental results show that SMTC-Net achieved state-of-the-art performance on three UVOS datasets ( 89.2% J&amp;F on DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video salient object detection (VSOD) benchmarks with the notable increase, demonstrating its effectiveness and superiority over previous methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Humans Subtle Differences with DIFFusion</title>
<link>https://arxiv.org/abs/2504.08046</link>
<guid>https://arxiv.org/abs/2504.08046</guid>
<content:encoded><![CDATA[
arXiv:2504.08046v2 Announce Type: replace 
Abstract: Scientific expertise often requires recognizing subtle visual differences that remain challenging to articulate even for domain experts. We present a system that leverages generative models to automatically discover and visualize minimal discriminative features between categories while preserving instance identity. Our method generates counterfactual visualizations with subtle, targeted transformations between classes, performing well even in domains where data is sparse, examples are unpaired, and category boundaries resist verbal description. Experiments across six domains, including black hole simulations, butterfly taxonomy, and medical imaging, demonstrate accurate transitions with limited training data, highlighting both established discriminative features and novel subtle distinctions that measurably improved category differentiation. User studies confirm our generated counterfactuals significantly outperform traditional approaches in teaching humans to correctly differentiate between fine-grained classes, showing the potential of generative models to advance visual learning and scientific research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada</title>
<link>https://arxiv.org/abs/2504.13231</link>
<guid>https://arxiv.org/abs/2504.13231</guid>
<content:encoded><![CDATA[
arXiv:2504.13231v2 Announce Type: replace 
Abstract: Rapid information access is vital during wildfires, yet traditional data sources are slow and costly. Social media offers real-time updates, but extracting relevant insights remains a challenge. We present WildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian wildfires, annotated across twelve key themes. Evaluating both vision-language models and custom-trained classifiers, we show that while zero-shot prompting offers quick deployment, even simple trained models outperform them when labelled data is available. Our best-performing transformer-based fine-tuned model reaches 83% f-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this model can be used to uncover trends during wildfires. Our findings highlight the enduring importance of tailored datasets and task-specific training. Importantly, such datasets should be localized, as disaster response requirements vary across regions and contexts.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</title>
<link>https://arxiv.org/abs/2405.18418</link>
<guid>https://arxiv.org/abs/2405.18418</guid>
<content:encoded><![CDATA[
arXiv:2405.18418v3 Announce Type: replace-cross 
Abstract: Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis: T1w MRI to Tau PET</title>
<link>https://arxiv.org/abs/2406.12632</link>
<guid>https://arxiv.org/abs/2406.12632</guid>
<content:encoded><![CDATA[
arXiv:2406.12632v2 Announce Type: replace-cross 
Abstract: There is a demand for medical image synthesis or translation to generate synthetic images of missing modalities from available data. This need stems from challenges such as restricted access to high-cost imaging devices, government regulations, or failure to follow up with patients or study participants. In medical imaging, preserving high-level semantic features is often more critical than achieving pixel-level accuracy. Perceptual loss functions are widely employed to train medical image synthesis or translation models, as they quantify differences in high-level image features using a pre-trained feature extraction network. While 3D and 2.5D perceptual losses are used in 3D medical image synthesis, they face challenges, such as the lack of pre-trained 3D models or difficulties in balancing loss reduction across different planes. In this work, we focus on synthesizing 3D tau PET images from 3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that sequentially computes the 2D average perceptual loss for each of the axial, coronal, and sagittal planes over epochs, with the cycle duration gradually decreasing. Additionally, we process tau PET images using by-manufacturer standardization to enhance the preservation of high-SUVR regions indicative of tau pathology and mitigate SUVR variability caused by inter-manufacturer differences. We combine the proposed loss with SSIM and MSE losses and demonstrate its effectiveness in improving both quantitative and qualitative performance across various generative models, including U-Net, UNETR, SwinUNETR, CycleGAN, and Pix2Pix.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Action Pretraining from Videos</title>
<link>https://arxiv.org/abs/2410.11758</link>
<guid>https://arxiv.org/abs/2410.11758</guid>
<content:encoded><![CDATA[
arXiv:2410.11758v2 Announce Type: replace-cross 
Abstract: We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI for ECG to CMR Translation Study</title>
<link>https://arxiv.org/abs/2411.13602</link>
<guid>https://arxiv.org/abs/2411.13602</guid>
<content:encoded><![CDATA[
arXiv:2411.13602v2 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of global mortality, necessitating accessible and accurate diagnostic tools. While cardiac magnetic resonance imaging (CMR) provides gold-standard insights into cardiac structure and function, its clinical utility is limited by high cost and complexity. In contrast, electrocardiography (ECG) is inexpensive and widely available but lacks the granularity of CMR. We propose CardioNets, a deep learning framework that translates 12-lead ECG signals into CMR-level functional parameters and synthetic images, enabling scalable cardiac assessment. CardioNets integrates cross-modal contrastive learning and generative pretraining, aligning ECG with CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via a masked autoregressive model. Trained on 159,819 samples from five cohorts, including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and externally validated on independent clinical datasets (n=3,767), CardioNets achieved strong performance across disease screening and phenotype estimation tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8% and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human physicians using both ECG and real CMR. These results suggest that CardioNets offers a promising, low-cost alternative to CMR for large-scale CVD screening, particularly in resource-limited settings. Future efforts will focus on clinical deployment and regulatory validation of ECG-based synthetic imaging.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Black box Adversarial Attacks</title>
<link>https://arxiv.org/abs/2411.16782</link>
<guid>https://arxiv.org/abs/2411.16782</guid>
<content:encoded><![CDATA[
arXiv:2411.16782v2 Announce Type: replace-cross 
Abstract: Adversarial examples usually exhibit good cross-model transferability, enabling attacks on black-box models with limited information about their architectures and parameters, which are highly threatening in commercial black-box scenarios. Model ensembling is an effective strategy to improve the transferability of adversarial examples by attacking multiple surrogate models. However, since prior studies usually adopt few models in the ensemble, there remains an open question of whether scaling the number of models can further improve black-box attacks. Inspired by the scaling law of large foundation models, we investigate the scaling laws of black-box adversarial attacks in this work. Through theoretical analysis and empirical evaluations, we conclude with clear scaling laws that using more surrogate models enhances adversarial transferability. Comprehensive experiments verify the claims on standard image classifiers, diverse defended models and multimodal large language models using various adversarial attack methods. Specifically, by scaling law, we achieve 90%+ transfer attack success rate on even proprietary models like GPT-4o. Further visualization indicates that there is also a scaling law on the interpretability and semantics of adversarial perturbations.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An unsupervised method for MRI recovery: Deep image prior with structured sparsity</title>
<link>https://arxiv.org/abs/2501.01482</link>
<guid>https://arxiv.org/abs/2501.01482</guid>
<content:encoded><![CDATA[
arXiv:2501.01482v2 Announce Type: replace-cross 
Abstract: Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Trust-Guided Approach to MR Image Reconstruction with Side Information</title>
<link>https://arxiv.org/abs/2501.03021</link>
<guid>https://arxiv.org/abs/2501.03021</guid>
<content:encoded><![CDATA[
arXiv:2501.03021v2 Announce Type: replace-cross 
Abstract: Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust- Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates</title>
<link>https://arxiv.org/abs/2504.08353</link>
<guid>https://arxiv.org/abs/2504.08353</guid>
<content:encoded><![CDATA[
arXiv:2504.08353v2 Announce Type: replace-cross 
Abstract: Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization</title>
<link>https://arxiv.org/abs/2505.03186</link>
<guid>https://arxiv.org/abs/2505.03186</guid>
<content:encoded><![CDATA[
arXiv:2505.03186v2 Announce Type: replace-cross 
Abstract: The inherent synchronization between a speaker's lip movements, voice, and the underlying linguistic content offers a rich source of information for improving speech processing tasks, especially in challenging conditions where traditional audio-only systems falter. We introduce CoGenAV, a powerful and data-efficient model designed to learn versatile audio-visual representations applicable across a wide range of speech and audio-visual tasks. CoGenAV is trained by optimizing a dual objective derived from natural audio-visual synchrony, contrastive feature alignment and generative text prediction, using only 223 hours of labeled data from the LRS2 dataset. This contrastive-generative synchronization strategy effectively captures fundamental cross-modal correlations. We showcase the effectiveness and versatility of the learned CoGenAV representations on multiple benchmarks. When utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these representations contribute to achieving a state-of-the-art Word Error Rate (WER) of 1.27. They also enable strong performance in Visual Speech Recognition (VSR) with a WER of 20.5 on LRS2, and significantly improve performance in noisy environments by over 70%. Furthermore, CoGenAV representations benefit speech reconstruction tasks, boosting performance in Speech Enhancement and Separation, and achieve competitive results in audio-visual synchronization tasks like Active Speaker Detection (ASD). Our model will be open-sourced to facilitate further development and collaboration within both academia and industry.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features</title>
<link>https://arxiv.org/abs/2505.08800</link>
<guid>https://arxiv.org/abs/2505.08800</guid>
<content:encoded><![CDATA[
<div> Keywords: railway safety, driver fatigue, behavior-based monitoring system, Directed-Graph Neural Network, vision-based technologies

Summary: 
A new study introduces an online behavior-based monitoring system using a custom Directed-Graph Neural Network (DGNN) to classify train driver states as alert, not alert, or pathological. Input representations were optimized through an ablation study comparing skeletal-only, facial-only, and combined features, with the combined features achieving the highest accuracy of 80.88% in the three-class model. The combination of facial and skeletal features also resulted in over 99% accuracy in binary alertness classification. A novel dataset was introduced that includes simulated pathological conditions, expanding the assessment of risks related to fatigue and health in train driver monitoring. This research represents a significant advancement in enhancing railway safety through the utilization of vision-based technologies for advanced online monitoring.

<br /><br />Summary: <div>
arXiv:2505.08800v1 Announce Type: new 
Abstract: Driver fatigue poses a significant challenge to railway safety, with traditional systems like the dead-man switch offering limited and basic alertness checks. This study presents an online behavior-based monitoring system utilizing a customised Directed-Graph Neural Network (DGNN) to classify train driver's states into three categories: alert, not alert, and pathological. To optimize input representations for the model, an ablation study was performed, comparing three feature configurations: skeletal-only, facial-only, and a combination of both. Experimental results show that combining facial and skeletal features yields the highest accuracy (80.88%) in the three-class model, outperforming models using only facial or skeletal features. Furthermore, this combination achieves over 99% accuracy in the binary alertness classification. Additionally, we introduced a novel dataset that, for the first time, incorporates simulated pathological conditions into train driver monitoring, broadening the scope for assessing risks related to fatigue and health. This work represents a step forward in enhancing railway safety through advanced online monitoring using vision-based technologies.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions</title>
<link>https://arxiv.org/abs/2505.08801</link>
<guid>https://arxiv.org/abs/2505.08801</guid>
<content:encoded><![CDATA[
<div> Skeletal model, OptiGait-LGBM model, RUET-GAIT dataset, gait recognition, real-world scenarios 
<br />
Summary: 
The paper introduces the OptiGait-LGBM model for gait recognition in uncontrolled outdoor environments using a skeletal model approach. It addresses challenges such as varying illumination and non-overlapping camera views by utilizing landmark positions and generating numerical datasets. The RUET-GAIT dataset is introduced to represent complex gait sequences in real-world scenarios. The model outperforms ensemble techniques like Random Forest and CatBoost in terms of accuracy, memory usage, and training time, providing a low-cost and memory-efficient solution. The approach aims to recognize person re-identification with minimal computational cost compared to existing methods, making it suitable for practical applications requiring gait authentication from a distance. 
<br /> <div>
arXiv:2505.08801v1 Announce Type: new 
Abstract: Gait recognition, known for its ability to identify individuals from a distance, has gained significant attention in recent times due to its non-intrusive verification. While video-based gait identification systems perform well on large public datasets, their performance drops when applied to real-world, unconstrained gait data due to various factors. Among these, uncontrolled outdoor environments, non-overlapping camera views, varying illumination, and computational efficiency are core challenges in gait-based authentication. Currently, no dataset addresses all these challenges simultaneously. In this paper, we propose an OptiGait-LGBM model capable of recognizing person re-identification under these constraints using a skeletal model approach, which helps mitigate inconsistencies in a person's appearance. The model constructs a dataset from landmark positions, minimizing memory usage by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to represent uncontrolled gait sequences in complex outdoor environments. The process involves extracting skeletal joint landmarks, generating numerical datasets, and developing an OptiGait-LGBM gait classification model. Our aim is to address the aforementioned challenges with minimal computational cost compared to existing methods. A comparative analysis with ensemble techniques such as Random Forest and CatBoost demonstrates that the proposed approach outperforms them in terms of accuracy, memory usage, and training time. This method provides a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction</title>
<link>https://arxiv.org/abs/2505.08808</link>
<guid>https://arxiv.org/abs/2505.08808</guid>
<content:encoded><![CDATA[
<div> sparse representation, HD map construction, geometric cues, semantic cues, centerline detection

Summary:
In this paper, the authors propose a novel approach to sparse HD map construction that rivals dense representations in efficiency and performance. They enhance sparse representation techniques with a dedicated network architecture for feature extraction, a sparse-dense segmentation auxiliary task to leverage geometric and semantic cues, and a denoising module guided by physical priors. Their method, SparseMeXt, achieves state-of-the-art results on the nuScenes dataset, with SparseMeXt-Large reaching an mAP of 68.9% at over 20 fps. By challenging the traditional reliance on dense representations, this work demonstrates the untapped potential of sparse methods in HD map construction and sets a new benchmark for efficiency-performance trade-offs in the field. <div>
arXiv:2505.08808v1 Announce Type: new 
Abstract: Recent advancements in high-definition \emph{HD} map construction have demonstrated the effectiveness of dense representations, which heavily rely on computationally intensive bird's-eye view \emph{BEV} features. While sparse representations offer a more efficient alternative by avoiding dense BEV processing, existing methods often lag behind due to the lack of tailored designs. These limitations have hindered the competitiveness of sparse representations in online HD map construction. In this work, we systematically revisit and enhance sparse representation techniques, identifying key architectural and algorithmic improvements that bridge the gap with--and ultimately surpass--dense approaches. We introduce a dedicated network architecture optimized for sparse map feature extraction, a sparse-dense segmentation auxiliary task to better leverage geometric and semantic cues, and a denoising module guided by physical priors to refine predictions. Through these enhancements, our method achieves state-of-the-art performance on the nuScenes dataset, significantly advancing HD map construction and centerline detection. Specifically, SparseMeXt-Tiny reaches a mean average precision \emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for sparse representations in HD map construction. These results underscore the untapped potential of sparse methods, challenging the conventional reliance on dense representations and redefining efficiency-performance trade-offs in the field.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian</title>
<link>https://arxiv.org/abs/2505.08811</link>
<guid>https://arxiv.org/abs/2505.08811</guid>
<content:encoded><![CDATA[
<div> Keywords: Underwater 3D scene reconstruction, Tensorized Underwater Gaussian Splatting (TUGS), Adaptive Medium Estimation (AME), light attenuation, backscatter effects

Summary:
TUGS addresses the challenges in underwater 3D scene reconstruction by effectively modeling complex interactions between object geometries and water media. It utilizes tensorized higher-order Gaussians and an adaptive medium estimation module to accurately simulate light attenuation and backscatter effects underwater. TUGS outperforms existing methods in rendering high-quality underwater images with faster speeds and less memory usage. The lightweight design of TUGS allows for efficient reconstruction using limited parameters, making it suitable for memory-constrained underwater UAV applications. Extensive experiments on real-world datasets validate the superior reconstruction quality achieved by TUGS. <br /><br />Summary: <div>
arXiv:2505.08811v1 Announce Type: new 
Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems. Therefore, we propose Tensorized Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction. TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments. Compared to other NeRF-based and GS-based methods designed for underwater, TUGS is able to render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Deep Learning Model in Image Recognition via Coverage Test</title>
<link>https://arxiv.org/abs/2505.08814</link>
<guid>https://arxiv.org/abs/2505.08814</guid>
<content:encoded><![CDATA[
<div> coverage metrics, neural networks, deep neural networks, security testing, empirical research

Summary:
This paper investigates the relationships and patterns of four coverage metrics - primary functionality, boundary, hierarchy, and structural coverage - in deep neural networks (DNNs). Empirical experiments were conducted using different DNN architectures (LeNet, VGG, ResNet) and models of varying depths (5 to 54 layers) to analyze the relationships between model depth, configuration information, and neural network coverage. Additionally, the relationships between modified decision/condition coverage and dataset size were explored. The study aims to contribute to the understanding of how different depths and configurations impact the coverage of neural networks and provide insights into improving the security testing of DNN models. Three potential future directions for research in this area are also proposed. 

<br /><br />Summary: <div>
arXiv:2505.08814v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards SFW sampling for diffusion models via external conditioning</title>
<link>https://arxiv.org/abs/2505.08817</link>
<guid>https://arxiv.org/abs/2505.08817</guid>
<content:encoded><![CDATA[
<div> Conditional Trajectory Correction, Multimodal Models, Contrastive Language Image Pre-training, NSFW Detection, Image Quality <br />
Summary: <br />
Score-based generative models (SBMs), known as diffusion models, are widely used for image generation but have faced challenges generating NSFW content. This study proposes a safe-for-work (SFW) sampler that utilizes external sources, such as multimodal models and CLIP, to guide SBMs away from undesired regions and prevent unsafe outputs. The SFW sampler incorporates a Conditional Trajectory Correction step to improve safety without compromising image quality. Results on the Stable Diffusion model demonstrate that the SFW sampler effectively reduces explicit content generation and aligns well with existing approaches for NSFW detection. The method allows for user-defined NSFW classes and minimally impacts images that do not require correction. Overall, the study highlights the potential of model-agnostic conditioning in enhancing SBM performance and ensuring the responsible use of image synthesis technology. <br /> <div>
arXiv:2505.08817v1 Announce Type: new 
Abstract: Score-based generative models (SBM), also known as diffusion models, are the de facto state of the art for image synthesis. Despite their unparalleled performance, SBMs have recently been in the spotlight for being tricked into creating not-safe-for-work (NSFW) content, such as violent images and non-consensual nudity. Current approaches that prevent unsafe generation are based on the models' own knowledge, and the majority of them require fine-tuning. This article explores the use of external sources for ensuring safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional Trajectory Correction step that guides the samples away from undesired regions in the ambient space using multimodal models as the source of conditioning. Furthermore, using Contrastive Language Image Pre-training (CLIP), our method admits user-defined NSFW classes, which can vary in different settings. Our experiments on the text-to-image SBM Stable Diffusion validate that the proposed SFW sampler effectively reduces the generation of explicit content while being competitive with other fine-tuning-based approaches, as assessed via independent NSFW detectors. Moreover, we evaluate the impact of the SFW sampler on image quality and show that the proposed correction scheme comes at a minor cost with negligible effect on samples not needing correction. Our study confirms the suitability of the SFW sampler towards aligned SBM models and the potential of using model-agnostic conditioning for the prevention of unwanted images.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.08833</link>
<guid>https://arxiv.org/abs/2505.08833</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, urban planning, satellite imagery, OpenStreetMap, controlled urban imagery generation

Summary: 
Generative AI, specifically a Stable Diffusion model with ControlNet, is utilized to automate urban planning by generating high-fidelity satellite imagery based on land use descriptions, infrastructure, and natural environments. Data integration from OpenStreetMap enhances the model's accuracy and diversity in creating realistic urban landscapes across different cities. The model's performance is evaluated using FID and KID scores, demonstrating robustness and quality in producing varied urban designs. By incorporating language prompts and control imagery, the model effectively captures design constraints and preferences, generating images preferred by urban planners and the general public. This research sets a benchmark for controlled urban imagery generation, showcasing the potential of generative AI in improving planning workflows and engaging the public.<br /><br />Summary: <div>
arXiv:2505.08833v1 Announce Type: new 
Abstract: Generative AI offers new opportunities for automating urban planning by creating site-specific urban layouts and enabling flexible design exploration. However, existing approaches often struggle to produce realistic and practical designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion model, extended with ControlNet, to generate high-fidelity satellite imagery conditioned on land use descriptions, infrastructure, and natural environments. To overcome data availability limitations, we spatially link satellite imagery with structured land use and constraint information from OpenStreetMap. Using data from three major U.S. cities, we demonstrate that the proposed diffusion model generates realistic and diverse urban landscapes by varying land-use configurations, road networks, and water bodies, facilitating cross-city learning and design diversity. We also systematically evaluate the impacts of varying language prompts and control imagery on the quality of satellite imagery generation. Our model achieves high FID and KID scores and demonstrates robustness across diverse urban contexts. Qualitative assessments from urban planners and the general public show that generated images align closely with design descriptions and constraints, and are often preferred over real images. This work establishes a benchmark for controlled urban imagery generation and highlights the potential of generative AI as a tool for enhancing planning workflows and public engagement.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd Scene Analysis using Deep Learning Techniques</title>
<link>https://arxiv.org/abs/2505.08834</link>
<guid>https://arxiv.org/abs/2505.08834</guid>
<content:encoded><![CDATA[
<div> crowd scene analysis, crowd counting, anomaly detection, self-supervised training, MultiColumn CNN

Summary: 
The research focuses on crowd scene analysis, specifically crowd counting and anomaly detection. To address the challenges of data annotation and model effectiveness, a novel approach combining self-supervised training and MultiColumn CNN is proposed for crowd counting. This enables the model to learn features at different levels, improving its performance on various crowd scene challenges. For anomaly detection, a spatiotemporal model based on VGG19 is developed, extracting spatial and temporal features using CNN and LSTM blocks. The model is designed for binary classification to detect normal or abnormal behaviors in crowds, with improved performance achieved by utilizing dense residual blocks instead of fully connected layers. Experimental evaluations on datasets such as the Hockey Fight dataset and SCVD dataset demonstrate the superiority of the proposed models over existing state-of-the-art approaches.<br /><br />Summary: <div>
arXiv:2505.08834v1 Announce Type: new 
Abstract: Our research is focused on two main applications of crowd scene analysis crowd counting and anomaly detection In recent years a large number of researches have been presented in the domain of crowd counting We addressed two main challenges in this domain 1 Deep learning models are datahungry paradigms and always need a large amount of annotated data for the training of algorithm It is timeconsuming and costly task to annotate such large amount of data Selfsupervised training is proposed to deal with this challenge 2 MCNN consists of multicolumns of CNN with different sizes of filters by presenting a novel approach based on a combination of selfsupervised training and MultiColumn CNN This enables the model to learn features at different levels and makes it effective in dealing with challenges of occluded scenes nonuniform density complex backgrounds and scale invariation The proposed model was evaluated on publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly detection addressing challenges like lighting environmental conditions unexpected objects and scalability The model extracts spatial and temporal features allowing it to be generalized to realworld scenes Spatial features are learned using CNN while temporal features are learned using LSTM blocks The model works on binary classification and can detect normal or abnormal behavior The models performance is improved by replacing fully connected layers with dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset show our models outperform other stateoftheart approaches
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Autonomous Driving: Frontiers and Opportunities</title>
<link>https://arxiv.org/abs/2505.08854</link>
<guid>https://arxiv.org/abs/2505.08854</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, autonomous driving, Level 5 autonomy, image generation, LiDAR

Summary: 
Generative Artificial Intelligence (GenAI) is a transformative technology revolutionizing industries with its content creation, reasoning, and planning capabilities. This survey explores the role of GenAI in achieving fully autonomous driving, particularly at Level 5 autonomy. It delves into the principles of generative modeling, including VAEs, GANs, and Large Language Models (LLMs), and their applications in image and LiDAR generation, decision making, and smart transportation networks. The survey also addresses challenges such as generalization, safety checks, regulatory compliance, and ethical concerns. Research plans are proposed to address theoretical assurances, trust metrics, transport integration, and socio-technical influence. This comprehensive review serves as a valuable resource for researchers, engineers, and policymakers navigating the intersection of generative AI and advanced autonomous mobility. An online repository of cited works is also available for reference. 

<br /><br />Summary: <div>
arXiv:2505.08854v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety</title>
<link>https://arxiv.org/abs/2505.08882</link>
<guid>https://arxiv.org/abs/2505.08882</guid>
<content:encoded><![CDATA[
<div> Keywords: transportation safety, road damage detection, potholes, cracks, deep learning

Summary: 
A new study introduces a system designed to enhance transportation safety by detecting road anomalies such as potholes and cracks. The system utilizes deep learning technology and Raspberry Pi to detect and classify various types of road damage, providing real-time data to authorities. It also warns nearby vehicles of potential hazards on the road, aiming to prevent accidents. The innovative solution can count road anomalies and transmit data to the cloud for further action. By proactively notifying authorities and drivers of road defects, the system seeks to mitigate accidents caused by road hazards. This comprehensive approach aims to improve overall road safety and create a safer environment for the community.<br /><br />Summary: <div>
arXiv:2505.08882v1 Announce Type: new 
Abstract: This study aims to improve transportation safety, especially traffic safety. Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. To tackle this problem and improve road safety, a comprehensive system has been developed to detect potholes, cracks (e.g. alligator, transverse, longitudinal), classify their sizes, and transmit this data to the cloud for appropriate action by authorities. The system also broadcasts warning signals to nearby vehicles warning them if a severe anomaly is detected on the road. Moreover, the system can count road anomalies in real-time. It is emulated through the utilization of Raspberry Pi, a camera module, deep learning model, laptop, and cloud service. Deploying this innovative solution aims to proactively enhance road safety by notifying relevant authorities and drivers about the presence of potholes and cracks to take actions, thereby mitigating potential accidents arising from this prevalent road hazard leading to safer road conditions for the whole community.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images</title>
<link>https://arxiv.org/abs/2505.08886</link>
<guid>https://arxiv.org/abs/2505.08886</guid>
<content:encoded><![CDATA[
<div> Keywords: skin cancer, artificial intelligence, diagnostic aids, image processing, machine learning

Summary: 
- The study addresses the growing concern over skin cancer rates and the need for advanced diagnostic tools due to limited awareness and expertise. 
- Artificial Intelligence (AI) shows promise in differentiating between malignant and benign skin lesions, but its integration into clinical practice is still developing. 
- The research utilizes a combination of image processing techniques and machine learning algorithms, specifically neuro-fuzzy and colonial competition methods. 
- By applying this approach to dermoscopic images from the ISIC database, the method achieved a high accuracy of 94% on a dataset of 560 images. 
- The results suggest that this methodology can significantly aid clinicians in the early detection of melanoma, thereby improving skin cancer diagnostics. 

<br /><br />Summary: <div>
arXiv:2505.08886v1 Announce Type: new 
Abstract: The rising incidence of skin cancer, coupled with limited public awareness and a shortfall in clinical expertise, underscores an urgent need for advanced diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool in this domain, particularly for distinguishing malignant from benign skin lesions. Leveraging publicly available datasets of skin lesions, researchers have been developing AI-based diagnostic solutions. However, the integration of such computer systems in clinical settings is still nascent. This study aims to bridge this gap by employing a fusion of image processing techniques and machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches. Applied to dermoscopic images from the ISIC database, our method achieved a notable accuracy of 94% on a dataset of 560 images. These results underscore the potential of our approach in aiding clinicians in the early detection of melanoma, thereby contributing significantly to skin cancer diagnostics.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems</title>
<link>https://arxiv.org/abs/2505.08909</link>
<guid>https://arxiv.org/abs/2505.08909</guid>
<content:encoded><![CDATA[
<div> PnP methods, deep denoisers, Poisson inverse problems, CoCo denoiser, Hamiltonian regularization, spectral regularization<br />
<br />
Summary:<br />
The article introduces a new approach for Plug-and-Play (PnP) methods in imaging problems using deep denoisers. Traditional PnP methods require strong convexity or smoothness in the fidelity term and a non-expansive denoiser, which may not be suitable for Poisson inverse problems. The proposed CoCo denoiser is conservative and may be expansive, leading to improved denoising performance. A novel training strategy incorporating Hamiltonian and spectral regularization is used to train the CoCo denoiser. The CoCo denoiser is proven to be the proximal operator of a weakly convex function, allowing for a restoration model with an implicit weakly convex prior. The global convergence of PnP methods to a stationary point of the restoration model is established. Experimental results demonstrate the superiority of the proposed approach in terms of visual quality and quantitative metrics. <div>
arXiv:2505.08909v1 Announce Type: new 
Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results in imaging problems. They typically require strong convexity or smoothness of the fidelity term and a (residual) non-expansive denoiser for convergence. These assumptions, however, are violated in Poisson inverse problems, and non-expansiveness can hinder denoising performance. To address these challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be (residual) expansive, leading to improved denoising. By leveraging the generalized Helmholtz decomposition, we introduce a novel training strategy that combines Hamiltonian regularization to promote conservativeness and spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser is a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior. The global convergence of PnP methods to a stationary point of this restoration model is established. Extensive experimental results demonstrate that our approach outperforms closely related methods in both visual quality and quantitative metrics.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind Maya: Building a Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2505.08910</link>
<guid>https://arxiv.org/abs/2505.08910</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Multilingual, Low-resource languages, Cultural contexts, Open-source 

Summary: 
Maya is introduced as an open-source Multilingual Vision-Language Model (VLM) to address the limitations faced by existing models. It focuses on improving performance on low-resource languages and varied cultural contexts, which have been overlooked in current VLMs. The key contributions of Maya include a multilingual image-text pretraining dataset in eight languages, as well as a multilingual image-text model that supports these languages. By leveraging the LLaVA pretraining dataset, Maya enhances cultural and linguistic comprehension in vision-language tasks. The code for Maya is available on GitHub, providing a valuable resource for researchers and developers working in the field of VLMs. This initiative aims to bridge the gap in performance between widely spoken languages and underrepresented linguistic groups, paving the way for more inclusive and diverse applications of VLM technology. 

<br /><br />Summary: <div>
arXiv:2505.08910v1 Announce Type: new 
Abstract: In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Channel Selection in Self-Attention For Person Re-Identification</title>
<link>https://arxiv.org/abs/2505.08961</link>
<guid>https://arxiv.org/abs/2505.08961</guid>
<content:encoded><![CDATA[
<div> Differentiable Channel Selection Attention module, DCS-Attention, self-attention, Information Bottleneck, Re-ID task 

Summary:<br />
The paper introduces the Differentiable Channel Selection Attention module, DCS-Attention, which selectively chooses informative channels during attention weight computation. It can be integrated with fixed or learnable backbones and is motivated by the Information Bottleneck principle. A novel variational upper bound for the IB loss is derived for training the networks with DCS-Attention. Experimental results on person Re-ID benchmarks using DCS-FB and DCS-DNAS demonstrate improved prediction accuracy for identifying person identities. The code for the work is available on GitHub. <div>
arXiv:2505.08961v1 Announce Type: new 
Abstract: In this paper, we propose a novel attention module termed the Differentiable Channel Selection Attention module, or the DCS-Attention module. In contrast with conventional self-attention, the DCS-Attention module features selection of informative channels in the computation of the attention weights. The selection of the feature channels is performed in a differentiable manner, enabling seamless integration with DNN training. Our DCS-Attention is compatible with either fixed neural network backbones or learnable backbones with Differentiable Neural Architecture Search (DNAS), leading to DCS with Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our DCS-Attention is motivated by the principle of Information Bottleneck (IB), and a novel variational upper bound for the IB loss, which can be optimized by SGD, is derived and incorporated into the training loss of the networks with the DCS-Attention modules. In this manner, a neural network with DCS-Attention modules is capable of selecting the most informative channels for feature extraction so that it enjoys state-of-the-art performance for the Re-ID task. Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention in learning discriminative features critical to identifying person identities. The code of our work is available at https://github.com/Statistical-Deep-Learning/DCS-Attention.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training</title>
<link>https://arxiv.org/abs/2505.08971</link>
<guid>https://arxiv.org/abs/2505.08971</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, pre-training, next-token prediction, importance sampling, scaling properties

Summary: 
PRIOR is a new approach for vision-language model pre-training that addresses the issue of fitting to noise and hallucination by prioritizing image-related tokens. This is achieved through the use of a reference model, a text-only language model trained on captions without images, to weight each token based on its probability for training the vision-language model. Tokens directly related to visual inputs are given lower probabilities by the text-only model, leading to a token-specific re-weighting in the training loss. PRIOR outperforms the standard next-token prediction approach by 19% and 8% on various benchmarks for LVLMs with and without visual encoders, respectively. Additionally, PRIOR demonstrates superior scaling properties, offering higher scaling coefficients that indicate greater potential for performance improvements with increasing compute and data. <br /><br />Summary: <div>
arXiv:2505.08971v1 Announce Type: new 
Abstract: In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking</title>
<link>https://arxiv.org/abs/2505.08999</link>
<guid>https://arxiv.org/abs/2505.08999</guid>
<content:encoded><![CDATA[
<div> Keywords: visual tracking, adversarial attacks, deep learning, meta-learning, transferability<br />
<br />
Summary: <br />
Visual tracking methods based on convolutional neural networks and Transformers have shown excellent performance but are vulnerable to security issues. Addressing these vulnerabilities is essential for real-world applications. The proposed adaptive meta-gradient adversarial attack (AMGA) method integrates multi-model ensembles and meta-learning strategies to enhance attack effectiveness. By combining momentum mechanisms and Gaussian smoothing, AMGA improves the transferability of adversarial examples across different models. The method randomly selects models and optimizes gradient directions to minimize the gap between white- and black-box attacks, achieving superior performance in black-box scenarios. Experimental results on datasets like OTB2015 and LaSOT demonstrate the efficacy of AMGA in improving attack performance, transferability, and deception of adversarial examples. The code and data for AMGA are available for further research and development. <br /> <div>
arXiv:2505.08999v1 Announce Type: new 
Abstract: In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction</title>
<link>https://arxiv.org/abs/2505.09018</link>
<guid>https://arxiv.org/abs/2505.09018</guid>
<content:encoded><![CDATA[
<div> CGM, deep learning, dietary monitoring, caloric estimation, multimodal sensing  
Summary:  
- The study addresses the challenge of accurately estimating caloric intake for Type 2 diabetes management using a new multimodal deep learning framework.  
- By combining CGM time-series data, Demographic/Microbiome data, and pre-meal food images, the model enhances caloric estimation accuracy.  
- The model incorporates attention-based encoding, convolutional feature extraction for meal imagery, and multi-layer perceptrons for CGM and microbiome data.  
- Evaluation on a dataset of 40 participants showed that the model achieved a Root Mean Squared Relative Error (RMSRE) of 0.2544, surpassing baseline models by over 50%.  
- The results demonstrate the potential of multimodal sensing in improving automated dietary assessment tools for chronic disease management.  

<br /><br />Summary: <div>
arXiv:2505.09018v1 Announce Type: new 
Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet accurately estimating caloric intake remains a major challenge. While continuous glucose monitors (CGMs) offer valuable physiological data, they often fall short in capturing the full nutritional profile of meals due to inter-individual and meal-specific variability. In this work, we introduce a multimodal deep learning framework that jointly leverages CGM time-series data, Demographic/Microbiome, and pre-meal food images to enhance caloric estimation. Our model utilizes attention based encoding and a convolutional feature extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome data followed by a late fusion strategy for joint reasoning. We evaluate our approach on a curated dataset of over 40 participants, incorporating synchronized CGM, Demographic and Microbiome data and meal photographs with standardized caloric labels. Our model achieves a Root Mean Squared Relative Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These findings demonstrate the potential of multimodal sensing to improve automated dietary assessment tools for chronic disease management.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition</title>
<link>https://arxiv.org/abs/2505.09073</link>
<guid>https://arxiv.org/abs/2505.09073</guid>
<content:encoded><![CDATA[
<div> Keywords: facial recognition, pose differences, domain adaptation, attention mapping, joint entropy loss

Summary:
This article addresses the challenge of pose differences affecting facial recognition performance by introducing a domain adaptive framework. The framework leverages shared attention mapping to highlight common patterns between 2D facial images and 3D facial data, improving pose invariance. Additionally, a joint entropy regularizing loss is used to enhance consistency between the intersecting 2D and 3D representations. The proposed framework outperforms existing methods on FaceScape and ARL-VTF datasets, showing significant improvements in profile (90°) TAR @ 1% FAR. By emphasizing correlations and promoting consistency, this novel approach demonstrates the potential to enhance facial recognition performance across large pose differences. <div>
arXiv:2505.09073v1 Announce Type: new 
Abstract: Despite recent advances in facial recognition, there remains a fundamental issue concerning degradations in performance due to substantial perspective (pose) differences between enrollment and query (probe) imagery. Therefore, we propose a novel domain adaptive framework to facilitate improved performances across large discrepancies in pose by enabling image-based (2D) representations to infer properties of inherently pose invariant point cloud (3D) representations. Specifically, our proposed framework achieves better pose invariance by using (1) a shared (joint) attention mapping to emphasize common patterns that are most correlated between 2D facial images and 3D facial data and (2) a joint entropy regularizing loss to promote better consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D and 3D representations$\unicode{x2014}$by leveraging both attention maps. This framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$) TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and 1.57$\unicode{x0025}$, respectively.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions</title>
<link>https://arxiv.org/abs/2505.09092</link>
<guid>https://arxiv.org/abs/2505.09092</guid>
<content:encoded><![CDATA[
<div> dataset, Lane Keeping Assist, OpenLKA, driving data, vehicle models <br />
Summary: <br />
OpenLKA introduces a new dataset for evaluating Lane Keeping Assist (LKA) systems in real-world conditions. The dataset includes 400 hours of driving data from over 50 vehicle models, collected in various scenarios such as challenging road geometries, adverse weather, and traffic. It provides a multimodal approach, combining CAN bus streams, high-resolution dash-cam video, Openpilot outputs, and scene annotations for comprehensive analysis. By integrating vehicle-internal signals with perception and semantic context, OpenLKA aims to benchmark production LKA systems, identify critical operational scenarios, and evaluate road infrastructure for autonomous driving readiness. The dataset is open-access and available on GitHub for researchers and developers to use for LKA improvement and evaluation purposes. <br /> <div>
arXiv:2505.09092v1 Announce Type: new 
Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access. This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement. It includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing in Tampa, Florida and global contributions from the Comma.ai driving community. The dataset spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions and surrounding traffic. The dataset is multimodal, comprising: i) full CAN bus streams, decoded using custom reverse-engineered DBC files to extract key LKA events (e.g., system disengagements, lane detection failures); ii) synchronized high-resolution dash-cam video; iii) real-time outputs from Openpilot, providing accurate estimates of road curvature and lane positioning; iv) enhanced scene annotations generated by Vision Language Models, describing lane visibility, pavement quality, weather, lighting, and traffic conditions. By integrating vehicle-internal signals with high-fidelity perception and rich semantic context, OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving. The dataset is publicly available at: https://github.com/OpenLKA/OpenLKA.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning</title>
<link>https://arxiv.org/abs/2505.09118</link>
<guid>https://arxiv.org/abs/2505.09118</guid>
<content:encoded><![CDATA[
<div> interaction, scene graphs, vision-language models, reasoning, spatial relationships

Summary: 
The paper introduces Interaction-augmented Scene Graph Reasoning (ISGR) framework to enhance vision-language models' (VLMs) interactional reasoning capabilities. The framework addresses challenges in conventional scene graphs by improving relationship sets through a dual-stream graph constructor combining spatial relation extraction and interaction-aware captioning. It activates VLMs' knowledge of object functionalities through targeted interaction queries, enabling active reasoning about object interactions. Additionally, a long-term memory reinforcement learning strategy with specialized rewards enhances VLMs' ability to generalize interaction reasoning to new scenes. Experimental results show significant performance improvements, particularly on tasks requiring complex scene understanding. The ISGR framework provides a novel approach to enhancing interaction reasoning in VLMs, improving their ability to reason about complex interactions in visual scenes. <div>
arXiv:2505.09118v1 Announce Type: new 
Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting vision-language models' (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMs' interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMs' latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a lone-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at https://github.com/open_upon_acceptance.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance</title>
<link>https://arxiv.org/abs/2505.09123</link>
<guid>https://arxiv.org/abs/2505.09123</guid>
<content:encoded><![CDATA[
<div> Keywords: Big model, Camouflaged Object Detection, Segment Anything Model, Promotion Point Targeting Network, Key point selection

Summary:
This study introduces a new framework for Camouflaged Object Detection (COD) using the Segment Anything Model (SAM), which previous studies deemed unsuitable for this task. The researchers develop the Promotion Point Targeting Network (PPT-net) to predict camouflaged objects' presence at candidate points in the image using multi-scale features. They also implement a key point selection (KPS) algorithm to guide segmentation by promoting positive and negative points contrastively. This approach marks the first application of big models in COD and outperforms existing methods on three datasets across six metrics. By leveraging SAM as an off-the-shelf solution, the study demonstrates the advantages of using pre-existing models over designing new ones from scratch, making COD a less challenging task that only requires finding informative promotions rather than precise ones.

<br /><br />Summary: <div>
arXiv:2505.09123v1 Announce Type: new 
Abstract: Big model has emerged as a new research paradigm that can be applied to various down-stream tasks with only minor effort for domain adaption. Correspondingly, this study tackles Camouflaged Object Detection (COD) leveraging the Segment Anything Model (SAM). The previous studies declared that SAM is not workable for COD but this study reveals that SAM works if promoted properly, for which we devise a new framework to render point promotions: First, we develop the Promotion Point Targeting Network (PPT-net) to leverage multi-scale features in predicting the probabilities of camouflaged objects' presences at given candidate points over the image. Then, we develop a key point selection (KPS) algorithm to deploy both positive and negative point promotions contrastively to SAM to guide the segmentation. It is the first work to facilitate big model for COD and achieves plausible results experimentally over the existing methods on 3 data sets under 6 metrics. This study demonstrates an off-the-shelf methodology for COD by leveraging SAM, which gains advantage over designing professional models from scratch, not only in performance, but also in turning the problem to a less challenging task, that is, seeking informative but not exactly precise promotions.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</title>
<link>https://arxiv.org/abs/2505.09129</link>
<guid>https://arxiv.org/abs/2505.09129</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, surveillance video, color features, tactical mission, deployability <br />
<br />
Summary: 
This paper presents a lightweight anomaly detection framework for surveillance videos in high sensitivity tactical missions. The framework combines unsupervised KMeans clustering with RGB channel histogram modeling to detect structural anomalies and color mutation signals in key frames. The method was tested on operation surveillance video footage from an African country and successfully identified highly anomalous frames related to various threats. The approach demonstrated effectiveness in tactical assassination warning, object screening, and environmental monitoring without access to original data. The study highlights the significance of color features as carriers of low semantic battlefield signals and suggests future enhancements through graph neural networks and temporal modeling. The proposed framework offers deployability and tactical interpretation value in high-risk security tasks. <br /> <div>
arXiv:2505.09129v1 Announce Type: new 
Abstract: The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.09139</link>
<guid>https://arxiv.org/abs/2505.09139</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, object detection, prompt refinement, Contrastive Class Alignment Score, automatic prompt selection

Summary:
In this paper, the authors propose a method for improving object detection accuracy in vision-language models (VLMs) by automating prompt refinement. They introduce a novel metric called the Contrastive Class Alignment Score (CCAS), which evaluates the semantic alignment of prompts with target object classes while avoiding similarity to confounding classes. Using a large language model, diverse prompt candidates are generated and filtered through CCAS, computed with prompt embeddings from a sentence transformer. The evaluation on challenging object categories shows that the automated selection of high-precision prompts enhances object detection accuracy without requiring additional model training or labeled data. This scalable and model-agnostic pipeline offers a systematic approach to prompt engineering for VLM-based detection systems. 

<br /><br />Summary: <div>
arXiv:2505.09139v1 Announce Type: new 
Abstract: Vision-language models (VLMs) offer flexible object detection through natural language prompts but suffer from performance variability depending on prompt phrasing. In this paper, we introduce a method for automated prompt refinement using a novel metric called the Contrastive Class Alignment Score (CCAS), which ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes. Our method generates diverse prompt candidates via a large language model and filters them through CCAS, computed using prompt embeddings from a sentence transformer. We evaluate our approach on challenging object categories, demonstrating that our automatic selection of high-precision prompts improves object detection accuracy without the need for additional model training or labeled data. This scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation</title>
<link>https://arxiv.org/abs/2505.09140</link>
<guid>https://arxiv.org/abs/2505.09140</guid>
<content:encoded><![CDATA[
<div> Transformer, 3D point cloud generation, topology-aware, bottleneck structure, feature extraction

Summary:
Topology-Aware Diffusion Transformer (TopoDiT-3D) is proposed to enhance 3D point cloud generation by incorporating global topological information often overlooked in existing models. The bottleneck structure, utilizing Perceiver Resampler, integrates topological data extracted through persistent homology and filters redundant local features to improve training efficiency. TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency, highlighting the importance of rich topological information in 3D point cloud generation. The synergistic approach of incorporating topology-aware mechanisms with conventional local feature learning demonstrates improved performance in capturing complex geometries and maintaining shape consistency. The availability of videos and code on GitHub allows for further exploration and implementation of TopoDiT-3D in various applications. <div>
arXiv:2505.09140v1 Announce Type: new 
Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly improved 3D point cloud generation. However, existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries. To address this limitation, we propose TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure for 3D point cloud generation. Specifically, we design the bottleneck structure utilizing Perceiver Resampler, which not only offers a mode to integrate topological information extracted through persistent homology into feature learning, but also adaptively filters out redundant local features to improve training efficiency. Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning. Videos and code are available at https://github.com/Zechao-Guan/TopoDiT-3D.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection</title>
<link>https://arxiv.org/abs/2505.09155</link>
<guid>https://arxiv.org/abs/2505.09155</guid>
<content:encoded><![CDATA[
<div> segmentation, circuit schematics, net detection, AMSnet 2.0, positional information <br />
<br />
The paper introduces a novel net detection mechanism based on segmentation for understanding circuit schematics. This method improves upon existing techniques by providing high robustness and recovering positional information for digital reconstruction of schematics. The proposed approach addresses the limitations of current multimodal large language models (MLLMs) in recognizing circuit schematics, particularly in dealing with complex or noisy schematics. The authors expand the AMSnet dataset to create AMSnet 2.0, which includes 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and detailed positional information for circuit components and nets. This dataset enhancement enables the development of more accurate and reliable schematic parsing algorithms, facilitating improved understanding and analysis of circuit schematics for various applications. <br /><br />Summary: <div>
arXiv:2505.09155v1 Announce Type: new 
Abstract: Current multimodal large language models (MLLMs) struggle to understand circuit schematics due to their limited recognition capabilities. This could be attributed to the lack of high-quality schematic-netlist training data. Existing work such as AMSnet applies schematic parsing to generate netlists. However, these methods rely on hard-coded heuristics and are difficult to apply to complex or noisy schematics in this paper. We therefore propose a novel net detection mechanism based on segmentation with high robustness. The proposed method also recovers positional information, allowing digital reconstruction of schematics. We then expand AMSnet dataset with schematic images from various sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information for circuit components and nets, whereas AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2505.09168</link>
<guid>https://arxiv.org/abs/2505.09168</guid>
<content:encoded><![CDATA[
<div> Keywords: Camouflage Object Detection, DRRNet, Global Context, Local Details, Reverse Refinement

Summary:
DRRNet addresses the challenges in Camouflage Object Detection by introducing a four-stage architecture that focuses on capturing both global context and local details. The model includes an Omni-Context Feature Extraction Module to capture camouflage patterns and a Local Detail Extraction Module to enhance microstructural information. DRRNet integrates dual representations of scene understanding and structural awareness and utilizes a reverse refinement module to suppress background interference and enhance object boundaries. Through two rounds of inverse refinement, the model effectively improves the continuity of object boundaries. Experimental results show that DRRNet outperforms existing methods on benchmark datasets. The code for DRRNet is publicly available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2505.09168v1 Announce Type: new 
Abstract: The core challenge in Camouflage Object Detection (COD) lies in the indistinguishable similarity between targets and backgrounds in terms of color, texture, and shape. This causes existing methods to either lose edge details (such as hair-like fine structures) due to over-reliance on global semantic information or be disturbed by similar backgrounds (such as vegetation patterns) when relying solely on local features. We propose DRRNet, a four-stage architecture characterized by a "context-detail-fusion-refinement" pipeline to address these issues. Specifically, we introduce an Omni-Context Feature Extraction Module to capture global camouflage patterns and a Local Detail Extraction Module to supplement microstructural information for the full-scene context module. We then design a module for forming dual representations of scene understanding and structural awareness, which fuses panoramic features and local features across various scales. In the decoder, we also introduce a reverse refinement module that leverages spatial edge priors and frequency-domain noise suppression to perform a two-stage inverse refinement of the output. By applying two successive rounds of inverse refinement, the model effectively suppresses background interference and enhances the continuity of object boundaries. Experimental results demonstrate that DRRNet significantly outperforms state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/jerrySunning/DRRNet.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System</title>
<link>https://arxiv.org/abs/2505.09178</link>
<guid>https://arxiv.org/abs/2505.09178</guid>
<content:encoded><![CDATA[
<div> Efficiency, Adaptation Strategy, Plug-and-Play, Modular Architecture, Open-Source Platform
Summary:<br /><br />UniCAD is a unified architecture developed to simplify the creation and deployment of multi-task computer-aided diagnosis (CAD) systems for medical imaging. It leverages pre-trained vision models to handle both 2D and 3D medical images efficiently with minimal task-specific parameters. The key innovations of UniCAD are its low-rank adaptation strategy that achieves high performance with minimal trainable parameters and its modular architecture that allows for easy integration of different tasks and experts. The platform also provides an open-source environment for researchers to share and access lightweight CAD experts, fostering a more collaborative research ecosystem. Comprehensive experiments across 12 medical datasets show that UniCAD outperforms existing methods in terms of accuracy and deployment efficiency. The source code and project page for UniCAD are available at https://mii-laboratory.github.io/UniCAD/. <div>
arXiv:2505.09178v1 Announce Type: new 
Abstract: The growing complexity and scale of visual model pre-training have made developing and deploying multi-task computer-aided diagnosis (CAD) systems increasingly challenging and resource-intensive. Furthermore, the medical imaging community lacks an open-source CAD platform to enable the rapid creation of efficient and extendable diagnostic models. To address these issues, we propose UniCAD, a unified architecture that leverages the robust capabilities of pre-trained vision foundation models to seamlessly handle both 2D and 3D medical images while requiring only minimal task-specific parameters. UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation strategy is employed to adapt a pre-trained visual model to the medical image domain, achieving performance on par with fully fine-tuned counterparts while introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular architecture that combines a frozen foundation model with multiple plug-and-play experts, enabling diverse tasks and seamless functionality expansion. Building on this unified CAD architecture, we establish an open-source platform where researchers can share and access lightweight CAD experts, fostering a more equitable and efficient research ecosystem. Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency. The source code and project page are available at https://mii-laboratory.github.io/UniCAD/.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Quantization: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.09188</link>
<guid>https://arxiv.org/abs/2505.09188</guid>
<content:encoded><![CDATA[
<div> Zero-shot Quantization, deep learning models, network quantization, data generation strategies, privacy constraints

Summary:
Zero-shot Quantization (ZSQ) is a method for reducing memory and computational demands of deep learning models without requiring access to training data. This approach is beneficial for deployment on resource-constrained devices in scenarios where data privacy, security, or regulatory constraints limit data access. The paper provides a formal definition of the ZSQ problem and categorizes existing ZSQ methods based on data generation strategies. It highlights the key challenges, motivations, core ideas, and key takeaways of these methods, offering a comprehensive overview of the field. The paper also suggests future research directions to address remaining limitations and enhance ZSQ techniques. This survey is the first to delve into ZSQ methods, presenting a valuable resource for researchers and practitioners in the field. 

<br /><br />Summary: <div>
arXiv:2505.09188v1 Announce Type: new 
Abstract: Network quantization has proven to be a powerful approach to reduce the memory and computational demands of deep learning models for deployment on resource-constrained devices. However, traditional quantization methods often rely on access to training data, which is impractical in many real-world scenarios due to privacy, security, or regulatory constraints. Zero-shot Quantization (ZSQ) emerges as a promising solution, achieving quantization without requiring any real data. In this paper, we provide a comprehensive overview of ZSQ methods and their recent advancements. First, we provide a formal definition of the ZSQ problem and highlight the key challenges. Then, we categorize the existing ZSQ methods into classes based on data generation strategies, and analyze their motivations, core ideas, and key takeaways. Lastly, we suggest future research directions to address the remaining limitations and advance the field of ZSQ. To the best of our knowledge, this paper is the first in-depth survey on ZSQ.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2505.09196</link>
<guid>https://arxiv.org/abs/2505.09196</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-light image enhancement, gene effect, parameter dynamic evolution, neural network models, computational photography 

Summary: 
The paper introduces the concept of the gene effect in low-light image enhancement, where randomly resetting certain parameters can unexpectedly improve performance. This phenomenon limits the effectiveness of complex neural network models as random parameters sometimes outperform learned ones. The authors attribute the gene effect to static parameters, which hinder adaptability to different image environments. Inspired by biological evolution, they propose parameter dynamic evolution (PDE) as a solution. PDE simulates gene mutation and recombination through parameter orthogonal generation, allowing models to adapt and mitigate the gene effect. Experimental results confirm the effectiveness of PDE in enhancing image quality. The code for the proposed techniques will be made available to the public. 

<br /><br />Summary: <div>
arXiv:2505.09196v1 Announce Type: new 
Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational photography, aiming to improve illumination, reduce noise, and enhance image quality. While recent advancements focus on designing increasingly complex neural network models, we observe a peculiar phenomenon: resetting certain parameters to random values unexpectedly improves enhancement performance for some images. Drawing inspiration from biological genes, we term this phenomenon the gene effect. The gene effect limits enhancement performance, as even random parameters can sometimes outperform learned ones, preventing models from fully utilizing their capacity. In this paper, we investigate the reason and propose a solution. Based on our observations, we attribute the gene effect to static parameters, analogous to how fixed genetic configurations become maladaptive when environments change. Inspired by biological evolution, where adaptation to new environments relies on gene mutation and recombination, we propose parameter dynamic evolution (PDE) to adapt to different images and mitigate the gene effect. PDE employs a parameter orthogonal generation technique and the corresponding generated parameters to simulate gene recombination and gene mutation, separately. Experiments validate the effectiveness of our techniques. The code will be released to the public.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures</title>
<link>https://arxiv.org/abs/2505.09251</link>
<guid>https://arxiv.org/abs/2505.09251</guid>
<content:encoded><![CDATA[
<div> Keywords: Metasurface, radar absorbing structures, convolutional neural network, electromagnetic response, computational efficiency

Summary: 
Metasurface-based radar absorbing structures (RAS) are essential for stealth technology and electromagnetic shielding due to their ability to selectively absorb specific frequencies with minimal thickness. Conventional design methods for these structures are time-consuming and computationally intensive. To address this, a surrogate model using a convolutional neural network (CNN) with Huber loss function was developed to accelerate electromagnetic response prediction for multi-layered RAS. The model achieved high accuracy with cosine similarity of 99.9% and mean square error of 0.001 after 1000 epochs of training. Validated through full-wave simulations and experiments, the model significantly reduced computational time while maintaining accuracy. <div>
arXiv:2505.09251v1 Announce Type: new 
Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for applications like stealth technology, electromagnetic (EM) shielding, etc. due to their capability to achieve frequency selective absorption characteristics with minimal thickness and reduced weight penalty. However, the conventional approach for the EM design and optimization of these structures relies on forward simulations, using full wave simulation tools, to predict the electromagnetic (EM) response of candidate meta atoms. This process is computationally intensive, extremely time consuming and requires exploration of large design spaces. To overcome this challenge, we propose a surrogate model that significantly accelerates the prediction of EM responses of multi-layered metasurface-based RAS. A convolutional neural network (CNN) based architecture with Huber loss function has been employed to estimate the reflection characteristics of the RAS model. The proposed model achieved a cosine similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of training. The efficiency of the model has been established via full wave simulations as well as experiment where it demonstrated significant reduction in computational time while maintaining high predictive accuracy.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping</title>
<link>https://arxiv.org/abs/2505.09252</link>
<guid>https://arxiv.org/abs/2505.09252</guid>
<content:encoded><![CDATA[
<div> Keywords: intracranial hemorrhage, non-contrast computed tomography, deep learning models, multi-modal large language models, medical imaging analysis

Summary: 
This study evaluated the performance of zero-shot multi-modal large language models (MLLMs) in comparison to traditional deep learning methods for intracranial hemorrhage (ICH) classification and subtyping on non-contrast computed tomography. Traditional deep learning models outperformed MLLMs in both binary classification of ICH presence and subtype classification. The MLLMs, including GPT-4o and Gemini 2.0 Flash, exhibited inferior performance in subtype classification tasks. While MLLMs have enhanced interpretability through language interactions, their accuracy in ICH subtyping remains lower than that of deep networks. The study emphasizes the potential of MLLMs in medical imaging analysis but suggests the need for further refinement and development of more precise models for improving performance in three-dimensional medical image processing.<br /><br />Summary: <div>
arXiv:2505.09252v1 Announce Type: new 
Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes on non-contrast computed tomography is critical for prognosis prediction and therapeutic decision-making, yet remains challenging due to low contrast and blurring boundaries. This study evaluates the performance of zero-shot multi-modal large language models (MLLMs) compared to traditional deep learning methods in ICH binary classification and subtyping. Methods: We utilized a dataset provided by RSNA, comprising 192 NCCT volumes. The study compares various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2, with conventional deep learning models, including ResNet50 and Vision Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such as ICH presence, subtype classification, localization, and volume estimation. Results: The results indicate that in the ICH binary classification task, traditional deep learning models outperform MLLMs comprehensively. For subtype classification, MLLMs also exhibit inferior performance compared to traditional deep learning models, with Gemini 2.0 Flash achieving an macro-averaged precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While MLLMs excel in interactive capabilities, their overall accuracy in ICH subtyping is inferior to deep networks. However, MLLMs enhance interpretability through language interactions, indicating potential in medical imaging analysis. Future efforts will focus on model refinement and developing more precise MLLMs to improve performance in three-dimensional medical image processing.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Augmentation for Pose-invariant Face Recognition</title>
<link>https://arxiv.org/abs/2505.09256</link>
<guid>https://arxiv.org/abs/2505.09256</guid>
<content:encoded><![CDATA[
<div> novel approach, face recognition, head poses, synthetic data, feature aggregation <br />
Summary: 
This paper introduces a novel approach called Pose-TTA to enhance face recognition performance by augmenting head poses during testing without the need for additional training. By using a portrait animator, the method aligns faces at inference time, generating matching side-profile images for comparison instead of frontalising them. A weighted feature aggregation strategy is implemented to address any distortions or biases in the synthetic data, improving the reliability of the augmented images. Extensive experiments on various datasets show that Pose-TTA consistently enhances inference performance across different pre-trained face recognition models. The method is easy to integrate into existing face recognition pipelines as it does not require retraining or fine-tuning of the underlying models. <br />Summary: <div>
arXiv:2505.09256v1 Announce Type: new 
Abstract: The goal of this paper is to enhance face recognition performance by augmenting head poses during the testing phase. Existing methods often rely on training on frontalised images or learning pose-invariant representations, yet both approaches typically require re-training and testing for each dataset, involving a substantial amount of effort. In contrast, this study proposes Pose-TTA, a novel approach that aligns faces at inference time without additional training. To achieve this, we employ a portrait animator that transfers the source image identity into the pose of a driving image. Instead of frontalising a side-profile face -- which can introduce distortion -- Pose-TTA generates matching side-profile images for comparison, thereby reducing identity information loss. Furthermore, we propose a weighted feature aggregation strategy to address any distortions or biases arising from the synthetic data, thus enhancing the reliability of the augmented images. Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance. Moreover, our method is straightforward to integrate into existing face recognition pipelines, as it requires no retraining or fine-tuning of the underlying recognition models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</title>
<link>https://arxiv.org/abs/2505.09263</link>
<guid>https://arxiv.org/abs/2505.09263</guid>
<content:encoded><![CDATA[
<div> few-shot Anomaly-driven Generation, realistic anomalies, diverse anomalies, weakly-supervised anomaly detection, industrial inspection
Summary:<br />
- The paper addresses the challenge of anomaly detection in industrial inspection with limited anomaly samples by proposing a few-shot Anomaly-driven Generation (AnoGen) method.
- AnoGen guides a diffusion model to generate realistic and diverse anomalies using only a few real anomalies, improving anomaly detection model training.
- The method comprises three stages: learning anomaly distribution, guiding the diffusion model to generate anomalies on specific objects, and leveraging weakly-supervised anomaly detection for model training.
- Experimental results on the MVTec dataset show that the generated anomalies enhance anomaly classification and segmentation performance, with DRAEM and DesTSeg achieving significant improvements.
- The code and generated anomalous data are available on the project's GitHub repository for further research and implementation. 
Summary: <div>
arXiv:2505.09263v1 Announce Type: new 
Abstract: Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt</title>
<link>https://arxiv.org/abs/2505.09264</link>
<guid>https://arxiv.org/abs/2505.09264</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised, anomaly detection, self-attention transformers, reconstruction networks, One Normal Image Prompt

Summary: 
OneNIP introduces a novel method for unified anomaly detection using unsupervised reconstruction networks with self-attention transformers. By reconstructing normal features and restoring anomaly features with just one normal image prompt, OneNIP improves anomaly detection performance significantly. This approach addresses the issue of perfect reconstruction for both normal and anomaly features, enhancing detection accuracy. Additionally, a supervised refiner is proposed to improve pixel-level anomaly segmentation by regressing reconstruction errors using real normal and synthesized anomalous images. The effectiveness of OneNIP is demonstrated on industry benchmarks such as MVTec, BTAD, and VisA, outperforming previous methods. The code and pre-trained models for OneNIP are publicly available for further research and implementation. <br /><br />Summary: <div>
arXiv:2505.09264v1 Announce Type: new 
Abstract: Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning</title>
<link>https://arxiv.org/abs/2505.09265</link>
<guid>https://arxiv.org/abs/2505.09265</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot visual anomaly segmentation, vision-language models, MetaUAS, change segmentation, universal anomaly segmentation 

Summary: 
Zero- and few-shot visual anomaly segmentation typically rely on vision-language models using textual prompts, limiting their flexibility. This paper introduces a novel approach using a pure visual foundation model for universal anomaly segmentation. By unifying anomaly segmentation into change segmentation, the model can leverage synthetic image pairs for training, independent of specific anomaly datasets. The proposed MetaUAS framework utilizes a one-prompt meta-learning approach and a soft feature alignment module to handle geometric variations for improved segmentation accuracy. This method does not require pre-trained visual-language models or specialized anomaly detection datasets, making it versatile for segmenting unseen anomalies without language guidance. MetaUAS outperforms existing zero-shot, few-shot, and full-shot anomaly segmentation methods in terms of efficiency and effectiveness. The code and pre-trained models are publicly available for further research and application. 

Summary: <br /><br />Zero- and few-shot visual anomaly segmentation typically rely on vision-language models using textual prompts, limiting their flexibility. This paper introduces a novel approach using a pure visual foundation model for universal anomaly segmentation. By unifying anomaly segmentation into change segmentation, the model can leverage synthetic image pairs for training, independent of specific anomaly datasets. The proposed MetaUAS framework utilizes a one-prompt meta-learning approach and a soft feature alignment module to handle geometric variations for improved segmentation accuracy. This method does not require pre-trained visual-language models or specialized anomaly detection datasets, making it versatile for segmenting unseen anomalies without language guidance. MetaUAS outperforms existing zero-shot, few-shot, and full-shot anomaly segmentation methods in terms of efficiency and effectiveness. The code and pre-trained models are publicly available for further research and application. <div>
arXiv:2505.09265v1 Announce Type: new 
Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Medical Imaging Segmentation: A Survey</title>
<link>https://arxiv.org/abs/2505.09274</link>
<guid>https://arxiv.org/abs/2505.09274</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, segmentation, Generative AI, Few-Shot Learning, Foundation Models

Summary:<br /><br />Medical imaging plays a crucial role in healthcare, with segmentation being a challenging task due to various factors such as data complexity and privacy constraints. Recent advancements in methodologies like Generative AI, Few-Shot Learning, Foundation Models, and Universal Models show promise in addressing these challenges. These approaches aim to improve generalization and domain adaptation, offering potential solutions to longstanding issues in medical image segmentation. The survey provides an in-depth exploration of theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Despite advancements, limitations and unresolved issues remain, highlighting the need for further research to enhance the practicality and accessibility of segmentation models in medical imaging. A GitHub Repository is being maintained to track and update innovations in this evolving field. <div>
arXiv:2505.09274v1 Announce Type: new 
Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care. Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints. Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise. This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models. These approaches offer promising solutions to longstanding challenges. We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging. We are maintaining a \href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting butterfly species presence from satellite imagery using soft contrastive regularisation</title>
<link>https://arxiv.org/abs/2505.09306</link>
<guid>https://arxiv.org/abs/2505.09306</guid>
<content:encoded><![CDATA[
<div> biodiversity monitoring, remote sensing data, butterfly species presence, Resnet-based model, contrastive regularisation loss <br />
Summary:<br />
The article introduces a new dataset for predicting butterfly species presence in the UK using satellite data. It discusses the shift towards using remote sensing data for biodiversity monitoring and the challenges and opportunities it presents. The study focuses on optimizing a Resnet-based model for predicting multi-species presence from satellite images, with a particular emphasis on locations with high biodiversity. A new, soft supervised contrastive regularization loss is proposed to improve prediction accuracy, especially for probabilistic labels like species presence data. The research aims to address the growing need for scalable biodiversity monitoring methods and highlights the importance of accurately predicting species biodiversity from remote sensing data for efficient monitoring and conservation efforts. <div>
arXiv:2505.09306v1 Announce Type: new 
Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage. Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images. This paper presents a new data set for predicting butterfly species presence from satellite data in the United Kingdom. We experimentally optimise a Resnet-based model to predict multi-species presence from 4-band satellite images, and find that this model especially outperforms the mean rate baseline for locations with high species biodiversity. To improve performance, we develop a soft, supervised contrastive regularisation loss that is tailored to probabilistic labels (such as species-presence data), and demonstrate that this improves prediction accuracy. In summary, our new data set and contrastive regularisation method contribute to the open challenge of accurately predicting species biodiversity from remote sensing data, which is key for efficient biodiversity monitoring.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Video Compression using 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.09324</link>
<guid>https://arxiv.org/abs/2505.09324</guid>
<content:encoded><![CDATA[
<div> Neural Video Codecs, Deep Learning, Video Compression, ROI, Gaussian Splatting <br />
Summary:<br />
The research community is moving towards neural video codecs (NVC) that utilize deep learning techniques instead of traditional codec pipelines. These NVCs offer content-aware compression strategies and higher efficiency. However, their computational demands limit real-time applications like video conferencing. To address this issue, a region-of-interest (ROI) based neural video compression model using 2D Gaussian Splatting is proposed. This approach allows for real-time decoding and requires fewer data points compared to traditional methods. By leveraging a content-aware initialization strategy and a novel redundancy-reduction mechanism, the encoding time of the Gaussian splatting-based image codec is accelerated by 88%. This advancement paves the way for NVCs to be used in video conferencing applications, offering improved adaptability and compression efficiency. <br />Summary: <div>
arXiv:2505.09324v1 Announce Type: new 
Abstract: The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis</title>
<link>https://arxiv.org/abs/2505.09329</link>
<guid>https://arxiv.org/abs/2505.09329</guid>
<content:encoded><![CDATA[
<div> scaling, medical vision, foundation models, self-supervised learning, BioVFM-21M

Summary:<br />
- The study explores scaling behaviors in developing scalable medical vision foundation models through self-supervised learning.
- A large-scale biomedical image dataset, BioVFM-21M, is introduced to support scalable pretraining.
- Scaling up provides benefits but varies across tasks in the medical domain.
- Factors correlated with scaling benefits include task characteristics, data diversity, pretraining methods, and computational efficiency.
- The proposed BioVFM, pretrained on 21 million biomedical images, outperforms previous state-of-the-art foundation models across 12 medical benchmarks. <br /> <div>
arXiv:2505.09329v1 Announce Type: new 
Abstract: Scaling up model and data size have demonstrated impressive performance improvement over a wide range of tasks. Despite extensive studies on scaling behaviors for general-purpose tasks, medical images exhibit substantial differences from natural data. It remains unclear the key factors in developing medical vision foundation models at scale due to the absence of an extensive understanding of scaling behavior in the medical domain. In this paper, we explored the scaling behavior across model sizes, training algorithms, data sizes, and imaging modalities in developing scalable medical vision foundation models by self-supervised learning. To support scalable pretraining, we introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a wide range of biomedical image modalities and anatomies. We observed that scaling up does provide benefits but varies across tasks. Additional analysis reveals several factors correlated with scaling benefits. Finally, we propose BioVFM, a large-scale medical vision foundation model pretrained on 21 million biomedical images, which outperforms the previous state-of-the-art foundation models across 12 medical benchmarks. Our results highlight that while scaling up is beneficial for pursuing better performance, task characteristics, data diversity, pretraining methods, and computational efficiency remain critical considerations for developing scalable medical foundation models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2505.09336</link>
<guid>https://arxiv.org/abs/2505.09336</guid>
<content:encoded><![CDATA[
<div> MultiviewVLM, vision-language model, unsupervised contrastive multiview representation learning, facial emotions, 3D/4D data<br />
<br />
Summary:<br />
MultiviewVLM is a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. The architecture integrates pseudo-labels from generated textual prompts for aligning emotional semantics. It proposes a joint embedding space to align multiview representations without explicit supervision and enhances model discriminability through a novel multiview contrastive learning strategy using stable positive-negative pair sampling. A gradient-friendly loss function ensures smoother convergence, and the model is optimized for distributed training for scalability. Extensive experiments show that MultiviewVLM surpasses existing methods and can be easily adapted to various real-world applications with minimal modifications.<br /> <div>
arXiv:2505.09336v1 Announce Type: new 
Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. Our architecture integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics. To capture shared information across multi-views, we propose a joint embedding space that aligns multiview representations without requiring explicit supervision. We further enhance the discriminability of our model through a novel multiview contrastive learning strategy that leverages stable positive-negative pair sampling. A gradient-friendly loss function is introduced to promote smoother and more stable convergence, and the model is optimized for distributed training to ensure scalability. Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis</title>
<link>https://arxiv.org/abs/2505.09358</link>
<guid>https://arxiv.org/abs/2505.09358</guid>
<content:encoded><![CDATA[
<div> conditional generative models, pretrained latent diffusion models, dense image analysis tasks, zero-shot generalization, Marigold

Summary:
Marigold introduces a new approach to fine-tuning pretrained latent diffusion models like Stable Diffusion for dense image analysis tasks. By leveraging the knowledge extracted from these models, Marigold achieves state-of-the-art zero-shot generalization on tasks such as monocular depth estimation, surface normals prediction, and intrinsic decomposition. The approach requires minimal modification of the original model's architecture and can be trained on small synthetic datasets using just a single GPU over a few days. This method capitalizes on the success of text-to-image generative models, particularly those using denoising diffusion in a latent space, to enhance the performance of deep learning models in data-scarce settings. The Marigold project showcases the potential of leveraging pretrained models for efficient transfer learning and demonstrates the depth of understanding these models possess of the visual world. <div>
arXiv:2505.09358v1 Announce Type: new 
Abstract: The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo</title>
<link>https://arxiv.org/abs/2505.09368</link>
<guid>https://arxiv.org/abs/2505.09368</guid>
<content:encoded><![CDATA[
<div> Dataset, Robustness, Optical Flow, Scene Flow, Stereo Vision <br />
Summary:
RobustSpring is introduced as a dataset and benchmark focusing on evaluating the robustness of models to image corruptions in optical flow, scene flow, and stereo vision tasks. By applying 20 different image corruptions in a consistent manner to the high-resolution Spring dataset, RobustSpring creates challenging conditions for model evaluation. A new corruption robustness metric allows for comparisons of model robustness, with integration into the Spring benchmark enabling dual evaluations of accuracy and robustness. Initial model benchmarking reveals that accuracy does not necessarily correlate with robustness, and that robustness can vary widely across different types of corruptions. RobustSpring aims to promote the development of computer vision models that excel in both accuracy and resilience, with the dataset and benchmark being made publicly available for future research and applications. <br /><br />Summary: <div>
arXiv:2505.09368v1 Announce Type: new 
Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision algorithms generally focus on model accuracy rather than robustness to image corruptions like noise or rain. Hence, the resilience of models to such real-world perturbations is largely unquantified. To address this, we present RobustSpring, a comprehensive dataset and benchmark for evaluating robustness to image corruptions for optical flow, scene flow, and stereo models. RobustSpring applies 20 different image corruptions, including noise, blur, color changes, quality degradations, and weather distortions, in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, creating a suite of 20,000 corrupted images that reflect challenging conditions. RobustSpring enables comparisons of model robustness via a new corruption robustness metric. Integration with the Spring benchmark enables public two-axis evaluations of both accuracy and robustness. We benchmark a curated selection of initial models, observing that accurate models are not necessarily robust and that robustness varies widely by corruption type. RobustSpring is a new computer vision benchmark that treats robustness as a first-class citizen to foster models that combine accuracy with resilience. It will be available at https://spring-benchmark.org.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment</title>
<link>https://arxiv.org/abs/2505.09372</link>
<guid>https://arxiv.org/abs/2505.09372</guid>
<content:encoded><![CDATA[
<div> Keywords: dermatology, vision-language pretraining, multimodal challenge, knowledge-enhanced, MAKE framework

Summary:
MAKE is a novel framework designed for enhancing vision-language pretraining in dermatology. It addresses the limitations of traditional text-based approaches by incorporating multiple knowledge aspects, utilizing a multi-aspect contrastive learning strategy. The framework breaks down clinical narratives into knowledge-enhanced sub-texts and aligns them with image features for more accurate diagnosis. Additionally, a diagnosis-guided weighting scheme prioritizes clinically significant sub-captions. By pretraining on a large dataset of dermatological image-text pairs, MAKE surpasses existing VLP models in skin disease classification, concept annotation, and cross-modal retrieval tasks. The code for MAKE will be publicly available, offering a valuable tool for advancing AI applications in dermatology.

<br /><br />Summary: <div>
arXiv:2505.09372v1 Announce Type: new 
Abstract: Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision-language pretraining (VLP) has advanced medical AI, its effectiveness in dermatology is limited by text length constraints and the lack of structured texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced vision-language pretraining framework for zero-shot dermatological tasks. Recognizing that comprehensive dermatological descriptions require multiple knowledge aspects that exceed standard text constraints, our framework introduces: (1) a multi-aspect contrastive learning strategy that decomposes clinical narratives into knowledge-enhanced sub-texts through large language models, (2) a fine-grained alignment mechanism that connects subcaptions with diagnostically relevant image features, and (3) a diagnosis-guided weighting scheme that adaptively prioritizes different sub-captions based on clinical significance prior. Through pretraining on 403,563 dermatological image-text pairs collected from education resources, MAKE significantly outperforms state-of-the-art VLP models on eight datasets across zero-shot skin disease classification, concept annotation, and cross-modal retrieval tasks. Our code will be made publicly available at https: //github.com/SiyuanYan1/MAKE.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-driven Motion Generation: Overview, Challenges and Directions</title>
<link>https://arxiv.org/abs/2505.09379</link>
<guid>https://arxiv.org/abs/2505.09379</guid>
<content:encoded><![CDATA[
<div> Keywords: text-driven motion generation, human movements, natural language, motion synthesis, language-driven human motion synthesis <br />
Summary: 
Text-driven motion generation is a valuable approach for creating human movements directly from natural language, offering flexibility and accessibility in controlling animated characters. Traditional motion synthesis models focus on predicting future poses from initial sequences, often conditioned on action labels. Modern text-to-motion generation approaches are categorized based on architecture (VAE-based, diffusion-based, hybrid models) and motion representation (discrete and continuous strategies). Popular datasets, evaluation methods, and benchmarks have shaped progress in this area. The review aims to highlight the current state of the field, key challenges, limitations, and future directions for research. Researchers and practitioners in virtual reality, gaming, human-computer interaction, and robotics can benefit from this comprehensive survey. <br /> <br />Summary: <div>
arXiv:2505.09379v1 Announce Type: new 
Abstract: Text-driven motion generation offers a powerful and intuitive way to create human movements directly from natural language. By removing the need for predefined motion inputs, it provides a flexible and accessible approach to controlling animated characters. This makes it especially useful in areas like virtual reality, gaming, human-computer interaction, and robotics. In this review, we first revisit the traditional perspective on motion synthesis, where models focused on predicting future poses from observed initial sequences, often conditioned on action labels. We then provide a comprehensive and structured survey of modern text-to-motion generation approaches, categorizing them from two complementary perspectives: (i) architectural, dividing methods into VAE-based, diffusion-based, and hybrid models; and (ii) motion representation, distinguishing between discrete and continuous motion generation strategies. In addition, we explore the most widely used datasets, evaluation methods, and recent benchmarks that have shaped progress in this area. With this survey, we aim to capture where the field currently stands, bring attention to its key challenges and limitations, and highlight promising directions for future exploration. We hope this work offers a valuable starting point for researchers and practitioners working to push the boundaries of language-driven human motion synthesis.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform</title>
<link>https://arxiv.org/abs/2505.09380</link>
<guid>https://arxiv.org/abs/2505.09380</guid>
<content:encoded><![CDATA[
<div> AI, radiology, intracranial hemorrhage, NeoMedSys, model refinement <br />
<br />
Summary: The study introduced NeoMedSys, a radiology software platform, for deploying and refining AI models in clinical settings. The platform was tested in two large hospitals in Norway, focusing on improving the VIOLA-AI model for intracranial hemorrhage detection. By iteratively retraining the model based on real-time radiologist feedback, significant improvements were observed in sensitivity (90.3% from 79.2%), specificity (89.3% from 80.7%), and AUC (0.949 from 0.873). The study demonstrated the effectiveness of NeoMedSys in facilitating the enhancement of AI model performance through automated bleed detection and segmentation. This approach highlights the importance of real-time radiologist involvement in the refinement process, showcasing the potential for improving diagnostic accuracy in radiology with AI tools. <div>
arXiv:2505.09380v1 Announce Type: new 
Abstract: Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2). We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining. Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873). Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization</title>
<link>https://arxiv.org/abs/2505.09385</link>
<guid>https://arxiv.org/abs/2505.09385</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, Semantic segmentation, Class consistency, Domain shift, Adversarial mechanism<br />
<br />
Summary: 
FedSaaS is a novel federated segmentation framework that addresses the issue of class consistency in semantic segmentation tasks, particularly in the presence of domain shift. By utilizing class exemplars at both local and global levels, FedSaaS ensures that class representations align effectively. The server-side modeling of class prototypes supervises the global branch of clients, while an adversarial mechanism on the client side harmonizes global and local contributions for consistent outputs. Multilevel contrastive losses further enforce consistency between the two-level representations within the semantic space. Experimental results on various driving scene segmentation datasets show that FedSaaS outperforms existing methods, significantly improving segmentation accuracy and effectively tackling the class-consistency representation problem.<br /> 
Summary: <div>
arXiv:2505.09385v1 Announce Type: new 
Abstract: Federated semantic segmentation enables pixel-level classification in images through collaborative learning while maintaining data privacy. However, existing research commonly overlooks the fine-grained class relationships within the semantic space when addressing heterogeneous problems, particularly domain shift. This oversight results in ambiguities between class representation. To overcome this challenge, we propose a novel federated segmentation framework that strikes class consistency, termed FedSaaS. Specifically, we introduce class exemplars as a criterion for both local- and global-level class representations. On the server side, the uploaded class exemplars are leveraged to model class prototypes, which supervise global branch of clients, ensuring alignment with global-level representation. On the client side, we incorporate an adversarial mechanism to harmonize contributions of global and local branches, leading to consistent output. Moreover, multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space. Extensive experiments on several driving scene segmentation datasets demonstrate that our framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy and effectively addressing the class-consistency representation problem.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling</title>
<link>https://arxiv.org/abs/2505.09406</link>
<guid>https://arxiv.org/abs/2505.09406</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic scene reconstruction, Autonomous driving, Neural Radiance Fields, Monocular camera, Optical flow

Summary: 
FreeDriveRF is a novel method for dynamic scene reconstruction in autonomous driving using only sequential RGB images, eliminating the need for poses inputs. By decoupling dynamic and static parts early in the sampling process with semantic supervision, image blurring and artifacts are reduced. A warped ray-guided dynamic object rendering consistency loss is introduced to address challenges posed by object motion and occlusion in monocular cameras. This loss utilizes optical flow to improve the dynamic modeling process. The method also incorporates estimated dynamic flow to enhance the stability and accuracy of unbounded scene reconstruction through pose optimization. Experimental results on KITTI and Waymo datasets demonstrate the superior performance of FreeDriveRF in dynamic scene modeling for autonomous driving. 

<br /><br />Summary: <div>
arXiv:2505.09406v1 Announce Type: new 
Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to perceive and interpret complex scene changes more precisely. Dynamic Neural Radiance Fields (NeRFs) have recently shown promising capability in scene modeling. However, many existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity. To address this, we propose FreeDriveRF, which reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs. We innovatively decouple dynamic and static parts at the early sampling level using semantic supervision, mitigating image blurring and artifacts. To overcome the challenges posed by object motion and occlusion in monocular camera, we introduce a warped ray-guided dynamic object rendering consistency loss, utilizing optical flow to better constrain the dynamic modeling process. Additionally, we incorporate estimated dynamic flow to constrain the pose optimization process, improving the stability and accuracy of unbounded scene reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of our method in dynamic scene modeling for autonomous driving.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians</title>
<link>https://arxiv.org/abs/2505.09413</link>
<guid>https://arxiv.org/abs/2505.09413</guid>
<content:encoded><![CDATA[
<div> prediction, point clouds, 2D Gaussians, rendering, generalization

Summary:<br />
The article introduces a novel point cloud rendering method that predicts 2D Gaussians from point clouds. The method consists of two identical modules with an entire-patch architecture that can be applied to multiple datasets. It normalizes and initializes the Gaussians using point cloud information and employs splitting decoders to refine the initial predictions. This allows the method to effectively handle sparse point clouds and generalize to different categories of point clouds. The predicted Gaussians are directly used for rendering without the need for additional refinement, leading to high-quality results. Extensive experiments on various datasets demonstrate that the proposed method achieves state-of-the-art performance and superior generalization. The code for the method is available on GitHub for reference. <br />Summary: <div>
arXiv:2505.09413v1 Announce Type: new 
Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds to achieve photo-realistic rendering but still depend on categorical priors, dense point clouds, or additional refinements. Hence, we introduce a novel point cloud rendering method by predicting 2D Gaussians from point clouds. Our method incorporates two identical modules with an entire-patch architecture enabling the network to be generalized to multiple datasets. The module normalizes and initializes the Gaussians utilizing the point cloud information including normals, colors and distances. Then, splitting decoders are employed to refine the initial Gaussians by duplicating them and predicting more accurate results, making our methodology effectively accommodate sparse point clouds as well. Once trained, our approach exhibits direct generalization to point clouds across different categories. The predicted Gaussians are employed directly for rendering without additional refinement on the rendered images, retaining the benefits of 2D Gaussians. We conduct extensive experiments on various datasets, and the results demonstrate the superiority and generalization of our method, which achieves SOTA performance. The code is available at https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.09415</link>
<guid>https://arxiv.org/abs/2505.09415</guid>
<content:encoded><![CDATA[
<div> Keywords: Face anti-spoofing, multimodal large language models, FaceShield, spoof-aware vision perception, prompt-guided vision token masking

Summary:
FaceShield is a multimodal large language model designed for face anti-spoofing (FAS) tasks, with pre-training and fine-tuning datasets. It can authenticate faces, detect spoofing attacks, provide reasoning, and localize attack areas. The model uses spoof-aware vision perception and prompt-guided vision token masking to enhance performance. Extensive experiments show that FaceShield outperforms existing deep learning models and general MLLMs in FAS tasks, including classification, reasoning, and attack localization. The proposed datasets, protocols, and codes will be made available soon. The model's interpretability and reasoning capabilities make it a valuable tool for protecting facial recognition systems from presentation attacks.
<br /><br />Summary: <div>
arXiv:2505.09415v1 Announce Type: new 
Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems from presentation attacks. Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. Recently, multimodal large language models (MLLMs) have shown strong capabilities in perception, reasoning, and decision-making in visual tasks. However, there is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task. To address this gap, we propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the model's generalization ability. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization. Our instruction datasets, protocols, and codes will be released soon.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.09422</link>
<guid>https://arxiv.org/abs/2505.09422</guid>
<content:encoded><![CDATA[
<div> fusion, 4D radar, LiDAR, MoRAL, object detection  
Summary:  
- MoRAL is a motion-aware fusion framework for 3D object detection, addressing inter-frame misalignment and leveraging dynamic information from 4D radar.
- The Motion-aware Radar Encoder (MRE) compensates for radar misalignment caused by object movement during accumulation.
- The Motion Attention Gated Fusion (MAGF) module integrates radar motion features to guide LiDAR features towards dynamic foreground objects.
- Extensive evaluations on the VoD dataset show MoRAL outperforming existing methods, with the highest mAP of 73.30% in the entire area and 88.68% in the driving corridor.
- MoRAL achieves the best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in the driving corridor.  
<br /><br />Summary: <div>
arXiv:2505.09422v1 Announce Type: new 
Abstract: Reliable autonomous driving systems require accurate detection of traffic participants. To this end, multi-modal fusion has emerged as an effective strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame radar point clouds have demonstrated the effectiveness in bridging the point density gap. However, they often neglect radar point clouds' inter-frame misalignment caused by object movement during accumulation and do not fully exploit the object dynamic information from 4D radar. In this paper, we propose MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is designed to compensate for inter-frame radar misalignment from moving objects. Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion features to guide LiDAR features to focus on dynamic foreground objects. Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL outperforms existing methods, achieving the highest mAP of 73.30% in the entire area and 88.68% in the driving corridor. Notably, our method also achieves the best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in the driving corridor.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient LiDAR Reflectance Compression via Scanning Serialization</title>
<link>https://arxiv.org/abs/2505.09433</link>
<guid>https://arxiv.org/abs/2505.09433</guid>
<content:encoded><![CDATA[
arXiv:2505.09433v1 Announce Type: new 
Abstract: Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods. To address this, we introduce SerLiC, a serialization-based neural compression framework to fully exploit the intrinsic characteristics of LiDAR reflectance. SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization, offering a device-centric perspective for reflectance analysis. Each point is then tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance, for effective dependencies exploration. For efficient sequential modeling, Mamba is incorporated with a dual parallelization scheme, enabling simultaneous autoregressive dependency capture and fast processing. Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. Moreover, a lightweight version of SerLiC achieves > 10 fps (frames per second) with just 111K parameters, which is attractive for real-world applications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records</title>
<link>https://arxiv.org/abs/2505.09435</link>
<guid>https://arxiv.org/abs/2505.09435</guid>
<content:encoded><![CDATA[
arXiv:2505.09435v1 Announce Type: new 
Abstract: Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIP's three-stage framework--cleansing, attunement, and unification--addresses these challenges by (1) removing background frames, (2) leveraging large language models to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level cross-attention to resolve multi-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy</title>
<link>https://arxiv.org/abs/2505.09450</link>
<guid>https://arxiv.org/abs/2505.09450</guid>
<content:encoded><![CDATA[
arXiv:2505.09450v1 Announce Type: new 
Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally invasive diagnostic procedure. However, an aspiration needle tracker addressing rapid reciprocating motion is still missing. MrTrack, an aspiration needle tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a Mamba-based register extractor to sequentially distill global context from each historical search map, storing these temporal cues in a register bank. The Mamba-based register retriever then retrieves temporal prompts from the register bank to provide external cues when current vision features are temporarily unusable due to rapid reciprocating motion and imaging degradation. A self-supervised register diversify loss is proposed to encourage feature diversity and dimension independence within the learned register, mitigating feature collapse. Comprehensive experiments conducted on both motorized and manual aspiration datasets demonstrate that MrTrack not only outperforms state-of-the-art trackers in accuracy and robustness but also achieves superior inference efficiency.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos</title>
<link>https://arxiv.org/abs/2505.09455</link>
<guid>https://arxiv.org/abs/2505.09455</guid>
<content:encoded><![CDATA[
arXiv:2505.09455v1 Announce Type: new 
Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show promising results for extracting soccer events from broadcast videos. However, when operated in the high-recall, low-precision regime required for exhaustive event coverage in soccer analytics, their lack of contextual understanding becomes apparent: many false positives could be resolved by considering a broader sequence of actions and game-state information. In this work, we address this limitation by reasoning at the game level and improving STAD through the addition of a denoising sequence transduction task. Sequences of noisy, context-free player-centric predictions are processed alongside clean game state information using a Transformer-based encoder-decoder model. By modeling extended temporal context and reasoning jointly over team-level dynamics, our method leverages the "language of soccer" - its tactical regularities and inter-player dependencies - to generate "denoised" sequences of actions. This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video and complementing existing pixel-based methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A 2D Semantic-Aware Position Encoding for Vision Transformers</title>
<link>https://arxiv.org/abs/2505.09466</link>
<guid>https://arxiv.org/abs/2505.09466</guid>
<content:encoded><![CDATA[
arXiv:2505.09466v1 Announce Type: new 
Abstract: Vision transformers have demonstrated significant advantages in computer vision tasks due to their ability to capture long-range dependencies and contextual relationships through self-attention. However, existing position encoding techniques, which are largely borrowed from natural language processing, fail to effectively capture semantic-aware positional relationships between image patches. Traditional approaches like absolute position encoding and relative position encoding primarily focus on 1D linear position relationship, often neglecting the semantic similarity between distant yet contextually related patches. These limitations hinder model generalization, translation equivariance, and the ability to effectively handle repetitive or structured patterns in images. In this paper, we propose 2-Dimensional Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding method with semantic awareness that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates. Our method enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the gap between position encoding and perceptual similarity, thereby improving performance on computer vision tasks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2505.09484</link>
<guid>https://arxiv.org/abs/2505.09484</guid>
<content:encoded><![CDATA[
arXiv:2505.09484v1 Announce Type: new 
Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance. Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts. To address these challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential \textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features. Furthermore, the \textbf{R}epresentation \textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions. We also design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation (\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance. These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations. Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy. The code will be released soon.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput</title>
<link>https://arxiv.org/abs/2505.09498</link>
<guid>https://arxiv.org/abs/2505.09498</guid>
<content:encoded><![CDATA[
arXiv:2505.09498v1 Announce Type: new 
Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks. Our approach includes tailored architectural choices, token compression mechanisms, data curation, training schemes, and a novel image processing technique called implicit semantic stitching that effectively balances computational load and model performance. Through extensive evaluations on 11 standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves state-of-the-art results in both speed and accuracy, making it a promising solution for deployment in resource-constrained environments and large-scale real-time applications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2505.09528</link>
<guid>https://arxiv.org/abs/2505.09528</guid>
<content:encoded><![CDATA[
arXiv:2505.09528v1 Announce Type: new 
Abstract: In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems. Code is available at https://github.com/jwen307/quality_uq.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contactless Cardiac Pulse Monitoring Using Event Cameras</title>
<link>https://arxiv.org/abs/2505.09529</link>
<guid>https://arxiv.org/abs/2505.09529</guid>
<content:encoded><![CDATA[
arXiv:2505.09529v1 Announce Type: new 
Abstract: Time event cameras are a novel technology for recording scene information at extremely low latency and with low power consumption. Event cameras output a stream of events that encapsulate pixel-level light intensity changes within the scene, capturing information with a higher dynamic range and temporal resolution than traditional cameras. This study investigates the contact-free reconstruction of an individual's cardiac pulse signal from time event recording of their face using a supervised convolutional neural network (CNN) model. An end-to-end model is trained to extract the cardiac signal from a two-dimensional representation of the event stream, with model performance evaluated based on the accuracy of the calculated heart rate. The experimental results confirm that physiological cardiac information in the facial region is effectively preserved within the event stream, showcasing the potential of this novel sensor for remote heart rate monitoring. The model trained on event frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm) compared to the RMSE of 2.92 bpm achieved by the baseline model trained on standard camera frames. Furthermore, models trained on event frames generated at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an RMSE of 2.54 and 2.13 bpm, respectively.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes</title>
<link>https://arxiv.org/abs/2505.09562</link>
<guid>https://arxiv.org/abs/2505.09562</guid>
<content:encoded><![CDATA[
arXiv:2505.09562v1 Announce Type: new 
Abstract: Autonomous vehicles need a complete map of their surroundings to plan and act. This has sparked research into the tasks of 3D occupancy prediction, 3D scene completion, and 3D panoptic scene completion, which predict a dense map of the ego vehicle's surroundings as a voxel grid. Scene completion extends occupancy prediction by predicting occluded regions of the voxel grid, and panoptic scene completion further extends this task by also distinguishing object instances within the same class; both aspects are crucial for path planning and decision-making. However, 3D panoptic scene completion is currently underexplored. This work introduces a novel framework for 3D panoptic scene completion that extends existing 3D semantic scene completion models. We propose an Object Module and Panoptic Module that can easily be integrated with 3D occupancy and scene completion methods presented in the literature. Our approach leverages the available annotations in occupancy benchmarks, allowing individual object shapes to be learned as a differentiable problem. The code is available at https://github.com/nicolamarinello/OffsetOcc .
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation</title>
<link>https://arxiv.org/abs/2505.09564</link>
<guid>https://arxiv.org/abs/2505.09564</guid>
<content:encoded><![CDATA[
arXiv:2505.09564v1 Announce Type: new 
Abstract: Cardiac image segmentation is an important step in many cardiac image analysis and modeling tasks such as motion tracking or simulations of cardiac mechanics. While deep learning has greatly advanced segmentation in clinical settings, there is limited work on pre-clinical imaging, notably in porcine models, which are often used due to their anatomical and physiological similarity to humans. However, differences between species create a domain shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown promise for robust medical image segmentation; yet their applicability to porcine data remains largely unexplored. In this work, we investigate whether foundation models can generate sufficiently accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels. Our method requires no manually annotated pig data, relying instead on iterative updates to improve segmentation quality. We demonstrate that this self-training process not only enhances segmentation accuracy but also smooths out temporal inconsistencies across consecutive frames. Although our results are encouraging, there remains room for improvement, for example by incorporating more sophisticated self-training strategies and by exploring additional foundation models and other cardiac imaging technologies.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</title>
<link>https://arxiv.org/abs/2505.09568</link>
<guid>https://arxiv.org/abs/2505.09568</guid>
<content:encoded><![CDATA[
arXiv:2505.09568v1 Announce Type: new 
Abstract: Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Forget your Inverse DDIM for Image Editing</title>
<link>https://arxiv.org/abs/2505.09571</link>
<guid>https://arxiv.org/abs/2505.09571</guid>
<content:encoded><![CDATA[
arXiv:2505.09571v1 Announce Type: new 
Abstract: The field of text-to-image generation has undergone significant advancements with the introduction of diffusion models. Nevertheless, the challenge of editing real images persists, as most methods are either computationally intensive or produce poor reconstructions. This paper introduces SAGE (Self-Attention Guidance for image Editing) - a novel technique leveraging pre-trained diffusion models for image editing. SAGE builds upon the DDIM algorithm and incorporates a novel guidance mechanism utilizing the self-attention layers of the diffusion U-Net. This mechanism computes a reconstruction objective based on attention maps generated during the inverse DDIM process, enabling efficient reconstruction of unedited regions without the need to precisely reconstruct the entire input image. Thus, SAGE directly addresses the key challenges in image editing. The superiority of SAGE over other methods is demonstrated through quantitative and qualitative evaluations and confirmed by a statistically validated comprehensive user study, in which all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE ranks as the top-performing method in seven out of 10 quantitative analyses and secures second and third places in the remaining three.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.09591</link>
<guid>https://arxiv.org/abs/2505.09591</guid>
<content:encoded><![CDATA[
arXiv:2505.09591v1 Announce Type: new 
Abstract: Despite remarkable progress in multimodal models for Visual Question Answering (VQA), there remain major reliability concerns because the models can often be overconfident and miscalibrated, especially in out-of-distribution (OOD) settings. Plenty has been done to address such issues for unimodal models, but little work exists for multimodal cases. Here, we address unreliability in multimodal models by proposing a Variational VQA approach. Specifically, instead of fine-tuning vision-language models by using AdamW, we employ a recently proposed variational algorithm called IVON, which yields a posterior distribution over model parameters. Through extensive experiments, we show that our approach improves calibration and abstentions without sacrificing the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce Expected Calibration Error by more than 50% compared to the AdamW baseline and raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of distribution shifts, the performance gain is even higher, achieving 8% Coverage (@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we present variational learning as a viable option to enhance the reliability of multimodal models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightLab: Controlling Light Sources in Images with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.09608</link>
<guid>https://arxiv.org/abs/2505.09608</guid>
<content:encoded><![CDATA[
arXiv:2505.09608v1 Announce Type: new 
Abstract: We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing</title>
<link>https://arxiv.org/abs/2505.09615</link>
<guid>https://arxiv.org/abs/2505.09615</guid>
<content:encoded><![CDATA[
arXiv:2505.09615v1 Announce Type: new 
Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Learning for Label-Efficient Cancer Image Classification in Oncology</title>
<link>https://arxiv.org/abs/2505.08798</link>
<guid>https://arxiv.org/abs/2505.08798</guid>
<content:encoded><![CDATA[
arXiv:2505.08798v1 Announce Type: cross 
Abstract: The application of AI in oncology has been limited by its reliance on large, annotated datasets and the need for retraining models for domain-specific diagnostic tasks. Taking heed of these limitations, we investigated in-context learning as a pragmatic alternative to model retraining by allowing models to adapt to new diagnostic tasks using only a few labeled examples at inference, without the need for retraining. Using four vision-language models (VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our knowledge, this is the first study to compare the performance of multiple VLMs on different oncology classification tasks. Without any parameter updates, all models showed significant gains with few-shot prompting, with GPT-4o reaching an F1 score of 0.81 in binary classification and 0.60 in multi-class classification settings. While these results remain below the ceiling of fully fine-tuned systems, they highlight the potential of ICL to approximate task-specific behavior using only a handful of examples, reflecting how clinicians often reason from prior cases. Notably, open-source models like Paligemma and CLIP demonstrated competitive gains despite their smaller size, suggesting feasibility for deployment in computing constrained clinical environments. Overall, these findings highlight the potential of ICL as a practical solution in oncology, particularly for rare cancers and resource-limited contexts where fine-tuning is infeasible and annotated data is difficult to obtain.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thoughts on Objectives of Sparse and Hierarchical Masked Image Model</title>
<link>https://arxiv.org/abs/2505.08819</link>
<guid>https://arxiv.org/abs/2505.08819</guid>
<content:encoded><![CDATA[
arXiv:2505.08819v1 Announce Type: cross 
Abstract: Masked image modeling is one of the most poplular objectives of training. Recently, the SparK model has been proposed with superior performance among self-supervised learning models. This paper proposes a new mask pattern for this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the effect of the mask pattern used for image masking in pre-training on performance.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores</title>
<link>https://arxiv.org/abs/2505.08835</link>
<guid>https://arxiv.org/abs/2505.08835</guid>
<content:encoded><![CDATA[
arXiv:2505.08835v1 Announce Type: cross 
Abstract: The advent of convenient and efficient fully unmanned stores equipped with artificial intelligence-based automated checkout systems marks a new era in retail. However, these systems have inherent artificial intelligence security vulnerabilities, which are exploited via adversarial patch attacks, particularly in physical environments. This study demonstrated that adversarial patches can severely disrupt object detection models used in unmanned stores, leading to issues such as theft, inventory discrepancies, and interference. We investigated three types of adversarial patch attacks -- Hiding, Creating, and Altering attacks -- and highlighted their effectiveness. We also introduce the novel color histogram similarity loss function by leveraging attacker knowledge of the color information of a target class object. Besides the traditional confusion-matrix-based attack success rate, we introduce a new bounding-boxes-based metric to analyze the practical impact of these attacks. Starting with attacks on object detection models trained on snack and fruit datasets in a digital environment, we evaluated the effectiveness of adversarial patches in a physical testbed that mimicked a real unmanned store with RGB cameras and realistic conditions. Furthermore, we assessed the robustness of these attacks in black-box scenarios, demonstrating that shadow attacks can enhance success rates of attacks even without direct access to model parameters. Our study underscores the necessity for robust defense strategies to protect unmanned stores from adversarial threats. Highlighting the limitations of the current defense mechanisms in real-time detection systems and discussing various proactive measures, we provide insights into improving the robustness of object detection models and fortifying unmanned retail environments against these attacks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08837</link>
<guid>https://arxiv.org/abs/2505.08837</guid>
<content:encoded><![CDATA[
arXiv:2505.08837v1 Announce Type: cross 
Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is complex and dynamic. Static security policies have become inadequate as threats evolve and cloud resources exhibit elasticity [1]. This paper addresses the limitations of static policies by proposing a security policy management framework that uses reinforcement learning (RL) to adapt dynamically. Specifically, we employ deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, enabling the learning and continuous adjustment of controls such as firewall rules and Identity and Access Management (IAM) policies. The proposed RL based solution leverages cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies, maximizing threat mitigation, and compliance while minimizing resource impact. Experimental results demonstrate that our adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and substantially reducing incident detection and response times by 58%. In addition, it maintains high conformity with security requirements and efficient resource usage. These findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts</title>
<link>https://arxiv.org/abs/2505.08838</link>
<guid>https://arxiv.org/abs/2505.08838</guid>
<content:encoded><![CDATA[
arXiv:2505.08838v1 Announce Type: cross 
Abstract: Ultrasound (US) report generation is a challenging task due to the variability of US images, operator dependence, and the need for standardized text. Unlike X-ray and CT, US imaging lacks consistent datasets, making automation difficult. In this study, we propose a unified framework for multi-organ and multilingual US report generation, integrating fragment-based multilingual training and leveraging the standardized nature of US reports. By aligning modular text fragments with diverse imaging data and curating a bilingual English-Chinese dataset, the method achieves consistent and clinically accurate text generation across organ sites and languages. Fine-tuning with selective unfreezing of the vision transformer (ViT) further improves text-image alignment. Compared to the previous state-of-the-art KMVE method, our approach achieves relative gains of about 2\% in BLEU scores, approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly reducing errors such as missing or incorrect content. By unifying multi-organ and multi-language report generation into a single, scalable framework, this work demonstrates strong potential for real-world clinical workflows.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Total Variation-Based Image Decomposition and Denoising for Microscopy Images</title>
<link>https://arxiv.org/abs/2505.08843</link>
<guid>https://arxiv.org/abs/2505.08843</guid>
<content:encoded><![CDATA[
arXiv:2505.08843v1 Announce Type: cross 
Abstract: Experimentally acquired microscopy images are unavoidably affected by the presence of noise and other unwanted signals, which degrade their quality and might hide relevant features. With the recent increase in image acquisition rate, modern denoising and restoration solutions become necessary. This study focuses on image decomposition and denoising of microscopy images through a workflow based on total variation (TV), addressing images obtained from various microscopy techniques, including atomic force microscopy (AFM), scanning tunneling microscopy (STM), and scanning electron microscopy (SEM). Our approach consists in restoring an image by extracting its unwanted signal components and subtracting them from the raw one, or by denoising it. We evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving this goal in distinct study cases. Huber-ROF proved to be the most flexible one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a wider applicability of this method in microscopy, restricted not only to STM, AFM, and SEM images. The Python code used for this study is publicly available as part of AiSurf. It is designed to be integrated into experimental workflows for image acquisition or can be used to denoise previously acquired images.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of Conformal Prediction in Cervical Atypia Classification</title>
<link>https://arxiv.org/abs/2505.08845</link>
<guid>https://arxiv.org/abs/2505.08845</guid>
<content:encoded><![CDATA[
arXiv:2505.08845v1 Announce Type: cross 
Abstract: Deep learning based cervical cancer classification can potentially increase access to screening in low-resource regions. However, deep learning models are often overconfident and do not reliably reflect diagnostic uncertainty. Moreover, they are typically optimized to generate maximum-likelihood predictions, which fail to convey uncertainty or ambiguity in their results. Such challenges can be addressed using conformal prediction, a model-agnostic framework for generating prediction sets that contain likely classes for trained deep-learning models. The size of these prediction sets indicates model uncertainty, contracting as model confidence increases. However, existing conformal prediction evaluation primarily focuses on whether the prediction set includes or covers the true class, often overlooking the presence of extraneous classes. We argue that prediction sets should be truthful and valuable to end users, ensuring that the listed likely classes align with human expectations rather than being overly relaxed and including false positives or unlikely classes. In this study, we comprehensively validate conformal prediction sets using expert annotation sets collected from multiple annotators. We evaluate three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification. Our expert annotation-based analysis reveals that conventional coverage-based evaluations overestimate performance and that current conformal prediction methods often produce prediction sets that are not well aligned with human labels. Additionally, we explore the capabilities of the conformal prediction methods in identifying ambiguous and out-of-distribution data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntrinsicEdit: Precise generative image manipulation in intrinsic space</title>
<link>https://arxiv.org/abs/2505.08889</link>
<guid>https://arxiv.org/abs/2505.08889</guid>
<content:encoded><![CDATA[
arXiv:2505.08889v1 Announce Type: cross 
Abstract: Generative diffusion models have advanced image editing with high-quality results and intuitive interfaces such as prompts and semantic drawing. However, these interfaces lack precise control, and the associated methods typically specialize on a single editing task. We introduce a versatile, generative workflow that operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision for a range of editing operations. Building atop the RGB-X diffusion framework, we address key challenges of identity preservation and intrinsic-channel entanglement. By incorporating exact diffusion inversion and disentangled channel manipulation, we enable precise, efficient editing with automatic resolution of global illumination effects -- all without additional data collection or model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including color and texture adjustments, object insertion and removal, global relighting, and their combinations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</title>
<link>https://arxiv.org/abs/2505.08919</link>
<guid>https://arxiv.org/abs/2505.08919</guid>
<content:encoded><![CDATA[
arXiv:2505.08919v1 Announce Type: cross 
Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical treatment planning for lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to assess the reconstruction comprehensively. Further, due to the absence of publicly available shape datasets to benchmark reconstruction algorithms, we developed a shape dataset named Lung3D, including the 3D models of 800 labeled pulmonary segments and the corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/M3DV/ImPulSe.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery</title>
<link>https://arxiv.org/abs/2505.08932</link>
<guid>https://arxiv.org/abs/2505.08932</guid>
<content:encoded><![CDATA[
arXiv:2505.08932v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and forest monitoring, including seed dispersal in hard-to-reach terrains. However, a detailed understanding of the forest floor remains a challenge due to high natural variability, quickly changing environmental parameters, and ambiguous annotations due to unclear definitions. To address this issue, we adapt the Segment Anything Model (SAM), a vision foundation model with strong generalization capabilities, to segment forest floor objects such as tree stumps, vegetation, and woody debris. To this end, we employ parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of additional model parameters while keeping the original weights fixed. We adjust SAM's mask decoder to generate masks corresponding to our dataset categories, allowing for automatic segmentation without manual prompting. Our results show that the adapter-based PEFT method achieves the highest mean intersection over union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a lightweight alternative for resource-constrained UAV platforms.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-step manipulation task and motion planning guided by video demonstration</title>
<link>https://arxiv.org/abs/2505.08949</link>
<guid>https://arxiv.org/abs/2505.08949</guid>
<content:encoded><![CDATA[
arXiv:2505.08949v1 Announce Type: cross 
Abstract: This work aims to leverage instructional video to solve complex multi-step task-and-motion planning tasks in robotics. Towards this goal, we propose an extension of the well-established Rapidly-Exploring Random Tree (RRT) planner, which simultaneously grows multiple trees around grasp and release states extracted from the guiding video. Our key novelty lies in combining contact states and 3D object poses extracted from the guiding video with a traditional planning algorithm that allows us to solve tasks with sequential dependencies, for example, if an object needs to be placed at a specific location to be grasped later. We also investigate the generalization capabilities of our approach to go beyond the scene depicted in the instructional video. To demonstrate the benefits of the proposed video-guided planning approach, we design a new benchmark with three challenging tasks: (I) 3D re-arrangement of multiple objects between a table and a shelf, (ii) multi-step transfer of an object through a tunnel, and (iii) transferring objects using a tray similar to a waiter transfers dishes. We demonstrate the effectiveness of our planning algorithm on several robots, including the Franka Emika Panda and the KUKA KMR iiwa. For a seamless transfer of the obtained plans to the real robot, we develop a trajectory refinement approach formulated as an optimal control problem (OCP).
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ</title>
<link>https://arxiv.org/abs/2505.08990</link>
<guid>https://arxiv.org/abs/2505.08990</guid>
<content:encoded><![CDATA[
arXiv:2505.08990v1 Announce Type: cross 
Abstract: Live video streaming is increasingly popular on social media platforms. With the growth of live streaming comes an increased need for robust content moderation to remove dangerous, illegal, or otherwise objectionable content. Whereas video on demand distribution enables offline content analysis, live streaming imposes restrictions on latency for both analysis and distribution. In this paper, we present extensions to the in-progress Media Over QUIC Transport protocol that enable real-time content moderation in one-to-many video live streams. Importantly, our solution removes only the video segments that contain objectionable content, allowing playback resumption as soon as the stream conforms to content policies again. Content analysis tasks may be transparently distributed to arbitrary client devices. We implement and evaluate our system in the context of light strobe removal for photosensitive viewers, finding that streaming clients experience an increased latency of only one group-of-pictures duration.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural BRDF Importance Sampling by Reparameterization</title>
<link>https://arxiv.org/abs/2505.08998</link>
<guid>https://arxiv.org/abs/2505.08998</guid>
<content:encoded><![CDATA[
arXiv:2505.08998v1 Announce Type: cross 
Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged as popular material representations for enhancing realism in physically-based rendering. Yet their importance sampling remains a significant challenge. In this paper, we introduce a reparameterization-based formulation of neural BRDF importance sampling that seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples. The reparameterization-based formulation transfers the distribution learning task to a problem of identifying BRDF integral substitutions. In contrast to previous methods that rely on invertible networks and multi-step inference to reconstruct BRDF distributions, our model removes these constraints, which offers greater flexibility and efficiency. Our variance and performance analysis demonstrates that our reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-cache: Efficient Robot Trajectory Retrieval System</title>
<link>https://arxiv.org/abs/2505.09040</link>
<guid>https://arxiv.org/abs/2505.09040</guid>
<content:encoded><![CDATA[
arXiv:2505.09040v1 Announce Type: cross 
Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that accelerates real-world robot inference by leveraging big-data retrieval and learning from experience. While modern Vision-Language-Action (VLA) models can handle diverse robotic tasks, they often incur high per-step inference costs, resulting in significant latency, sometimes minutes per task. In contrast, RT-cache stores a large-scale Memory of previously successful robot trajectories and retrieves relevant multistep motion snippets, drastically reducing inference overhead. By integrating a Memory Builder with a Trajectory Retrieval, we develop an efficient retrieval process that remains tractable even for extremely large datasets. RT-cache flexibly accumulates real-world experiences and replays them whenever the current scene matches past states, adapting quickly to new or unseen environments with only a few additional samples. Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks both faster and more successfully than a baseline lacking retrieval, suggesting a practical, data-driven solution for real-time manipulation.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis</title>
<link>https://arxiv.org/abs/2505.09091</link>
<guid>https://arxiv.org/abs/2505.09091</guid>
<content:encoded><![CDATA[
arXiv:2505.09091v1 Announce Type: cross 
Abstract: In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis</title>
<link>https://arxiv.org/abs/2505.09109</link>
<guid>https://arxiv.org/abs/2505.09109</guid>
<content:encoded><![CDATA[
arXiv:2505.09109v1 Announce Type: cross 
Abstract: Due to the deformability of garments, generating a large amount of high-quality data for robotic garment manipulation tasks is highly challenging. In this paper, we present a synthetic garment dataset that can be used for robotic garment folding. We begin by constructing geometric garment templates based on keypoints and applying generative models to generate realistic texture patterns. Leveraging these keypoint annotations, we generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. To improve robustness, we propose KG-DAgger, which uses a keypoint-based strategy to generate demonstration data for recovering from failures. KG-DAgger significantly improves the model performance, boosting the real-world success rate by 25\%. After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75\% success rate in the real world. Experiments in both simulation and real-world settings validate the effectiveness of our proposed framework.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Urban Critical Green Space Development Using Machine Learning</title>
<link>https://arxiv.org/abs/2505.09175</link>
<guid>https://arxiv.org/abs/2505.09175</guid>
<content:encoded><![CDATA[
arXiv:2505.09175v1 Announce Type: cross 
Abstract: This paper presents a novel framework for prioritizing urban green space development in Tehran using diverse socio-economic, environmental, and sensitivity indices. The indices were derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the Weather Research & Forecasting (WRF) model. The WRF model was used to estimate the air temperature at a 1 km resolution due to insufficient meteorological stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C, respectively. After data preparation, several machine learning models were used for binary vegetation cover classification including XGBoost, LightGBM, Random Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94% in Overall Accuracy, Recall, and F1-score. Then, the probability of areas lacking vegetation cover was assessed using socio-economic, environmental and sensitivity indices. This resulted in the RF generating an urban green space development prioritization map. Feature Importance Analysis revealed that the most significant indices were nightly land surface temperature (LST) and sensitive population. Finally, the framework performance was validated through microclimate simulation to assess the critical areas after and before the green space development by green roofs. The simulation demonstrated reducing air temperature by up to 0.67{\deg}C after utilizing the green roof technology in critical areas. As a result, this framework provides a valuable tool for urban planners to develop green spaces.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression</title>
<link>https://arxiv.org/abs/2505.09193</link>
<guid>https://arxiv.org/abs/2505.09193</guid>
<content:encoded><![CDATA[
arXiv:2505.09193v1 Announce Type: cross 
Abstract: Recent forward prediction-based learned video compression (LVC) methods have achieved impressive results, even surpassing VVC reference software VTM under the Low Delay B (LDB) configuration. In contrast, learned bidirectional video compression (BVC) remains underexplored and still lags behind its forward-only counterparts. This performance gap is mainly due to the limited ability to extract diverse and accurate contexts: most existing BVCs primarily exploit temporal motion while neglecting non-local correlations across frames. Moreover, they lack the adaptability to dynamically suppress harmful contexts arising from fast motion or occlusion. To tackle these challenges, we propose BiECVC, a BVC framework that incorporates diversified local and non-local context modeling along with adaptive context gating. For local context enhancement, BiECVC reuses high-quality features from lower layers and aligns them using decoded motion vectors without introducing extra motion overhead.To model non-local dependencies efficiently, we adopt a linear attention mechanism that balances performance and complexity. To further mitigate the impact of inaccurate context prediction, we introduce Bidirectional Context Gating, inspired by data-dependent decay in recent autoregressive language models, to dynamically filter contextual information based on conditional coding results. Extensive experiments demonstrate that BiECVC achieves state-of-the-art performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2 under the Random Access (RA) configuration with intra periods of 32 and 64, respectively. To our knowledge, BiECVC is the first learned video codec to surpass VTM 13.2 RA across all standard test datasets. Code will be available at https://github.com/JiangWeibeta/ECVC.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDBench: Large-Scale Electron Density Data for Molecular Modeling</title>
<link>https://arxiv.org/abs/2505.09262</link>
<guid>https://arxiv.org/abs/2505.09262</guid>
<content:encoded><![CDATA[
arXiv:2505.09262v1 Announce Type: cross 
Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on the learning of atoms, molecules, and simple quantum chemical properties (such as energy and force), but ignore the importance of electron density (ED) $\rho(r)$ in accurately understanding molecular force fields (MFFs). ED describes the probability of finding electrons at specific locations around atoms or molecules, which uniquely determines all ground state properties (such as energy, molecular structure, etc.) of interactive multi-particle systems according to the Hohenberg-Kohn theorem. However, the calculation of ED relies on the time-consuming first-principles density functional theory (DFT) which leads to the lack of large-scale ED data and limits its application in MLFFs. In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED designed to advance learning-based research at the electronic scale. Built upon the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million molecules. To comprehensively evaluate the ability of models to understand and utilize electronic information, we design a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation. Our evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, we show that learning-based method can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.09315</link>
<guid>https://arxiv.org/abs/2505.09315</guid>
<content:encoded><![CDATA[
arXiv:2505.09315v1 Announce Type: cross 
Abstract: In recent years, diffusion model has shown its potential across diverse domains from vision generation to language modeling. Transferring its capabilities to modern autonomous driving systems has also emerged as a promising direction.In this work, we propose TransDiffuser, an encoder-decoder based generative trajectory planning model for end-to-end autonomous driving. The encoded scene information serves as the multi-modal conditional input of the denoising decoder. To tackle the mode collapse dilemma in generating high-quality diverse trajectories, we introduce a simple yet effective multi-modal representation decorrelation optimization mechanism during the training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis</title>
<link>https://arxiv.org/abs/2505.09323</link>
<guid>https://arxiv.org/abs/2505.09323</guid>
<content:encoded><![CDATA[
arXiv:2505.09323v1 Announce Type: cross 
Abstract: This study, we propose a novel Q-space Guided Collaborative Attention Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI (MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly acquired structural MRI data. Q-CATN employs a collaborative attention mechanism to effectively extract complementary information from multiple modalities and dynamically adjust its internal representations based on flexible q-space information, eliminating the need for fixed sampling schemes. Additionally, we introduce a range of task-specific constraints to preserve anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic relationships between directional DWI signal distributions and q-space. Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD, and QGAN, in estimating parameter maps and fiber tracts both quantitatively and qualitatively, while preserving fine-grained details. Notably, its ability to accommodate flexible q-space sampling highlights its potential as a promising toolkit for clinical and research applications. Our code is available at https://github.com/Idea89560041/Q-CATN.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images</title>
<link>https://arxiv.org/abs/2505.09334</link>
<guid>https://arxiv.org/abs/2505.09334</guid>
<content:encoded><![CDATA[
arXiv:2505.09334v1 Announce Type: cross 
Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early detection and accurate diagnosis are critical for improving survival rates. While deep learning, particularly convolutional neural networks (CNNs), has revolutionized medical image analysis by detecting subtle patterns indicative of early-stage lung cancer, its adoption faces challenges. These models are often computationally expensive and require significant resources, making them unsuitable for resource constrained environments. Additionally, their lack of transparency hinders trust and broader adoption in sensitive fields like healthcare. Knowledge distillation addresses these challenges by transferring knowledge from large, complex models (teachers) to smaller, lightweight models (students). We propose a knowledge distillation-based approach for lung cancer detection, incorporating explainable AI (XAI) techniques to enhance model transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3, and VGG16, are evaluated as teacher models. We developed and trained a lightweight student model, Distilled Custom Student Network (DCSNet) using ResNet50 as the teacher. This approach not only ensures high diagnostic performance in resource-constrained settings but also addresses transparency concerns, facilitating the adoption of AI-driven diagnostic tools in healthcare.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks</title>
<link>https://arxiv.org/abs/2505.09344</link>
<guid>https://arxiv.org/abs/2505.09344</guid>
<content:encoded><![CDATA[
arXiv:2505.09344v1 Announce Type: cross 
Abstract: Determining the performance of a Deep Neural Network during Neural Architecture Search processes is essential for identifying optimal architectures and hyperparameters. Traditionally, this process requires training and evaluation of each network, which is time-consuming and resource-intensive. Zero-cost proxies estimate performance without training, serving as an alternative to traditional training. However, recent proxies often lack generalization across diverse scenarios and provide only relative rankings rather than predicted accuracies. To address these limitations, we propose GreenFactory, an ensemble of zero-cost proxies that leverages a random forest regressor to combine multiple predictors' strengths and directly predict model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust results across multiple datasets. Specifically, GreenFactory achieves high Kendall correlations on NATS-Bench-SSS, indicating substantial agreement between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120, showcasing its reliability in both search spaces.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression</title>
<link>https://arxiv.org/abs/2505.09356</link>
<guid>https://arxiv.org/abs/2505.09356</guid>
<content:encoded><![CDATA[
arXiv:2505.09356v1 Announce Type: cross 
Abstract: Precise initialization plays a critical role in the performance of localization algorithms, especially in the context of robotics, autonomous driving, and computer vision. Poor localization accuracy is often a consequence of inaccurate initial poses, particularly noticeable in GNSS-denied environments where GPS signals are primarily relied upon for initialization. Recent advances in leveraging deep neural networks for pose regression have led to significant improvements in both accuracy and robustness, especially in estimating complex spatial relationships and orientations. In this paper, we introduce APR-Transformer, a model architecture inspired by state-of-the-art methods, which predicts absolute pose (3D position and 3D orientation) using either image or LiDAR data. We demonstrate that our proposed method achieves state-of-the-art performance on established benchmark datasets such as the Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our experiments to include our custom complex APR-BeIntelli dataset. Additionally, we validate the reliability of our approach in GNSS-denied environments by deploying the model in real-time on an autonomous test vehicle. This showcases the practical feasibility and effectiveness of our approach. The source code is available at:https://github.com/GT-ARC/APR-Transformer.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units</title>
<link>https://arxiv.org/abs/2505.09393</link>
<guid>https://arxiv.org/abs/2505.09393</guid>
<content:encoded><![CDATA[
arXiv:2505.09393v1 Announce Type: cross 
Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net</title>
<link>https://arxiv.org/abs/2505.09521</link>
<guid>https://arxiv.org/abs/2505.09521</guid>
<content:encoded><![CDATA[
arXiv:2505.09521v1 Announce Type: cross 
Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain CNNs that fail to capture cross-channel time-frequency cues or on heavy transformer/GAN decoders that strain memory and stability. We propose Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts these issues via a Multi-directional Time-Frequency Convolutional Attention Encoder, stacking temporal, spectral and joint convolutions with self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space blocks enable efficient long-range spatial modelling. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2505.09565</link>
<guid>https://arxiv.org/abs/2505.09565</guid>
<content:encoded><![CDATA[
arXiv:2505.09565v1 Announce Type: cross 
Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple motion-corrupted low-resolution 2D slices constitutes a critical step in image-based diagnostics of moving subjects, such as fetal brain Magnetic Resonance Imaging (MRI). Existing solutions struggle with image artifacts and severe subject motion or require slice pre-alignment to achieve satisfying reconstruction performance. We propose a novel SVR method to enable fast and accurate MRI reconstruction even in cases of severe image and motion corruption. Our approach performs motion correction, outlier handling, and super-resolution reconstruction with all operations being entirely based on implicit neural representations. The model can be initialized with task-specific priors through fully self-supervised meta-learning on either simulated or real-world data. In extensive experiments including over 480 reconstructions of simulated and clinical MRI brain data from different centers, we prove the utility of our method in cases of severe subject motion and image artifacts. Our results demonstrate improvements in reconstruction quality, especially in the presence of severe motion, compared to state-of-the-art methods, and up to 50% reduction in reconstruction time.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image</title>
<link>https://arxiv.org/abs/2207.14425</link>
<guid>https://arxiv.org/abs/2207.14425</guid>
<content:encoded><![CDATA[
arXiv:2207.14425v2 Announce Type: replace 
Abstract: In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting conditions by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting conditions, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EiHi Net: Out-of-Distribution Generalization Paradigm</title>
<link>https://arxiv.org/abs/2209.14946</link>
<guid>https://arxiv.org/abs/2209.14946</guid>
<content:encoded><![CDATA[
arXiv:2209.14946v3 Announce Type: replace 
Abstract: This paper develops a new EiHi net to solve the out-of-distribution (OoD) generalization problem in deep learning. EiHi net is a model learning paradigm that can be blessed on any visual backbone. This paradigm can change the previous learning method of the deep model, namely find out correlations between inductive sample features and corresponding categories, which suffers from pseudo correlations between indecisive features and labels. We fuse SimCLR and VIC-Reg via explicitly and dynamically establishing the original - positive - negative sample pair as a minimal learning element, the deep model iteratively establishes a relationship close to the causal one between features and labels, while suppressing pseudo correlations. To further validate the proposed model, and strengthen the established causal relationships, we develop a human-in-the-loop strategy, with few guidance samples, to prune the representation space directly. Finally, it is shown that the developed EiHi net makes significant improvements in the most difficult and typical OoD dataset Nico, compared with the current SOTA results, without any domain ($e.g.$ background, irrelevant features) information.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2401.07378</link>
<guid>https://arxiv.org/abs/2401.07378</guid>
<content:encoded><![CDATA[
arXiv:2401.07378v3 Announce Type: replace 
Abstract: Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F$^3$Loc: Fusion and Filtering for Floorplan Localization</title>
<link>https://arxiv.org/abs/2403.03370</link>
<guid>https://arxiv.org/abs/2403.03370</guid>
<content:encoded><![CDATA[
arXiv:2403.03370v2 Announce Type: replace 
Abstract: In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2407.19708</link>
<guid>https://arxiv.org/abs/2407.19708</guid>
<content:encoded><![CDATA[
arXiv:2407.19708v5 Announce Type: replace 
Abstract: Low-light image enhancement is an important task in computer vision, essential for improving the visibility and quality of images captured in non-optimal lighting conditions. Inadequate illumination can lead to significant information loss and poor image quality, impacting various applications such as surveillance. photography, or even autonomous driving. In this regard, automated methods have been developed to automatically adjust illumination in the image for a better visual perception. Current enhancement techniques often use specific datasets to enhance low-light images, but still present challenges when adapting to diverse real-world conditions, where illumination degradation may be localized to specific regions. To address this challenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose main approach is the use of a classification mechanism to determine whether local or global illumination enhancement is required. Subsequently, estimator networks adjust illumination based on this classification and simultaneously enhance color fidelity. ALEN integrates the Light Classification Network (LCNet) for illuminance categorization, complemented by the Single-Channel Network (SCNet), and Multi-Channel Network (MCNet) for precise estimation of illumination and color, respectively. Extensive experiments on publicly available datasets for low-light conditions were carried out to underscore ALEN's robust generalization capabilities, demonstrating superior performance in both quantitative metrics and qualitative assessments when compared to recent state-of-the-art methods. The ALEN not only enhances image quality in terms of visual perception but also represents an advancement in high-level vision tasks, such as semantic segmentation, as presented in this work. The code of this method is available at https://github.com/xingyumex/ALEN
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOP-Distrib: Revisiting 6D Pose Estimation Benchmarks for Better Evaluation under Visual Ambiguities</title>
<link>https://arxiv.org/abs/2408.17297</link>
<guid>https://arxiv.org/abs/2408.17297</guid>
<content:encoded><![CDATA[
arXiv:2408.17297v3 Announce Type: replace 
Abstract: 6D pose estimation aims at determining the object pose that best explains the camera observation. The unique solution for non-ambiguous objects can turn into a multi-modal pose distribution for symmetrical objects or when occlusions of symmetry-breaking elements happen, depending on the viewpoint. Currently, 6D pose estimation methods are benchmarked on datasets that consider, for their ground truth annotations, visual ambiguities as only related to global object symmetries, whereas they should be defined per-image to account for the camera viewpoint. We thus first propose an automatic method to re-annotate those datasets with a 6D pose distribution specific to each image, taking into account the object surface visibility in the image to correctly determine the visual ambiguities. Second, given this improved ground truth, we re-evaluate the state-of-the-art single pose methods and show that this greatly modifies the ranking of these methods. Third, as some recent works focus on estimating the complete set of solutions, we derive a precision/recall formulation to evaluate them against our image-wise distribution ground truth, making it the first benchmark for pose distribution methods on real images.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Homography is All You Need: IMM-based Joint Homography and Multiple Object State Estimation</title>
<link>https://arxiv.org/abs/2409.02562</link>
<guid>https://arxiv.org/abs/2409.02562</guid>
<content:encoded><![CDATA[
arXiv:2409.02562v3 Announce Type: replace 
Abstract: A novel online MOT algorithm, IMM Joint Homography State Estimation (IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the only additional 3D information, whereas other 3D MOT methods use regular 3D measurements. By jointly modelling the homography matrix and its dynamics as part of track state vectors, IMM-JHSE removes the explicit influence of camera motion compensation techniques on predicted track position states, which was prevalent in previous approaches. Expanding upon this, static and dynamic camera motion models are combined using an IMM filter. A simple bounding box motion model is used to predict bounding box positions to incorporate image plane information. In addition to applying an IMM to camera motion, a non-standard IMM approach is applied where bounding-box-based BIoU scores are mixed with ground-plane-based Mahalanobis distances in an IMM-like fashion to perform association only, making IMM-JHSE robust to motion away from the ground plane. Finally, IMM-JHSE makes use of dynamic process and measurement noise estimation techniques. IMM-JHSE improves upon related techniques, including UCMCTrack, OC-SORT, C-BIoU and ByteTrack on the DanceTrack and KITTI-car datasets, increasing HOTA by 2.64 and 2.11, respectively, while offering competitive performance on the MOT17, MOT20 and KITTI-pedestrian datasets. Using publicly available detections, IMM-JHSE outperforms almost all other 2D MOT methods and is outperformed only by 3D MOT methods -- some of which are offline -- on the KITTI-car dataset. Compared to tracking-by-attention methods, IMM-JHSE shows remarkably similar performance on the DanceTrack dataset and outperforms them on the MOT17 dataset. The code is publicly available: https://github.com/Paulkie99/imm-jhse.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features</title>
<link>https://arxiv.org/abs/2409.18769</link>
<guid>https://arxiv.org/abs/2409.18769</guid>
<content:encoded><![CDATA[
arXiv:2409.18769v5 Announce Type: replace 
Abstract: Periorbital distances are critical markers for diagnosing and monitoring a range of oculoplastic and craniofacial conditions. Manual measurement, however, is subjective and prone to intergrader variability. Automated methods have been developed but remain limited by standardized imaging requirements, small datasets, and a narrow focus on individual measurements. We developed a segmentation pipeline trained on a domain-specific dataset of healthy eyes and compared its performance against the Segment Anything Model (SAM) and the prior benchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple disease classes and imaging conditions. We further investigated the use of predicted periorbital distances as features for disease classification under in-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow classifiers, CNNs, and fusion models. Our segmentation model achieved state-of-the-art accuracy across all datasets, with error rates within intergrader variability and superior performance relative to SAM and PeriorbitAI. In classification tasks, models trained on periorbital distances matched CNN performance on ID data (77--78\% accuracy) and substantially outperformed CNNs under OOD conditions (63--68\% accuracy vs. 14\%). Fusion models achieved the highest ID accuracy (80\%) but were sensitive to degraded CNN features under OOD shifts. Segmentation-derived periorbital distances provide robust, explainable features for disease classification and generalize better under domain shift than CNN image classifiers. These results establish a new benchmark for periorbital distance prediction and highlight the potential of anatomy-based AI pipelines for real-world deployment in oculoplastic and craniofacial care.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos</title>
<link>https://arxiv.org/abs/2410.07795</link>
<guid>https://arxiv.org/abs/2410.07795</guid>
<content:encoded><![CDATA[
arXiv:2410.07795v4 Announce Type: replace 
Abstract: Human motion capture from monocular videos has made significant progress in recent years. However, modern approaches often produce temporal artifacts, e.g. in form of jittery motion and struggle to achieve smooth and physically plausible motions. Explicitly integrating physics, in form of internal forces and exterior torques, helps alleviating these artifacts. Current state-of-the-art approaches make use of an automatic PD controller to predict torques and reaction forces in order to re-simulate the input kinematics, i.e. the joint angles of a predefined skeleton. However, due to imperfect physical models, these methods often require simplifying assumptions and extensive preprocessing of the input kinematics to achieve good performance. To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach. We develop a control loop as a meta-PD controller to predict internal joint torques and external reaction forces, followed by a physics-based motion simulation. A recurrent neural network is introduced to realize a Kalman filter that attentively balances the kinematics input and simulated motion, resulting in an optimal-state dynamics prediction. We show that this filtering step is crucial to provide an online supervision that helps balancing the shortcoming of the respective input motions, thus being important for not only capturing accurate global motion trajectories but also producing physically plausible human poses. The proposed approach excels in the physics-based human pose estimation task and demonstrates the physical plausibility of the predictive dynamics, compared to state of the art. The code is available on https://github.com/cuongle1206/OSDCap
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflecting Topology Consistency and Abnormality via Learnable Attentions for Airway Labeling</title>
<link>https://arxiv.org/abs/2410.23854</link>
<guid>https://arxiv.org/abs/2410.23854</guid>
<content:encoded><![CDATA[
arXiv:2410.23854v2 Announce Type: replace 
Abstract: Accurate airway anatomical labeling is crucial for clinicians to identify and navigate complex bronchial structures during bronchoscopy. Automatic airway anatomical labeling is challenging due to significant individual variability and anatomical variations. Previous methods are prone to generate inconsistent predictions, which is harmful for preoperative planning and intraoperative navigation. This paper aims to address these challenges by proposing a novel method that enhances topological consistency and improves the detection of abnormal airway branches. We propose a novel approach incorporating two modules: the Soft Subtree Consistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module constructs a soft subtree to capture clinically relevant topological relationships, allowing for flexible feature aggregation within and across subtrees. The ABS module facilitates the interaction between node features and prototypes to distinguish abnormal branches, preventing the erroneous aggregation of features between normal and abnormal nodes. Evaluated on a challenging dataset characterized by severe airway distortion and atrophy, our method achieves superior performance compared to state-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the segmental level and an 83.7% accuracy at the subsegmental level, representing a 1.4% increase in subsegmental accuracy and a 3.1% increase in topological consistency. Notably, the method demonstrates reliable performance in cases with disease-induced airway deformities, ensuring consistent and accurate labeling.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment</title>
<link>https://arxiv.org/abs/2412.01986</link>
<guid>https://arxiv.org/abs/2412.01986</guid>
<content:encoded><![CDATA[
arXiv:2412.01986v2 Announce Type: replace 
Abstract: Mesh quality assessment (MQA) models play a critical role in the design, optimization, and evaluation of mesh operation systems in a wide variety of applications. Current MQA models, whether model-based methods using topology-aware features or projection-based approaches working on rendered 2D projections, often fail to capture the intricate interactions between texture and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid full-reference colored MQA framework that integrates model-based and projection-based approaches, capturing complex interactions between textural information and 3D structures for enriched quality representations. Our method employs graph learning to extract detailed 3D representations, which are then projected to 2D using a novel feature rendering process that precisely aligns them with colored projections. This enables the exploration of geometry-texture interactions via cross-attention, producing comprehensive mesh quality representations. Extensive experiments demonstrate HybridMQA's superior performance across diverse datasets, highlighting its ability to effectively leverage geometry-texture interactions for a thorough understanding of mesh quality. Our implementation will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model Trained with a Single GPU in Just One Day</title>
<link>https://arxiv.org/abs/2412.05888</link>
<guid>https://arxiv.org/abs/2412.05888</guid>
<content:encoded><![CDATA[
arXiv:2412.05888v2 Announce Type: replace 
Abstract: Medical image segmentation involves partitioning medical images into meaningful regions, with a focus on identifying anatomical structures and lesions. It has broad applications in healthcare, and deep learning methods have enabled significant advancements in automating this process. Recently, the introduction of the Segmentation Anything Model (SAM), the first foundation model for segmentation task, has prompted researchers to adapt it for the medical domain to improve performance across various tasks. However, SAM's large model size and high GPU requirements hinder its scalability and development in the medical domain. In this work, we propose MCP-MedSAM, a powerful and lightweight medical SAM model designed to be trainable on a single A100 GPU with 40GB of memory within one day while delivering superior segmentation performance. Recognizing the significant internal differences between modalities and the need for direct segmentation target information within bounding boxes, we introduce two kinds of prompts: the modality prompt and the content prompt. After passing through the prompt encoder, their embedding representations can further improve the segmentation performance by incorporating more relevant information without adding significant training overhead. Additionally, we adopt an effective modality-based data sampling strategy to address data imbalance between modalities, ensuring more balanced performance across all modalities. Our method was trained and evaluated using a large-scale challenge dataset, compared to top-ranking methods on the challenge leaderboard, MCP-MedSAM achieved superior performance while requiring only one day of training on a single GPU. The code is publicly available at \textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An ocean front detection and tracking algorithm</title>
<link>https://arxiv.org/abs/2502.15250</link>
<guid>https://arxiv.org/abs/2502.15250</guid>
<content:encoded><![CDATA[
arXiv:2502.15250v4 Announce Type: replace 
Abstract: Existing ocean front detection methods--including histogram-based variance analysis, Lyapunov exponent, gradient thresholding, and machine learning--suffer from critical limitations: discontinuous outputs, over-detection, reliance on single-threshold decisions, and lack of open-source implementations. To address these challenges, this paper proposes the Bayesian Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA). The framework introduces three innovations: (1) a Bayesian decision mechanism that integrates gradient priors and field operators to eliminate manual threshold sensitivity; (2) morphological refinement algorithms for merging fragmented fronts, deleting spurious rings, and thinning frontal zones to pixel-level accuracy; and (3) a novel metric space definition for temporal front tracking, enabling systematic analysis of front evolution. Validated on global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\%$ compared to histogram-based methods while achieving superior intensity ($0.16^\circ$C/km), continuity, and spatiotemporal coherence. The open-source release bridges a critical gap in reproducible oceanographic research.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-R1: Reinforcing Video Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2503.21776</link>
<guid>https://arxiv.org/abs/2503.21776</guid>
<content:encoded><![CDATA[
arXiv:2503.21776v2 Announce Type: replace 
Abstract: Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data are released in: https://github.com/tulerfeng/Video-R1.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation</title>
<link>https://arxiv.org/abs/2504.14988</link>
<guid>https://arxiv.org/abs/2504.14988</guid>
<content:encoded><![CDATA[
arXiv:2504.14988v2 Announce Type: replace 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal perception capabilities, garnering significant attention. While numerous evaluation studies have emerged, assessing LVLMs both holistically and on specialized tasks, fine-grained image tasks-fundamental to computer vision-remain largely unexplored. To fill this gap, we introduce a comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 1.01 million questions and 0.33 million images. Our evaluation systematically examines LVLMs from both human-oriented and machine-oriented perspectives, focusing on their semantic recognition and fine-grained feature representation capabilities. Through extensive experiments on twelve representative LVLMs/VLMs, we uncover key findings regarding the influence of training paradigms, modality alignment, perturbation susceptibility, and fine-grained category reasoning on task performance. This work provides critical insights into the limitations of current LVLMs and offers guidance for future data construction and model design in the development of more advanced LVLMs. Our code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction</title>
<link>https://arxiv.org/abs/2505.02126</link>
<guid>https://arxiv.org/abs/2505.02126</guid>
<content:encoded><![CDATA[
arXiv:2505.02126v2 Announce Type: replace 
Abstract: Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects</title>
<link>https://arxiv.org/abs/2204.03139</link>
<guid>https://arxiv.org/abs/2204.03139</guid>
<content:encoded><![CDATA[
arXiv:2204.03139v2 Announce Type: replace-cross 
Abstract: Research in manipulation of deformable objects is typically conducted on a limited range of scenarios, because handling each scenario on hardware takes significant effort. Realistic simulators with support for various types of deformations and interactions have the potential to speed up experimentation with novel tasks and algorithms. However, for highly deformable objects it is challenging to align the output of a simulator with the behavior of real objects. Manual tuning is not intuitive, hence automated methods are needed. We view this alignment problem as a joint perception-inference challenge and demonstrate how to use recent neural network architectures to successfully perform simulation parameter inference from real point clouds. We analyze the performance of various architectures, comparing their data and training requirements. Furthermore, we propose to leverage differentiable point cloud sampling and differentiable simulation to significantly reduce the time to achieve the alignment. We employ an efficient way to propagate gradients from point clouds to simulated meshes and further through to the physical simulation parameters, such as mass and stiffness. Experiments with highly deformable objects show that our method can achieve comparable or better alignment with real object behavior, while reducing the time needed to achieve this by more than an order of magnitude. Videos and supplementary material are available at https://diffcloud.github.io.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error correcting 2D-3D cascaded network for myocardial infarct scar segmentation on late gadolinium enhancement cardiac magnetic resonance images</title>
<link>https://arxiv.org/abs/2306.14725</link>
<guid>https://arxiv.org/abs/2306.14725</guid>
<content:encoded><![CDATA[
arXiv:2306.14725v2 Announce Type: replace-cross 
Abstract: Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is considered the in vivo reference standard for assessing infarct size (IS) and microvascular obstruction (MVO) in ST-elevation myocardial infarction (STEMI) patients. However, the exact quantification of those markers of myocardial infarct severity remains challenging and very time-consuming. As LGE distribution patterns can be quite complex and hard to delineate from the blood pool or epicardial fat, automatic segmentation of LGE CMR images is challenging. In this work, we propose a cascaded framework of two-dimensional and three-dimensional convolutional neural networks (CNNs) which enables to calculate the extent of myocardial infarction in a fully automated way. By artificially generating segmentation errors which are characteristic for 2D CNNs during training of the cascaded framework we are enforcing the detection and correction of 2D segmentation errors and hence improve the segmentation accuracy of the entire method. The proposed method was trained and evaluated on two publicly available datasets. We perform comparative experiments where we show that our framework outperforms state-of-the-art reference methods in segmentation of myocardial infarction. Furthermore, in extensive ablation studies we show the advantages that come with the proposed error correcting cascaded method. The code of this project is publicly available at https://github.com/matthi99/EcorC.git
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Call to Arms: AI Should be Critical for Social Media Analysis of Conflict Zones</title>
<link>https://arxiv.org/abs/2311.00810</link>
<guid>https://arxiv.org/abs/2311.00810</guid>
<content:encoded><![CDATA[
arXiv:2311.00810v3 Announce Type: replace-cross 
Abstract: The massive proliferation of social media data represents a transformative opportunity for conflict studies and for tracking the proliferation and use of weaponry, as conflicts are increasingly documented in these online spaces. At the same time, the scale and types of data available are problematic for traditional open-source intelligence. This paper focuses on identifying specific weapon systems and the insignias of the armed groups using them as documented in the Ukraine war, as these tasks are critical to operational intelligence and tracking weapon proliferation, especially given the scale of international military aid given to Ukraine. The large scale of social media makes manual assessment difficult, however, so this paper presents early work that uses computer vision models to support this task. We demonstrate that these models can both identify weapons embedded in images shared in social media and how the resulting collection of military-relevant images and their post times interact with the offline, real-world conflict. Not only can we then track changes in the prevalence of images of tanks, land mines, military trucks, etc., we find correlations among time series data associated with these images and the daily fatalities in this conflict. This work shows substantial opportunity for examining similar online documentation of conflict contexts, and we also point to future avenues where computer vision can be further improved for these open-source intelligence tasks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues</title>
<link>https://arxiv.org/abs/2403.02043</link>
<guid>https://arxiv.org/abs/2403.02043</guid>
<content:encoded><![CDATA[
arXiv:2403.02043v2 Announce Type: replace-cross 
Abstract: Light field cameras and multi-camera arrays have emerged as promising solutions for accurately estimating depth by passively capturing light information. This is possible because the 3D information of a scene is embedded in the 4D light field geometry. Commonly, depth estimation methods extract this information relying on gradient information, heuristic-based optimisation models, or learning-based approaches. This paper focuses mainly on explicitly understanding and exploiting 4D geometrical cues for light field depth estimation. Thus, a novel method is proposed, based on a non-learning-based optimisation approach for depth estimation that explicitly considers surface normal accuracy and occlusion regions by utilising a fully explainable 4D geometric model of the light field. The 4D model performs depth/disparity estimation by determining the orientations and analysing the intersections of key 2D planes in 4D space, which are the images of 3D-space points in the 4D light field. Experimental results show that the proposed method outperforms both learning-based and non-learning-based state-of-the-art methods in terms of surface normal angle accuracy, achieving a Median Angle Error on planar surfaces, on average, 26.3$\%$ lower than the state-of-the-art, and still being competitive with state-of-the-art methods in terms of MSE ${\times}$ 100 and Badpix 0.07.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Quantum Annealing and Spiking Neuromorphic Computing for Sampling Binary Sparse Coding QUBO Problems</title>
<link>https://arxiv.org/abs/2405.20525</link>
<guid>https://arxiv.org/abs/2405.20525</guid>
<content:encoded><![CDATA[
arXiv:2405.20525v2 Announce Type: replace-cross 
Abstract: We consider the problem of computing a sparse binary representation of an image. To be precise, given an image and an overcomplete, non-orthonormal basis, we aim to find a sparse binary vector indicating the minimal set of basis vectors that when added together best reconstruct the given input. We formulate this problem with an $L_2$ loss on the reconstruction error, and an $L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing sparsity. This yields a quadratic unconstrained binary optimization problem (QUBO), whose optimal solution(s) in general is NP-hard to find. The contribution of this work is twofold. First, we solve the sparse representation QUBOs by solving them both on a D-Wave quantum annealer with Pegasus chip connectivity via minor embedding, as well as on the Intel Loihi 2 spiking neuromorphic processor using a stochastic Non-equilibrium Boltzmann Machine (NEBM). Second, we deploy Quantum Evolution Monte Carlo with Reverse Annealing and iterated warm starting on Loihi 2 to evolve the solution quality from the respective machines. The solutions are benchmarked against simulated annealing, a classical heuristic, and the optimal solutions are computed using CPLEX. Iterated reverse quantum annealing performs similarly to simulated annealing, although simulated annealing is always able to sample the optimal solution whereas quantum annealing was not always able to. The Loihi 2 solutions that are sampled are on average more sparse than the solutions from any of the other methods. We demonstrate that both quantum annealing and neuromorphic computing are suitable for binary sparse coding QUBOs, and that Loihi 2 outperforms a D-Wave quantum annealer standard linear-schedule anneal, while iterated reverse quantum annealing performs much better than both unmodified linear-schedule quantum annealing and iterated warm starting on Loihi 2.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METDrive: Multi-modal End-to-end Autonomous Driving with Temporal Guidance</title>
<link>https://arxiv.org/abs/2409.12667</link>
<guid>https://arxiv.org/abs/2409.12667</guid>
<content:encoded><![CDATA[
arXiv:2409.12667v3 Announce Type: replace-cross 
Abstract: Multi-modal end-to-end autonomous driving has shown promising advancements in recent work. By embedding more modalities into end-to-end networks, the system's understanding of both static and dynamic aspects of the driving environment is enhanced, thereby improving the safety of autonomous driving. In this paper, we introduce METDrive, an end-to-end system that leverages temporal guidance from the embedded time series features of ego states, including rotation angles, steering, throttle signals, and waypoint vectors. The geometric features derived from perception sensor data and the time series features of ego state data jointly guide the waypoint prediction with the proposed temporal guidance loss function. We evaluated METDrive on the CARLA leaderboard benchmarks, achieving a driving score of 70%, a route completion score of 94%, and an infraction score of 0.78.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2409.13498</link>
<guid>https://arxiv.org/abs/2409.13498</guid>
<content:encoded><![CDATA[
arXiv:2409.13498v2 Announce Type: replace-cross 
Abstract: Recent advancements in computer vision, particularly in detection, segmentation, and classification, have significantly impacted various domains. However, these advancements are tied to RGB-based systems, which are insufficient for applications in industries like waste sorting, pharmaceuticals, and defense, where advanced object characterization beyond shape or color is necessary. Hyperspectral (HS) imaging, capturing both spectral and spatial information, addresses these limitations and offers advantages over conventional technologies such as X-ray fluorescence and Raman spectroscopy, particularly in terms of speed, cost, and safety.
  This study evaluates the potential of combining HS imaging with deep learning for material characterization. The research involves: i) designing an experimental setup with HS camera, conveyor, and controlled lighting; ii) generating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with semi-automated mask generation and Raman spectroscopy-based labeling; and iii) developing a deep learning model trained on HS images for pixel-level material classification. The model achieved 99.94\% classification accuracy, demonstrating robustness in color, size, and shape invariance, and effectively handling material overlap. Limitations, such as challenges with black objects, are also discussed. Extending computer vision beyond RGB to HS imaging proves feasible, overcoming major limitations of traditional methods and showing strong potential for future applications.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2409.18872</link>
<guid>https://arxiv.org/abs/2409.18872</guid>
<content:encoded><![CDATA[
arXiv:2409.18872v2 Announce Type: replace-cross 
Abstract: This paper presents a method for virtual contrast enhancement in breast MRI, offering a promising non-invasive alternative to traditional contrast agent-based DCE-MRI acquisition. Using a conditional generative adversarial network, we predict DCE-MRI images, including jointly-generated sequences of multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs, enabling tumor localization and characterization without the associated health risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe), assessing their utility in a tumor segmentation downstream task, and conclude with an analysis of the temporal patterns in multi-sequence DCE-MRI generation. Our approach demonstrates promising results in generating realistic and useful DCE-MRI sequences, highlighting the potential of virtual contrast enhancement for improving breast cancer diagnosis and treatment, particularly for patients where contrast agent administration is contraindicated.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Traffic Anomalies from Generative Models on Real-Time Observations</title>
<link>https://arxiv.org/abs/2502.01391</link>
<guid>https://arxiv.org/abs/2502.01391</guid>
<content:encoded><![CDATA[
arXiv:2502.01391v2 Announce Type: replace-cross 
Abstract: Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
arXiv:2503.21696v2 Announce Type: replace-cross 
Abstract: Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling</title>
<link>https://arxiv.org/abs/2504.14219</link>
<guid>https://arxiv.org/abs/2504.14219</guid>
<content:encoded><![CDATA[
arXiv:2504.14219v2 Announce Type: replace-cross 
Abstract: We present PRISM, a unified framework that enables multiple image generation and editing tasks in a single foundational model. Starting from a pre-trained text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy to produce RGB images along with intrinsic maps (referred to as X layers) simultaneously. Unlike previous approaches, which infer intrinsic properties individually or require separate models for decomposition and conditional generation, PRISM maintains consistency across modalities by generating all intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX generation, RGB-to-X decomposition, and X-to-RGBX conditional generation. Additionally, PRISM enables both global and local image editing through conditioning on selected intrinsic layers and text prompts. Extensive experiments demonstrate the competitive performance of PRISM both for intrinsic image decomposition and conditional image generation while preserving the base model's text-to-image generation capability.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets</title>
<link>https://arxiv.org/abs/2504.21227</link>
<guid>https://arxiv.org/abs/2504.21227</guid>
<content:encoded><![CDATA[
arXiv:2504.21227v2 Announce Type: replace-cross 
Abstract: Deep learning models have great potential in medical imaging, including orthodontics and skeletal maturity assessment. However, applying a model to data different from its training set can lead to unreliable predictions that may impact patient care. To address this, we propose a comprehensive verification framework that evaluates model suitability through multiple complementary strategies. First, we introduce a Gradient Attention Map (GAM)-based approach that analyzes attention patterns using Grad-CAM and compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance. Second, we extend verification to early convolutional feature maps, capturing structural mis-alignments missed by attention alone. Finally, we incorporate an additional garbage class into the classification model to explicitly reject out-of-distribution inputs. Experimental results demonstrate that these combined methods effectively identify unsuitable models and inputs, promoting safer and more reliable deployment of deep learning in medical imaging.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
<link>https://arxiv.org/abs/2505.02831</link>
<guid>https://arxiv.org/abs/2505.02831</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion transformers, representation guidance, Self-Representation Alignment, generative training, self-distillation  

<br /><br />Summary: This study addresses the challenges in improving generative training by proposing a novel method called Self-Representation Alignment (SRA). Recent works have shown that meaningful internal representations can enhance both the training speed and output quality in diffusion transformers. Traditional methods either require complex external representation frameworks or rely on large pre-trained foundation models for guidance. In contrast, SRA leverages the inherent discriminative process of diffusion transformers, allowing them to generate representation guidance internally during generative training. The method works by aligning the output latent representations from earlier layers (which have higher noise) with those from later layers (with lower noise). This alignment process enhances overall representation learning without the need for external components. Experimental results demonstrate that applying SRA to Diffusion Transformers (DiTs) and Simple Transformers (SiTs) results in consistent performance improvements. Notably, SRA surpasses existing approaches that depend on auxiliary representation frameworks and achieves performance levels comparable to methods reliant on sophisticated external representation priors, highlighting its efficacy and simplicity in optimizing generative processes. <div>
arXiv:2505.02831v3 Announce Type: replace 
Abstract: Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance the generation quality of diffusion transformers. However, existing approaches necessitate to either introduce an external and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation Alignment (SRA), a simple yet straightforward method that obtains representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in the earlier layer with higher noise to that in the later layer with lower noise to progressively enhance the overall representation learning during only the generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that are heavily dependent on powerful external representation priors.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing</title>
<link>https://arxiv.org/abs/2505.07984</link>
<guid>https://arxiv.org/abs/2505.07984</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal language model, remote sensing imagery, fine-tuning, Group Relative Policy Optimization, military installations

Summary: 
MilChat is a lightweight multimodal language model designed for analyzing remote sensing imagery in secluded areas, particularly focusing on challenging missile launch sites. A new dataset, MilData, was compiled to train the model with expert-verified aerial images and detailed captions highlighting military installations. The model underwent supervised fine-tuning using CoT reasoning annotations and utilized Group Relative Policy Optimization to improve domain-specific cue detection while minimizing false positives. Empirical evaluations showed that MilChat outperformed larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification tasks. Achieving over 80% recall and 98% precision on the MilData benchmark, the study highlights the effectiveness of targeted fine-tuning and reinforcement learning in specialized domains. 

<br /><br />Summary: <div>
arXiv:2505.07984v1 Announce Type: new 
Abstract: Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed MilChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, MilData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that MilChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed MilData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Foundation Model Embedding-Based Semantic Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.07998</link>
<guid>https://arxiv.org/abs/2505.07998</guid>
<content:encoded><![CDATA[
<div> semantic anomaly detection, visual embeddings, autonomous systems, instance segmentation, filtering

Summary: 
This paper investigates semantic anomaly detection in autonomous systems using state-of-the-art vision foundation models. The framework proposed compares local vision embeddings from runtime images to a database of safe scenarios to identify anomalies. Two variants of the framework are considered, one using raw grid-based embeddings and the other leveraging instance segmentation for object-centric representations. A filtering mechanism is introduced to reduce false positives and enhance robustness. Evaluation on CARLA-simulated anomalies shows that the instance-based method with filtering achieves performance comparable to GPT-4o, with precise anomaly localization. These results demonstrate the potential of vision embeddings from foundation models for real-time anomaly detection in autonomous systems. 

Summary: <div>
arXiv:2505.07998v1 Announce Type: new 
Abstract: Semantic anomalies are contextually invalid or unusual combinations of familiar visual elements that can cause undefined behavior and failures in system-level reasoning for autonomous systems. This work explores semantic anomaly detection by leveraging the semantic priors of state-of-the-art vision foundation models, operating directly on the image. We propose a framework that compares local vision embeddings from runtime images to a database of nominal scenarios in which the autonomous system is deemed safe and performant. In this work, we consider two variants of the proposed framework: one using raw grid-based embeddings, and another leveraging instance segmentation for object-centric representations. To further improve robustness, we introduce a simple filtering mechanism to suppress false positives. Our evaluations on CARLA-simulated anomalies show that the instance-based method with filtering achieves performance comparable to GPT-4o, while providing precise anomaly localization. These results highlight the potential utility of vision embeddings from foundation models for real-time anomaly detection in autonomous systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</title>
<link>https://arxiv.org/abs/2505.08013</link>
<guid>https://arxiv.org/abs/2505.08013</guid>
<content:encoded><![CDATA[
<div> Keywords: feature detection, structure-from-motion, SLAM, deformable transformer, keypoint detection

Summary:
Robust feature detection and description in structure-from-motion and SLAM applications remain challenging, particularly in cases of significant viewpoint changes. Existing methods focus on local features but fail to capture long-range relationships. The Robust Deformable Detector (RDD) introduced in this study leverages deformable transformers to incorporate global context and geometric invariance through deformable self-attention mechanisms. By focusing on key locations, the deformable attention effectively reduces search space complexity and models geometric invariance. The RDD outperforms state-of-the-art methods in keypoint detection/description tasks and can handle semi-dense matching. The study also introduces two new benchmarks, emphasizing large viewpoint and scale variations, as well as an Air-to-Ground benchmark, showcasing the method's effectiveness for 3D reconstruction across different altitudes.

<br /><br />Summary: <div>
arXiv:2505.08013v1 Announce Type: new 
Abstract: As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Interpretable Subtask Reasoning for Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.08084</link>
<guid>https://arxiv.org/abs/2505.08084</guid>
<content:encoded><![CDATA[
<div> Keywords: visual question answering, multimodal large language models, reasoning, interpretability, subtask-driven training

Summary:<br />
The article introduces VISTAR, a framework for enhancing interpretability and reasoning in visual question answering tasks. VISTAR utilizes subtask-driven training to generate structured reasoning sequences within multimodal large language models. By fine-tuning these models to produce step-by-step rationales, VISTAR improves reasoning accuracy while maintaining interpretability. The framework addresses the limitations of current methods by integrating both textual and visual explanations directly into the model, eliminating the need for external models. Experimental results on two benchmarks demonstrate the effectiveness of VISTAR in improving reasoning performance. The code and dataset for VISTAR will be made available for further research and development. 

Summary: <div>
arXiv:2505.08084v1 Announce Type: new 
Abstract: Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)</title>
<link>https://arxiv.org/abs/2505.08086</link>
<guid>https://arxiv.org/abs/2505.08086</guid>
<content:encoded><![CDATA[
<div> Keywords: wound classification, Artificial Intelligence, transfer learning, multi-modal network, medical image analysis <br />
Summary: <br />
- Effective diagnosis of acute and hard-to-heal wounds is crucial for providing proper patient care, especially in cases of infection, peripheral vascular disease, and increasing wound depth.
- Utilizing Artificial Intelligence (AI) in medical image interpretation can significantly improve early disease detection.
- A multi-modal AI model based on transfer learning (TL) was proposed, combining Xception and GMRNN architectures for wound classification with high accuracy.
- The model incorporated features extracted by TL algorithm and location features for classifying diabetic, pressure, surgical, and venous ulcers.
- The proposed methodology showcased exceptional accuracy in accurately classifying the most commonly occurring wound types using images and their locations. <div>
arXiv:2505.08086v1 Announce Type: new 
Abstract: The effective diagnosis of acute and hard-to-heal wounds is crucial for wound care practitioners to provide effective patient care. Poor clinical outcomes are often linked to infection, peripheral vascular disease, and increasing wound depth, which collectively exacerbate these comorbidities. However, diagnostic tools based on Artificial Intelligence (AI) speed up the interpretation of medical images and improve early detection of disease. In this article, we propose a multi-modal AI model based on transfer learning (TL), which combines two state-of-the-art architectures, Xception and GMRNN, for wound classification. The multi-modal network is developed by concatenating the features extracted by a transfer learning algorithm and location features to classify the wound types of diabetic, pressure, surgical, and venous ulcers. The proposed method is comprehensively compared with deep neural networks (DNN) for medical image analysis. The experimental results demonstrate a notable wound-class classifications (containing only diabetic, pressure, surgical, and venous) vary from 78.77 to 100\% in various experiments. The results presented in this study showcase the exceptional accuracy of the proposed methodology in accurately classifying the most commonly occurring wound types using wound images and their corresponding locations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing</title>
<link>https://arxiv.org/abs/2505.08101</link>
<guid>https://arxiv.org/abs/2505.08101</guid>
<content:encoded><![CDATA[
<div> Point cloud processing, autonomous driving, 3D object recognition, Point Transformer V3, distillation framework<br />
<br />
Summary: 
This paper introduces a novel distillation framework for transferring knowledge from a high-capacity teacher model to a lightweight student model in point cloud processing applications. The framework leverages topology-aware representations and gradient-guided knowledge distillation to capture geometric structures and guide the learning process effectively. Experimental results on Nuscenes, SemanticKITTI, and Waymo datasets show competitive performance with a 16x reduction in model size and nearly 1.9x decrease in inference time compared to the teacher model. The proposed method achieves state-of-the-art performance in segmentation on NuScenes, surpassing prior knowledge distillation baselines in LiDAR data training. Available publicly on GitHub at: https://github.com/HySonLab/PointDistill<br /><br /> <div>
arXiv:2505.08101v1 Announce Type: new 
Abstract: Point cloud processing has gained significant attention due to its critical role in applications such as autonomous driving and 3D object recognition. However, deploying high-performance models like Point Transformer V3 in resource-constrained environments remains challenging due to their high computational and memory demands. This work introduces a novel distillation framework that leverages topology-aware representations and gradient-guided knowledge distillation to effectively transfer knowledge from a high-capacity teacher to a lightweight student model. Our approach captures the underlying geometric structures of point clouds while selectively guiding the student model's learning process through gradient-based feature alignment. Experimental results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the proposed method achieves competitive performance, with an approximately 16x reduction in model size and a nearly 1.9x decrease in inference time compared to its teacher model. Notably, on NuScenes, our method achieves state-of-the-art performance among knowledge distillation techniques trained solely on LiDAR data, surpassing prior knowledge distillation baselines in segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors</title>
<link>https://arxiv.org/abs/2505.08111</link>
<guid>https://arxiv.org/abs/2505.08111</guid>
<content:encoded><![CDATA[
<div> sleep position classification, pressure-sensitive mats, deep learning, transfer learning, polysomnography

Summary:
The study focuses on classifying four-way sleep positions using data from pressure-sensitive mats (PSMs) placed under mattresses in a sleep clinic. Sleep positions can impact sleep quality and the prevalence of sleep disorders like apnea. The researchers used transfer learning techniques to adapt pre-trained deep learning models for accurate sleep position estimation from a low-resolution PSM dataset collected in a polysomnography sleep lab. They leveraged Vision Transformer models pre-trained on ImageNet and a model for human pose estimation. Their approach surpassed previous methods using deep learning and traditional machine learning models. The performance was evaluated on 112 nights of patient recordings and validated on a higher-resolution dataset from 13 patients. Despite challenges with low-resolution data, the approach shows promise for real-world application in clinical settings.<br /><br />Summary: <div>
arXiv:2505.08111v1 Announce Type: new 
Abstract: Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of monitoring patients during sleep. We focus on four-way sleep position classification using data collected from a PSM placed under a mattress in a sleep clinic. Sleep positions can affect sleep quality and the prevalence of sleep disorders, such as apnea. Measurements were performed on patients with suspected sleep disorders referred for assessments at a sleep clinic. Training deep learning models can be challenging in clinical settings due to the need for large amounts of labeled data. To overcome the shortage of labeled training data, we utilize transfer learning to adapt pre-trained deep learning models to accurately estimate sleep positions from a low-resolution PSM dataset collected in a polysomnography sleep lab. Our approach leverages Vision Transformer models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a pre-trained model for human pose estimation (ViTPose). These approaches outperform previous work from PSM-based sleep pose classification using deep learning (TCN) as well as traditional machine learning models (SVM, XGBoost, Random Forest) that use engineered features. We evaluate the performance of sleep position classification from 112 nights of patient recordings and validate it on a higher resolution 13-patient dataset. Despite the challenges of differentiating between sleep positions from low-resolution PSM data, our approach shows promise for real-world deployment in clinical settings
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Now you see it, Now you don't: Damage Label Agreement in Drone &amp; Satellite Post-Disaster Imagery</title>
<link>https://arxiv.org/abs/2505.08117</link>
<guid>https://arxiv.org/abs/2505.08117</guid>
<content:encoded><![CDATA[
<div> Keywords: damage labels, satellite imagery, drone imagery, machine learning, building damage assessment

Summary: 
This paper examines discrepancies in damage labels from satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey. The analysis reveals a significant 29.02% label disagreement between the sources, highlighting potential risks in deploying machine learning damage assessment systems. Satellite-derived labels were found to under-report damage by at least 20.43% compared to drone-derived labels, indicating a misrepresentation of actual conditions. The differing distributions of satellite and drone-derived labels suggest that computer vision and machine learning models trained on either dataset may not accurately represent real-world scenarios. To address these issues and prevent potential societal harm, the paper offers four recommendations for enhancing reliability and transparency in deploying CV/ML damage assessment systems. 

<br /><br />Summary: <div>
arXiv:2505.08117v1 Announce Type: new 
Abstract: This paper audits damage labels derived from coincident satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey, finding 29.02% label disagreement and significantly different distributions between the two sources, which presents risks and potential harms during the deployment of machine learning damage assessment systems. Currently, there is no known study of label agreement between drone and satellite imagery for building damage assessment. The only prior work that could be used to infer if such imagery-derived labels agree is limited by differing damage label schemas, misaligned building locations, and low data quantities. This work overcomes these limitations by comparing damage labels using the same damage label schemas and building locations from three hurricanes, with the 15,814 buildings representing 19.05 times more buildings considered than the most relevant prior work. The analysis finds satellite-derived labels significantly under-report damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and satellite- and drone-derived labels represent significantly different distributions (p<5.1x10^-175). This indicates that computer vision and machine learning (CV/ML) models trained on at least one of these distributions will misrepresent actual conditions, as the differing satellite and drone-derived distributions cannot simultaneously represent the distribution of actual conditions in a scene. This potential misrepresentation poses ethical risks and potential societal harm if not managed. To reduce the risk of future societal harms, this paper offers four recommendations to improve reliability and transparency to decisio-makers when deploying CV/ML damage assessment systems in practice
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections</title>
<link>https://arxiv.org/abs/2505.08123</link>
<guid>https://arxiv.org/abs/2505.08123</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-material decomposition, SEMMD, JSover, spectral CT, implicit neural representation

Summary: 
JSover is a new one-step framework for multi-material decomposition (MMD) in CT imaging that improves the accuracy and efficiency of traditional methods. Unlike previous two-step approaches, JSover simultaneously reconstructs tissue compositions and estimates the energy spectrum directly from single-energy CT projections. By incorporating physics-informed spectral priors, it creates a virtual spectral CT system from SE acquisitions, reducing beam hardening artifacts. Additionally, JSover leverages implicit neural representation (INR) to enhance material map estimation using unsupervised deep learning. Experimental results demonstrate that JSover surpasses existing SEMMD techniques in accuracy and computational performance on both simulated and real CT datasets. This advancement in MMD technology holds potential for a wide range of clinical applications by enabling more reliable quantitative tissue composition reconstructions from single-energy CT scans.<br /><br />Summary: <div>
arXiv:2505.08123v1 Announce Type: new 
Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLAG: Scalable Language-Augmented Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08124</link>
<guid>https://arxiv.org/abs/2505.08124</guid>
<content:encoded><![CDATA[
<div> framework, language-augmented, scene representations, Gaussian splatting, scalability
Summary:
SLAG is a multi-GPU framework that accelerates the encoding of language-augmented scene representations for robotics applications. By integrating 2D visual-language model features into 3D scenes using SAM and CLIP, SLAG eliminates the need for loss functions and enables highly parallelized scene encoding. The method achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while maintaining embedding quality on datasets like ScanNet and LERF. Additionally, SLAG introduces a vector database for efficient embedding storage and retrieval, making it suitable for time-sensitive and data-intensive scenarios. The framework holds promise for use in search-and-rescue operations, smart cities, mining, and other large-scale robotics applications. Visit the project website for more information: https://slag-project.github.io/. 

<br /><br />Summary: <div>
arXiv:2505.08124v1 Announce Type: new 
Abstract: Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Multi-Object Tracking with an Event Camera</title>
<link>https://arxiv.org/abs/2505.08126</link>
<guid>https://arxiv.org/abs/2505.08126</guid>
<content:encoded><![CDATA[
<div> Event cameras, Asynchronous Event Multi-Object Tracking, AEMOT, object detection, object tracking<br />
Summary:<br />
Events cameras with their low latency output, high temporal resolution, and dynamic range, are ideal for detecting and tracking objects in dynamic environments. The Asynchronous Event Multi-Object Tracking (AEMOT) algorithm processes individual raw events asynchronously to detect and track multiple objects. By identifying salient event blob features through optical flow analysis, AEMOT tracks candidate objects using the Asynchronous Event Blob (AEB) tracker. A novel validation stage distinguishes and estimates object characteristics such as position, velocity, size, and orientation in real time. Tested on the Bee Swarm Dataset, AEMOT outperforms other event-based detection and tracking algorithms with precision and recall rates exceeding 37%. The algorithm and the labeled event dataset will be made open source. <br />Summary: <div>
arXiv:2505.08126v1 Announce Type: new 
Abstract: Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoKD: Multi-Task Optimization for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.08170</link>
<guid>https://arxiv.org/abs/2505.08170</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Multi-Task Optimization, Gradient Conflicts, Gradient Dominance, Image Classification, Object Detection <br />
<br />Summary: <br />Compact models can be effectively trained through Knowledge Distillation (KD), with two main challenges being balancing guidance from the teacher model and the task objective, and handling knowledge representation disparities between teacher and student models. To address these challenges, Multi-Task Optimization for Knowledge Distillation (MoKD) is proposed. MoKD reformulates KD as a multi-objective optimization problem to balance objectives and introduces a subspace learning framework to improve knowledge transfer by projecting feature representations into a high-dimensional space. Results from experiments on ImageNet-1K dataset and COCO dataset show that MoKD outperforms existing methods in image classification and object detection, achieving state-of-the-art performance with greater efficiency. Additionally, MoKD models achieve state-of-the-art performance compared to models trained from scratch. <div>
arXiv:2505.08170v1 Announce Type: new 
Abstract: Compact models can be effectively trained through Knowledge Distillation (KD), a technique that transfers knowledge from larger, high-performing teacher models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing learning from the teacher's guidance and the task objective, and 2) handling the disparity in knowledge representation between teacher and student models. To address these, we propose Multi-Task Optimization for Knowledge Distillation (MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where task-specific and distillation gradients are misaligned, and b) Gradient Dominance, where one objective's gradient dominates, causing imbalance. MoKD reformulates KD as a multi-objective optimization problem, enabling better balance between objectives. Additionally, it introduces a subspace learning framework to project feature representations into a high-dimensional space, improving knowledge transfer. Our MoKD is demonstrated to outperform existing methods through extensive experiments on image classification using the ImageNet-1K dataset and object detection using the COCO dataset, achieving state-of-the-art performance with greater efficiency. To the best of our knowledge, MoKD models also achieve state-of-the-art performance compared to models trained from scratch.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification</title>
<link>https://arxiv.org/abs/2505.08173</link>
<guid>https://arxiv.org/abs/2505.08173</guid>
<content:encoded><![CDATA[
<div> causal inference, long-tail classification, Visual Transformers, TSCNet, multi-scale causal interventions <br />
<br />
Summary: 
This paper addresses the challenge of handling biases introduced by class imbalance in long-tail classification by investigating the influence of existing causal models on CNNs and Visual Transformers. The study shows that Visual Transformers' global feature representation makes it difficult for causal methods to model associations between fine-grained features and predictions, especially for tail classes with similar visual appearance. To tackle this issue, the paper proposes TSCNet, a two-stage causal modeling method that employs multi-scale causal interventions. The first stage focuses on hierarchical causal representation learning by decoupling background and objects and applying backdoor interventions. The second stage, counterfactual logits bias calibration, refines the model's decision boundary by constructing a counterfactual balanced data distribution. Experimental results demonstrate that TSCNet outperforms existing methods in eliminating biases caused by data imbalance. <br /><br /> <div>
arXiv:2505.08173v1 Announce Type: new 
Abstract: Causal inference has emerged as a promising approach to mitigate long-tail classification by handling the biases introduced by class imbalance. However, along with the change of advanced backbone models from Convolutional Neural Networks (CNNs) to Visual Transformers (ViT), existing causal models may not achieve an expected performance gain. This paper investigates the influence of existing causal models on CNNs and ViT variants, highlighting that ViT's global feature representation makes it hard for causal methods to model associations between fine-grained features and predictions, which leads to difficulties in classifying tail classes with similar visual appearance. To address these issues, this paper proposes TSCNet, a two-stage causal modeling method to discover fine-grained causal associations through multi-scale causal interventions. Specifically, in the hierarchical causal representation learning stage (HCRL), it decouples the background and objects, applying backdoor interventions at both the patch and feature level to prevent model from using class-irrelevant areas to infer labels which enhances fine-grained causal representation. In the counterfactual logits bias calibration stage (CLBC), it refines the optimization of model's decision boundary by adaptive constructing counterfactual balanced data distribution to remove the spurious associations in the logits caused by data distribution. Extensive experiments conducted on various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate multiple biases introduced by data imbalance, which outperforms existing methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images</title>
<link>https://arxiv.org/abs/2505.08178</link>
<guid>https://arxiv.org/abs/2505.08178</guid>
<content:encoded><![CDATA[
<div> DGORNet, Disparity Estimation, Surgical Images, Occlusion, Monocular Depth<br />
<br />
Summary: <br />
The study introduces DGORNet, a novel Depth Guided Occlusion-Aware Disparity Refinement Network for improving stereo laparoscopic image disparity estimation. It addresses challenges such as occlusion and limited labeled data by leveraging monocular depth information and introducing a Position Embedding module for enhanced spatial context. The network also incorporates an Optical Flow Difference Loss for unlabeled data to enhance robustness in dynamic surgical scenes. Experimental results on the SCARED dataset demonstrate that DGORNet outperforms existing methods in terms of End-Point Error and Root Mean Squared Error, particularly in occlusion and texture-less regions. Ablation studies validate the significant contributions of the Position Embedding and Optical Flow Difference Loss in improving spatial and temporal consistency. DGORNet proves to be an effective solution for enhancing disparity estimation in laparoscopic surgery, providing practical insights for addressing challenges in disparity estimation and data scarcity. <div>
arXiv:2505.08178v1 Announce Type: new 
Abstract: Occlusion and the scarcity of labeled surgical data are significant challenges in disparity estimation for stereo laparoscopic images. To address these issues, this study proposes a Depth Guided Occlusion-Aware Disparity Refinement Network (DGORNet), which refines disparity maps by leveraging monocular depth information unaffected by occlusion. A Position Embedding (PE) module is introduced to provide explicit spatial context, enhancing the network's ability to localize and refine features. Furthermore, we introduce an Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal continuity across video frames to improve robustness in dynamic surgical scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean Squared Error (RMSE), particularly in occlusion and texture-less regions. Ablation studies confirm the contributions of the Position Embedding and Optical Flow Difference Loss, highlighting their roles in improving spatial and temporal consistency. These results underscore DGORNet's effectiveness in enhancing disparity estimation for laparoscopic surgery, offering a practical solution to challenges in disparity estimation and data limitations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2505.08190</link>
<guid>https://arxiv.org/abs/2505.08190</guid>
<content:encoded><![CDATA[
<div> diffusion models, image inpainting, raindrop removal, single image, Generative Adversarial Network <br />
<br />
Summary:
Raindrop removal from images is a challenging task, especially when relying on a single image. Common approaches involve detecting raindrop regions and then restoring the background based on these detections. The use of Generative Adversarial Networks (GANs) is prevalent for background restoration. However, recent advancements in diffusion models have improved image inpainting techniques significantly. This paper introduces a novel method for raindrop removal using diffusion-based image inpainting from a single image. By leveraging the capabilities of diffusion models, this technique offers a promising approach for effectively removing raindrops from images in a single-step process. <div>
arXiv:2505.08190v1 Announce Type: new 
Abstract: Raindrop removal is a challenging task in image processing. Removing raindrops while relying solely on a single image further increases the difficulty of the task. Common approaches include the detection of raindrop regions in the image, followed by performing a background restoration process conditioned on those regions. While various methods can be applied for the detection step, the most common architecture used for background restoration is the Generative Adversarial Network (GAN). Recent advances in the use of diffusion models have led to state-of-the-art image inpainting techniques. In this paper, we introduce a novel technique for raindrop removal from a single image using diffusion-based image inpainting.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2505.08196</link>
<guid>https://arxiv.org/abs/2505.08196</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, Dynamic Scene Reconstruction, Anchor-Based Structure, Rate-Distortion Optimization, Hierarchical Pipeline

Summary: 
ADC-GS introduces a novel approach for dynamic scene reconstruction by organizing Gaussian primitives into an anchor-based structure within a canonical space, improving performance by reducing redundancy and optimizing representation. It utilizes a hierarchical pipeline to capture motions at varying granularities and adopts rate-distortion optimization to achieve an optimal balance between bitrate consumption and representation fidelity. The method outperforms existing approaches in rendering speed by 300%-800% while maintaining state-of-the-art storage efficiency and rendering quality. The code for ADC-GS is available on GitHub at https://github.com/H-Huang774/ADC-GS.git. <div>
arXiv:2505.08196v1 Announce Type: new 
Abstract: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Watermarking in the Era of Diffusion Models: Advances and Challenges</title>
<link>https://arxiv.org/abs/2505.08197</link>
<guid>https://arxiv.org/abs/2505.08197</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, stable diffusion, visual watermarks, deepfake detection, diffusion models

Summary: 
This paper discusses the implications of advanced generative artificial intelligence technologies like Stable Diffusion on copyright infringement and the need for robust protection mechanisms such as visual watermarks. Traditional deepfake detection methods are ineffective against sophisticated manipulations, but diffusion models offer enhanced detection accuracy by learning features and embedding imperceptible watermarks. The integration of diffusion models and watermarking security is crucial in preserving ownership rights in the face of evolving forgery threats. The analysis focuses on the strengths and challenges of watermark techniques in the context of diffusion models, highlighting the importance of innovative solutions to protect digital content. The research aims to advance the discourse on safeguarding visual content and ensuring the integrity of ownership in the age of generative AI. 

<br /><br />Summary: <div>
arXiv:2505.08197v1 Announce Type: new 
Abstract: As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix</title>
<link>https://arxiv.org/abs/2505.08228</link>
<guid>https://arxiv.org/abs/2505.08228</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, adverse weather conditions, data augmentation, autonomous driving, robustness

Summary:<br /><br />Enhancing the robustness of object detection systems in adverse weather conditions is crucial for autonomous driving technology. The study introduces a novel approach using the diffusion model Instruct Pix2Pix to develop prompting methodologies for generating realistic datasets with weather-based augmentations. This aims to mitigate the impact of challenging weather on state-of-the-art object detection models like Faster R-CNN and YOLOv10. Experiments conducted in the CARLA simulator and real-world datasets BDD100K and ACDC demonstrate the effectiveness of the approach. The key contributions include quantifying the performance gap in object detection models in difficult weather conditions and showcasing how tailored data augmentation strategies can enhance model robustness. This research lays a solid foundation for enhancing perception systems in demanding environmental scenarios, offering pathways for future advancements in autonomous driving technology. 

Summary: <div>
arXiv:2505.08228v1 Announce Type: new 
Abstract: Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.
  The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective</title>
<link>https://arxiv.org/abs/2505.08231</link>
<guid>https://arxiv.org/abs/2505.08231</guid>
<content:encoded><![CDATA[
<div> Object detection, maritime navigation, dataset, HMPNet, shipborne object detection <br />
Summary: 
- The article introduces Navigation12, a dataset for object detection in maritime environments, addressing the lack of maritime-specific data.
- HMPNet, a lightweight architecture, is proposed for shipborne object detection, with improved accuracy and computational efficiency compared to existing methods.
- HMPNet incorporates a hierarchical dynamic modulation backbone for feature aggregation, a matrix cascading poly-scale neck, and a polymerization weight sharing detector for efficient multi-scale feature aggregation.
- Empirical evaluations show HMPNet outperforms the current state-of-the-art model YOLOv11n, with a 3.3% improvement in mean Average Precision and 23% reduction in parameters.
- The proposed dataset and architecture aim to enhance visual perception techniques in autonomous maritime navigation systems. 
<br /><br />Summary: <div>
arXiv:2505.08231v1 Announce Type: new 
Abstract: In the realm of intelligent maritime navigation, object detection from a shipborne perspective is paramount. Despite the criticality, the paucity of maritime-specific data impedes the deployment of sophisticated visual perception techniques, akin to those utilized in autonomous vehicular systems, within the maritime context. To bridge this gap, we introduce Navigation12, a novel dataset annotated for 12 object categories under diverse maritime environments and weather conditions. Based upon this dataset, we propose HMPNet, a lightweight architecture tailored for shipborne object detection. HMPNet incorporates a hierarchical dynamic modulation backbone to bolster feature aggregation and expression, complemented by a matrix cascading poly-scale neck and a polymerization weight sharing detector, facilitating efficient multi-scale feature aggregation. Empirical evaluations indicate that HMPNet surpasses current state-of-the-art methods in terms of both accuracy and computational efficiency, realizing a 3.3% improvement in mean Average Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition</title>
<link>https://arxiv.org/abs/2505.08233</link>
<guid>https://arxiv.org/abs/2505.08233</guid>
<content:encoded><![CDATA[
<div> Keyword: contactless fingerprint recognition, G-MSGINet, minutiae localization, identity embedding, robust

Summary:
G-MSGINet is a new framework for contactless fingerprint recognition that integrates minutiae localization and identity embedding directly from raw input images. It introduces the GMSGI layer, which combines pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modeling. This approach eliminates the need for orientation supervision and adapts graph connectivity from learned kernel descriptors. Experimental results on benchmark datasets show that G-MSGINet achieves high minutiae F1-scores and Rank-1 identification accuracies, with an Equal Error Rate as low as 0.5%. It outperforms previous methods in terms of F1-score and accuracy while using fewer parameters and floating-point operations. The results demonstrate the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.

Summary:<br /><br />Keyword: contactless fingerprint recognition, G-MSGINet, minutiae localization, identity embedding, robust <div>
arXiv:2505.08233v1 Announce Type: new 
Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust contactless fingerprint recognition that jointly performs minutiae localization and identity embedding directly from raw input images. Existing approaches rely on multi-branch architectures, orientation labels, or complex preprocessing steps, which limit scalability and generalization across real-world acquisition scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a novel computational module that integrates grouped pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modelling into a single processing unit. Stacked GMSGI layers progressively refine both local minutiae-sensitive features and global topological representations through end-to-end optimization. The architecture eliminates explicit orientation supervision and adapts graph connectivity directly from learned kernel descriptors, thereby capturing meaningful structural relationships among fingerprint regions without fixed heuristics. Extensive experiments on three benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that G-MSGINet consistently achieves minutiae F1-scores in the range of $0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%, while maintaining an Equal Error Rate (EER) as low as 0.5%. These results correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1 accuracy when compared to prior methods, using only 0.38 million parameters and 6.63 giga floating-point operations, which represents up to ten times fewer parameters than competitive baselines. This highlights the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Removing Watermarks with Partial Regeneration using Semantic Information</title>
<link>https://arxiv.org/abs/2505.08234</link>
<guid>https://arxiv.org/abs/2505.08234</guid>
<content:encoded><![CDATA[
<div> Vulnerability, SemanticRegen, Watermarking, Adversarial Attack, Image Manipulation

Summary:
SemanticRegen is a three-stage, label-free attack that targets state-of-the-art semantic and invisible watermarks by erasing them while preserving an image's apparent meaning. The pipeline utilizes a vision-language model to obtain captions, zero-shot segmentation for foreground masks, and inpainting with an LLM-guided diffusion model to maintain salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems, SemanticRegen successfully defeats the semantic TreeRing watermark and reduces bit-accuracy for the remaining schemes while maintaining high perceptual quality. The attack achieves up to 12 percent higher masked SSIM in foreground regions compared to prior diffusion-based attackers. These results underscore the gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, emphasizing the need for watermarking algorithms resilient to content-preserving regenerative attacks.<br /><br />Summary: <div>
arXiv:2505.08234v1 Announce Type: new 
Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2505.08235</link>
<guid>https://arxiv.org/abs/2505.08235</guid>
<content:encoded><![CDATA[
<div> diffusion model, event camera, video frame interpolation, denoising, event-based

Summary:
EventDiff is a novel event-based diffusion model framework for Video Frame Interpolation (VFI). It incorporates a lightweight Spatial-Temporal Cross Attention (STCA) module to fuse dynamic event streams with static frames, allowing for direct interpolation in the latent space through a denoising diffusion process. The method surpasses existing event-based VFI approaches by up to 1.98dB in PSNR on Vimeo90K-Triplet dataset and demonstrates superior performance on SNU-FILM tasks. In comparison to diffusion-based VFI methods, EventDiff achieves up to 5.72dB PSNR improvement on Vimeo90K-Triplet and is 4.24 times faster during inference. Through a two-stage training strategy involving pretraining the Event-Frame Hybrid AutoEncoder (HAE) and then jointly optimizing it with the diffusion model, EventDiff achieves state-of-the-art performance on multiple synthetic and real-world event VFI datasets. <br /><br />Summary: <div>
arXiv:2505.08235v1 Announce Type: new 
Abstract: Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Congenital Heart Disease recognition using Deep Learning/Transformer models</title>
<link>https://arxiv.org/abs/2505.08242</link>
<guid>https://arxiv.org/abs/2505.08242</guid>
<content:encoded><![CDATA[
<div> Keywords: Congenital Heart Disease, deep learning, dual-modality, diagnosis, accuracy 

Summary:
Deep learning models are investigated for their effectiveness in diagnosing Congenital Heart Disease (CHD), a major cause of infant morbidity and mortality. The study focuses on utilizing dual-modality (sound and image) deep learning methods for CHD screening. The research achieves 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset. By leveraging the automatic feature extraction abilities of deep learning, doctors can improve the accuracy of CHD detection, reducing the risk of false negatives commonly seen in current non-invasive screening methods. This innovative approach showcases the potential of deep learning in assisting healthcare professionals in diagnosing CHD more effectively and accurately. <div>
arXiv:2505.08242v1 Announce Type: new 
Abstract: Congenital Heart Disease (CHD) remains a leading cause of infant morbidity and mortality, yet non-invasive screening methods often yield false negatives. Deep learning models, with their ability to automatically extract features, can assist doctors in detecting CHD more effectively. In this work, we investigate the use of dual-modality (sound and image) deep learning methods for CHD diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Memorization of Diffusion Models through p-Laplace Analysis</title>
<link>https://arxiv.org/abs/2505.08246</link>
<guid>https://arxiv.org/abs/2505.08246</guid>
<content:encoded><![CDATA[
<div> score function, gradient, diffusion models, p-Laplace operators, memorization identification

Summary:
This study explores the use of diffusion models in estimating score functions to compute higher-order differentials known as p-Laplace operators. By utilizing the estimated score functions, researchers were able to accurately identify memorized training data. A numerical p-Laplace approximation was proposed based on the learned score functions, proving effective in detecting important features of the probability landscape. The analysis focused on Gaussian mixture models but also extended to image generative models, marking the first instance of memorization identification using the p-Laplace operator in image generation. This research sheds light on the potential of leveraging score functions in diffusion models for advanced data analysis applications. <br /><br />Summary: <div>
arXiv:2505.08246v1 Announce Type: new 
Abstract: Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely p-Laplace operators. We show here these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. We analyze the structured case of Gaussian mixture models, and demonstrate the results carry-over to image generative models, where memorization identification based on the p-Laplace operator is performed for the first time.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets</title>
<link>https://arxiv.org/abs/2505.08259</link>
<guid>https://arxiv.org/abs/2505.08259</guid>
<content:encoded><![CDATA[
<div> Convolutional, transformer, image classification, ResNet-18, Vision Transformer <br />
Summary: <br />
This study compares convolutional and transformer-based architectures for image classification, focusing on reducing inference latency and model complexity while maintaining accuracy. Using ResNet-18 as a baseline, four Vision Transformer variants (Tiny, Small, Base, Large) were fine-tuned on DermatologyMNIST and TinyImageNet datasets. Through hyperparameter tuning, the study shows that optimally fine-tuned Vision Transformers can match or exceed baseline performance, achieve faster inference, and operate with fewer parameters. This highlights the potential of Vision Transformers for applications in resource-constrained environments. <div>
arXiv:2505.08259v1 Announce Type: new 
Abstract: This study evaluates the trade-offs between convolutional and transformer-based architectures on both medical and general-purpose image classification benchmarks. We use ResNet-18 as our baseline and introduce a fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small, Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce inference latency and model complexity with acceptable accuracy degradation. Through systematic hyperparameter variations, we demonstrate that appropriately fine-tuned Vision Transformers can match or exceed the baseline's performance, achieve faster inference, and operate with fewer parameters, highlighting their viability for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Novel Category Discovery</title>
<link>https://arxiv.org/abs/2505.08260</link>
<guid>https://arxiv.org/abs/2505.08260</guid>
<content:encoded><![CDATA[
<div> discovery, transductive learning, few-shot learning, clustering, model adaptability
Summary: 
This paper introduces the Few-Shot Novel Category Discovery (FSNCD) setting, where a trained agent can switch between identifying known classes and clustering novel classes with only a few support examples. Two methods, Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC), are proposed to enhance the model's reasoning capabilities. The framework extends prior-based clustering algorithms to real-world open set scenarios, improving model adaptability in few-shot learning. Extensive experiments on five datasets show that the methods achieve high performance across various task settings and scenarios. <div>
arXiv:2505.08260v1 Announce Type: new 
Abstract: The recently proposed Novel Category Discovery (NCD) adapt paradigm of transductive learning hinders its application in more real-world scenarios. In fact, few labeled data in part of new categories can well alleviate this burden, which coincides with the ease that people can label few of new category data. Therefore, this paper presents a new setting in which a trained agent is able to flexibly switch between the tasks of identifying examples of known (labelled) classes and clustering novel (completely unlabeled) classes as the number of query examples increases by leveraging knowledge learned from only a few (handful) support examples. Drawing inspiration from the discovery of novel categories using prior-based clustering algorithms, we introduce a novel framework that further relaxes its assumptions to the real-world open set level by unifying the concept of model adaptability in few-shot learning. We refer to this setting as Few-Shot Novel Category Discovery (FSNCD) and propose Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC) to examine the model's reasoning capabilities. Extensive experiments and detailed analysis on five commonly used datasets demonstrate that our methods can achieve leading performance levels across different task settings and scenarios.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
<link>https://arxiv.org/abs/2505.08266</link>
<guid>https://arxiv.org/abs/2505.08266</guid>
<content:encoded><![CDATA[
<div> MPNN, SFs, link prediction, Graph Vision Network, visual perception <br />
<br />
Summary: 
Message-passing graph neural networks (MPNNs) and structural features (SFs) are crucial for link prediction. However, the potential of visual perception has been overlooked in MPNNs. The Graph Vision Network (GVN) framework integrates vision structural awareness into MPNNs for the first time. An efficient variant, E-GVN, is also proposed. Empirical results show that GVN consistently benefits from visual enhancement across various link prediction datasets, including large-scale graphs. GVN outperforms existing state-of-the-art methods, indicating a promising direction for link prediction. <div>
arXiv:2505.08266v1 Announce Type: new 
Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping</title>
<link>https://arxiv.org/abs/2505.08273</link>
<guid>https://arxiv.org/abs/2505.08273</guid>
<content:encoded><![CDATA[
<div> satellite imagery, irrigation mapping, dataset, deep learning, agriculture <br />
Summary: 
IrrMap is a large-scale dataset comprising 1.1 million patches for irrigation method mapping across regions. The dataset includes multi-resolution satellite imagery from LandSat and Sentinel, along with auxiliary data such as crop type and vegetation indices. With standardized GeoTIFF patches and multiple input modalities, IrrMap is ML-ready for seamless deep learning model training. The dataset covers 1,687,899 farms and 14,117,330 acres in multiple western U.S. states from 2013 to 2023, enabling comprehensive irrigation analysis. The dataset's complete pipeline allows for easy extension to new regions for irrigation data collection. Analysis of irrigation method distribution, spatial patterns, and irrigated area variations provides insights into regional and resolution-based differences. IrrMap is openly released with benchmark models and pipeline code on GitHub and a data repository for further exploration in agricultural and geospatial analysis. <br /> <div>
arXiv:2505.08273v1 Announce Type: new 
Abstract: We introduce IrrMap, the first large-scale dataset (1.1 million patches) for irrigation method mapping across regions. IrrMap consists of multi-resolution satellite imagery from LandSat and Sentinel, along with key auxiliary data such as crop type, land use, and vegetation indices. The dataset spans 1,687,899 farms and 14,117,330 acres across multiple western U.S. states from 2013 to 2023, providing a rich and diverse foundation for irrigation analysis and ensuring geospatial alignment and quality control. The dataset is ML-ready, with standardized 224x224 GeoTIFF patches, the multiple input modalities, carefully chosen train-test-split data, and accompanying dataloaders for seamless deep learning model training andbenchmarking in irrigation mapping. The dataset is also accompanied by a complete pipeline for dataset generation, enabling researchers to extend IrrMap to new regions for irrigation data collection or adapt it with minimal effort for other similar applications in agricultural and geospatial analysis. We also analyze the irrigation method distribution across crop groups, spatial irrigation patterns (using Shannon diversity indices), and irrigated area variations for both LandSat and Sentinel, providing insights into regional and resolution-based differences. To promote further exploration, we openly release IrrMap, along with the derived datasets, benchmark models, and pipeline code, through a GitHub repository: https://github.com/Nibir088/IrrMap and Data repository: https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and implementation details.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion</title>
<link>https://arxiv.org/abs/2505.08281</link>
<guid>https://arxiv.org/abs/2505.08281</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal image compression, residual-guided, semantic retrieval, diffusion-based generation, compression-aware diffusion model

Summary:
ResULIC is a new residual-guided ultra-low-rate image compression framework that integrates semantic retrieval, residual signals, and a compression-aware diffusion model for improved reconstruction fidelity and coding efficiency. It introduces Semantic Residual Coding (SRC) to capture semantic disparities and uses a perceptual fidelity optimizer for high-quality reconstruction. The Compression-aware Diffusion Model (CDM) aligns bitrates with diffusion time steps to enhance compression-reconstruction synergy. Experimental results show ResULIC outperforms state-of-the-art methods, achieving significant improvements in LPIPS and FID metrics. This approach offers a comprehensive solution for high-quality image compression by incorporating residual signals, semantic retrieval, and a compression-aware diffusion model. <br /><br />Summary: <div>
arXiv:2505.08281v1 Announce Type: new 
Abstract: Existing multimodal large model-based image compression frameworks often rely on a fragmented integration of semantic retrieval, latent compression, and generative models, resulting in suboptimal performance in both reconstruction fidelity and coding efficiency. To address these challenges, we propose a residual-guided ultra lowrate image compression named ResULIC, which incorporates residual signals into both semantic retrieval and the diffusion-based generation process. Specifically, we introduce Semantic Residual Coding (SRC) to capture the semantic disparity between the original image and its compressed latent representation. A perceptual fidelity optimizer is further applied for superior reconstruction quality. Additionally, we present the Compression-aware Diffusion Model (CDM), which establishes an optimal alignment between bitrates and diffusion time steps, improving compression-reconstruction synergy. Extensive experiments demonstrate the effectiveness of ResULIC, achieving superior objective and subjective performance compared to state-of-the-art diffusion-based methods with - 80.7%, -66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at https: //njuvision.github.io/ResULIC/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks</title>
<link>https://arxiv.org/abs/2505.08284</link>
<guid>https://arxiv.org/abs/2505.08284</guid>
<content:encoded><![CDATA[
<div> quantitative analysis, Ukiyo-e, creativity, machine learning, cultural evolution
Summary: 
- The research focuses on using machine learning to conduct a quantitative analysis of creativity in Ukiyo-e, a traditional Japanese art form, using a large database of high-resolution images.
- The study reveals that the overall creativity of Ukiyo-e has decreased with cultural maturation, but the style has become more segmented and maintained a high level of creativity.
- The analysis provides new insights into the study of Ukiyo-e and demonstrates its role in the ongoing cultural history of Eastern art.
<br /><br />Summary: <div>
arXiv:2505.08284v1 Announce Type: new 
Abstract: Artwork research has long relied on human sensibility and subjective judgment, but recent developments in machine learning have enabled the quantitative assessment of features that humans could not discover. In Western paintings, comprehensive analyses have been conducted from various perspectives in conjunction with large databases, but such extensive analysis has not been sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a traditional Japanese art form, as a case study of Eastern paintings, and conduct a quantitative analysis of creativity in works of art using 11,000 high-resolution images. This involves using the concept of calculating creativity from networks to analyze both the creativity of the artwork and that of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that the creativity of its appearance has declined with the maturation of culture, but in terms of style, it has become more segmented with the maturation of culture and has maintained a high level of creativity. This not only provides new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved within the ongoing cultural history, playing a culturally significant role in the analysis of Eastern art.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units</title>
<link>https://arxiv.org/abs/2505.08294</link>
<guid>https://arxiv.org/abs/2505.08294</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, deepfakes, facial action units, multimodal manipulations, cross-modal fusion<br />
Summary: 
The article introduces FauForensics, a novel framework for detecting realistic audio-visual deepfakes by using biologically invariant facial action units (FAUs) to capture subtle dynamics disrupted in synthetic content. This approach reduces domain dependency and addresses multi-modal forgery issues. FauForensics computes frame-wise audiovisual similarities through a fusion module with learnable cross-modal queries, effectively aligning temporal-spatial lip-audio relationships. The experiments on FakeAVCeleb and LAV-DF datasets demonstrate state-of-the-art performance and superior cross-dataset generalizability, outperforming existing methods by up to an average of 4.83%. This innovative framework provides robust detection capabilities for multimodal manipulations in the evolving landscape of generative AI and deepfake threats.<br /><br />Summary: <div>
arXiv:2505.08294v1 Announce Type: new 
Abstract: The rapid evolution of generative AI has increased the threat of realistic audio-visual deepfakes, demanding robust detection methods. Existing solutions primarily address unimodal (audio or visual) forgeries but struggle with multimodal manipulations due to inadequate handling of heterogeneous modality features and poor generalization across datasets. To this end, we propose a novel framework called FauForensics by introducing biologically invariant facial action units (FAUs), which is a quantitative descriptor of facial muscle activity linked to emotion physiology. It serves as forgery-resistant representations that reduce domain dependency while capturing subtle dynamics often disrupted in synthetic content. Besides, instead of comparing entire video clips as in prior works, our method computes fine-grained frame-wise audiovisual similarities via a dedicated fusion module augmented with learnable cross-modal queries. It dynamically aligns temporal-spatial lip-audio relationships while mitigating multi-modal feature heterogeneity issues. Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance and superior cross-dataset generalizability with up to an average of 4.83\% than existing methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing</title>
<link>https://arxiv.org/abs/2505.08302</link>
<guid>https://arxiv.org/abs/2505.08302</guid>
<content:encoded><![CDATA[
<div> Swin-Transformer, irrigation mapping, agricultural landscapes, satellite imagery, sustainable practices 
Summary:
- The article introduces Knowledge-Informed Irrigation Mapping (KIIM), a novel approach using Swin-Transformer for accurate irrigation mapping in complex agricultural landscapes.
- KIIM incorporates a specialized projection matrix for crop to irrigation probability encoding, a spatial attention map for land identification, bi-directional cross-attention for information integration, and a weighted ensemble for prediction combination.
- Experimentation in five US states demonstrates significant improvement over baseline methods, particularly in identifying drip irrigation.
- A two-phase transfer learning approach enhances cross-state mapping, resulting in a substantial boost in performance in states with limited labeled data.
- KIIM shows efficiency by achieving baseline performance with only 40% of the training data, reducing the need for extensive manual labeling efforts and promoting cost-effective large-scale automated irrigation mapping. 

<br /><br />Summary: <div>
arXiv:2505.08302v1 Announce Type: new 
Abstract: Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An incremental algorithm for non-convex AI-enhanced medical image processing</title>
<link>https://arxiv.org/abs/2505.08324</link>
<guid>https://arxiv.org/abs/2505.08324</guid>
<content:encoded><![CDATA[
<div> deep learning, non-convex optimization, medical imaging, image reconstruction, incremental model-based optimization

Summary:
The article introduces incDG, a hybrid framework that combines deep learning with incremental model-based optimization to efficiently solve non-convex regularized inverse problems in medical imaging. By leveraging the Deep Guess strategy, incDG uses a deep neural network to generate initializations for a non-convex variational solver, improving reconstruction through incremental iterations. The integration of AI tools and model-based optimization enhances robustness and stability in solving imaging inverse problems. Validation on TpV-regularized optimization tasks, including medical image deblurring and tomographic reconstruction, shows superior accuracy and stability compared to conventional solvers and deep learning-based methods. Furthermore, training incDG without ground truth minimally affects performance, making it a practical and powerful tool for addressing non-convex inverse problems in various imaging applications and beyond. 

<br /><br />Summary: <div>
arXiv:2505.08324v1 Announce Type: new 
Abstract: Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computer vision-based model for occupancy detection using low-resolution thermal images</title>
<link>https://arxiv.org/abs/2505.08336</link>
<guid>https://arxiv.org/abs/2505.08336</guid>
<content:encoded><![CDATA[
<div> occupancy detection, thermal images, computer vision techniques, YOLOv5 model, privacy concerns <br />
Summary: <br />
Occupancy detection is crucial for efficient HVAC system operation, with Advanced occupant-centric control (OCC) being a more personalized approach. While traditional methods use RGB images for occupancy detection, privacy concerns arise due to captured facial and body features. This study presents a model using low-resolution thermal images and computer vision techniques, reducing privacy risks and computational resources. The model, incorporating transfer learning with YOLOv5, exhibited high performance with precision and recall values nearing perfection. By utilizing thermal images, this novel approach not only ensures privacy protection but also delivers accurate occupancy information for HVAC system optimization. <div>
arXiv:2505.08336v1 Announce Type: new 
Abstract: Occupancy plays an essential role in influencing the energy consumption and operation of heating, ventilation, and air conditioning (HVAC) systems. Traditional HVAC typically operate on fixed schedules without considering occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in regulating HVAC operations. RGB images combined with computer vision (CV) techniques are widely used for occupancy detection, however, the detailed facial and body features they capture raise significant privacy concerns. Low-resolution thermal images offer a non-invasive solution that mitigates privacy issues. The study developed an occupancy detection model utilizing low-resolution thermal images and CV techniques, where transfer learning was applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The developed model ultimately achieved satisfactory performance, with precision, recall, mAP50, and mAP50 values approaching 1.000. The contributions of this model lie not only in mitigating privacy concerns but also in reducing computing resource demands.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning</title>
<link>https://arxiv.org/abs/2505.08349</link>
<guid>https://arxiv.org/abs/2505.08349</guid>
<content:encoded><![CDATA[
<div> Frequency Adaptation, Diversion, Cross-domain few-shot learning, spectral representations, generalization <br />
Summary: <br />
The paper introduces Frequency Adaptation and Diversion (FAD) for Cross-domain few-shot learning (CD-FSL), which tackles the issue of spectral variations in spatially similar images across domains. FAD utilizes a Frequency Diversion Adapter to transform features into the frequency domain, partition them into low, mid, and high-frequency bands, and adapt each band separately to capture different levels of semantic information. Experiment results on the Meta-Dataset benchmark show that FAD outperforms existing methods in both seen and unseen domains, highlighting the effectiveness of frequency-aware frameworks and band-wise adaptation for enhancing generalization in CD-FSL. <div>
arXiv:2505.08349v1 Announce Type: new 
Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from limited labeled samples under significant distribution shifts. While recent methods enhance adaptability through lightweight task-specific modules, they operate solely in the spatial domain and overlook frequency-specific variations that are often critical for robust transfer. We observe that spatially similar images across domains can differ substantially in their spectral representations, with low and high frequencies capturing complementary semantic information at coarse and fine levels. This indicates that uniform spatial adaptation may overlook these spectral distinctions, thus constraining generalization. To address this, we introduce Frequency Adaptation and Diversion (FAD), a frequency-aware framework that explicitly models and modulates spectral components. At its core is the Frequency Diversion Adapter, which transforms intermediate features into the frequency domain using the discrete Fourier transform (DFT), partitions them into low, mid, and high-frequency bands via radial masks, and reconstructs each band using inverse DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional branch with a kernel size tailored to its spectral scale, enabling targeted and disentangled adaptation across frequencies. Extensive experiments on the Meta-Dataset benchmark demonstrate that FAD consistently outperforms state-of-the-art methods on both seen and unseen domains, validating the utility of frequency-domain representations and band-wise adaptation for improving generalization in CD-FSL.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives</title>
<link>https://arxiv.org/abs/2505.08350</link>
<guid>https://arxiv.org/abs/2505.08350</guid>
<content:encoded><![CDATA[
<div> Keywords: StoryAnchors, story frame generation, temporal consistency, scene diversity, narrative richness

Summary:
StoryAnchors introduces a framework for generating high-quality, multi-scene story frames with strong temporal consistency. The bidirectional story generator integrates past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions. Specific conditions are introduced to enhance scene diversity and narrative richness. The framework incorporates Multi-Event Story Frame Labeling and Progressive Story Frame Training to capture overarching narrative flow and event-level dynamics, supporting editable and expandable story frames. Extensive experiments demonstrate that StoryAnchors surpasses existing models in consistency, coherence, and scene diversity. Its performance in narrative consistency and story richness rivals that of GPT-4o. StoryAnchors provides a scalable, flexible, and highly editable foundation for advancing story-driven frame generation. 

<br /><br />Summary: <div>
arXiv:2505.08350v1 Announce Type: new 
Abstract: This paper introduces StoryAnchors, a unified framework for generating high-quality, multi-scene story frames with strong temporal consistency. The framework employs a bidirectional story generator that integrates both past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions throughout the narrative. Specific conditions are introduced to distinguish story frame generation from standard video synthesis, facilitating greater scene diversity and enhancing narrative richness. To further improve generation quality, StoryAnchors integrates Multi-Event Story Frame Labeling and Progressive Story Frame Training, enabling the model to capture both overarching narrative flow and event-level dynamics. This approach supports the creation of editable and expandable story frames, allowing for manual modifications and the generation of longer, more complex sequences. Extensive experiments show that StoryAnchors outperforms existing open-source models in key areas such as consistency, narrative coherence, and scene diversity. Its performance in narrative consistency and story richness is also on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of story-driven frame generation, offering a scalable, flexible, and highly editable foundation for future research.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DArFace: Deformation Aware Robustness for Low Quality Face Recognition</title>
<link>https://arxiv.org/abs/2505.08423</link>
<guid>https://arxiv.org/abs/2505.08423</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial recognition, deep neural networks, low-quality images, deformation modeling, robustness

Summary: 
DArFace is a new framework for robust face recognition that addresses the challenges posed by low-quality facial images in real-world scenarios. It incorporates global transformations and local elastic deformations during training to simulate realistic degraded conditions without the need for paired high- and low-quality samples. The method also includes a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on benchmarks such as TinyFace, IJB-B, and IJB-C showcase DArFace's superior performance compared to existing methods, with significant improvements attributed to its incorporation of local deformation modeling. The framework leverages deep neural networks, advanced loss functions, and large-scale datasets to overcome the domain gap between high-quality training data and low-quality real-world images. <div>
arXiv:2505.08423v1 Announce Type: new 
Abstract: Facial recognition systems have achieved remarkable success by leveraging deep neural networks, advanced loss functions, and large-scale datasets. However, their performance often deteriorates in real-world scenarios involving low-quality facial images. Such degradations, common in surveillance footage or standoff imaging include low resolution, motion blur, and various distortions, resulting in a substantial domain gap from the high-quality data typically used during training. While existing approaches attempt to address robustness by modifying network architectures or modeling global spatial transformations, they frequently overlook local, non-rigid deformations that are inherently present in real-world settings. In this work, we introduce DArFace, a Deformation-Aware robust Face recognition framework that enhances robustness to such degradations without requiring paired high- and low-quality training samples. Our method adversarially integrates both global transformations (e.g., rotation, translation) and local elastic deformations during training to simulate realistic low-quality conditions. Moreover, we introduce a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with significant gains attributed to the inclusion of local deformation modeling.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation</title>
<link>https://arxiv.org/abs/2505.08426</link>
<guid>https://arxiv.org/abs/2505.08426</guid>
<content:encoded><![CDATA[
<div> Keywords: gaze estimation, deep learning, super-resolution, cross-attention, dataset correction

Summary: 
- The paper introduces DHECA-SuperGaze, a deep learning-based method for improved gaze estimation in unconstrained environments.
- The method incorporates super-resolution and a dual head-eye cross-attention module to enhance gaze prediction accuracy.
- Errors in the Gaze360 dataset annotations were identified and corrected, leading to improved performance evaluation results.
- DHECA-SuperGaze outperforms state-of-the-art methods in both within-dataset and cross-dataset testing scenarios.
- The proposed approach demonstrates superior angular error reduction in static and temporal gaze estimation configurations, showcasing its robust generalization capabilities.

<br /><br />Summary: <div>
arXiv:2505.08426v1 Announce Type: new 
Abstract: Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48{\deg} (Gaze360) and 2.95{\deg} (GFIE) in static configurations, and 0.59{\deg} (Gaze360) and 3.00{\deg} (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53{\deg} (Gaze360) and 3.99{\deg} (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Image Reconstruction from Brain Activity via Latent Representation</title>
<link>https://arxiv.org/abs/2505.08429</link>
<guid>https://arxiv.org/abs/2505.08429</guid>
<content:encoded><![CDATA[
<div> Keywords: visual image reconstruction, deep neural networks, generative models, compositional strategies, ethical considerations

Summary: 
Visual image reconstruction has progressed with the integration of deep neural networks and generative models. The evolution of the field from early classification approaches to detailed reconstructions emphasizes hierarchical latent representations, compositional strategies, and modular architectures. Challenges include achieving zero-shot generalization for unseen images and modeling subjective aspects of perception accurately. Diverse datasets, refined evaluation metrics aligned with human judgments, and compositional representations are needed for model robustness and generalizability. Ethical considerations such as privacy, consent, and potential misuse must be addressed responsibly. Visual image reconstruction provides insights into neural coding, enables new psychological measurements of visual experiences, and has applications in clinical diagnostics and brain-machine interfaces. 

<br /><br />Summary: <div>
arXiv:2505.08429v1 Announce Type: new 
Abstract: Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection</title>
<link>https://arxiv.org/abs/2505.08437</link>
<guid>https://arxiv.org/abs/2505.08437</guid>
<content:encoded><![CDATA[
<div> Dataset, Forgery Detection, Deepfake, Human Body, TikTok-DeepFake<br />
<br />
Summary:<br />
The article introduces the TikTok-DeepFake (TT-DF) dataset, which focuses on body forgery detection, addressing the lack of datasets and detection methods in this area. TT-DF consists of 6,120 forged videos and 1,378,857 synthetic frames, offering a wide range of forgery methods using advanced human image animation models. The dataset aims to simulate various forged data scenarios comprehensively. Additionally, a new body forgery detection model, Temporal Optical Flow Network (TOF-Net), is proposed, leveraging spatiotemporal inconsistencies and optical flow distribution differences between natural and forged data. Experimental results show that TOF-Net outperforms existing facial forgery detection models on the TT-DF dataset. The work provides a significant contribution to advancing research in human body forgery detection and offers the TT-DF dataset for further exploration in this domain. <br /> <div>
arXiv:2505.08437v1 Announce Type: new 
Abstract: The emergence and popularity of facial deepfake methods spur the vigorous development of deepfake datasets and facial forgery detection, which to some extent alleviates the security concerns about facial-related artificial intelligence technologies. However, when it comes to human body forgery, there has been a persistent lack of datasets and detection methods, due to the later inception and complexity of human body generation methods. To mitigate this issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic frames, specifically tailored for body forgery detection. TT-DF offers a wide variety of forgery methods, involving multiple advanced human image animation models utilized for manipulation, two generative configurations based on the disentanglement of identity and pose information, as well as different compressed versions. The aim is to simulate any potential unseen forged data in the wild as comprehensively as possible, and we also furnish a benchmark on TT-DF. Additionally, we propose an adapted body forgery detection model, Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal inconsistencies and optical flow distribution differences between natural data and forged data. Our experiments demonstrate that TOF-Net achieves favorable performance on TT-DF, outperforming current state-of-the-art extendable facial forgery detection models. For our TT-DF dataset, please refer to https://github.com/HashTAG00002/TT-DF.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering</title>
<link>https://arxiv.org/abs/2505.08438</link>
<guid>https://arxiv.org/abs/2505.08438</guid>
<content:encoded><![CDATA[
<div> Event cameras, 3D reconstruction, stereo, monocular, multimodal systems<br />
Summary:<br />
Event cameras are a promising sensor for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. This survey categorizes existing works into three types based on input modality and further classifies them by reconstruction approach. Methods are organized chronologically and summarized public datasets relevant to event-based 3D reconstruction. Current research limitations in data availability, evaluation, representation, and dynamic scene handling are highlighted. Future research directions in event-driven 3D reconstruction are outlined. <br /> <div>
arXiv:2505.08438v1 Announce Type: new 
Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. Unlike conventional frame-based cameras, they produce sparse and temporally rich data streams, which enable more accurate 3D reconstruction and open up the possibility of performing reconstruction in extreme environments such as high-speed motion, low light, or high dynamic range scenes. In this survey, we provide the first comprehensive review focused exclusively on 3D reconstruction using event cameras. The survey categorises existing works into three major types based on input modality - stereo, monocular, and multimodal systems, and further classifies them by reconstruction approach, including geometry-based, deep learning-based, and recent neural rendering techniques such as Neural Radiance Fields and 3D Gaussian Splatting. Methods with a similar research focus were organised chronologically into the most subdivided groups. We also summarise public datasets relevant to event-based 3D reconstruction. Finally, we highlight current research limitations in data availability, evaluation, representation, and dynamic scene handling, and outline promising future research directions. This survey aims to serve as a comprehensive reference and a roadmap for future developments in event-driven 3D reconstruction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models</title>
<link>https://arxiv.org/abs/2505.08455</link>
<guid>https://arxiv.org/abs/2505.08455</guid>
<content:encoded><![CDATA[
<div> benchmark, Video-based long-form Causal Reasoning, LVLM, procedural videos, Recognition-Reasoning Decomposition<br />
<br />
Summary: Large Video Language Models (LVLMs) lack the ability to perform video-based causal reasoning due to the absence of benchmarks. To address this, a new benchmark called Video-based long-form Causal Reasoning (VCRBench) is introduced. VCRBench tests LVLMs on their capability to identify, reason about, and sequence events in videos to achieve a specific goal. State-of-the-art LVLMs struggle with long-form causal reasoning in videos due to difficulty in modeling long-range causal dependencies from visual observations. To address this issue, a modular approach called Recognition-Reasoning Decomposition (RRD) is proposed, breaking video-based causal reasoning into video recognition and causal reasoning sub-tasks. Experiments show that RRD significantly improves accuracy on VCRBench, with gains up to 25.2%. Analysis reveals that LVLMs heavily rely on language knowledge for complex video-based causal reasoning tasks. <br /><br /> <div>
arXiv:2505.08455v1 Announce Type: new 
Abstract: Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images</title>
<link>https://arxiv.org/abs/2505.08517</link>
<guid>https://arxiv.org/abs/2505.08517</guid>
<content:encoded><![CDATA[
<div> deep learning, inhalation injuries, bronchoscopy images, enhanced StarGAN, classification accuracy

Summary:
This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images. The traditional methods for diagnosing and grading inhalation injuries have limitations, leading to subjective assessments and weak correlations with clinical outcomes. To overcome the scarcity of medical imaging data, enhanced StarGAN, a generative model, was used to improve the quality and clinical relevance of synthetic images. The augmented dataset generated by enhanced StarGAN significantly improved classification performance, achieving an accuracy of 77.78%. The image quality was assessed using the Fr\'echet Inception Distance, with enhanced StarGAN achieving the lowest FID of 30.06. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly in preserving bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.<br /><br />Summary: <div>
arXiv:2505.08517v1 Announce Type: new 
Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to the limitations of traditional methods, such as Abbreviated Injury Score (AIS), which rely on subjective assessments and show weak correlations with clinical outcomes. This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images with the duration of mechanical ventilation as an objective metric. To address the scarcity of medical imaging data, we propose enhanced StarGAN, a generative model that integrates Patch Loss and SSIM Loss to improve synthetic images' quality and clinical relevance. The augmented dataset generated by enhanced StarGAN significantly improved classification performance when evaluated using the Swin Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the original dataset. Image quality was assessed using the Fr\'echet Inception Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06, outperforming baseline models. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly the preservation of bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis</title>
<link>https://arxiv.org/abs/2505.08524</link>
<guid>https://arxiv.org/abs/2505.08524</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole slide image classification, domain shift, continual learning, generative latent replay, attention mechanism

Summary: 
The article introduces a novel framework, Attention-based Generative Latent Replay Continual Learning (AGLR-CL), for Whole Slide Image (WSI) classification in the presence of domain shifts. By utilizing Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, the method preserves past domain knowledge without storing original data. An attention-based filtering step focuses on salient patch embeddings, generating high-quality synthetic samples. This privacy-aware approach eliminates the need for replay buffers and outperforms buffer-free methods while matching buffer-based solutions' performance. The AGLR-CL framework is validated on tasks such as biomarker detection and molecular status prediction across diverse datasets, demonstrating its ability to retain previous knowledge and adapt to new domains effectively. The framework provides a privacy-preserving solution for domain incremental continual learning in WSI classification. 

Summary: <div>
arXiv:2505.08524v1 Announce Type: new 
Abstract: Whole slide image (WSI) classification has emerged as a powerful tool in computational pathology, but remains constrained by domain shifts, e.g., due to different organs, diseases, or institution-specific variations. To address this challenge, we propose an Attention-based Generative Latent Replay Continual Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for domain incremental WSI classification. Our method employs Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, preserving knowledge of past domains without explicitly storing original data. A novel attention-based filtering step focuses on the most salient patch embeddings, ensuring high-quality synthetic samples. This privacy-aware strategy obviates the need for replay buffers and outperforms other buffer-free counterparts while matching the performance of buffer-based solutions. We validate AGLR-CL on clinically relevant biomarker detection and molecular status prediction across multiple public datasets with diverse centers, organs, and patient cohorts. Experimental results confirm its ability to retain prior knowledge and adapt to new domains, offering an effective, privacy-preserving avenue for domain incremental continual learning in WSI classification.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation</title>
<link>https://arxiv.org/abs/2505.08525</link>
<guid>https://arxiv.org/abs/2505.08525</guid>
<content:encoded><![CDATA[
<div> Keywords: tubular structures, semantic segmentation, super-resolution, dynamic snake upsampling, boundary-skeleton weighted loss

Summary:
This paper introduces a novel approach for accurate segmentation of tubular topological structures, addressing the challenges posed by the slenderness and curvature of such structures. The proposed method combines dynamic snake upsampling operators with a boundary-skeleton weighted loss to improve subpixel-level feature recovery and topological continuity. The snake upsampling operators adaptively adjust the sampling stride and select subpixel sampling points along the serpentine path, enabling more precise feature recovery for tubular structures. The boundary-skeleton weighted loss prioritizes boundary alignment precision and topological consistency by adjusting weight allocation based on the mask class ratio and distance field. Experimental results demonstrate that the proposed method significantly enhances both pixel-wise segmentation accuracy and topological consistency across various datasets and backbone networks. Overall, this dynamic snake upsampling operator and boundary-skeleton weighted loss offer a promising solution for accurate segmentation of tubular structures in various applications. 

<br /><br />Summary: <div>
arXiv:2505.08525v1 Announce Type: new 
Abstract: Accurate segmentation of tubular topological structures (e.g., fissures and vasculature) is critical in various fields to guarantee dependable downstream quantitative analysis and modeling. However, in dense prediction tasks such as semantic segmentation and super-resolution, conventional upsampling operators cannot accommodate the slenderness of tubular structures and the curvature of morphology. This paper introduces a dynamic snake upsampling operators and a boundary-skeleton weighted loss tailored for topological tubular structures. Specifically, we design a snake upsampling operators based on an adaptive sampling domain, which dynamically adjusts the sampling stride according to the feature map and selects a set of subpixel sampling points along the serpentine path, enabling more accurate subpixel-level feature recovery for tubular structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted loss that trades off main body and boundary weight allocation based on mask class ratio and distance field, preserving main body overlap while enhancing focus on target topological continuity and boundary alignment precision. Experiments across various domain datasets and backbone networks show that this plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted loss boost both pixel-wise segmentation accuracy and topological consistency of results.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting</title>
<link>https://arxiv.org/abs/2505.08527</link>
<guid>https://arxiv.org/abs/2505.08527</guid>
<content:encoded><![CDATA[
<div> SFDA, segmentation, Segment Anything Model, Dual Feature Guided, box prompt search<br />
Summary:<br />
The article discusses source-free domain adaptation (SFDA) for segmentation using the Segment Anything Model (SAM). Existing SFDA approaches generate defective bounding box prompts due to the domain gap. To address this, a Dual Feature Guided (DFG) auto-prompting approach is proposed. The approach involves training the source model in a feature aggregation phase to adapt to the target domain and build a suitable feature distribution for box prompt search. The box prompt is gradually expanded based on target model features and SAM features to handle different target feature distributions. Postprocessing the pseudo-labels produced by SAM further refines the results. Experimental results on 3D and 2D datasets show superior performance compared to conventional methods. The code for the approach is available on GitHub. <div>
arXiv:2505.08527v1 Announce Type: new 
Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a model trained in the source domain to perform well in the target domain with only the source model and unlabeled target data.Inspired by the recent success of Segment Anything Model (SAM) which exhibits the generality of segmenting images of various modalities and in different domains given human-annotated prompts like bounding boxes or points, we for the first time explore the potentials of Segment Anything Model for SFDA via automatedly finding an accurate bounding box prompt. We find that the bounding boxes directly generated with existing SFDA approaches are defective due to the domain gap.To tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting approach to search for the box prompt. Specifically, the source model is first trained in a feature aggregation phase, which not only preliminarily adapts the source model to the target domain but also builds a feature distribution well-prepared for box prompt search. In the second phase, based on two feature distribution observations, we gradually expand the box prompt with the guidance of the target model feature and the SAM feature to handle the class-wise clustered target features and the class-wise dispersed target features, respectively. To remove the potentially enlarged false positive regions caused by the over-confident prediction of the target model, the refined pseudo-labels produced by SAM are further postprocessed based on connectivity analysis. Experiments on 3D and 2D datasets indicate that our approach yields superior performance compared to conventional methods. Code is available at https://github.com/zheangh/DFG.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning</title>
<link>https://arxiv.org/abs/2505.08537</link>
<guid>https://arxiv.org/abs/2505.08537</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, food quality assessment, raspberry grading, instance segmentation, industrial environment<br />
<br />
Summary: <br />
This research explores the use of computer vision for assessing food quality rapidly and accurately in an industrial setting, specifically focusing on real-time raspberry grading. A dataset called RaspGrade was created and meticulously annotated for this purpose. The study found that while accurate fruit-level masks can be generated through instance segmentation, classifying certain raspberry grades poses challenges due to color similarities and occlusion, while others are more easily distinguishable based on color. The RaspGrade dataset is now available for access on HuggingFace. This research sheds light on the potential of computer vision for non-invasive and efficient food quality assessment in industrial processes, highlighting the complexities and opportunities in real-time fruit grading. <br /> <div>
arXiv:2505.08537v1 Announce Type: new 
Abstract: This research investigates the application of computer vision for rapid, accurate, and non-invasive food quality assessment, focusing on the novel challenge of real-time raspberry grading into five distinct classes within an industrial environment as the fruits move along a conveyor belt. To address this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and meticulously annotated. Instance segmentation experiments revealed that accurate fruit-level masks can be obtained; however, the classification of certain raspberry grades presents challenges due to color similarities and occlusion, while others are more readily distinguishable based on color. The acquired and annotated RaspGrade dataset is accessible on HuggingFace at: https://huggingface.co/datasets/FBK-TeV/RaspGrade.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</title>
<link>https://arxiv.org/abs/2505.08552</link>
<guid>https://arxiv.org/abs/2505.08552</guid>
<content:encoded><![CDATA[
<div> detect, copyright infringement, generative AI, visual artworks, contrastive learning 

Summary: 
The article discusses the issue of copyright infringement and forgery in AI-generated visual artworks due to the inherent memorization capabilities of generative models. The proposed DFA-CON framework utilizes contrastive learning to detect copyright violations in AI-generated art by creating a discriminative representation space that distinguishes between original artworks and their forged counterparts. The model is trained across various attack types, such as inpainting, style transfer, adversarial perturbation, and cutmix, to improve detection performance. Evaluation results show that DFA-CON outperforms existing pretrained foundation models in detecting copyright infringement. The code and model checkpoints will be released publicly upon acceptance. <div>
arXiv:2505.08552v1 Announce Type: new 
Abstract: Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</title>
<link>https://arxiv.org/abs/2505.08561</link>
<guid>https://arxiv.org/abs/2505.08561</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked video modeling, Trajectory-Aware Adaptive Token Sampler, MAE, Proximal Policy Optimization, action recognition

Summary: 
Masked video modeling (MVM) has been proven effective for pre-training visual foundation models by reconstructing masked spatiotemporal tokens. However, the challenge lies in selecting the appropriate masking strategy. This study introduces the Trajectory-Aware Adaptive Token Sampler (TATS), which models motion dynamics to select motion-centric tokens, integrated into the masked autoencoder framework. A unified training strategy enables joint optimization of MAE and TATS using Proximal Policy Optimization (PPO) from scratch. The model allows aggressive masking without compromising action recognition performance, maintaining memory efficiency in pre-training. Experiments across benchmarks demonstrate the effectiveness, transferability, generalization, and efficiency of the proposed approach compared to state-of-the-art methods. <div>
arXiv:2505.08561v1 Announce Type: new 
Abstract: Masked video modeling~(MVM) has emerged as a highly effective pre-training strategy for visual foundation models, whereby the model reconstructs masked spatiotemporal tokens using information from visible tokens. However, a key challenge in such approaches lies in selecting an appropriate masking strategy. Previous studies have explored predefined masking techniques, including random and tube-based masking, as well as approaches that leverage key motion priors, optical flow and semantic cues from externally pre-trained models. In this work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token Sampler (TATS), which models the motion dynamics of tokens and can be seamlessly integrated into the masked autoencoder (MAE) framework to select motion-centric tokens in videos. Additionally, we propose a unified training strategy that enables joint optimization of both MAE and TATS from scratch using Proximal Policy Optimization (PPO). We show that our model allows for aggressive masking without compromising performance on the downstream task of action recognition while also ensuring that the pre-training remains memory efficient. Extensive experiments of the proposed approach across four benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51, demonstrate the effectiveness, transferability, generalization, and efficiency of our work compared to other state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections</title>
<link>https://arxiv.org/abs/2505.08568</link>
<guid>https://arxiv.org/abs/2505.08568</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, thermal detector, traffic light system, mobility restrictions, pedestrian safety

Summary:
The article introduces a new thermal detector-based traffic light system designed to cater to the needs of individuals with mobility restrictions and visual impairments. Traditional RGB camera-based systems often overlook these users, but the proposed system dynamically adjusts signal durations for those with walking impairments and triggers auditory signals for the visually impaired. To address the challenges of thermal imaging, a new YOLO-Thermal detector is developed, enhancing detection accuracy and robustness in thermal imagery. The system also includes the creation of a thermal dataset for people with mobility restrictions, capturing diverse pedestrian scenarios in various environmental conditions. Experimental results show that the proposed system outperforms existing detectors and effectively enhances barrier-free intersection for all users. The source codes and dataset are available for further research and development. <br /><br />Summary: The new thermal detector-based traffic light system addresses the needs of individuals with mobility restrictions and visual impairments by dynamically adjusting signal durations and triggering auditory signals. Developed with a focus on thermal imaging, the YOLO-Thermal detector improves detection accuracy, and the created thermal dataset captures diverse pedestrian scenarios. Experimental results show superior performance compared to existing detectors, highlighting its effectiveness in enhancing barrier-free intersection for all users. <div>
arXiv:2505.08568v1 Announce Type: new 
Abstract: Rapid advances in deep learning for computer vision have driven the adoption of RGB camera-based adaptive traffic light systems to improve traffic safety and pedestrian comfort. However, these systems often overlook the needs of people with mobility restrictions. Moreover, the use of RGB cameras presents significant challenges, including limited detection performance under adverse weather or low-visibility conditions, as well as heightened privacy concerns. To address these issues, we propose a fully automated, thermal detector-based traffic light system that dynamically adjusts signal durations for individuals with walking impairments or mobility burden and triggers the auditory signal for visually impaired individuals, thereby advancing towards barrier-free intersection for all users. To this end, we build the thermal dataset for people with mobility restrictions (TD4PWMR), designed to capture diverse pedestrian scenarios, particularly focusing on individuals with mobility aids or mobility burden under varying environmental conditions, such as different lighting, weather, and crowded urban settings. While thermal imaging offers advantages in terms of privacy and robustness to adverse conditions, it also introduces inherent hurdles for object detection due to its lack of color and fine texture details and generally lower resolution of thermal images. To overcome these limitations, we develop YOLO-Thermal, a novel variant of the YOLO architecture that integrates advanced feature extraction and attention mechanisms for enhanced detection accuracy and robustness in thermal imaging. Experiments demonstrate that the proposed thermal detector outperforms existing detectors, while the proposed traffic light system effectively enhances barrier-free intersection. The source codes and dataset are available at https://github.com/leon2014dresden/YOLO-THERMAL.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking</title>
<link>https://arxiv.org/abs/2505.08581</link>
<guid>https://arxiv.org/abs/2505.08581</guid>
<content:encoded><![CDATA[
arXiv:2505.08581v1 Announce Type: new 
Abstract: Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior</title>
<link>https://arxiv.org/abs/2505.08585</link>
<guid>https://arxiv.org/abs/2505.08585</guid>
<content:encoded><![CDATA[
arXiv:2505.08585v1 Announce Type: new 
Abstract: Machine learning has taken a critical role in seismic interpretation workflows, especially in fault delineation tasks. However, despite the recent proliferation of pretrained models and synthetic datasets, the field still lacks a systematic understanding of the generalizability limits of these models across seismic data representing a variety of geologic, acquisition and processing settings. Distributional shifts between different data sources, limitations in fine-tuning strategies and labeled data accessibility, and inconsistent evaluation protocols all represent major roadblocks in the deployment of reliable and robust models in real-world exploration settings. In this paper, we present the first large-scale benchmarking study explicitly designed to provide answers and guidelines for domain shift strategies in seismic interpretation. Our benchmark encompasses over $200$ models trained and evaluated on three heterogeneous datasets (synthetic and real data) including FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining, fine-tuning, and joint training strategies under varying degrees of domain shift. Our analysis highlights the fragility of current fine-tuning practices, the emergence of catastrophic forgetting, and the challenges of interpreting performance in a systematic manner. We establish a robust experimental baseline to provide insights into the tradeoffs inherent to current fault delineation workflows, and shed light on directions for developing more generalizable, interpretable and effective machine learning models for seismic interpretation. The insights and analyses reported provide a set of guidelines on the deployment of fault delineation models within seismic interpretation workflows.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrePrompt: Predictive prompting for class incremental learning</title>
<link>https://arxiv.org/abs/2505.08586</link>
<guid>https://arxiv.org/abs/2505.08586</guid>
<content:encoded><![CDATA[
arXiv:2505.08586v1 Announce Type: new 
Abstract: Class Incremental Learning (CIL) based on pre-trained models offers a promising direction for open-world continual learning. Existing methods typically rely on correlation-based strategies, where an image's classification feature is used as a query to retrieve the most related key prompts and select the corresponding value prompts for training. However, these approaches face an inherent limitation: fitting the entire feature space of all tasks with only a few trainable prompts is fundamentally challenging. We propose Predictive Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based limitations by leveraging pre-trained models' natural classification ability to predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a two-stage prediction framework: task-specific prompt prediction followed by label prediction. While theoretically appealing, this framework risks bias toward recent classes due to missing historical data for older classifier calibration. PrePrompt then mitigates this by incorporating feature translation, dynamically balancing stability and plasticity. Experiments across multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art prompt-based CIL methods. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment</title>
<link>https://arxiv.org/abs/2505.08589</link>
<guid>https://arxiv.org/abs/2505.08589</guid>
<content:encoded><![CDATA[
arXiv:2505.08589v1 Announce Type: new 
Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI) dataset comprising 2525 images taken by a drone flying over dense urban environments. MESSI is unique in two main features. First, it contains images from various altitudes, allowing us to investigate the effect of depth on semantic segmentation. Second, it includes images taken from several different urban regions (at different altitudes). This is important since the variety covers the visual richness captured by a drone's 3D flight, performing horizontal and vertical maneuvers. MESSI contains images annotated with location, orientation, and the camera's intrinsic parameters and can be used to train a deep neural network for semantic segmentation or other applications of interest (e.g., localization, navigation, and tracking). This paper describes the dataset and provides annotation details. It also explains how semantic segmentation was performed using several neural network models and shows several relevant statistics. MESSI will be published in the public domain to serve as an evaluation benchmark for semantic segmentation using images captured by a drone or similar vehicle flying over a dense urban environment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejoining fragmented ancient bamboo slips with physics-driven deep learning</title>
<link>https://arxiv.org/abs/2505.08601</link>
<guid>https://arxiv.org/abs/2505.08601</guid>
<content:encoded><![CDATA[
arXiv:2505.08601v1 Announce Type: new 
Abstract: Bamboo slips are a crucial medium for recording ancient civilizations in East Asia, and offers invaluable archaeological insights for reconstructing the Silk Road, studying material culture exchanges, and global history. However, many excavated bamboo slips have been fragmented into thousands of irregular pieces, making their rejoining a vital yet challenging step for understanding their content. Here we introduce WisePanda, a physics-driven deep learning framework designed to rejoin fragmented bamboo slips. Based on the physics of fracture and material deterioration, WisePanda automatically generates synthetic training data that captures the physical properties of bamboo fragmentations. This approach enables the training of a matching network without requiring manually paired samples, providing ranked suggestions to facilitate the rejoining process. Compared to the leading curve matching method, WisePanda increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using WisePanda have experienced substantial efficiency improvements (approximately 20 times faster) when rejoining fragmented bamboo slips. This research demonstrates that incorporating physical principles into deep learning models can significantly enhance their performance, transforming how archaeologists restore and study fragmented artifacts. WisePanda provides a new paradigm for addressing data scarcity in ancient artifact restoration through physics-driven machine learning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking</title>
<link>https://arxiv.org/abs/2505.08604</link>
<guid>https://arxiv.org/abs/2505.08604</guid>
<content:encoded><![CDATA[
arXiv:2505.08604v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability of deep learning models in medical imaging applications. This work is motivated by the observation that class activation maps (CAMs) for in-distribution (ID) data typically emphasize regions that are highly relevant to the model's predictions, whereas OOD data often lacks such focused activations. By masking input images with inverted CAMs, the feature representations of ID data undergo more substantial changes compared to those of OOD data, offering a robust criterion for differentiation. In this paper, we introduce a novel unsupervised OOD detection framework, Multi-Exit Class Activation Map (MECAM), which leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks that combine CAMs from varying resolutions and depths, our method captures both global and local feature representations, thereby enhancing the robustness of OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and PathMNIST, and test its performance against three medical OOD datasets, RSNA Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN. Comprehensive comparisons with state-of-the-art OOD detection methods validate the effectiveness of our approach. Our findings emphasize the potential of multi-exit networks and feature masking for advancing unsupervised OOD detection in medical imaging, paving the way for more reliable and interpretable models in clinical practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-Modal Information to Enhance Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.08605</link>
<guid>https://arxiv.org/abs/2505.08605</guid>
<content:encoded><![CDATA[
arXiv:2505.08605v1 Announce Type: new 
Abstract: Dataset distillation aims to create a compact and highly representative synthetic dataset that preserves the knowledge of a larger real dataset. While existing methods primarily focus on optimizing visual representations, incorporating additional modalities and refining object-level information can significantly improve the quality of distilled datasets. In this work, we introduce two key enhancements to dataset distillation: caption-guided supervision and object-centric masking. To integrate textual information, we propose two strategies for leveraging caption features: the feature concatenation, where caption embeddings are fused with visual features at the classification stage, and caption matching, which introduces a caption-based alignment loss during training to ensure semantic coherence between real and synthetic data. Additionally, we apply segmentation masks to isolate target objects and remove background distractions, introducing two loss functions designed for object-centric learning: masked feature alignment loss and masked gradient matching loss. Comprehensive evaluations demonstrate that integrating caption-based guidance and object-centric masking enhances dataset distillation, leading to synthetic datasets that achieve superior performance on downstream tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</title>
<link>https://arxiv.org/abs/2505.08607</link>
<guid>https://arxiv.org/abs/2505.08607</guid>
<content:encoded><![CDATA[
arXiv:2505.08607v1 Announce Type: new 
Abstract: Stereo matching methods rely on dense pixel-wise ground truth labels, which are laborious to obtain, especially for real-world datasets. The scarcity of labeled data and domain gaps between synthetic and real-world images also pose notable challenges. In this paper, we propose a novel framework, \textbf{BooSTer}, that leverages both vision foundation models and large-scale mixed image sources, including synthetic, real, and single-view images. First, to fully unleash the potential of large-scale single-view images, we design a data generation strategy combining monocular depth estimation and diffusion models to generate dense stereo matching data from single-view images. Second, to tackle sparse labels in real-world datasets, we transfer knowledge from monocular depth estimation models, using pseudo-mono depth labels and a dynamic scale- and shift-invariant loss for additional supervision. Furthermore, we incorporate vision foundation model as an encoder to extract robust and transferable features, boosting accuracy and generalization. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving significant improvements in accuracy over existing methods, particularly in scenarios with limited labeled data and domain shifts.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.08614</link>
<guid>https://arxiv.org/abs/2505.08614</guid>
<content:encoded><![CDATA[
arXiv:2505.08614v1 Announce Type: new 
Abstract: Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08617</link>
<guid>https://arxiv.org/abs/2505.08617</guid>
<content:encoded><![CDATA[
arXiv:2505.08617v1 Announce Type: new 
Abstract: While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08644</link>
<guid>https://arxiv.org/abs/2505.08644</guid>
<content:encoded><![CDATA[
arXiv:2505.08644v1 Announce Type: new 
Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
<link>https://arxiv.org/abs/2505.08665</link>
<guid>https://arxiv.org/abs/2505.08665</guid>
<content:encoded><![CDATA[
arXiv:2505.08665v1 Announce Type: new 
Abstract: Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results</title>
<link>https://arxiv.org/abs/2505.08685</link>
<guid>https://arxiv.org/abs/2505.08685</guid>
<content:encoded><![CDATA[
arXiv:2505.08685v1 Announce Type: new 
Abstract: Deep learning (DL) has become the dominant approach for medical image segmentation, yet ensuring the reliability and clinical applicability of these models requires addressing key challenges such as annotation variability, calibration, and uncertainty estimation. This is why we created the Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS), which highlights the critical role of multiple annotators in establishing a more comprehensive ground truth, emphasizing that segmentation is inherently subjective and that leveraging inter-annotator variability is essential for robust model evaluation. Seven teams participated in the challenge, submitting a variety of DL models evaluated using metrics such as Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and Continuous Ranked Probability Score (CRPS). By incorporating consensus and dissensus ground truth, we assess how DL models handle uncertainty and whether their confidence estimates align with true segmentation performance. Our findings reinforce the importance of well-calibrated models, as better calibration is strongly correlated with the quality of the results. Furthermore, we demonstrate that segmentation models trained on diverse datasets and enriched with pre-trained knowledge exhibit greater robustness, particularly in cases deviating from standard anatomical structures. Notably, the best-performing models achieved high DSC and well-calibrated uncertainty estimates. This work underscores the need for multi-annotator ground truth, thorough calibration assessments, and uncertainty-aware evaluations to develop trustworthy and clinically reliable DL-based medical image segmentation models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model</title>
<link>https://arxiv.org/abs/2505.08695</link>
<guid>https://arxiv.org/abs/2505.08695</guid>
<content:encoded><![CDATA[
arXiv:2505.08695v1 Announce Type: new 
Abstract: Given an arbitrary content and style image, arbitrary style transfer aims to render a new stylized
  image which preserves the content image's structure and possesses the style image's style. Existing
  arbitrary style transfer methods are based on either small models or pre-trained large-scale models.
  The small model-based methods fail to generate high-quality stylized images, bringing artifacts and
  disharmonious patterns. The pre-trained large-scale model-based methods can generate high-quality
  stylized images but struggle to preserve the content structure and cost long inference time. To this
  end, we propose a new framework, called SPAST, to generate high-quality stylized images with
  less inference time. Specifically, we design a novel Local-global Window Size Stylization Module
  (LGWSSM)tofuse style features into content features. Besides, we introduce a novel style prior loss,
  which can dig out the style priors from a pre-trained large-scale model into the SPAST and motivate
  the SPAST to generate high-quality stylized images with short inference time.We conduct abundant
  experiments to verify that our proposed method can generate high-quality stylized images and less
  inference time compared with the SOTA arbitrary style transfer methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Image Colorization with Instance-aware Texts and Masks</title>
<link>https://arxiv.org/abs/2505.08705</link>
<guid>https://arxiv.org/abs/2505.08705</guid>
<content:encoded><![CDATA[
arXiv:2505.08705v1 Announce Type: new 
Abstract: Recently, the application of deep learning in image colorization has received widespread attention. The maturation of diffusion models has further advanced the development of image colorization models. However, current mainstream image colorization models still face issues such as color bleeding and color binding errors, and cannot colorize images at the instance level. In this paper, we propose a diffusion-based colorization method MT-Color to achieve precise instance-aware colorization with use-provided guidance. To tackle color bleeding issue, we design a pixel-level mask attention mechanism that integrates latent features and conditional gray image features through cross-attention. We use segmentation masks to construct cross-attention masks, preventing pixel information from exchanging between different instances. We also introduce an instance mask and text guidance module that extracts instance masks and text representations of each instance, which are then fused with latent features through self-attention, utilizing instance masks to form self-attention masks to prevent instance texts from guiding the colorization of other areas, thus mitigating color binding errors. Furthermore, we apply a multi-instance sampling strategy, which involves sampling each instance region separately and then fusing the results. Additionally, we have created a specialized dataset for instance-level colorization tasks, GPT-color, by leveraging large visual language models on existing image datasets. Qualitative and quantitative experiments show that our model and dataset outperform previous methods and datasets.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</title>
<link>https://arxiv.org/abs/2505.08723</link>
<guid>https://arxiv.org/abs/2505.08723</guid>
<content:encoded><![CDATA[
arXiv:2505.08723v1 Announce Type: new 
Abstract: Satellite image time series (SITS) provide continuous observations of the Earth's surface, making them essential for applications such as environmental management and disaster assessment. However, existing spatiotemporal foundation models rely on plain vision transformers, which encode entire temporal sequences without explicitly capturing multiscale spatiotemporal relationships between land objects. This limitation hinders their effectiveness in downstream tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision transformer foundation model tailored for SITS analysis. At its core, we introduce a spatiotemporal gyroscope attention mechanism that dynamically captures evolving multiscale patterns across both time and space. For pre-training, we curate MillionST, a large-scale dataset of one million images from 100,000 geographic locations, each captured across 10 temporal phases over five years, encompassing diverse geospatial changes and seasonal variations. Leveraging this dataset, we adapt masked image modeling to pre-train TiMo, enabling it to effectively learn and encode generalizable spatiotemporal representations.Extensive experiments across multiple spatiotemporal tasks-including deforestation monitoring, land cover segmentation, crop type classification, and flood detection-demonstrate TiMo's superiority over state-of-the-art methods. Code, model, and dataset will be released at https://github.com/MiliLab/TiMo.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.08725</link>
<guid>https://arxiv.org/abs/2505.08725</guid>
<content:encoded><![CDATA[
arXiv:2505.08725v1 Announce Type: new 
Abstract: The Large Visual-Language Models (LVLMs) have significantly advanced image understanding. Their comprehension and reasoning capabilities enable promising applications in autonomous driving scenarios. However, existing research typically focuses on front-view perspectives and partial objects within scenes, struggling to achieve comprehensive scene understanding. Meanwhile, existing LVLMs suffer from the lack of mapping relationship between 2D and 3D and insufficient integration of 3D object localization and instruction understanding. To tackle these limitations, we first introduce NuInteract, a large-scale dataset with over 1.5M multi-view image language pairs spanning dense scene captions and diverse interactive tasks. Furthermore, we propose DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs with a spatial processor using a series of learnable queries. The spatial processor, designed as a plug-and-play component, can be initialized with pre-trained 3D detectors to improve 3D perception. Our experiments show that DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable improvement on the 3D visual grounding task. The dataset and code will be released at https://github.com/zc-zhao/DriveMonkey.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion</title>
<link>https://arxiv.org/abs/2505.08747</link>
<guid>https://arxiv.org/abs/2505.08747</guid>
<content:encoded><![CDATA[
arXiv:2505.08747v1 Announce Type: new 
Abstract: Nutrition estimation is an important component of promoting healthy eating and mitigating diet-related health risks. Despite advances in tasks such as food classification and ingredient recognition, progress in nutrition estimation is limited due to the lack of datasets with nutritional annotations. To address this issue, we introduce FastFood, a dataset with 84,446 images across 908 fast food categories, featuring ingredient and nutritional annotations. In addition, we propose a new model-agnostic Visual-Ingredient Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating visual and ingredient features. Ingredient robustness is improved through synonym replacement and resampling strategies during training. The ingredient-aware visual feature fusion module combines ingredient features and visual representation to achieve accurate nutritional prediction. During testing, ingredient predictions are refined using large multimodal models by data augmentation and majority voting. Our experiments on both FastFood and Nutrition5k datasets validate the effectiveness of our proposed method built in different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the importance of ingredient information in nutrition estimation. https://huiyanqi.github.io/fastfood-nutrition-estimation/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology</title>
<link>https://arxiv.org/abs/2505.08765</link>
<guid>https://arxiv.org/abs/2505.08765</guid>
<content:encoded><![CDATA[
arXiv:2505.08765v1 Announce Type: new 
Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target objects using visual and textual cues without external guidance. Existing approaches struggle in complex urban environments due to redundant semantic processing, similar object distinction, and the exploration-exploitation dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS, the first benchmark dataset for autonomous search of common urban objects. This dataset comprises 2,420 tasks across six object categories with varying difficulty levels, enabling comprehensive evaluation of UAV agents' search capabilities. To solve the AVOS tasks, we also propose PRPSearcher (Perception-Reasoning-Planning Searcher), a novel agentic method powered by multi-modal large language models (MLLMs) that mimics human three-tier cognition. Specifically, PRPSearcher constructs three specialized maps: an object-centric dynamic semantic map enhancing spatial perception, a 3D cognitive map based on semantic attraction values for target reasoning, and a 3D uncertainty map for balanced exploration-exploitation search. Also, our approach incorporates a denoising mechanism to mitigate interference from similar objects and utilizes an Inspiration Promote Thought (IPT) prompting mechanism for adaptive action planning. Experimental results on CityAVOS demonstrate that PRPSearcher surpasses existing baselines in both success rate and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and -46.40% NE). While promising, the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities in AVOS tasks. This work establishes a foundation for future advances in embodied target search. Dataset and source code are available at https://anonymous.4open.science/r/CityAVOS-3DF8.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation</title>
<link>https://arxiv.org/abs/2505.07840</link>
<guid>https://arxiv.org/abs/2505.07840</guid>
<content:encoded><![CDATA[
arXiv:2505.07840v1 Announce Type: cross 
Abstract: Precision farming relies on accurate vegetation monitoring to enhance crop productivity and promote sustainable agricultural practices. This study presents a comprehensive evaluation of UAV-based imaging for vegetation health assessment in a palm tree cultivation region in Dubai. By comparing multispectral and RGB image data, we demonstrate that RGBbased vegetation indices offer performance comparable to more expensive multispectral indices, providing a cost-effective alternative for large-scale agricultural monitoring. Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI were computed to categorize vegetation into healthy, moderate, and stressed conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered similar results in vegetation classification and stress detection. Our findings highlight the practical benefits of integrating RGB imagery into precision farming, reducing operational costs while maintaining accuracy in plant health monitoring. This research underscores the potential of UAVbased RGB imaging as a powerful tool for precision agriculture, enabling broader adoption of data-driven decision-making in crop management. By leveraging the strengths of both multispectral and RGB imaging, this work advances the state of UAV applications in agriculture, paving the way for more efficient and scalable farming solutions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding</title>
<link>https://arxiv.org/abs/2505.07851</link>
<guid>https://arxiv.org/abs/2505.07851</guid>
<content:encoded><![CDATA[
arXiv:2505.07851v1 Announce Type: cross 
Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing high-resolution, real-time imaging of cardiac structures. However, existing navigation methods rely on electromagnetic (EM) tracking, which is susceptible to interference and position drift, or require manual adjustments based on operator expertise. To overcome these limitations, we propose a novel anatomy-aware pose estimation system that determines the ICE catheter position and orientation solely from ICE images, eliminating the need for external tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep learning model, which captures spatial relationships between ICE images and anatomical structures. The model is trained on a clinically acquired dataset of 851 subjects, including ICE images paired with position and orientation labels normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16 embeddings and processed through a transformer network, where a [CLS] token independently predicts position and orientation via separate linear layers. The model is optimized using a Mean Squared Error (MSE) loss function, balancing positional and orientational accuracy. Experimental results demonstrate an average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98 deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy. Qualitative assessments further validate alignment between predicted and target views within 3D cardiac meshes. This AI-driven system enhances procedural efficiency, reduces operator workload, and enables real-time ICE catheter localization for tracking-free procedures. The proposed method can function independently or complement existing mapping systems like CARTO, offering a transformative approach to ICE-guided interventions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding</title>
<link>https://arxiv.org/abs/2505.07864</link>
<guid>https://arxiv.org/abs/2505.07864</guid>
<content:encoded><![CDATA[
arXiv:2505.07864v1 Announce Type: cross 
Abstract: Flowcharts are indispensable tools in software design and business-process analysis, yet current vision-language models (VLMs) frequently misinterpret the directional arrows and graph topology that set these diagrams apart from natural images. We introduce a seven-stage pipeline grouped into three broader processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical character recognition (OCR) to extract node text; and (3) construction of a structured prompt that guides the VLMs. Tested on a 90-question benchmark distilled from 30 annotated flowcharts, the method raises overall accuracy from 80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp); branch-result questions improve more modestly, and before-step questions remain difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same trends, reinforcing the advantage of explicit arrow encoding. Limitations include dependence on detector and OCR precision, the small evaluation set, and residual errors at nodes with multiple incoming edges. Future work will enlarge the benchmark with synthetic and handwritten flowcharts and assess the approach on Business Process Model and Notation (BPMN) and Unified Modeling Language (UML).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2505.07866</link>
<guid>https://arxiv.org/abs/2505.07866</guid>
<content:encoded><![CDATA[
arXiv:2505.07866v1 Announce Type: cross 
Abstract: The diffusion model has recently emerged as a potent approach in computer vision, demonstrating remarkable performances in the field of generative artificial intelligence. Capable of producing high-quality synthetic images, diffusion models have been successfully applied across a range of applications. However, a significant challenge remains with the high computational cost associated with training and generating these models. This study focuses on the efficiency and inference time of diffusion-based generative models, highlighting their applications in both natural and medical imaging. We present the most recent advances in diffusion models by categorizing them into three key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play a crucial role in medical imaging, where producing fast, reliable, and high-quality medical images is essential for accurate analysis of abnormalities and disease diagnosis. We first investigate the general framework of DDPM, LDM, and WDM and discuss the computational complexity gap filled by these models in natural and medical imaging. We then discuss the current limitations of these models as well as the opportunities and future research directions in medical imaging.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2505.07879</link>
<guid>https://arxiv.org/abs/2505.07879</guid>
<content:encoded><![CDATA[
arXiv:2505.07879v1 Announce Type: cross 
Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monocular Online Reconstruction with Enhanced Detail Preservation</title>
<link>https://arxiv.org/abs/2505.07887</link>
<guid>https://arxiv.org/abs/2505.07887</guid>
<content:encoded><![CDATA[
arXiv:2505.07887v1 Announce Type: cross 
Abstract: We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the Hierarchical Gaussian Management Module for effective Gaussian distribution and the Global Consistency Optimization Module for maintaining alignment and coherence at all scales. In addition, we present the Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors</title>
<link>https://arxiv.org/abs/2505.07906</link>
<guid>https://arxiv.org/abs/2505.07906</guid>
<content:encoded><![CDATA[
arXiv:2505.07906v1 Announce Type: cross 
Abstract: Microstructure often dictates materials performance, yet it is rarely treated as an explicit design variable because microstructure is hard to quantify, predict, and optimize. Here, we introduce an image centric, closed-loop framework that makes microstructural morphology into a controllable objective and demonstrate its use case with Li- and Mn-rich layered oxide cathode precursors. This work presents an integrated, AI driven framework for the predictive design and optimization of lithium-ion battery cathode precursor synthesis. This framework integrates a diffusion-based image generation model, a quantitative image analysis pipeline, and a particle swarm optimization (PSO) algorithm. By extracting key morphological descriptors such as texture, sphericity, and median particle size (D50) from SEM images, the platform accurately predicts SEM like morphologies resulting from specific coprecipitation conditions, including reaction time-, solution concentration-, and pH-dependent structural changes. Optimization then pinpoints synthesis parameters that yield user defined target morphologies, as experimentally validated by the close agreement between predicted and synthesized structures. This framework offers a practical strategy for data driven materials design, enabling both forward prediction and inverse design of synthesis conditions and paving the way toward autonomous, image guided microstructure engineering.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny</title>
<link>https://arxiv.org/abs/2505.07908</link>
<guid>https://arxiv.org/abs/2505.07908</guid>
<content:encoded><![CDATA[
arXiv:2505.07908v1 Announce Type: cross 
Abstract: In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA (Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fr\'{e}chet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids</title>
<link>https://arxiv.org/abs/2505.08082</link>
<guid>https://arxiv.org/abs/2505.08082</guid>
<content:encoded><![CDATA[
arXiv:2505.08082v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fr\'{e}chet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Neighborhood Environments with Large Language Models</title>
<link>https://arxiv.org/abs/2505.08163</link>
<guid>https://arxiv.org/abs/2505.08163</guid>
<content:encoded><![CDATA[
arXiv:2505.08163v1 Announce Type: cross 
Abstract: Neighborhood environments include physical and environmental conditions such as housing quality, roads, and sidewalks, which significantly influence human health and well-being. Traditional methods for assessing these environments, including field surveys and geographic information systems (GIS), are resource-intensive and challenging to evaluate neighborhood environments at scale. Although machine learning offers potential for automated analysis, the laborious process of labeling training data and the lack of accessible models hinder scalability. This study explores the feasibility of large language models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood environments (e.g., sidewalk and powerline) at scale. We train a robust YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting six environmental indicators, including streetlight, sidewalk, powerline, apartment, single-lane road, and multilane road. We then evaluate four LLMs, including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility, robustness, and limitations in identifying these indicators, with a focus on the impact of prompting strategies and fine-tuning. We apply majority voting with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs could be a useful tool to decode the neighborhood environment without any training effort.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices</title>
<link>https://arxiv.org/abs/2505.08191</link>
<guid>https://arxiv.org/abs/2505.08191</guid>
<content:encoded><![CDATA[
arXiv:2505.08191v1 Announce Type: cross 
Abstract: Neural rendering has gained prominence for its high-quality output, which is crucial for AR/VR applications. However, its large voxel grid data size and irregular access patterns challenge real-time processing on edge devices. While previous works have focused on improving data locality, they have not adequately addressed the issue of large voxel grid sizes, which necessitate frequent off-chip memory access and substantial on-chip memory. This paper introduces SpNeRF, a software-hardware co-design solution tailored for sparse volumetric neural rendering. We first identify memory-bound rendering inefficiencies and analyze the inherent sparsity in the voxel grid data of neural rendering. To enhance efficiency, we propose novel preprocessing and online decoding steps, reducing the memory size for voxel grid. The preprocessing step employs hash mapping to support irregular data access while maintaining a minimal memory size. The online decoding step enables efficient on-chip sparse voxel grid processing, incorporating bitmap masking to mitigate PSNR loss caused by hash collisions. To further optimize performance, we design a dedicated hardware architecture supporting our sparse voxel grid processing technique. Experimental results demonstrate that SpNeRF achieves an average 21.07$\times$ reduction in memory size while maintaining comparable PSNR levels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and NeuRex.Edge, our design achieves speedups of 95.1$\times$, 63.5$\times$, 1.5$\times$ and 10.3$\times$, and improves energy efficiency by 625.6$\times$, 529.1$\times$, 4$\times$, and 4.4$\times$, respectively.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image</title>
<link>https://arxiv.org/abs/2505.08239</link>
<guid>https://arxiv.org/abs/2505.08239</guid>
<content:encoded><![CDATA[
arXiv:2505.08239v1 Announce Type: cross 
Abstract: We introduce adaptive view planning to multi-view synthesis, aiming to improve both occlusion revelation and 3D consistency for single-view 3D reconstruction. Instead of generating an unordered set of views independently or simultaneously, we generate a sequence of views, leveraging temporal consistency to enhance 3D coherence. Most importantly, our view sequence is not determined by a pre-determined camera setup. Instead, we compute an adaptive camera trajectory (ACT), specifically, an orbit of camera views, which maximizes the visibility of occluded regions of the 3D object to be reconstructed. Once the best orbit is found, we feed it to a video diffusion model to generate novel views around the orbit, which in turn, are passed to a multi-view 3D reconstruction model to obtain the final reconstruction. Our multi-view synthesis pipeline is quite efficient since it involves no run-time training/optimization, only forward inferences by applying the pre-trained models for occlusion analysis and multi-view synthesis. Our method predicts camera trajectories that reveal occlusions effectively and produce consistent novel views, significantly improving 3D reconstruction over SOTA on the unseen GSO dataset, both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis</title>
<link>https://arxiv.org/abs/2505.08247</link>
<guid>https://arxiv.org/abs/2505.08247</guid>
<content:encoded><![CDATA[
arXiv:2505.08247v1 Announce Type: cross 
Abstract: Medical image synthesis plays a crucial role in providing anatomically accurate images for diagnosis and treatment. Hallux valgus, which affects approximately 19% of the global population, requires frequent weight-bearing X-rays for assessment, placing additional strain on both patients and healthcare providers. Existing X-ray models often struggle to balance image fidelity, skeletal consistency, and physical constraints, particularly in diffusion-based methods that lack skeletal guidance. We propose the Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a foot evaluation method utilizing skeletal landmarks. SCCDM incorporates multi-scale feature extraction and attention mechanisms, improving the Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves an average score of 0.85, demonstrating strong clinical applicability. The code is available at https://github.com/midisec/SCCDM.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted</title>
<link>https://arxiv.org/abs/2505.08255</link>
<guid>https://arxiv.org/abs/2505.08255</guid>
<content:encoded><![CDATA[
arXiv:2505.08255v1 Announce Type: cross 
Abstract: With the advancement of AI generative techniques, Deepfake faces have become incredibly realistic and nearly indistinguishable to the human eye. To counter this, Deepfake detectors have been developed as reliable tools for assessing face authenticity. These detectors are typically developed on Deep Neural Networks (DNNs) and trained using third-party datasets. However, this protocol raises a new security risk that can seriously undermine the trustfulness of Deepfake detectors: Once the third-party data providers insert poisoned (corrupted) data maliciously, Deepfake detectors trained on these datasets will be injected ``backdoors'' that cause abnormal behavior when presented with samples containing specific triggers. This is a practical concern, as third-party providers may distribute or sell these triggers to malicious users, allowing them to manipulate detector performance and escape accountability.
  This paper investigates this risk in depth and describes a solution to stealthily infect Deepfake detectors. Specifically, we develop a trigger generator, that can synthesize passcode-controlled, semantic-suppression, adaptive, and invisible trigger patterns, ensuring both the stealthiness and effectiveness of these triggers. Then we discuss two poisoning scenarios, dirty-label poisoning and clean-label poisoning, to accomplish the injection of backdoors. Extensive experiments demonstrate the effectiveness, stealthiness, and practicality of our method compared to several baselines.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities</title>
<link>https://arxiv.org/abs/2505.08283</link>
<guid>https://arxiv.org/abs/2505.08283</guid>
<content:encoded><![CDATA[
arXiv:2505.08283v1 Announce Type: cross 
Abstract: Multimodal learning enhances deep learning models by enabling them to perceive and understand information from multiple data modalities, such as visual and textual inputs. However, most existing approaches assume the availability of all modalities, an assumption that often fails in real-world applications. Recent works have introduced learnable missing-case-aware prompts to mitigate performance degradation caused by missing modalities while reducing the need for extensive model fine-tuning. Building upon the effectiveness of missing-case-aware handling for missing modalities, we propose a novel decoupled prototype-based output head, which leverages missing-case-aware class-wise prototypes tailored for each individual modality. This approach dynamically adapts to different missing modality scenarios and can be seamlessly integrated with existing prompt-based methods. Extensive experiments demonstrate that our proposed output head significantly improves performance across a wide range of missing-modality scenarios and varying missing rates.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</title>
<link>https://arxiv.org/abs/2505.08293</link>
<guid>https://arxiv.org/abs/2505.08293</guid>
<content:encoded><![CDATA[
arXiv:2505.08293v1 Announce Type: cross 
Abstract: Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2505.08299</link>
<guid>https://arxiv.org/abs/2505.08299</guid>
<content:encoded><![CDATA[
arXiv:2505.08299v1 Announce Type: cross 
Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged as powerful alternatives to Transformers for sequence modeling, offering linear-time complexity and competitive performance across diverse tasks. However, their large parameter counts pose significant challenges for deployment in resource-constrained environments. We propose a novel unstructured pruning framework tailored for Mamba models that achieves up to 70\% parameter reduction while retaining over 95\% of the original performance. Our approach integrates three key innovations: (1) a gradient-aware magnitude pruning technique that combines weight magnitude and gradient information to identify less critical parameters, (2) an iterative pruning schedule that gradually increases sparsity to maintain model stability, and (3) a global pruning strategy that optimizes parameter allocation across the entire model. Through extensive experiments on WikiText-103, Long Range Arena, and ETT time-series benchmarks, we demonstrate significant efficiency gains with minimal performance degradation. Our analysis of pruning effects on Mamba's components reveals critical insights into the architecture's redundancy and robustness, enabling practical deployment in resource-constrained settings while broadening Mamba's applicability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
<link>https://arxiv.org/abs/2505.08316</link>
<guid>https://arxiv.org/abs/2505.08316</guid>
<content:encoded><![CDATA[
arXiv:2505.08316v1 Announce Type: cross 
Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</title>
<link>https://arxiv.org/abs/2505.08414</link>
<guid>https://arxiv.org/abs/2505.08414</guid>
<content:encoded><![CDATA[
arXiv:2505.08414v1 Announce Type: cross 
Abstract: Current deep learning models are mostly task specific and lack a user-friendly interface to operate. We present Meta-EyeFM, a multi-function foundation model that integrates a large language model (LLM) with vision foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a routing mechanism to enable accurate task-specific analysis based on text queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and systemic diseases, differentiate ocular disease severity, and identify common ocular signs. The model achieved 100% accuracy in routing fundus images to appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection, $\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification. Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o LMMs in detecting various eye diseases and comparable to an ophthalmologist. This system offers enhanced usability and diagnostic performance, making it a valuable decision support tool for primary eye care or an online LLM for fundus evaluation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI</title>
<link>https://arxiv.org/abs/2505.08430</link>
<guid>https://arxiv.org/abs/2505.08430</guid>
<content:encoded><![CDATA[
arXiv:2505.08430v1 Announce Type: cross 
Abstract: Tertiary lymphoid structures (TLS) are organized clusters of immune cells, whose maturity and area can be quantified in whole slide image (WSI) for various prognostic tasks. Existing methods for assessing these characteristics typically rely on cell proxy tasks and require additional post-processing steps. In this work, We focus on a novel task-TLS Semantic Segmentation (TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in an end-to-end manner. Due to the extensive scale of WSI and patch-based segmentation strategies, TLS-SS necessitates integrating from neighboring patches to guide target patch (target) segmentation. Previous techniques often employ on multi-resolution approaches, constraining the capacity to leverage the broader neighboring context while tend to preserve coarse-grained information. To address this, we propose a GNN-based Neighboring Context Aggregation Framework (GNCAF), which progressively aggregates multi-hop neighboring context from the target and employs a self-attention mechanism to guide the segmentation of the target. GNCAF can be integrated with various segmentation models to enhance their ability to perceive contextual information outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly available. Experiments on these datasets demonstrate the superiority of GNCAF, achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU, respectively. Additionally, we also validate the task scalability of GNCAF on segmentation of lymph node metastases.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?</title>
<link>https://arxiv.org/abs/2505.08468</link>
<guid>https://arxiv.org/abs/2505.08468</guid>
<content:encoded><![CDATA[
arXiv:2505.08468v1 Announce Type: cross 
Abstract: Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2505.08528</link>
<guid>https://arxiv.org/abs/2505.08528</guid>
<content:encoded><![CDATA[
arXiv:2505.08528v1 Announce Type: cross 
Abstract: In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A portable diagnosis model for Keratoconus using a smartphone</title>
<link>https://arxiv.org/abs/2505.08616</link>
<guid>https://arxiv.org/abs/2505.08616</guid>
<content:encoded><![CDATA[
arXiv:2505.08616v1 Announce Type: cross 
Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized thinning and protrusion, leading to visual distortion. While Placido disc-based topography remains a standard in clinical diagnostics, its dependence on specialized equipment limits accessibility. In this paper, we propose a portable, smartphone-based diagnostic framework that captures corneal reflections of a Placido disc displayed on a phone screen and applies a two-stage detection pipeline, then validate on 3D-printed emulated eyeball models that simulate normal, moderate, and severe KC stages based on anterior chamber depth (ACD). The first step of the two-stage detection pipeline is classifying different stages of KC with features including height and width of extracted reflections using weighted support vector machine (WSVM). It achieves a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16 Pro. For the second step, we visualize the KC-affected protrusion regions on the corneas with color maps based on inter-disc distance, that provides an intuitive representation of disease severity and localization. Moreover, we validate the ability of the extracted features to differentiate between KC stages with ANOVA and Omega Squared, with significant p-values (e.g., $p < 10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models</title>
<link>https://arxiv.org/abs/2505.08622</link>
<guid>https://arxiv.org/abs/2505.08622</guid>
<content:encoded><![CDATA[
arXiv:2505.08622v1 Announce Type: cross 
Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Claycode: Stylable and Deformable 2D Scannable Codes</title>
<link>https://arxiv.org/abs/2505.08666</link>
<guid>https://arxiv.org/abs/2505.08666</guid>
<content:encoded><![CDATA[
arXiv:2505.08666v1 Announce Type: cross 
Abstract: This paper introduces Claycode, a novel 2D scannable code designed for extensive stylization and deformation. Unlike traditional matrix-based codes (e.g., QR codes), Claycodes encode their message in a tree structure. During the encoding process, bits are mapped into a topology tree, which is then depicted as a nesting of color regions drawn within the boundaries of a target polygon shape. When decoding, Claycodes are extracted and interpreted in real-time from a camera stream. We detail the end-to-end pipeline and show that Claycodes allow for extensive stylization without compromising their functionality. We then empirically demonstrate Claycode's high tolerance to heavy deformations, outperforming traditional 2D scannable codes in scenarios where they typically fail.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Coder:Text-Guided CAD Files Code Generation</title>
<link>https://arxiv.org/abs/2505.08686</link>
<guid>https://arxiv.org/abs/2505.08686</guid>
<content:encoded><![CDATA[
arXiv:2505.08686v1 Announce Type: cross 
Abstract: Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D models of real-world products. Traditional CAD typically relies on hand-drawing by experts or modifications of existing library files, which doesn't allow for rapid personalization. With the emergence of generative artificial intelligence, convenient and efficient personalized CAD generation has become possible. However, existing generative methods typically produce outputs that lack interactive editability and geometric annotations, limiting their practical applications in manufacturing. To enable interactive generative CAD, we propose CAD-Coder, a framework that transforms natural language instructions into CAD script codes, which can be executed in Python environments to generate human-editable CAD files (.Dxf). To facilitate the generation of editable CAD sketches with annotation information, we construct a comprehensive dataset comprising 29,130 Dxf files with their corresponding script codes, where each sketch preserves both editability and geometric annotations. We evaluate CAD-Coder on various 2D/3D CAD generation tasks against existing methods, demonstrating superior interactive capabilities while uniquely providing editable sketches with geometric annotations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation</title>
<link>https://arxiv.org/abs/2505.08693</link>
<guid>https://arxiv.org/abs/2505.08693</guid>
<content:encoded><![CDATA[
arXiv:2505.08693v1 Announce Type: cross 
Abstract: Self-supervised pretrain techniques have been widely used to improve the downstream tasks' performance. However, real-world magnetic resonance (MR) studies usually consist of different sets of contrasts due to different acquisition protocols, which poses challenges for the current deep learning methods on large-scale pretrain and different downstream tasks with different input requirements, since these methods typically require a fixed set of input modalities or, contrasts. To address this challenge, we propose variable-input ViT (VIViT), a transformer-based framework designed for self-supervised pretraining and segmentation finetuning for variable contrasts in each study. With this ability, our approach can maximize the data availability in pretrain, and can transfer the learned knowledge from pretrain to downstream tasks despite variations in input requirements. We validate our method on brain infarct and brain tumor segmentation, where our method outperforms current CNN and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively. These results highlight the efficacy of our design for better adaptability and performance on tasks with real-world heterogeneous MR data.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aya Vision: Advancing the Frontier of Multilingual Multimodality</title>
<link>https://arxiv.org/abs/2505.08751</link>
<guid>https://arxiv.org/abs/2505.08751</guid>
<content:encoded><![CDATA[
arXiv:2505.08751v1 Announce Type: cross 
Abstract: Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</title>
<link>https://arxiv.org/abs/2505.08787</link>
<guid>https://arxiv.org/abs/2505.08787</guid>
<content:encoded><![CDATA[
arXiv:2505.08787v1 Announce Type: cross 
Abstract: Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Anytime Optical Flow Estimation with Event Cameras</title>
<link>https://arxiv.org/abs/2307.05033</link>
<guid>https://arxiv.org/abs/2307.05033</guid>
<content:encoded><![CDATA[
arXiv:2307.05033v3 Announce Type: replace 
Abstract: Event cameras respond to changes in log-brightness at the millisecond level, making them ideal for optical flow estimation. However, existing datasets from event cameras provide only low frame rate ground truth for optical flow, limiting the research potential of event-driven optical flow. To address this challenge, we introduce a low-latency event representation, Unified Voxel Grid, and propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. Furthermore, we propose the Rectified Flow Warp Loss (RFWL) for the unsupervised assessment of intermediate optical flow. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-based interactive segmentation in remote sensing</title>
<link>https://arxiv.org/abs/2308.13174</link>
<guid>https://arxiv.org/abs/2308.13174</guid>
<content:encoded><![CDATA[
arXiv:2308.13174v3 Announce Type: replace 
Abstract: Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal Consistency to Frame-Based Domain Translation Approaches</title>
<link>https://arxiv.org/abs/2310.00868</link>
<guid>https://arxiv.org/abs/2310.00868</guid>
<content:encoded><![CDATA[
arXiv:2310.00868v2 Announce Type: replace 
Abstract: Fourteen million colonoscopies are performed annually just in the U.S. However, the videos from these colonoscopies are not saved due to storage constraints (each video from a high-definition colonoscope camera can be in tens of gigabytes). Instead, a few relevant individual frames are saved for documentation/reporting purposes and these are the frames on which most current colonoscopy AI models are trained on. While developing new unsupervised domain translation methods for colonoscopy (e.g. to translate between real optical and virtual/CT colonoscopy), it is thus typical to start with approaches that initially work for individual frames without temporal consistency. Once an individual-frame model has been finalized, additional contiguous frames are added with a modified deep learning architecture to train a new model from scratch for temporal consistency. This transition to temporally-consistent deep learning models, however, requires significantly more computational and memory resources for training. In this paper, we present a lightweight solution with a tunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual frame-based approaches that reduces training requirements by a factor of 5. We demonstrate the effectiveness of our approach on two challenging use cases in colonoscopy: haustral fold segmentation (indicative of missed surface) and realistic colonoscopy simulator video generation. We also release a first-of-its kind temporal dataset for colonoscopy for the above use cases. The datasets, accompanying code, and pretrained models will be made available on our Computational Endoscopy Platform GitHub (https://github.com/nadeemlab/CEP). The supplementary video is available at https://youtu.be/UMVP-uIXwWk.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized View and Geometry Distillation from Multi-view Diffuser</title>
<link>https://arxiv.org/abs/2312.06198</link>
<guid>https://arxiv.org/abs/2312.06198</guid>
<content:encoded><![CDATA[
arXiv:2312.06198v4 Announce Type: replace 
Abstract: Generating multi-view images from a single input view using image-conditioned diffusion models is a recent advancement and has shown considerable potential. However, issues such as the lack of consistency in synthesized views and over-smoothing in extracted geometry persist. Previous methods integrate multi-view consistency modules or impose additional supervisory to enhance view consistency while compromising on the flexibility of camera positioning and limiting the versatility of view synthesis. In this study, we consider the radiance field optimized during geometry extraction as a more rigid consistency prior, compared to volume and ray aggregation used in previous works. We further identify and rectify a critical bias in the traditional radiance field optimization process through score distillation from a multi-view diffuser. We introduce an Unbiased Score Distillation (USD) that utilizes unconditioned noises from a 2D diffusion model, greatly refining the radiance field fidelity. We leverage the rendered views from the optimized radiance field as the basis and develop a two-step specialization process of a 2D diffusion model, which is adept at conducting object-specific denoising and generating high-quality multi-view images. Finally, we recover faithful geometry and texture directly from the refined multi-view images. Empirical evaluations demonstrate that our optimized geometry and view distillation technique generates comparable results to the state-of-the-art models trained on extensive datasets, all while maintaining freedom in camera positioning. Please see our project page at https://youjiazhang.github.io/USD/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation</title>
<link>https://arxiv.org/abs/2403.17525</link>
<guid>https://arxiv.org/abs/2403.17525</guid>
<content:encoded><![CDATA[
arXiv:2403.17525v2 Announce Type: replace 
Abstract: When benefiting graphic sketch representation with sketch drawing orders, recent studies have linked sketch patches as graph edges by drawing orders in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since the contextual relationships between patches may be inconsistent with the sequential positions in drawing orders, due to variants of sketch drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for sketch learning. We introduce a sinusoidal absolute PE to embed the sequential positions in drawing orders, and a learnable relative PE to encode the unseen contextual relationships between patches. Both types of PEs never attend the construction of graph edges, but are injected into graph nodes to cooperate with the visual patterns captured from patches. After linking nodes by semantic proximity, during message aggregation via graph convolutional networks, each node receives both semantic features from patches and contextual information from PEs from its neighbors, which equips local patch patterns with global contextual information, further obtaining drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis. The source codes could be found at https://github.com/SCZang/DC-gra2seq.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2404.11824</link>
<guid>https://arxiv.org/abs/2404.11824</guid>
<content:encoded><![CDATA[
arXiv:2404.11824v5 Announce Type: replace 
Abstract: Text-to-image (T2I) generation has made remarkable progress in producing high-quality images, but a fundamental challenge remains: creating backgrounds that naturally accommodate text placement without compromising image quality. This capability is non-trivial for real-world applications like graphic design, where clear visual hierarchy between content and text is essential. Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating text-friendly backgrounds. We present TextCenGen, a training-free dynamic background adaptation in the blank region for text-friendly image generation. Instead of directly reducing attention in text areas, which degrades image quality, we relocate conflicting objects before background optimization. Our method analyzes cross-attention maps to identify conflicting objects overlapping with text regions and uses a force-directed graph approach to guide their relocation, followed by attention excluding constraints to ensure smooth backgrounds. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality. Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across four seed datasets, TextCenGen outperforms existing methods by achieving 23% lower saliency overlap in text regions while maintaining 98% of the semantic fidelity measured by CLIP score and our proposed Visual-Textual Concordance Metric (VTCM).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminative and Consistent Representation Distillation</title>
<link>https://arxiv.org/abs/2407.11802</link>
<guid>https://arxiv.org/abs/2407.11802</guid>
<content:encoded><![CDATA[
arXiv:2407.11802v5 Announce Type: replace 
Abstract: Knowledge Distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. While contrastive learning has shown promise in self-supervised learning by creating discriminative representations, its application in knowledge distillation remains limited and focuses primarily on discrimination, neglecting the structural relationships captured by the teacher model. To address this limitation, we propose Discriminative and Consistent Distillation (DCD), which employs a contrastive loss along with a consistency regularization to minimize the discrepancy between the distributions of teacher and student representations. Our method introduces learnable temperature and bias parameters that adapt during training to balance these complementary objectives, replacing the fixed hyperparameters commonly used in contrastive learning approaches. Through extensive experiments on CIFAR-100 and ImageNet ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance, with the student model sometimes surpassing the teacher's accuracy. Furthermore, we show that DCD's learned representations exhibit superior cross-dataset generalization when transferred to Tiny ImageNet and STL-10.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Representation Distillation</title>
<link>https://arxiv.org/abs/2407.12073</link>
<guid>https://arxiv.org/abs/2407.12073</guid>
<content:encoded><![CDATA[
arXiv:2407.12073v5 Announce Type: replace 
Abstract: Knowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Feature Matching for Large-Scale Structure from Motion</title>
<link>https://arxiv.org/abs/2409.02310</link>
<guid>https://arxiv.org/abs/2409.02310</guid>
<content:encoded><![CDATA[
arXiv:2409.02310v4 Announce Type: replace 
Abstract: Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2410.01723</link>
<guid>https://arxiv.org/abs/2410.01723</guid>
<content:encoded><![CDATA[
arXiv:2410.01723v5 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR Scene Semantic Segmentation</title>
<link>https://arxiv.org/abs/2411.11935</link>
<guid>https://arxiv.org/abs/2411.11935</guid>
<content:encoded><![CDATA[
arXiv:2411.11935v2 Announce Type: replace 
Abstract: Reliable deep learning models require not only accurate predictions but also well-calibrated confidence estimates to ensure dependable uncertainty estimation. This is crucial in safety-critical applications like autonomous driving, which depend on rapid and precise semantic segmentation of LiDAR point clouds for real-time 3D scene understanding. In this work, we introduce a sampling-free approach for estimating well-calibrated confidence values for classification tasks, achieving alignment with true classification accuracy and significantly reducing inference time compared to sampling-based methods. Our evaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic segmentation shows that our approach maintains well-calibrated confidence values while achieving increased processing speed compared to a sampling baseline. Additionally, reliability diagrams reveal that our method produces underconfidence rather than overconfident predictions, an advantage for safety-critical applications. Our sampling-free approach offers well-calibrated and time-efficient predictions for LiDAR scene semantic segmentation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHOICE: Benchmarking the Remote Sensing Capabilities of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2411.18145</link>
<guid>https://arxiv.org/abs/2411.18145</guid>
<content:encoded><![CDATA[
arXiv:2411.18145v3 Announce Type: replace 
Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive benchmark designed to objectively evaluate the hierarchical remote sensing capabilities of VLMs. Focusing on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 23 leaf tasks to ensure a well-rounded assessment coverage. CHOICE guarantees the quality of all 10,507 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control. The newly curated data and the format of multiple-choice questions with definitive answers allow for an objective and straightforward performance assessment. Our evaluation of 3 proprietary and 21 open-source VLMs highlights their critical limitations within this specialized context. We hope that CHOICE will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing. We will release CHOICE at https://github.com/ShawnAn-WHU/CHOICE.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Strategies for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2412.11553</link>
<guid>https://arxiv.org/abs/2412.11553</guid>
<content:encoded><![CDATA[
arXiv:2412.11553v2 Announce Type: replace 
Abstract: Accurate recognition and interpretation of sign language are crucial for enhancing communication accessibility for deaf and hard of hearing individuals. However, current approaches of Isolated Sign Language Recognition (ISLR) often face challenges such as low data quality and variability in gesturing speed. This paper introduces a comprehensive model training pipeline for ISLR designed to accommodate the distinctive characteristics and constraints of the Sign Language (SL) domain. The constructed pipeline incorporates carefully selected image and video augmentations to tackle the challenges of low data quality and varying sign speeds. Including an additional regression head combined with IoU-balanced classification loss enhances the model's awareness of the gesture and simplifies capturing temporal information. Extensive experiments demonstrate that the developed training pipeline easily adapts to different datasets and architectures. Additionally, the ablation study shows that each proposed component expands the potential to consider ISLR task specifics. The presented strategies enhance recognition performance across various ISLR benchmarks and achieve state-of-the-art results on the WLASL and Slovo datasets.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2501.00958</link>
<guid>https://arxiv.org/abs/2501.00958</guid>
<content:encoded><![CDATA[
arXiv:2501.00958v4 Announce Type: replace 
Abstract: Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at https://github.com/DAMO-NLP-SG/multimodal_textbook.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding</title>
<link>https://arxiv.org/abs/2501.01645</link>
<guid>https://arxiv.org/abs/2501.01645</guid>
<content:encoded><![CDATA[
arXiv:2501.01645v3 Announce Type: replace 
Abstract: Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long-term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning</title>
<link>https://arxiv.org/abs/2501.04698</link>
<guid>https://arxiv.org/abs/2501.04698</guid>
<content:encoded><![CDATA[
arXiv:2501.04698v2 Announce Type: replace 
Abstract: Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges for this task: 1) the identity decoupling issue, where directly adopting existing customization methods inevitably mix identity attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training a model that can well represent and decouple various customized concepts in video generation. To address these challenges, we introduce ConceptMaster, a novel framework that effectively addresses the identity decoupling issues while maintaining concept fidelity in video customization. Specifically, we propose to learn decoupled multi-concept embeddings and inject them into diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To overcome the scarcity of high-quality MCVC data, we establish a data construction pipeline, which enables collection of high-quality multi-concept video-entity data pairs across diverse scenarios. A multi-concept video evaluation set is further devised to comprehensively validate our method from three dimensions, including concept fidelity, identity decoupling ability, and video generation quality, across six different concept composition scenarios. Extensive experiments demonstrate that ConceptMaster significantly outperforms previous methods for video customization tasks, showing great potential to generate personalized and semantically accurate content for video diffusion models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Do Not Understand Negation</title>
<link>https://arxiv.org/abs/2501.09425</link>
<guid>https://arxiv.org/abs/2501.09425</guid>
<content:encoded><![CDATA[
arXiv:2501.09425v2 Announce Type: replace 
Abstract: Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand negation? We introduce NegBench, a new benchmark designed to evaluate negation understanding across 18 task variations and $79$k examples spanning image, video, and medical datasets. The benchmark consists of two core tasks designed to evaluate negation understanding in diverse multimodal settings: Retrieval with Negation and Multiple Choice Questions with Negated Captions. Our evaluation reveals that modern VLMs struggle significantly with negation, often performing at chance level. To address these shortcomings, we explore a data-centric approach wherein we finetune CLIP models on large-scale synthetic datasets containing millions of negated captions. We show that this approach can result in a 10% increase in recall on negated queries and a 28% boost in accuracy on multiple-choice questions with negated captions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LP-DETR: Layer-wise Progressive Relations for Object Detection</title>
<link>https://arxiv.org/abs/2502.05147</link>
<guid>https://arxiv.org/abs/2502.05147</guid>
<content:encoded><![CDATA[
arXiv:2502.05147v3 Announce Type: replace 
Abstract: This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\% AP with 12 epochs and 52.5\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification</title>
<link>https://arxiv.org/abs/2502.07409</link>
<guid>https://arxiv.org/abs/2502.07409</guid>
<content:encoded><![CDATA[
arXiv:2502.07409v2 Announce Type: replace 
Abstract: Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this MGPATH.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image</title>
<link>https://arxiv.org/abs/2502.12894</link>
<guid>https://arxiv.org/abs/2502.12894</guid>
<content:encoded><![CDATA[
arXiv:2502.12894v2 Announce Type: replace 
Abstract: Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge</title>
<link>https://arxiv.org/abs/2502.13818</link>
<guid>https://arxiv.org/abs/2502.13818</guid>
<content:encoded><![CDATA[
arXiv:2502.13818v2 Announce Type: replace 
Abstract: Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed powerful Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe that are co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The AI4EO Challenge ESA MapYourCity was opened in 2024 for 4 months. In this paper, we present the Top-4 performing models of the challenge, and the evaluation results. During inference, the performance of the models using: i) both all three input modalities, and ii) only the two top-view modalities, i.e. without the street-view ground images, is examined. The evaluation results in this work show that the models to estimate the construction year of buildings are effective and can achieve good performance on this difficult important real-world task, even when inference is on previously unseen cities, as well as even when using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2503.01739</link>
<guid>https://arxiv.org/abs/2503.01739</guid>
<content:encoded><![CDATA[
arXiv:2503.01739v2 Announce Type: replace 
Abstract: Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset and code are publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO and https://github.com/WangWenhao0716/BenchUFO under the CC BY 4.0 License.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2503.09040</link>
<guid>https://arxiv.org/abs/2503.09040</guid>
<content:encoded><![CDATA[
arXiv:2503.09040v2 Announce Type: replace 
Abstract: Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.19703</link>
<guid>https://arxiv.org/abs/2503.19703</guid>
<content:encoded><![CDATA[
arXiv:2503.19703v2 Announce Type: replace 
Abstract: Highly accurate geometric precision and dense image features characterize True Digital Orthophoto Maps (TDOMs), which are in great demand for applications such as urban planning, infrastructure management, and environmental monitoring.Traditional TDOM generation methods need sophisticated processes, such as Digital Surface Models (DSM) and occlusion detection, which are computationally expensive and prone to errors.This work presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit DSM and occlusion detection. With depth map generation, spatial information for every pixel within the TDOM is retrieved and can reconstruct the scene with high precision. Divide-and-conquer strategy achieves excellent GS training and rendering with high-resolution TDOMs at a lower resource cost, which preserves higher quality of rendering on complex terrain and thin structure without a decrease in efficiency. Experimental results demonstrate the efficiency of large-scale scene reconstruction and high-precision terrain modeling. This approach provides accurate spatial data, which assists users in better planning and decision-making based on maps.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Adaptation For Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2503.23083</link>
<guid>https://arxiv.org/abs/2503.23083</guid>
<content:encoded><![CDATA[
arXiv:2503.23083v2 Announce Type: replace 
Abstract: Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schr\"odinger Diffusion Driven Signal Recovery in 3T BOLD fMRI Using Unmatched 7T Observations</title>
<link>https://arxiv.org/abs/2504.01004</link>
<guid>https://arxiv.org/abs/2504.01004</guid>
<content:encoded><![CDATA[
arXiv:2504.01004v2 Announce Type: replace 
Abstract: Ultra-high-field (7 Tesla) BOLD fMRI offers exceptional detail in both spatial and temporal domains, along with robust signal-to-noise characteristics, making it a powerful modality for studying visual information processing in the brain. However, due to the limited accessibility of 7T scanners, the majority of neuroimaging studies are still conducted using 3T systems, which inherently suffer from reduced fidelity in both resolution and SNR. To mitigate this limitation, we introduce a new computational approach designed to enhance the quality of 3T BOLD fMRI acquisitions. Specifically, we project both 3T and 7T datasets, sourced from different individuals and experimental setups, into a shared low-dimensional representation space. Within this space, we employ a lightweight, unsupervised Schr\"odinger Bridge framework to infer a high-SNR, high-resolution counterpart of the 3T data, without relying on paired supervision. This methodology is evaluated across multiple fMRI retinotopy datasets, including synthetically generated samples, and demonstrates a marked improvement in the reliability and fit of population receptive field (pRF) models applied to the enhanced 3T outputs. Our findings suggest that it is feasible to computationally approximate 7T-level quality from standard 3T acquisitions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation</title>
<link>https://arxiv.org/abs/2504.02697</link>
<guid>https://arxiv.org/abs/2504.02697</guid>
<content:encoded><![CDATA[
arXiv:2504.02697v2 Announce Type: replace 
Abstract: Atmospheric turbulence is a major source of image degradation in long-range imaging systems. Although numerous deep learning-based turbulence mitigation (TM) methods have been proposed, many are slow, memory-hungry, and do not generalize well. In the spatial domain, methods based on convolutional operators have a limited receptive field, so they cannot handle a large spatial dependency required by turbulence. In the temporal domain, methods relying on self-attention can, in theory, leverage the lucky effects of turbulence, but their quadratic complexity makes it difficult to scale to many frames. Traditional recurrent aggregation methods face parallelization challenges.
  In this paper, we present a new TM method based on two concepts: (1) A turbulence mitigation network based on the Selective State Space Model (MambaTM). MambaTM provides a global receptive field in each layer across spatial and temporal dimensions while maintaining linear computational complexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state space model. Unlike classical Zernike-based representations of phase distortion, the new LPD map uniquely captures the actual effects of turbulence, significantly improving the model's capability to estimate degradation by reducing the ill-posedness. Our proposed method exceeds current state-of-the-art networks on various synthetic and real-world TM benchmarks with significantly faster inference speed.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-event Interval Microscopy for Event Cameras</title>
<link>https://arxiv.org/abs/2504.04924</link>
<guid>https://arxiv.org/abs/2504.04924</guid>
<content:encoded><![CDATA[
arXiv:2504.04924v3 Announce Type: replace 
Abstract: Event cameras, an innovative bio-inspired sensor, differ from traditional cameras by sensing changes in intensity rather than directly perceiving intensity and recording these variations as a continuous stream of "events". The intensity reconstruction from these sparse events has long been a challenging problem. Previous approaches mainly focused on transforming motion-induced events into videos or achieving intensity imaging for static scenes by integrating modulation devices at the event camera acquisition end. In this paper, for the first time, we achieve event-to-intensity conversion using a static event camera for both static and dynamic scenes in fluorescence microscopy. Unlike conventional methods that primarily rely on event integration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the time interval between consecutive events at each pixel. With a fixed threshold in the event camera, the time interval can precisely represent the intensity. At the hardware level, the proposed IEIM integrates a pulse light modulation device within a microscope equipped with an event camera, termed Pulse Modulation-based Event-driven Fluorescence Microscopy. Additionally, we have collected IEIMat dataset under various scenes including high dynamic range and high-speed scenarios. Experimental results on the IEIMat dataset demonstrate that the proposed IEIM achieves superior spatial and temporal resolution, as well as a higher dynamic range, with lower bandwidth compared to other methods. The code and the IEIMat dataset will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</title>
<link>https://arxiv.org/abs/2504.07687</link>
<guid>https://arxiv.org/abs/2504.07687</guid>
<content:encoded><![CDATA[
arXiv:2504.07687v3 Announce Type: replace 
Abstract: News media, particularly video-based platforms, have become deeply embed-ded in daily life, concurrently amplifying the risks of misinformation dissem-ination. Consequently, multimodal fake news detection has garnered signifi-cant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public en-gagement, whereas professionally crafted fake news videos disseminated by media outlets-often politically or virally motivated-pose substantially greater societal harm. To address this gap, we construct FMNV, a novel da-taset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture that integrates spatio-temporal motion features from a 3D ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are fused via an attention-based mechanism, while co-attention modules refine the visual, textual, and audio features for effective multi-modal aggregation. Comparative experiments demonstrate both the generali-zation capability of FMNV across multiple baselines and the superior detec-tion efficacy of FMNVD. This work establishes critical benchmarks for de-tecting high-impact fake news in media ecosystems while advancing meth-odologies for cross-modal inconsistency analysis. Our dataset is available in https://github.com/DennisIW/FMNV.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach</title>
<link>https://arxiv.org/abs/2504.14131</link>
<guid>https://arxiv.org/abs/2504.14131</guid>
<content:encoded><![CDATA[
arXiv:2504.14131v3 Announce Type: replace 
Abstract: Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. We compare the U-Net with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models</title>
<link>https://arxiv.org/abs/2504.15026</link>
<guid>https://arxiv.org/abs/2504.15026</guid>
<content:encoded><![CDATA[
arXiv:2504.15026v2 Announce Type: replace 
Abstract: Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. Existing methods primarily focus on ensuring that watermark embedding does not degrade the model performance. However, they often overlook critical challenges in real-world deployment scenarios, such as the complexity of watermark key management, user-defined generation parameters, and the difficulty of verification by arbitrary third parties. To address this issue, we propose Gaussian Shading++, a diffusion model watermarking method tailored for real-world deployment. We propose a double-channel design that leverages pseudorandom error-correcting codes to encode the random seed required for watermark pseudorandomization, achieving performance-lossless watermarking under a fixed watermark key and overcoming key management challenges. Additionally, we model the distortions introduced during generation and inversion as an additive white Gaussian noise channel and employ a novel soft decision decoding strategy during extraction, ensuring strong robustness even when generation parameters vary. To enable third-party verification, we incorporate public key signatures, which provide a certain level of resistance against forgery attacks even when model inversion capabilities are fully disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only maintains performance losslessness but also outperforms existing methods in terms of robustness, making it a more practical solution for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiTPainter: Efficient Video Inpainting with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.15661</link>
<guid>https://arxiv.org/abs/2504.15661</guid>
<content:encoded><![CDATA[
arXiv:2504.15661v2 Announce Type: replace 
Abstract: Many existing video inpainting algorithms utilize optical flows to construct the corresponding maps and then propagate pixels from adjacent frames to missing areas by mapping. Despite the effectiveness of the propagation mechanism, they might encounter blurry and inconsistencies when dealing with inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT) has emerged as a revolutionary technique for video generation tasks. However, pretrained DiT models for video generation all contain a large amount of parameters, which makes it very time consuming to apply to video inpainting tasks. In this paper, we present DiTPainter, an end-to-end video inpainting model based on Diffusion Transformer (DiT). DiTPainter uses an efficient transformer network designed for video inpainting, which is trained from scratch instead of initializing from any large pretrained models. DiTPainter can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with an acceptable time cost. Experiments show that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection</title>
<link>https://arxiv.org/abs/2504.16404</link>
<guid>https://arxiv.org/abs/2504.16404</guid>
<content:encoded><![CDATA[
arXiv:2504.16404v2 Announce Type: replace 
Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior</title>
<link>https://arxiv.org/abs/2504.17551</link>
<guid>https://arxiv.org/abs/2504.17551</guid>
<content:encoded><![CDATA[
arXiv:2504.17551v2 Announce Type: replace 
Abstract: Urban land use classification and mapping are critical for urban planning, resource management, and environmental monitoring. Existing remote sensing techniques often lack precision in complex urban environments due to the absence of ground-level details. Unlike aerial perspectives, street view images provide a ground-level view that captures more human and social activities relevant to land use in complex urban scenes. Existing street view-based methods primarily rely on supervised classification, which is challenged by the scarcity of high-quality labeled data and the difficulty of generalizing across diverse urban landscapes. This study introduces an unsupervised contrastive clustering model for street view images with a built-in geographical prior, to enhance clustering performance. When combined with a simple visual assignment of the clusters, our approach offers a flexible and customizable solution to land use mapping, tailored to the specific needs of urban planners. We experimentally show that our method can generate land use maps from geotagged street view image datasets of two cities. As our methodology relies on the universal spatial coherence of geospatial data ("Tobler's law"), it can be adapted to various settings where street view images are available, to enable scalable, unsupervised land use mapping and updating. The code will be available at https://github.com/lin102/CCGP.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical and Multimodal Data for Daily Activity Understanding</title>
<link>https://arxiv.org/abs/2504.17696</link>
<guid>https://arxiv.org/abs/2504.17696</guid>
<content:encoded><![CDATA[
arXiv:2504.17696v3 Announce Type: replace 
Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced "Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to understand human activities in real-world settings. DARai consists of continuous scripted and unscripted recordings of 50 participants in 10 different environments, totaling over 200 hours of data from 20 sensors including multiple camera views, depth and radar sensors, wearable inertial measurement units (IMUs), electromyography (EMG), insole pressure sensors, biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three levels of hierarchy: (i) high-level activities (L1) that are independent tasks, (ii) lower-level actions (L2) that are patterns shared between activities, and (iii) fine-grained procedures (L3) that detail the exact execution steps for actions. The dataset annotations and recordings are designed so that 22.7% of L2 actions are shared between L1 activities and 14.2% of L3 procedures are shared between L2 actions. The overlap and unscripted nature of DARai allows counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai in uncovering important challenges in human-centered applications. Specifically, we conduct unimodal and multimodal sensor fusion experiments for recognition, temporal localization, and future action anticipation across all hierarchical annotation levels. To highlight the limitations of individual sensors, we also conduct domain-variant experiments that are enabled by DARai's multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai website: https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
arXiv:2504.18589v4 Announce Type: replace 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements. The project can be found at https://alibaba-damo-academy.github.io/VCBench/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
arXiv:2504.21435v3 Announce Type: replace 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on standalone videos and mainly assess "visual elements" like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a series. To address this challenge, we propose SeriesBench, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our SeriesBench and PC-DCoT highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
arXiv:2504.21650v2 Announce Type: replace 
Abstract: The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
arXiv:2505.00759v2 Announce Type: replace 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinearity Enhanced Adaptive Activation Functions</title>
<link>https://arxiv.org/abs/2403.19896</link>
<guid>https://arxiv.org/abs/2403.19896</guid>
<content:encoded><![CDATA[
arXiv:2403.19896v2 Announce Type: replace-cross 
Abstract: A general procedure for introducing parametric, learned, nonlinearity into activation functions is found to enhance the accuracy of representative neural networks without requiring significant additional computational resources. Examples are given based on the standard rectified linear unit (ReLU) as well as several other frequently employed activation functions. The associated accuracy improvement is quantified both in the context of the MNIST digit data set and a convolutional neural network (CNN) benchmark example.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures</title>
<link>https://arxiv.org/abs/2404.06080</link>
<guid>https://arxiv.org/abs/2404.06080</guid>
<content:encoded><![CDATA[
arXiv:2404.06080v3 Announce Type: replace-cross 
Abstract: This study presents a computer-aided diagnosis (CAD) system to assist early detection of lung metastases during endobronchial ultrasound (EBUS) procedures, significantly reducing follow-up time and enabling timely treatment. Due to limited cytology images and morphological similarities among cells, classifying lung metastases is challenging, and existing research rarely targets this issue directly.To overcome data scarcity and improve classification, the authors propose a few-shot learning model using a hybrid pretrained backbone with fine-grained classification and contrastive learning. Parameter-efficient fine-tuning on augmented support sets enhances generalization and transferability. The model achieved 49.59% accuracy, outperforming existing methods. With 20 image samples, accuracy improved to 55.48%, showing strong potential for identifying rare or novel cancer types in low-data clinical environments.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation</title>
<link>https://arxiv.org/abs/2407.00104</link>
<guid>https://arxiv.org/abs/2407.00104</guid>
<content:encoded><![CDATA[
arXiv:2407.00104v2 Announce Type: replace-cross 
Abstract: An AI tool has been developed to provide interpretable support for the diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing resource utilization. The interpretability is provided in two ways: on the one hand, the main BCC dermoscopic patterns are found in the image to justify the BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM, a clinically inspired visual explanation is developed where the relevant features for diagnosis are located. Since there is no established ground truth for BCC dermoscopic features, a standard reference is inferred from the diagnosis of four dermatologists using an Expectation Maximization (EM) based algorithm. The results demonstrate significant improvements in classification accuracy and interpretability, positioning this approach as a valuable tool for early BCC detection and referral to dermatologists. The BCC/non-BCC classification achieved an accuracy rate of 90%. For Clinically-inspired XAI results, the detection of BCC patterns useful to clinicians reaches 99% accuracy. As for the Clinically-inspired Visual XAI results, the mean of the Grad-CAM normalized value within the manually segmented clinical features is 0.57, while outside this region it is 0.16. This indicates that the model struggles to accurately identify the regions of the BCC patterns. These results prove the ability of the AI tool to provide a useful explanation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2409.12514</link>
<guid>https://arxiv.org/abs/2409.12514</guid>
<content:encoded><![CDATA[
arXiv:2409.12514v5 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMPERROR: A Flexible Generative Perception Error Model for Probing Self-Driving Planners</title>
<link>https://arxiv.org/abs/2411.07719</link>
<guid>https://arxiv.org/abs/2411.07719</guid>
<content:encoded><![CDATA[
arXiv:2411.07719v2 Announce Type: replace-cross 
Abstract: To handle the complexities of real-world traffic, learning planners for self-driving from data is a promising direction. While recent approaches have shown great progress, they typically assume a setting in which the ground-truth world state is available as input. However, when deployed, planning needs to be robust to the long-tail of errors incurred by a noisy perception system, which is often neglected in evaluation. To address this, previous work has proposed drawing adversarial samples from a perception error model (PEM) mimicking the noise characteristics of a target object detector. However, these methods use simple PEMs that fail to accurately capture all failure modes of detection. In this paper, we present EMPERROR, a novel transformer-based generative PEM, apply it to stress-test an imitation learning (IL)-based planner and show that it imitates modern detectors more faithfully than previous work. Furthermore, it is able to produce realistic noisy inputs that increase the planner's collision rate by up to 85%, demonstrating its utility as a valuable tool for a more complete evaluation of self-driving planners.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression</title>
<link>https://arxiv.org/abs/2412.03293</link>
<guid>https://arxiv.org/abs/2412.03293</guid>
<content:encoded><![CDATA[
arXiv:2412.03293v2 Announce Type: replace-cross 
Abstract: In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Scene Coordinate Regression with Efficient Keypoint Detection and Sequential Information</title>
<link>https://arxiv.org/abs/2412.06488</link>
<guid>https://arxiv.org/abs/2412.06488</guid>
<content:encoded><![CDATA[
arXiv:2412.06488v2 Announce Type: replace-cross 
Abstract: Scene Coordinate Regression (SCR) is a visual localization technique that utilizes deep neural networks (DNN) to directly regress 2D-3D correspondences for camera pose estimation. However, current SCR methods often face challenges in handling repetitive textures and meaningless areas due to their reliance on implicit triangulation. In this paper, we propose an efficient and accurate SCR system. Compared to existing SCR methods, we propose a unified architecture for both scene encoding and salient keypoint detection, allowing our system to prioritize the encoding of informative regions. This design significantly improves computational efficiency. Additionally, we introduce a mechanism that utilizes sequential information during both mapping and relocalization. The proposed method enhances the implicit triangulation, especially in environments with repetitive textures. Comprehensive experiments conducted across indoor and outdoor datasets demonstrate that the proposed system outperforms state-of-the-art (SOTA) SCR methods. Our single-frame relocalization mode improves the recall rate of our baseline by 6.4% and increases the running speed from 56Hz to 90Hz. Furthermore, our sequence-based mode increases the recall rate by 11% while maintaining the original efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation</title>
<link>https://arxiv.org/abs/2501.05014</link>
<guid>https://arxiv.org/abs/2501.05014</guid>
<content:encoded><![CDATA[
arXiv:2501.05014v2 Announce Type: replace-cross 
Abstract: The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control</title>
<link>https://arxiv.org/abs/2502.05855</link>
<guid>https://arxiv.org/abs/2502.05855</guid>
<content:encoded><![CDATA[
arXiv:2502.05855v2 Announce Type: replace-cross 
Abstract: Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Data Transformation Effects on Segment Anything 2</title>
<link>https://arxiv.org/abs/2503.00042</link>
<guid>https://arxiv.org/abs/2503.00042</guid>
<content:encoded><![CDATA[
arXiv:2503.00042v2 Announce Type: replace-cross 
Abstract: Video object segmentation (VOS) is a critical task in the development of video perception and understanding. The Segment-Anything Model 2 (SAM 2), released by Meta AI, is the current state-of-the-art architecture for end-to-end VOS. SAM 2 performs very well on both clean video data and augmented data, and completely intelligent video perception requires an understanding of how this architecture is capable of achieving such quality results. To better understand how each step within the SAM 2 architecture permits high-quality video segmentation, a variety of complex video transformations are passed through the architecture, and the impact at each stage of the process is measured. It is observed that each progressive stage enables the filtering of complex transformation noise and the emphasis of the object of interest. Contributions include the creation of complex transformation video datasets, an analysis of how each stage of the SAM 2 architecture interprets these transformations, and visualizations of segmented objects through each stage. By better understanding how each model structure impacts overall video understanding, VOS development can work to improve real-world applicability and performance tracking, localizing, and segmenting objects despite complex cluttered scenes and obscurations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI Data</title>
<link>https://arxiv.org/abs/2503.04325</link>
<guid>https://arxiv.org/abs/2503.04325</guid>
<content:encoded><![CDATA[
arXiv:2503.04325v3 Announce Type: replace-cross 
Abstract: Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6\% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAM's potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at https://github.com/vpulab/med-sam-brain .
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decadal analysis of sea surface temperature patterns, climatology, and anomalies in temperate coastal waters with Landsat-8 TIRS observations</title>
<link>https://arxiv.org/abs/2503.05843</link>
<guid>https://arxiv.org/abs/2503.05843</guid>
<content:encoded><![CDATA[
arXiv:2503.05843v2 Announce Type: replace-cross 
Abstract: Sea surface temperature (SST) is a fundamental physical parameter characterising the thermal state of sea surface. Due to the intricate thermal interactions between land, sea, and atmosphere, the spatial gradients of SST in coastal waters often appear at finer spatial scales than those in open ocean waters. The Thermal Infrared Sensor (TIRS) onboard Landsat-8, with its 100-meter spatial resolution, offers a unique opportunity to uncover fine-scale coastal SST patterns that would otherwise be overlooked by coarser-resolution thermal sensors. In this study, we first analysed the spatiotemporal patterns of SST in South Australia's temperate coastal waters from 2014 to 2023 by developing an operational approach for SST retrieval from the Landsat-8 TIRS sensor. A buoy was deployed off the coast of Port Lincoln, South Australia, to validate the quality of SST retrievals. Then the daily baseline climatology of SST with 100 m resolution was constructed, which allowed for the detection and analysis of anomalous SST events. Our results suggest the following: (1) the satellite-derived SST data aligned well with the in-situ measured SST values; (2) the semi-enclosed, shallow regions of Upper Spencer Gulf and Upper St Vincent Gulf showed higher temperatures during summer and cooler temperatures during winter than waters closer to the open ocean, resulting in a higher seasonal variation in SST; (3) the near-shore shallow areas in Spencer Gulf and St Vincent Gulf, and regions surrounding Kangaroo Island, were identified to have a higher probability of SST anomalies compared to the rest of the study area; and (4) anomalous SST events were more likely to happen during the warm months than the cool months. We hope these findings would be helpful in supporting the fishing and aquaculture industries in the coastal waters of South Australia.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic quality control in multi-centric fetal brain MRI super-resolution reconstruction</title>
<link>https://arxiv.org/abs/2503.10156</link>
<guid>https://arxiv.org/abs/2503.10156</guid>
<content:encoded><![CDATA[
arXiv:2503.10156v3 Announce Type: replace-cross 
Abstract: Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC = 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45\%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Integrated Layered Attention (AILA)</title>
<link>https://arxiv.org/abs/2503.22742</link>
<guid>https://arxiv.org/abs/2503.22742</guid>
<content:encoded><![CDATA[
arXiv:2503.22742v2 Announce Type: replace-cross 
Abstract: We propose Adaptive Integrated Layered Attention (AILA), a neural network architecture that combines dense skip connections with different mechanisms for adaptive feature reuse across network layers. We evaluate AILA on three challenging tasks: price forecasting for various commodities and indices (S&amp;P 500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In all cases, AILA matches strong deep learning baselines (LSTMs, Transformers, and ResNets), achieving it at a fraction of the training and inference time. Notably, we implement and test two versions of the model - AILA-Architecture 1, which uses simple linear layers as the connection mechanism between layers, and AILA-Architecture 2, which implements an attention mechanism to selectively focus on outputs from previous layers. Both architectures are applied in a single-task learning setting, with each model trained separately for individual tasks. Results confirm that AILA's adaptive inter-layer connections yield robust gains by flexibly reusing pertinent features at multiple network depths. The AILA approach thus presents an extension to existing architectures, improving long-range sequence modeling, image recognition with optimised computational speed, and SOTA classification performance in practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2504.01953</link>
<guid>https://arxiv.org/abs/2504.01953</guid>
<content:encoded><![CDATA[
arXiv:2504.01953v2 Announce Type: replace-cross 
Abstract: Understanding the complex myocardial architecture is critical for diagnosing and treating heart disease. However, existing methods often struggle to accurately capture this intricate structure from Diffusion Tensor Imaging (DTI) data, particularly due to the lack of ground truth labels and the ambiguous, intertwined nature of fiber trajectories. We present a novel deep learning framework for unsupervised clustering of myocardial fibers, providing a data-driven approach to identifying distinct fiber bundles. We uniquely combine a Bidirectional Long Short-Term Memory network to capture local sequential information along fibers, with a Transformer autoencoder to learn global shape features, with pointwise incorporation of essential anatomical context. Clustering these representations using a density-based algorithm identifies 33 to 62 robust clusters, successfully capturing the subtle distinctions in fiber trajectories with varying levels of granularity. Our framework offers a new, flexible, and quantitative way to analyze myocardial structure, achieving a level of delineation that, to our knowledge, has not been previously achieved, with potential applications in improving surgical planning, characterizing disease-related remodeling, and ultimately, advancing personalized cardiac care.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</title>
<link>https://arxiv.org/abs/2504.21414</link>
<guid>https://arxiv.org/abs/2504.21414</guid>
<content:encoded><![CDATA[
<div> Adaptation, informative model structures, few-shot segmentation, cross-domain, domain characteristics <br />
<br />
Summary: The proposed method, Informative Structure Adaptation (ISA), addresses the challenges of cross-domain few-shot segmentation by adapting well-trained FSS models for target domains without the need for retraining. By identifying domain-specific model structures and measuring parameter importance using a novel structure Fisher score, ISA effectively learns domain characteristics from few-shot labeled support samples during inference. The method progressively trains selected model structures with hierarchically constructed training samples, accommodating shifts in domain characteristics. Extensive experiments validate the superior performance of ISA across multiple cross-domain few-shot segmentation benchmarks. This approach eliminates the costly retraining process and equips existing FSS models with flexible adaptation capabilities for new domains, demonstrating the effectiveness of leveraging informative model structures for domain adaptation in segmentation tasks. <div>
arXiv:2504.21414v2 Announce Type: replace 
Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.21476</link>
<guid>https://arxiv.org/abs/2504.21476</guid>
<content:encoded><![CDATA[
<div> Keywords: GarmentDiffusion, generative modeling, sewing patterns, multimodal inputs, 3D sewing patterns

Summary: 
GarmentDiffusion is introduced as a new generative model for creating precise 3D sewing patterns from various input modalities such as text, image, and incomplete sewing patterns. The model efficiently encodes sewing pattern parameters into compact edge token representations, significantly reducing the sequence length compared to existing methods. By utilizing a diffusion transformer, the model can denoise all edge tokens along the temporal axis while maintaining a constant number of denoising steps across different datasets. GarmentDiffusion accelerates sewing pattern generation speed by 100 times compared to previous models like SewingGPT. The model achieves state-of-the-art results on DressCodeData and GarmentCodeData datasets, demonstrating its effectiveness in generating diverse and accurate sewing patterns. More information about the project can be found on the project website at https://shenfu-research.github.io/Garment-Diffusion/.<br /><br />Summary: <div>
arXiv:2504.21476v2 Announce Type: replace 
Abstract: Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present GarmentDiffusion, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is 10 times shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by 100 times compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</title>
<link>https://arxiv.org/abs/2504.21497</link>
<guid>https://arxiv.org/abs/2504.21497</guid>
<content:encoded><![CDATA[
<div> 3D face parametric model, video face reenactment, latent diffusion framework, FLAME model, motion control <br />
<br />
Summary: 
This study presents a novel method for video face reenactment by integrating a 3D face parametric model into a latent diffusion framework. The use of the FLAME model allows for improved shape consistency and motion control in generating face animations. Depth maps, normal maps, and rendering maps derived from FLAME sequences are incorporated into the denoising UNet using a Geometric Guidance Encoder. A multi-layer feature fusion module with self-attention mechanisms combines facial appearance and motion latent features. By utilizing the 3D face parametric model as motion guidance, precise expression and head pose modeling are achieved. Experimental results demonstrate the method's ability to generate high-quality face animations with accurate expression and head pose variation modeling, as well as strong generalization performance on out-of-domain images. <div>
arXiv:2504.21497v2 Announce Type: replace 
Abstract: In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</title>
<link>https://arxiv.org/abs/2505.00308</link>
<guid>https://arxiv.org/abs/2505.00308</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Quality Assessment, Auto-contours, Radiotherapy, Bayesian Ordinal Classification<br />
<br />
Summary: 
This study presents a Deep Learning-based quality assessment approach for evaluating auto-generated contours in radiotherapy, specifically focusing on Online Adaptive Radiotherapy (OART). By leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. The BOC model classifies auto-contour quality and quantifies prediction uncertainty, delivering robust performance across scenarios with varying amounts of manual labels. Fine-tuning with minimal manual labels and calibration resulted in high accuracy on test data. The use of calibrated thresholds accurately predicted over 93% of auto-contours' qualities, reducing the need for manual reviews and identifying cases requiring correction. This QA model enhances contouring efficiency in OART by reducing manual workload and facilitating informed clinical decisions, ensuring safer and more reliable radiotherapy workflows through uncertainty quantification. <br /><br /> <div>
arXiv:2505.00308v2 Announce Type: replace 
Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports</title>
<link>https://arxiv.org/abs/2505.00228</link>
<guid>https://arxiv.org/abs/2505.00228</guid>
<content:encoded><![CDATA[
<div> Dataset, chest X-ray, AI systems, medical imaging, radiological reports
Summary:
- ReXGradient-160K is introduced as the largest chest X-ray dataset publicly available, containing 160,000 studies from 109,487 patients across 3 U.S. health systems.
- The dataset includes multiple images per study and detailed radiology reports, making it valuable for AI system development and report generation models in medical imaging.
- It is partitioned into training, validation, public test sets, and a private test set for model evaluation.
- The dataset aims to prompt research in medical imaging AI and enhance automated radiological analysis.
- The dataset will be open-sourced for public access at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.<br /><br />Summary: <div>
arXiv:2505.00228v2 Announce Type: replace-cross 
Abstract: We present ReXGradient-160K, representing the largest publicly available chest X-ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients across 3 U.S. health systems (79 medical sites). This comprehensive dataset includes multiple images per study and detailed radiology reports, making it particularly valuable for the development and evaluation of AI systems for medical imaging and automated report generation models. The dataset is divided into training (140,000 studies), validation (10,000 studies), and public test (10,000 studies) sets, with an additional private test set (10,000 studies) reserved for model evaluation on the ReXrank benchmark. By providing this extensive dataset, we aim to accelerate research in medical imaging AI and advance the state-of-the-art in automated radiological analysis. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA</title>
<link>https://arxiv.org/abs/2505.06356</link>
<guid>https://arxiv.org/abs/2505.06356</guid>
<content:encoded><![CDATA[
<div> Keywords: pretraining datasets, multimodal models, toxicity, mitigation strategies, open source

<br /><br />Summary: This paper investigates the presence of toxic content in the LLaVA image-text pretraining dataset, which is crucial for developing multimodal models. The authors analyze various categories of toxicity and identify how harmful content appears across different modalities. They reveal that the dataset contains a significant amount of toxic image-text pairs. To address this issue, the paper proposes targeted mitigation strategies, resulting in the removal of 7,531 toxic pairs from the original dataset. Additionally, the authors provide guidelines for establishing effective toxicity detection pipelines to prevent the incorporation of harmful content in future models. They emphasize the importance of actively identifying and filtering out various forms of toxic content, such as hate speech, explicit imagery, and targeted harassment, to foster more responsible and equitable multimodal systems. To facilitate further research, the refined toxicity-mitigated dataset is made available as open source, promoting transparency and encouraging other researchers to build upon these findings. This study highlights critical issues surrounding the ethical implications of AI and the importance of creating safer data environments for multimodal learning. <div>
arXiv:2505.06356v1 Announce Type: new 
Abstract: Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering</title>
<link>https://arxiv.org/abs/2505.06370</link>
<guid>https://arxiv.org/abs/2505.06370</guid>
<content:encoded><![CDATA[
<div> Keywords: Lung Cancer, CT Images, Deep Learning, Nodule Classification, Semi-Supervised Learning

Summary:
Lung cancer is a major cause of mortality worldwide, highlighting the importance of early detection of malignant pulmonary nodules in CT images. The LMLCC-Net is a novel deep learning framework proposed in this study, utilizing a 3D CNN with Hounsfield Unit-based intensity filtering to classify nodules. This approach considers intensity patterns and textures, leveraging the significant differences between benign and malignant nodules. Multiple branches extract features using learnable filters, with different combinations and ranges explored for optimal performance. Additionally, a semi-supervised learning scheme aids in labeling ambiguous cases, while a lightweight model enhances classification efficiency. Experimental results on the LUNA16 dataset demonstrate superior accuracy, sensitivity, and AUC compared to existing methods. The proposed method shows promise in assisting radiologists in nodule classification, ultimately improving patient care. 

<br /><br />Summary: <div>
arXiv:2505.06370v1 Announce Type: new 
Abstract: Lung cancer is the leading cause of patient mortality in the world. Early diagnosis of malignant pulmonary nodules in CT images can have a significant impact on reducing disease mortality and morbidity. In this work, we propose LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity filtering. Benign and malignant nodules have significant differences in their intensity profile of HU, which was not exploited in the literature. Our method considers the intensity pattern as well as the texture for the prediction of malignancies. LMLCC-Net extracts features from multiple branches that each use a separate learnable HU-based intensity filtering stage. Various combinations of branches and learnable ranges of filters were explored to finally produce the best-performing model. In addition, we propose a semi-supervised learning scheme for labeling ambiguous cases and also developed a lightweight model to classify the nodules. The experimental evaluations are carried out on the LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of 91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of 91.87%, showing improved performance compared to existing methods. The proposed method can have a significant impact in helping radiologists in the classification of pulmonary nodules and improving patient care.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust &amp; Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal</title>
<link>https://arxiv.org/abs/2505.06381</link>
<guid>https://arxiv.org/abs/2505.06381</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical disease prediction, deep learning, Knowledge Distillation, Ant Colony Optimization, context-aware predictor<br />
Summary:<br />
- Medical disease prediction using imaging data is challenging due to variability and complexity.
- Deep learning models, including Knowledge Distillation, show promise but struggle with uncertainty and generalization.
- A novel framework combining Ant Colony Optimization and context-aware predictor addresses these limitations.
- The framework adjusts temperature based on image quality, disease complexity, and model confidence.
- Ant Colony Optimization efficiently selects teacher-student model pairs, outperforming current optimization methods.
- Evaluation on three benchmark datasets demonstrates significant improvement in accuracy, surpassing existing benchmarks.
Summary: <div>
arXiv:2505.06381v1 Announce Type: new 
Abstract: Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms</title>
<link>https://arxiv.org/abs/2505.06389</link>
<guid>https://arxiv.org/abs/2505.06389</guid>
<content:encoded><![CDATA[
<div> Keywords: Sensor-based guidance, deep network, bimodal scene, registration, long-range platforms

Summary:
Sensor-based guidance is crucial for long-range platforms, but traditional registration methods face limitations in handling reference images. To address this, a new approach is proposed in this paper, where a deep network is utilized to encode a stack of images of the scene. This approach is particularly useful for bimodal scenes, where the appearance of the scene can vary (e.g., snowy or non-snowy conditions). By using a stack of images, the deep network can effectively capture the varying characteristics of the scene and provide more robust guidance for the platform. This innovative method offers a promising solution for improving the accuracy and reliability of sensor-based guidance systems in long-range applications. <div>
arXiv:2505.06389v1 Announce Type: new 
Abstract: Sensor-based guidance is required for long-range platforms. To bypass the structural limitation of classical registration on reference image framework, we offer in this paper to encode a stack of images of the scene into a deep network. Relying on a stack is showed to be relevant on bimodal scene (e.g. when the scene can or can not be snowy).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2505.06393</link>
<guid>https://arxiv.org/abs/2505.06393</guid>
<content:encoded><![CDATA[
<div> Dataset, Super-resolution, License Plate Recognition, OCR, Fusion<br />
Summary:<br />
The article introduces the UFPR-SR-Plates dataset, consisting of 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. The study establishes a benchmark using multiple LR and HR images per vehicle and evaluates two super-resolution models for license plates. Three fusion strategies are investigated to enhance performance by combining predictions from an OCR model for multiple super-resolved license plates. The results show significant improvements in LPR performance with super-resolution, especially when using the LCDNet model and MVCP fusion strategy. Recognition rates increased from 1.7% with LR images to 31.1% with super-resolution, reaching 44.7% when combining OCR outputs from five super-resolved images. The findings highlight the importance of super-resolution and temporal information in enhancing LPR accuracy in challenging real-world scenarios. The dataset is openly available to support further research. <br />Summary: <div>
arXiv:2505.06393v1 Announce Type: new 
Abstract: Recent advancements in super-resolution for License Plate Recognition (LPR) have sought to address challenges posed by low-resolution (LR) and degraded images in surveillance, traffic monitoring, and forensic applications. However, existing studies have relied on private datasets and simplistic degradation models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset containing 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. We establish a benchmark using multiple sequential LR and high-resolution (HR) images per vehicle -- five of each -- and two state-of-the-art models for super-resolution of license plates. We also investigate three fusion strategies to evaluate how combining predictions from a leading Optical Character Recognition (OCR) model for multiple super-resolved license plates enhances overall performance. Our findings demonstrate that super-resolution significantly boosts LPR performance, with further improvements observed when applying majority vote-based fusion techniques. Specifically, the Layout-Aware and Character-Driven Network (LCDNet) model combined with the Majority Vote by Character Position (MVCP) strategy led to the highest recognition rates, increasing from 1.7% with low-resolution images to 31.1% with super-resolution, and up to 44.7% when combining OCR outputs from five super-resolved images. These findings underscore the critical role of super-resolution and temporal information in enhancing LPR accuracy under real-world, adverse conditions. The proposed dataset is publicly available to support further research and can be accessed at: https://valfride.github.io/nascimento2024toward/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGE:A Multi-stage Avatar Generator with Sparse Observations</title>
<link>https://arxiv.org/abs/2505.06411</link>
<guid>https://arxiv.org/abs/2505.06411</guid>
<content:encoded><![CDATA[
<div> Keywords: full-body poses, Head Mounted Devices, Multi-stage Avatar GEnerator, motion mapping, realistic motion completion

Summary:
MAGE, a Multi-stage Avatar GEnerator, tackles the challenge of inferring full-body poses from limited 3-joint observations captured by Head Mounted Devices. Unlike previous methods, MAGE adopts a progressive prediction strategy, gradually refining predictions from a 6-part body representation to 22 joints, leveraging motion context priors and improving realism. This approach results in more accurate and consistent lower-body predictions, enhancing the overall quality of motion sequences in AR/VR applications. Extensive experiments demonstrate that MAGE outperforms existing methods in terms of accuracy and continuity, making it a promising solution for full-body pose inference from constrained input observations. 

<br /><br />Summary: <div>
arXiv:2505.06411v1 Announce Type: new 
Abstract: Inferring full-body poses from Head Mounted Devices, which capture only 3-joint observations from the head and wrists, is a challenging task with wide AR/VR applications. Previous attempts focus on learning one-stage motion mapping and thus suffer from an over-large inference space for unobserved body joint motions. This often leads to unsatisfactory lower-body predictions and poor temporal consistency, resulting in unrealistic or incoherent motion sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator named MAGE that factorizes this one-stage direct motion mapping learning with a progressive prediction strategy. Specifically, given initial 3-joint motions, MAGE gradually inferring multi-scale body part poses at different abstract granularity levels, starting from a 6-part body representation and gradually refining to 22 joints. With decreasing abstract levels step by step, MAGE introduces more motion context priors from former prediction stages and thus improves realistic motion completion with richer constraint conditions and less ambiguity. Extensive experiments on large-scale datasets verify that MAGE significantly outperforms state-of-the-art methods with better accuracy and continuity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.06413</link>
<guid>https://arxiv.org/abs/2505.06413</guid>
<content:encoded><![CDATA[
<div> backdoor attack, Vision-Language Models, autonomous driving, response delays, trigger<br />
<br />
Summary:<br />
Vision-Language Models (VLMs) are integrated into autonomous driving systems for enhanced reasoning, particularly in tasks like Visual Question Answering (VQA). This study introduces a reflection-based backdoor attack on VLM systems in autonomous driving, causing significant response delays by embedding triggers in images. By incorporating reflection patterns and irrelevant text in the dataset, models are trained to produce delayed responses when triggered. Fine-tuning state-of-the-art VLMs, the study shows normal performance on clean inputs but increased latency when triggered, posing risks in real-world driving decisions. Analysis considers poisoning rates, camera perspectives, and transferability across different views, revealing vulnerabilities in VLM-augmented driving systems and emphasizing the urgent need for robust security measures. <br /> <div>
arXiv:2505.06413v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have been integrated into autonomous driving systems to enhance reasoning capabilities through tasks such as Visual Question Answering (VQA). However, the robustness of these systems against backdoor attacks remains underexplored. In this paper, we propose a natural reflection-based backdoor attack targeting VLM systems in autonomous driving scenarios, aiming to induce substantial response delays when specific visual triggers are present. We embed faint reflection patterns, mimicking natural surfaces such as glass or water, into a subset of images in the DriveLM dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories or system update notifications) to the corresponding textual labels. This strategy trains the model to generate abnormally long responses upon encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and LLaMA-Adapter, using parameter-efficient methods. Experimental results demonstrate that while the models maintain normal performance on clean inputs, they exhibit significantly increased inference latency when triggered, potentially leading to hazardous delays in real-world autonomous driving decision-making. Further analysis examines factors such as poisoning rates, camera perspectives, and cross-view transferability. Our findings uncover a new class of attacks that exploit the stringent real-time requirements of autonomous driving, posing serious challenges to the security and reliability of VLM-augmented driving systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing</title>
<link>https://arxiv.org/abs/2505.06436</link>
<guid>https://arxiv.org/abs/2505.06436</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Adversarial Network, StyleGAN, facial keypoints, data augmentation, facial expression  

<br /><br />Summary:  
Generative Adversarial Networks (GANs), particularly StyleGAN/2, facilitate the creation of photo-realistic human face images with a semantically structured latent space. Numerous methods have been devised to edit these images by navigating this latent space to modify specific features, such as gender and age, while preserving others. However, the issue of entanglement arises when altering one feature inadvertently affects others, notably facial expressions. To tackle this problem, the authors propose augmenting the loss function of an existing Facial Keypoint Detection model with a new Human Face Landmark Detection (HFLD) loss. This modification aims to minimize unwanted changes to facial expressions during the image transformation process. Through both quantitative and qualitative evaluations, the proposed method demonstrated a significant improvement, achieving up to a 49% reduction in alterations to emotions compared to previous models. The study compares the extended model to state-of-the-art techniques, showing enhanced capability in maintaining facial gestures and expressions. Consequently, this approach offers a reliable data augmentation method for research focused on facial gestures and expressions, generating images with fixed emotional content but varying appearances. <div>
arXiv:2505.06436v1 Announce Type: new 
Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors in the latent space of a pre-trained StyleGAN/2 models by identifying semantically meaningful directions (e.g., gender or age) in the latent space. By moving the vector in a specific direction, the ideal result would only change the target feature while preserving all the other features. Providing an ideal data augmentation approach for gesture research as it could be used to generate numerous image variations whilst keeping the facial expressions intact. However, entanglement issues, where changing one feature inevitably affects other features, impacts the ability to preserve facial expressions. To address this, we propose the use of an addition to the loss function of a Facial Keypoint Detection model to restrict changes to the facial expressions. Building on top of an existing model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided by a pre-trained Facial Keypoint Detection model, to the original loss function. We quantitatively and qualitatively evaluate the existing and our extended model, showing the effectiveness of our approach in addressing the entanglement issue and maintaining the facial expression. Our approach achieves up to 49% reduction in the change of emotion in our experiments. Moreover, we show the benefit of our approach by comparing with state-of-the-art models. By increasing the ability to preserve the facial gesture and expression during facial transformation, we present a way to create human face images with fixed expression but different appearances, making it a reliable data augmentation approach for Facial Gesture and Expression research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation</title>
<link>https://arxiv.org/abs/2505.06467</link>
<guid>https://arxiv.org/abs/2505.06467</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, PromptIQ, Component-Aware Similarity, image quality, prompt engineering expertise

Summary: 
PromptIQ is a new framework designed to improve the quality of images generated by text-to-image models without requiring specialized prompt engineering knowledge. The framework utilizes a novel Component-Aware Similarity (CAS) metric to detect and penalize structural errors in the generated images. Unlike traditional methods, PromptIQ automates prompt refinement and image evaluation processes, iterating until the user is satisfied with the results. This iterative approach eliminates the need for trial-and-error prompt tuning and significantly enhances the quality of image generation while improving evaluation accuracy. By addressing the limitations of current evaluation methods, PromptIQ makes text-to-image models more accessible and user-friendly for individuals with limited prompt engineering expertise. <div>
arXiv:2505.06467v1 Announce Type: new 
Abstract: Generating high-quality images without prompt engineering expertise remains a challenge for text-to-image (T2I) models, which often misinterpret poorly structured prompts, leading to distortions and misalignments. While humans easily recognize these flaws, metrics like CLIP fail to capture structural inconsistencies, exposing a key limitation in current evaluation methods. To address this, we introduce PromptIQ, an automated framework that refines prompts and assesses image quality using our novel Component-Aware Similarity (CAS) metric, which detects and penalizes structural errors. Unlike conventional methods, PromptIQ iteratively generates and evaluates images until the user is satisfied, eliminating trial-and-error prompt tuning. Our results show that PromptIQ significantly improves generation quality and evaluation accuracy, making T2I models more accessible for users with little to no prompt engineering expertise.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.06512</link>
<guid>https://arxiv.org/abs/2505.06512</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image synthesis, Hierarchical Cross-Modal Alignment, diffusion sampling, semantic fidelity, spatial control<br /><br />Summary: The paper introduces a new framework called Hierarchical Cross-Modal Alignment (HCMA) to enhance text-to-image synthesis. This approach addresses the challenge of achieving high-level semantic fidelity while providing explicit spatial control, especially when generating scenes with multiple objects and intricate relationships. HCMA consists of two key alignment modules integrated into each diffusion sampling step. The global module aligns latent representations with textual descriptions, ensuring overall scene coherence. In contrast, the local module employs bounding-box layouts to accurately position objects, allowing for detailed spatial control. The authors conducted extensive experiments using the MS-COCO 2014 validation set to evaluate HCMA's effectiveness. The results indicate that HCMA outperforms existing state-of-the-art methods, with significant improvements reflected in a 0.69 increase in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These findings highlight HCMA's capability to faithfully capture complex textual semantics while adhering to spatial constraints defined by the user, marking it as a strong solution for grounded image generation. The code for HCMA is publicly available, further promoting research in this area. <div>
arXiv:2505.06512v1 Announce Type: new 
Abstract: Text-to-image synthesis has progressed to the point where models can generate visually compelling images from natural language prompts. Yet, existing methods often fail to reconcile high-level semantic fidelity with explicit spatial control, particularly in scenes involving multiple objects, nuanced relations, or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal Alignment (HCMA) framework for grounded text-to-image generation. HCMA integrates two alignment modules into each diffusion sampling step: a global module that continuously aligns latent representations with textual descriptions to ensure scene-level coherence, and a local module that employs bounding-box layouts to anchor objects at specified locations, enabling fine-grained spatial control. Extensive experiments on the MS-COCO 2014 validation set show that HCMA surpasses state-of-the-art baselines, achieving a 0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These results demonstrate HCMA's effectiveness in faithfully capturing intricate textual semantics while adhering to user-defined spatial constraints, offering a robust solution for semantically grounded image generation.Our code is available at https://github.com/hwang-cs-ime/HCMA
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation</title>
<link>https://arxiv.org/abs/2505.06515</link>
<guid>https://arxiv.org/abs/2505.06515</guid>
<content:encoded><![CDATA[
<div> Keywords: BEV, semantic segmentation, RESAR-BEV, autonomous driving, nuScenes  

<br /><br />Summary: Bird's-Eye-View (BEV) semantic segmentation enhances environmental perception for autonomous driving, but faces challenges like multi-modal misalignment and sensor noise. To address these, the proposed framework, RESAR-BEV, utilizes a progressive refinement method through residual autoregressive learning, decomposing BEV segmentation into interpretable stages via the Drive-Transformer and Modifier-Transformer architecture. It features a robust BEV representation that integrates ground-proximity voxels with adaptive height offsets and employs dual-path voxel feature encoding to facilitate efficient feature extraction. Additionally, RESAR-BEV implements decoupled supervision, allowing for offline Ground Truth decomposition and online joint optimization, which helps mitigate overfitting while maintaining structural coherence in the predictions. Extensive experiments conducted on the nuScenes dataset have shown that RESAR-BEV achieves a state-of-the-art performance with a mean Intersection over Union (mIoU) of 54.0% across seven crucial driving scene categories, while also ensuring real-time processing capabilities at 14.6 frames per second. The framework demonstrates robust performance even in challenging scenarios, including long-range perception and adverse weather conditions. <div>
arXiv:2505.06515v1 Announce Type: new 
Abstract: Bird's-Eye-View (BEV) semantic segmentation provides comprehensive environmental perception for autonomous driving but suffers multi-modal misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement framework that advances beyond single-step end-to-end approaches: (1) progressive refinement through residual autoregressive learning that decomposes BEV segmentation into interpretable coarse-to-fine stages via our Drive-Transformer and Modifier-Transformer residual prediction cascaded architecture, (2) robust BEV representation combining ground-proximity voxels with adaptive height offsets and dual-path voxel feature encoding (max+attention pooling) for efficient feature extraction, and (3) decoupled supervision with offline Ground Truth decomposition and online joint optimization to prevent overfitting while ensuring structural coherence. Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art performance with 54.0% mIoU across 7 essential driving-scene categories while maintaining real-time capability at 14.6 FPS. The framework exhibits robustness in challenging scenarios of long-range perception and adverse weather conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.06516</link>
<guid>https://arxiv.org/abs/2505.06516</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Dempster-Shafer Theory, Quantum Conflict Indicator, Conflict Fusion, Out-of-Distribution Detection, Quantum Mass Function

Summary:
Quantum Dempster-Shafer Theory (QDST) utilizes quantum interference effects to derive a Quantum Mass Function (QMF) from various data sources and employs quantum parallel computing for faster computation. This study addresses the challenge of managing conflicts between multiple QMFs by introducing a Quantum Conflict Indicator (QCI) that measures conflicts between two QMFs in decision-making. The QCI exhibits desirable properties such as non-negativity, symmetry, boundedness, extreme consistency, and insensitivity to refinement. It is then applied in conflict fusion methods, showing superior performance compared to existing approaches. The Class Description Domain Space (C-DDS) and its optimized version C-DDS+ utilize the QCI-based fusion method for Out-of-Distribution (OOD) detection, yielding better performance than state-of-the-art baseline methods. Experimental results demonstrate an increase in AUC and a decrease in FPR95, highlighting the effectiveness of the proposed approach in OOD detection tasks. 

<br /><br />Summary: <div>
arXiv:2505.06516v1 Announce Type: new 
Abstract: Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to derive a quantum mass function (QMF) as a fuzzy metric type from information obtained from various data sources. In addition, QDST uses quantum parallel computing to speed up computation. Nevertheless, the effective management of conflicts between multiple QMFs in QDST is a challenging question. This work aims to address this problem by proposing a Quantum Conflict Indicator (QCI) that measures the conflict between two QMFs in decision-making. Then, the properties of the QCI are carefully investigated. The obtained results validate its compliance with desirable conflict measurement properties such as non-negativity, symmetry, boundedness, extreme consistency and insensitivity to refinement. We then apply the proposed QCI in conflict fusion methods and compare its performance with several commonly used fusion approaches. This comparison demonstrates the superiority of the QCI-based conflict fusion method. Moreover, the Class Description Domain Space (C-DDS) and its optimized version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to address the Out-of-Distribution (OOD) detection task. The experimental results show that the proposed approach gives better OOD performance with respect to several state-of-the-art baseline OOD detection methods. Specifically, it achieves an average increase in Area Under the Receiver Operating Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the optimal baseline method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation</title>
<link>https://arxiv.org/abs/2505.06517</link>
<guid>https://arxiv.org/abs/2505.06517</guid>
<content:encoded><![CDATA[
<div> Keywords: visual-inertial odometry, long-tracked features, accumulated matching errors, real-time performance, IoT navigation

Summary: 
This paper introduces a visual-inertial odometry (VIO) method that utilizes long-tracked features to reduce localization drift. Long-tracked features can provide constraints on more visual frames but may also lead to accumulated matching errors. To address this, the proposed method includes an active decoupling mechanism to handle accumulated errors in long-tracked feature utilization. Additionally, a visual reference frame reset strategy eliminates tracking errors, while a depth prediction strategy leverages long-term constraints. To ensure real-time performance, the system incorporates efficient state estimation strategies, such as a parallel elimination strategy and an elimination skipping strategy. Experimental results demonstrate that the method offers higher positioning accuracy with short consumption time, making it well-suited for edge-enabled low-altitude IoT navigation applications. The code for the method will be made available on GitHub.<br /><br />Summary: <div>
arXiv:2505.06517v1 Announce Type: new 
Abstract: This paper presents a visual-inertial odometry (VIO) method using long-tracked features. Long-tracked features can constrain more visual frames, reducing localization drift. However, they may also lead to accumulated matching errors and drift in feature tracking. Current VIO methods adjust observation weights based on re-projection errors, yet this approach has flaws. Re-projection errors depend on estimated camera poses and map points, so increased errors might come from estimation inaccuracies, not actual feature tracking errors. This can mislead the optimization process and make long-tracked features ineffective for suppressing localization drift. Furthermore, long-tracked features constrain a larger number of frames, which poses a significant challenge to real-time performance of the system. To tackle these issues, we propose an active decoupling mechanism for accumulated errors in long-tracked feature utilization. We introduce a visual reference frame reset strategy to eliminate accumulated tracking errors and a depth prediction strategy to leverage the long-term constraint. To ensure real time preformane, we implement three strategies for efficient system state estimation: a parallel elimination strategy based on predefined elimination order, an inverse-depth elimination simplification strategy, and an elimination skipping strategy. Experiments on various datasets show that our method offers higher positioning accuracy with relatively short consumption time, making it more suitable for edge-enabled low-altitude IoT navigation, where high-accuracy positioning and real-time operation on edge device are required. The code will be published at github.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation</title>
<link>https://arxiv.org/abs/2505.06524</link>
<guid>https://arxiv.org/abs/2505.06524</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, open-vocabulary multi-entity segmentation, prompt bias, causal prompt, CPC-SAM

Summary:<br /><br /> The Segment Anything Model (SAM) faces generalization challenges in open-vocabulary multi-entity segmentation (OVMS) due to prompt bias. This bias stems from task-irrelevant generating factors in prompts, affecting generalization. To address this issue, a method is proposed to calibrate prompts for accurate OVMS by generating causal prompts containing only task-relevant factors. The proposed CPC-SAM integrates a causal prompt learner (CaPL) into SAM to obtain causal prompts. CaPL optimizes prompts by enforcing causal multi-distribution consistency, alternating between optimizing CaPL and SAM for accurate OVMS. The method is proven superior through extensive experiments. <div>
arXiv:2505.06524v1 Announce Type: new 
Abstract: Despite the strength of the Segment Anything Model (SAM), it struggles with generalization issues in open-vocabulary multi-entity segmentation (OVMS). Through empirical and causal analyses, we find that (i) the prompt bias is the primary cause of the generalization issues; (ii) this bias is closely tied to the task-irrelevant generating factors within the prompts, which act as confounders and affect generalization. To address the generalization issues, we aim to propose a method that can calibrate prompts to eliminate confounders for accurate OVMS. Building upon the causal analysis, we propose that the optimal prompt for OVMS should contain only task-relevant causal factors. We define it as the causal prompt, serving as the goal of calibration. Next, our theoretical analysis, grounded by causal multi-distribution consistency theory, proves that this prompt can be obtained by enforcing segmentation consistency and optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration method for SAM to achieve accurate OVMS. It integrates a lightweight causal prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first generate multiple prompts using random annotations to simulate diverse distributions and then reweight them via CaPL by enforcing causal multi-distribution consistency in both task and entity levels. To ensure obtaining causal prompts, CaPL is optimized by minimizing the cumulative segmentation loss across the reweighted prompts to achieve consistency and optimality. A bi-level optimization strategy alternates between optimizing CaPL and SAM, ensuring accurate OVMS. Extensive experiments validate its superiority.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization of Medical Image Registration Foundation Model</title>
<link>https://arxiv.org/abs/2505.06527</link>
<guid>https://arxiv.org/abs/2505.06527</guid>
<content:encoded><![CDATA[
<div> Keywords: Deformable registration, deep learning, medical image processing, foundation models, Sharpness-Aware Minimization

Summary:
Deformable registration in medical image processing is essential for aligning images accurately through establishing nonlinear correspondences. Traditional methods provide adaptability and interpretability but lack computational efficiency. On the other hand, deep learning approaches have improved speed and accuracy but struggle with flexibility and generalizability. Foundation models have shown promise in learning universal features for image registration but still face challenges in generalization. This paper introduces Sharpness-Aware Minimization (SAM) to enhance foundation models' generalization and robustness in medical image registration. By optimizing the loss landscape, SAM improves model stability across diverse data distributions and complex clinical scenarios. The experimental results demonstrate that integrating SAM with foundation models leads to significant improvements in cross-dataset registration performance, providing new pathways for advancing medical image registration technology.

<br /><br />Summary: <div>
arXiv:2505.06527v1 Announce Type: new 
Abstract: Deformable registration is a fundamental task in medical image processing, aiming to achieve precise alignment by establishing nonlinear correspondences between images. Traditional methods offer good adaptability and interpretability but are limited by computational efficiency. Although deep learning approaches have significantly improved registration speed and accuracy, they often lack flexibility and generalizability across different datasets and tasks. In recent years, foundation models have emerged as a promising direction, leveraging large and diverse datasets to learn universal features and transformation patterns for image registration, thus demonstrating strong cross-task transferability. However, these models still face challenges in generalization and robustness when encountering novel anatomical structures, varying imaging conditions, or unseen modalities. To address these limitations, this paper incorporates Sharpness-Aware Minimization (SAM) into foundation models to enhance their generalization and robustness in medical image registration. By optimizing the flatness of the loss landscape, SAM improves model stability across diverse data distributions and strengthens its ability to handle complex clinical scenarios. Experimental results show that foundation models integrated with SAM achieve significant improvements in cross-dataset registration performance, offering new insights for the advancement of medical image registration technology. Our code is available at https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection</title>
<link>https://arxiv.org/abs/2505.06528</link>
<guid>https://arxiv.org/abs/2505.06528</guid>
<content:encoded><![CDATA[
<div> Keywords: Deepfake, Convolutional Neural Networks, MTCNN, EfficientNet-B5, Kaggle DFDC 

Summary: 
Deepfake videos, created using advanced AI techniques, present a significant challenge to the authenticity of digital media. Detecting these videos requires sophisticated methods capable of identifying subtle inconsistencies. This paper focuses on using deep learning, specifically convolutional neural networks, to detect deepfake videos. The approach includes MTCNN as a face detector and EfficientNet-B5 as an encoder model to predict the authenticity of videos. The model was trained and evaluated on the Kaggle DFDC dataset, achieving a log loss of 42.78%, an AUC of 93.80%, and an F1 score of 86.82%. The results demonstrate the effectiveness of the deep learning approach in detecting deepfakes with high accuracy and reliability. The use of these advanced techniques highlights the importance of staying ahead of the evolving landscape of digital manipulation to preserve the integrity of media content.<br /><br />Summary: <div>
arXiv:2505.06528v1 Announce Type: new 
Abstract: Deepfake videos, produced through advanced artificial intelligence methods now a days, pose a new challenge to the truthfulness of the digital media. As Deepfake becomes more convincing day by day, detecting them requires advanced methods capable of identifying subtle inconsistencies. The primary motivation of this paper is to recognize deepfake videos using deep learning techniques, specifically by using convolutional neural networks. Deep learning excels in pattern recognition, hence, makes it an ideal approach for detecting the intricate manipulations in deepfakes. In this paper, we consider using MTCNN as a face detector and EfficientNet-B5 as encoder model to predict if a video is deepfake or not. We utilize training and evaluation dataset from Kaggle DFDC. The results shows that our deepfake detection model acquired 42.78% log loss, 93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition</title>
<link>https://arxiv.org/abs/2505.06536</link>
<guid>https://arxiv.org/abs/2505.06536</guid>
<content:encoded><![CDATA[
<div> Transformer-based; Adaptive Cross-modal Fusion Network; multimodal emotion recognition; cross-modal attention; feature reinforcement <br />
Summary: <br />
The study introduces a novel Transformer-based Adaptive Cross-modal Fusion Network (TACFN) for multimodal emotion recognition. TACFN addresses the issue of redundant features in cross-modal attention by implementing intra-modal feature selection through self-attention. This allows for adaptive and efficient interaction between modalities. Additionally, TACFN captures complementary information by splicing and using fused weight vectors for feature reinforcement. Experiments on RAVDESS and IEMOCAP datasets demonstrate the effectiveness of TACFN, showing significant performance improvement compared to other methods and achieving state-of-the-art results. The proposed approach, available on GitHub, showcases the potential of using selective and adaptive fusion techniques for enhancing multimodal emotion recognition tasks. <br /> <div>
arXiv:2505.06536v1 Announce Type: new 
Abstract: The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</title>
<link>https://arxiv.org/abs/2505.06537</link>
<guid>https://arxiv.org/abs/2505.06537</guid>
<content:encoded><![CDATA[
<div> Keywords: Fashion video generation, multiple reference images, Pose-aware Prototype Aggregator, Flow-enhanced Prototype Instantiator, MRFashion-7K dataset  

<br /><br />Summary:  
Fashion video generation involves creating temporally consistent videos based on reference images of a specific character. Existing diffusion-based approaches are limited to using a single reference image, constraining their ability to produce view-consistent videos, particularly when clothing patterns vary from different angles. Additionally, current motion modules fail to adequately model human body movement, leading to issues with spatiotemporal consistency. To resolve these challenges, we introduce ProFashion, a framework that employs multiple reference images to enhance both view consistency and temporal coherence. ProFashion features a Pose-aware Prototype Aggregator which selects and combines reference features based on pose information, generating frame-wise prototypes that guide the denoising process while maintaining efficiency. Moreover, we implement a Flow-enhanced Prototype Instantiator that utilizes human keypoint motion flow to direct an advanced spatiotemporal attention process within the denoiser, improving motion consistency. We validate the efficacy of ProFashion through comprehensive evaluations on the MRFashion-7K dataset, which we compiled from online sources. The results indicate that ProFashion surpasses existing methods on the UBC Fashion dataset, demonstrating its effectiveness in fashion video generation. <div>
arXiv:2505.06537v1 Announce Type: new 
Abstract: Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fashion videos, especially when there are different patterns on the clothes from different perspectives. Moreover, the widely adopted motion module does not sufficiently model human body movement, leading to sub-optimal spatiotemporal consistency. To address these issues, we propose ProFashion, a fashion video generation framework leveraging multiple reference images to achieve improved view consistency and temporal coherency. To effectively leverage features from multiple reference images while maintaining a reasonable computational cost, we devise a Pose-aware Prototype Aggregator, which selects and aggregates global and fine-grained reference features according to pose information to form frame-wise prototypes, which serve as guidance in the denoising process. To further enhance motion consistency, we introduce a Flow-enhanced Prototype Instantiator, which exploits the human keypoint motion flow to guide an extra spatiotemporal attention process in the denoiser. To demonstrate the effectiveness of ProFashion, we extensively evaluate our method on the MRFashion-7K dataset we collected from the Internet. ProFashion also outperforms previous methods on the UBC Fashion dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06543</link>
<guid>https://arxiv.org/abs/2505.06543</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual text rendering, Hierarchical Disentangled Glyph-Based framework, Multi-Linguistic GlyphNet, Glyph-Aware Perceptual Loss, LD-TSR

Summary:
The article introduces a novel approach called Hierarchical Disentangled Glyph-Based framework (HDGlyph) for improved visual text rendering in images. The framework addresses challenges with long-tail text cases, particularly for unseen or small-sized text. At the training stage, HDGlyph disentangles pixel-level representations using Multi-Linguistic GlyphNet and Glyph-Aware Perceptual Loss. This ensures robust rendering even for unseen characters. At inference time, HDGlyph employs Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme to refine background and small-sized text. Extensive evaluations demonstrate that HDGlyph outperforms existing methods, achieving accuracy gains of 5.08% in English and 11.7% in Chinese text rendering while maintaining high image quality. The framework excels in long-tail scenarios, showcasing strong accuracy and visual performance.

<br /><br />Summary: <div>
arXiv:2505.06543v1 Announce Type: new 
Abstract: Visual text rendering, which aims to accurately integrate specified textual content within generated images, is critical for various applications such as commercial design. Despite recent advances, current methods struggle with long-tail text cases, particularly when handling unseen or small-sized text. In this work, we propose a novel Hierarchical Disentangled Glyph-Based framework (HDGlyph) that hierarchically decouples text generation from non-text visual synthesis, enabling joint optimization of both common and long-tail text rendering. At the training stage, HDGlyph disentangles pixel-level representations via the Multi-Linguistic GlyphNet and the Glyph-Aware Perceptual Loss, ensuring robust rendering even for unseen characters. At inference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both background and small-sized text. Extensive evaluations show our model consistently outperforms others, with 5.08% and 11.7% accuracy gains in English and Chinese text rendering while maintaining high image quality. It also excels in long-tail scenarios with strong accuracy and visual performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</title>
<link>https://arxiv.org/abs/2505.06557</link>
<guid>https://arxiv.org/abs/2505.06557</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised temporal sentence grounding, positive sample mining, contrastive loss, rank loss, video-language correspondence

Summary: 
The article introduces a novel framework called Positive Sample Mining (PSM) for weakly supervised temporal sentence grounding (WSTSG) in untrimmed videos. Existing methods struggle with negative sample generation, especially when dealing with highly similar samples to an anchor sample. PSM addresses this by mining positive samples from the training set based on semantic similarity. By partitioning the training set into similar and dissimilar subsets, PSM provides more discriminative supervision. The framework incorporates PSM-guided contrastive and rank losses to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Experimental results on WSTSG and VideoQA tasks demonstrate the effectiveness and superiority of PSM in enhancing performance in temporal sentence grounding. <div>
arXiv:2505.06557v1 Announce Type: new 
Abstract: The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search</title>
<link>https://arxiv.org/abs/2505.06566</link>
<guid>https://arxiv.org/abs/2505.06566</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, person search, noise reduction, DURA framework, retrieval performance

<br /><br />Summary: This article addresses the challenge of text-to-image person search, which involves identifying individuals based on textual descriptions. To mitigate data collection costs, large-scale text-image datasets are generated from online co-occurrence pairs. However, this method can introduce noise, particularly through mismatched pairs, which negatively impacts retrieval performance. Many existing approaches emphasize the issue of negative samples, potentially exacerbating the noise problem. To overcome these challenges, the authors introduce the Dynamic Uncertainty and Relational Alignment (DURA) framework. This framework comprises the Key Feature Selector (KFS), which effectively captures and models noise uncertainty, thereby enhancing retrieval reliability. Additionally, it incorporates a novel loss function, Dynamic Softmax Hinge Loss (DSH-Loss), which intelligently adjusts the difficulty of negative samples, improving robustness against noisy environments. The researchers conduct experiments on three different datasets, demonstrating that their proposed method exhibits strong resistance to noise while significantly enhancing retrieval performance in both low- and high-noise scenarios. Overall, the DURA framework offers a promising solution for improving text-to-image person search in challenging data conditions. <div>
arXiv:2505.06566v1 Announce Type: new 
Abstract: Text-to-image person search aims to identify an individual based on a text description. To reduce data collection costs, large-scale text-image datasets are created from co-occurrence pairs found online. However, this can introduce noise, particularly mismatched pairs, which degrade retrieval performance. Existing methods often focus on negative samples, amplifying this noise. To address these issues, we propose the Dynamic Uncertainty and Relational Alignment (DURA) framework, which includes the Key Feature Selector (KFS) and a new loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and models noise uncertainty, improving retrieval reliability. The bidirectional evidence from cross-modal similarity is modeled as a Dirichlet distribution, enhancing adaptability to noisy data. DSH adjusts the difficulty of negative samples to improve robustness in noisy environments. Our experiments on three datasets show that the method offers strong noise resistance and improves retrieval performance in both low- and high-noise scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</title>
<link>https://arxiv.org/abs/2505.06573</link>
<guid>https://arxiv.org/abs/2505.06573</guid>
<content:encoded><![CDATA[
<div> sensor-based methods, distance measurement, power transmission lines, 3D lasers, ElectricSight<br />
<br />
Summary:
ElectricSight is introduced as a system for measuring and monitoring 3D distances between power lines and potential threats, addressing the challenge of balancing accuracy and cost in power line protection. By combining real-time images with environmental point cloud priors, ElectricSight provides cost-effective and precise measurements. A monocular depth estimation method integrates 3D point cloud data into image-based estimates, enhancing accuracy and reliability. Experimental tests in a real-world scenario show ElectricSight achieves an average distance measurement accuracy of 1.08m and an early warning accuracy of 92%, demonstrating its effectiveness in protecting power transmission lines and identifying hazards. <div>
arXiv:2505.06573v1 Announce Type: new 
Abstract: Protecting power transmission lines from potential hazards involves critical tasks, one of which is the accurate measurement of distances between power lines and potential threats, such as large cranes. The challenge with this task is that the current sensor-based methods face challenges in balancing accuracy and cost in distance measurement. A common practice is to install cameras on transmission towers, which, however, struggle to measure true 3D distances due to the lack of depth information. Although 3D lasers can provide accurate depth data, their high cost makes large-scale deployment impractical.
  To address this challenge, we present ElectricSight, a system designed for 3D distance measurement and monitoring of potential hazards to power transmission lines. This work's key innovations lie in both the overall system framework and a monocular depth estimation method. Specifically, the system framework combines real-time images with environmental point cloud priors, enabling cost-effective and precise 3D distance measurements. As a core component of the system, the monocular depth estimation method enhances the performance by integrating 3D point cloud data into image-based estimates, improving both the accuracy and reliability of the system.
  To assess ElectricSight's performance, we conducted tests with data from a real-world power transmission scenario. The experimental results demonstrate that ElectricSight achieves an average accuracy of 1.08 m for distance measurements and an early warning accuracy of 92%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</title>
<link>https://arxiv.org/abs/2505.06575</link>
<guid>https://arxiv.org/abs/2505.06575</guid>
<content:encoded><![CDATA[
<div> encoder-decoder, hierarchical feature extraction, 3D human contact estimation, point cloud, geometric structures
Summary: 
GRACE introduces a new approach called Geometry-level Reasoning for 3D Human-scene Contact Estimation that combines a point cloud encoder-decoder architecture with hierarchical feature extraction to accurately estimate contact points between humans and scenes. The framework integrates 3D human geometric structures with 2D interaction semantics derived from images, enabling the effective modeling of contact regions. By establishing an implicit mapping from geometric features to the vertex space of the 3D human mesh, GRACE achieves high prediction accuracy and demonstrates strong generalization capabilities across diverse human geometries. Extensive experiments on benchmark datasets show that GRACE outperforms existing methods in contact estimation, with additional results confirming its robust generalization to unstructured human point clouds. <div>
arXiv:2505.06575v1 Announce Type: new 
Abstract: Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Random Alternation Framework for Zero-Shot Pansharpening</title>
<link>https://arxiv.org/abs/2505.06576</link>
<guid>https://arxiv.org/abs/2505.06576</guid>
<content:encoded><![CDATA[
<div> Keywords: pansharpening, deep learning, two-stage framework, degradation-aware modeling, random alternation optimization

Summary: 
The article introduces a new approach called TRA-PAN for pansharpening, leveraging deep learning techniques. The proposed method addresses the challenge of acquiring real high-resolution images by utilizing reduced-resolution images and physical characteristics of full-resolution images. The TRA-PAN framework consists of two stages: a pre-training stage featuring Degradation-Aware Modeling (DAM) and a warm-up procedure, and a second stage employing Random Alternation Optimization (RAO) for model optimization. By primarily focusing on full-resolution images, the TRA-PAN method allows zero-shot training with just one image pair, eliminating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms current state-of-the-art methods in both quantitative metrics and visual quality, showcasing its practical applicability in real-world scenarios. <br /><br />Summary: <div>
arXiv:2505.06576v1 Announce Type: new 
Abstract: In recent years, pansharpening has seen rapid advancements with deep learning methods, which have demonstrated impressive fusion quality. However, the challenge of acquiring real high-resolution images limits the practical applicability of these methods. To address this, we propose a two-stage random alternating framework (TRA-PAN) that effectively integrates strong supervision constraints from reduced-resolution images with the physical characteristics of full-resolution images. The first stage introduces a pre-training procedure, which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral degradation mappings, alongside a warm-up procedure designed to reduce training time and mitigate the negative effects of reduced-resolution data. In the second stage, Random Alternation Optimization (RAO) is employed, where random alternating training leverages the strengths of both reduced- and full-resolution images, further optimizing the fusion model. By primarily relying on full-resolution images, our method enables zero-shot training with just a single image pair, obviating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in both quantitative metrics and visual quality in real-world scenarios, highlighting its strong practical applicability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform</title>
<link>https://arxiv.org/abs/2505.06578</link>
<guid>https://arxiv.org/abs/2505.06578</guid>
<content:encoded><![CDATA[
<div> FC layer weights sharing, two-dimensional separable transform, neural network architecture, image recognition, model parameter reduction <br />
Summary:<br />
The paper introduces a new two-dimensional separable transform (LST) for neural network architectures in image recognition tasks. The LST leverages weight sharing in fully connected (FC) layers to process rows and columns of an image efficiently. By using LST layers, the model significantly reduces the number of parameters compared to traditional stacked FC layers. A neural network classifier incorporating a single LST layer and an FC layer achieves a high accuracy of 98.02% on the MNIST dataset with just 9.5k parameters. The study also demonstrates the effectiveness of the LST approach by implementing a classifier for handwritten digit recognition on an FPGA platform. This work opens up new possibilities for designing compact and high-performance neural network models for image recognition tasks. <br /> <div>
arXiv:2505.06578v1 Announce Type: new 
Abstract: The paper presents a learned two-dimensional separable transform (LST) that can be considered as a new type of computational layer for constructing neural network (NN) architecture for image recognition tasks. The LST based on the idea of sharing the weights of one fullyconnected (FC) layer to process all rows of an image. After that, a second shared FC layer is used to process all columns of image representation obtained from the first layer. The use of LST layers in a NN architecture significantly reduces the number of model parameters compared to models that use stacked FC layers. We show that a NN-classifier based on a single LST layer followed by an FC layer achieves 98.02\% accuracy on the MNIST dataset, while having only 9.5k parameters. We also implemented a LST-based classifier for handwritten digit recognition on the FPGA platform to demonstrate the efficiency of the suggested approach for designing a compact and high-performance implementation of NN models. Git repository with supplementary materials: https://github.com/Mak-Sim/LST-2d
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning</title>
<link>https://arxiv.org/abs/2505.06592</link>
<guid>https://arxiv.org/abs/2505.06592</guid>
<content:encoded><![CDATA[
<div> Keywords: batch augmentation, fine-tuning, multimodal data, neural networks, dataloader<br />
Summary:<br />
This paper introduces a novel approach for detecting fetal organs in ultrasound images and clinical text data using batch augmentation and unimodal fine-tuning. The method involves pre-training initial layers with medical data before multimodal training, transferring initialization with batch augmentation for image data, and fine-tuning neural networks for feature extraction. By combining image features with textual information, the proposed approach achieves state-of-the-art performance on the UPMC Food-101 dataset. A custom dataloader script is implemented to load and augment the multimodal data, enhancing generalization capabilities. The multimodal large language model (LLM) with the proposed training methodology outperforms existing methods. The scripts for the proposed method are available on GitHub for comparative analysis with traditional approaches.<br /> 
Summary: <div>
arXiv:2505.06592v1 Announce Type: new 
Abstract: This paper proposes batch augmentation with unimodal fine-tuning to detect the fetus's organs from ultrasound images and associated clinical textual information. We also prescribe pre-training initial layers with investigated medical data before the multimodal training. At first, we apply a transferred initialization with the unimodal image portion of the dataset with batch augmentation. This step adjusts the initial layer weights for medical data. Then, we apply neural networks (NNs) with fine-tuned initial layers to images in batches with batch augmentation to obtain features. We also extract information from descriptions of images. We combine this information with features obtained from images to train the head layer. We write a dataloader script to load the multimodal data and use existing unimodal image augmentation techniques with batch augmentation for the multimodal data. The dataloader brings a new random augmentation for each batch to get a good generalization. We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The multimodal large language model (LLM) with the proposed training provides the best results among the investigated methods. We receive near state-of-the-art (SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the proposed method with traditional counterparts at the following repository: github.com/dipuk0506/multimodal
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.06603</link>
<guid>https://arxiv.org/abs/2505.06603</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Anomaly Detection, ReplayCAD, diffusion-driven generative replay, segmentation, pixel-level detailed features

Summary:
ReplayCAD is introduced as a novel approach to Continual Anomaly Detection (CAD), addressing challenges such as catastrophic forgetting and segmentation of small anomalous regions. The framework utilizes diffusion-driven generative replay to preserve pixel-level detailed features by compressing historical data based on class semantic embeddings. This allows for the accurate segmentation of anomalies. Additionally, ReplayCAD leverages spatial features to enhance spatial diversity in the compressed data, leading to improved performance in both classification and segmentation tasks. The method achieves state-of-the-art results, with significant enhancements in segmentation accuracy on benchmark datasets like VisA and MVTec. By making the source code available on GitHub, ReplayCAD offers a valuable resource for future research in anomaly detection. 

<br /><br />Summary: <div>
arXiv:2505.06603v1 Announce Type: new 
Abstract: Continual Anomaly Detection (CAD) enables anomaly detection models in learning new classes while preserving knowledge of historical classes. CAD faces two key challenges: catastrophic forgetting and segmentation of small anomalous regions. Existing CAD methods store image distributions or patch features to mitigate catastrophic forgetting, but they fail to preserve pixel-level detailed features for accurate segmentation. To overcome this limitation, we propose ReplayCAD, a novel diffusion-driven generative replay framework that replay high-quality historical data, thus effectively preserving pixel-level detailed features. Specifically, we compress historical data by searching for a class semantic embedding in the conditional space of the pre-trained diffusion model, which can guide the model to replay data with fine-grained pixel details, thus improving the segmentation performance. However, relying solely on semantic features results in limited spatial diversity. Hence, we further use spatial features to guide data compression, achieving precise control of sample space, thereby generating more diverse data. Our method achieves state-of-the-art performance in both classification and segmentation, with notable improvements in segmentation: 11.5% on VisA and 8.1% on MVTec. Our source code is available at https://github.com/HULEI7/ReplayCAD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization</title>
<link>https://arxiv.org/abs/2505.06635</link>
<guid>https://arxiv.org/abs/2505.06635</guid>
<content:encoded><![CDATA[
<div> regularization term, functional entropy, multi-modal learning, semantic segmentation, unimodal dominance

Summary:<br />
- A challenge in dense prediction tasks is fusing multi-modal inputs effectively, as models tend to rely on easily learnable modalities, resulting in unimodal dominance.
- A new regularization term based on functional entropy is proposed to balance the contribution of each visual modality to segmentation results without adding parameters or modules.
- The log-Sobolev inequality is utilized to bound functional entropy using functional-Fisher-information, maximizing the information from each visual modality to mitigate unimodal dominance.
- A multi-scale regularization module applies the proposed term on high-level features and segmentation predictions for more balanced multi-modal learning.
- Experimental results on three datasets show superior performance of the proposed method, achieving significant improvements without additional parameters. 

<br /><br />Summary: <div>
arXiv:2505.06635v1 Announce Type: new 
Abstract: Fusing and balancing multi-modal inputs from novel sensors for dense prediction tasks, particularly semantic segmentation, is critically important yet remains a significant challenge. One major limitation is the tendency of multi-modal frameworks to over-rely on easily learnable modalities, a phenomenon referred to as unimodal dominance or bias. This issue becomes especially problematic in real-world scenarios where the dominant modality may be unavailable, resulting in severe performance degradation. To this end, we apply a simple but effective plug-and-play regularization term based on functional entropy, which introduces no additional parameters or modules. This term is designed to intuitively balance the contribution of each visual modality to the segmentation results. Specifically, we leverage the log-Sobolev inequality to bound functional entropy using functional-Fisher-information. By maximizing the information contributed by each visual modality, our approach mitigates unimodal dominance and establishes a more balanced and robust segmentation framework. A multi-scale regularization module is proposed to apply our proposed plug-and-play term on high-level features and also segmentation predictions for more balanced multi-modal learning. Extensive experiments on three datasets demonstrate that our proposed method achieves superior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing any additional parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Distillation with Probabilistic Latent Features</title>
<link>https://arxiv.org/abs/2505.06647</link>
<guid>https://arxiv.org/abs/2505.06647</guid>
<content:encoded><![CDATA[
<div> Keywords: dataset distillation, synthetic data, low-rank multivariate normal, computational efficiency, classification tasks

<br /><br />Summary:  
As deep learning models become more complex and require larger datasets, reducing storage and computational costs is crucial. Dataset distillation offers a solution by synthesizing a smaller set of data to replace the original dataset in classification tasks. Traditional methods often map data from pixel space to the latent space of a generative model, but this study introduces a novel stochastic approach that models the joint distribution of latent features, enhancing the capture of spatial structures and diversity in the synthetic samples. The method employs a low-rank multivariate normal distribution, parameterized by a lightweight network, ensuring low computational complexity and compatibility with various matching networks in dataset distillation. Following the distillation process, synthetic images are generated by inputting the learned latent features into a pretrained generator. These images are then utilized to train classification models, with performance evaluated against real test sets. The proposed method was validated across various benchmarks, including subsets of ImageNet, CIFAR-10, and the MedMNIST histopathological dataset, achieving state-of-the-art performance across different backbone architectures, highlighting its generality and effectiveness in practical applications. <div>
arXiv:2505.06647v1 Announce Type: new 
Abstract: As deep learning models grow in complexity and the volume of training data increases, reducing storage and computational costs becomes increasingly important. Dataset distillation addresses this challenge by synthesizing a compact set of synthetic data that can effectively replace the original dataset in downstream classification tasks. While existing methods typically rely on mapping data from pixel space to the latent space of a generative model, we propose a novel stochastic approach that models the joint distribution of latent features. This allows our method to better capture spatial structures and produce diverse synthetic samples, which benefits model training. Specifically, we introduce a low-rank multivariate normal distribution parameterized by a lightweight network. This design maintains low computational complexity and is compatible with various matching networks used in dataset distillation. After distillation, synthetic images are generated by feeding the learned latent features into a pretrained generator. These synthetic images are then used to train classification models, and performance is evaluated on real test set. We validate our method on several benchmarks, including ImageNet subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach achieves state-of-the-art cross architecture performance across a range of backbone architectures, demonstrating its generality and effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection</title>
<link>https://arxiv.org/abs/2505.06663</link>
<guid>https://arxiv.org/abs/2505.06663</guid>
<content:encoded><![CDATA[
<div> Keywords: open-vocabulary, video visual relationship detection, object detection, relationship classification, mutual enhancement<br />
<br />
Summary: 
The paper introduces a framework called METOR for open-vocabulary video visual relationship detection. METOR aims to detect objects and their relationships in videos without predefined categories, by jointly modeling and mutually enhancing object detection and relationship classification. The framework includes a contextual refinement encoding module that refines text features and object queries using visual contexts. An iterative enhancement module is proposed to improve recognition performance by enhancing object and relationship representations alternately, exploiting their interdependence. METOR outperforms existing methods on the VidVRD and VidOR datasets, showcasing state-of-the-art performance in open-vocabulary video visual relationship detection. <div>
arXiv:2505.06663v1 Announce Type: new 
Abstract: Open-vocabulary video visual relationship detection aims to detect objects and their relationships in videos without being restricted by predefined object or relationship categories. Existing methods leverage the rich semantic knowledge of pre-trained vision-language models such as CLIP to identify novel categories. They typically adopt a cascaded pipeline to first detect objects and then classify relationships based on the detected objects, which may lead to error propagation and thus suboptimal performance. In this paper, we propose Mutual EnhancemenT of Objects and Relationships (METOR), a query-based unified framework to jointly model and mutually enhance object detection and relationship classification in open-vocabulary scenarios. Under this framework, we first design a CLIP-based contextual refinement encoding module that extracts visual contexts of objects and relationships to refine the encoding of text features and object queries, thus improving the generalization of encoding to novel categories. Then we propose an iterative enhancement module to alternatively enhance the representations of objects and relationships by fully exploiting their interdependence to improve recognition performance. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate that our framework achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning</title>
<link>https://arxiv.org/abs/2505.06665</link>
<guid>https://arxiv.org/abs/2505.06665</guid>
<content:encoded><![CDATA[
<div> Keywords: image fusion, semantic information, multi-task learning, segmentation, MultiTaskVIF

<br /><br />Summary:  
Visible and infrared image fusion (VIF) has gained significant attention, particularly with the focus on improving visual quality and incorporating semantic information into fusion models. Traditional segmentation-oriented VIF methods often rely on a cascade structure, which utilizes separate models for fusion and segmentation, leading to complexity and redundancy. The proposed solution addresses this issue by introducing MultiTaskVIF, a concise and universal training framework that allows for the simultaneous generation of both fused images and segmentation results. This innovative approach is inspired by multi-task learning, integrating semantic information directly into the fusion process without requiring a complete segmentation model. The framework features a multi-task head decoder (MTH) that replaces the conventional decoder of the fusion model, facilitating the efficient learning of semantic features during training. Extensive experimental evaluations have demonstrated the effectiveness and efficiency of MultiTaskVIF, proving it to be a simpler alternative to existing cascade frameworks. The authors plan to release the code for this method upon acceptance, thereby contributing to the field of image fusion and offering a streamlined approach for future research and applications. <div>
arXiv:2505.06665v1 Announce Type: new 
Abstract: Visible and infrared image fusion (VIF) has attracted significant attention in recent years. Traditional VIF methods primarily focus on generating fused images with high visual quality, while recent advancements increasingly emphasize incorporating semantic information into the fusion model during training. However, most existing segmentation-oriented VIF methods adopt a cascade structure comprising separate fusion and segmentation models, leading to increased network complexity and redundancy. This raises a critical question: can we design a more concise and efficient structure to integrate semantic information directly into the fusion model during training-Inspired by multi-task learning, we propose a concise and universal training framework, MultiTaskVIF, for segmentation-oriented VIF models. In this framework, we introduce a multi-task head decoder (MTH) to simultaneously output both the fused image and the segmentation result during training. Unlike previous cascade training frameworks that necessitate joint training with a complete segmentation model, MultiTaskVIF enables the fusion model to learn semantic features by simply replacing its decoder with MTH. Extensive experimental evaluations validate the effectiveness of the proposed method. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation</title>
<link>https://arxiv.org/abs/2505.06668</link>
<guid>https://arxiv.org/abs/2505.06668</guid>
<content:encoded><![CDATA[
<div> Keywords: StableMotion, image rectification, diffusion models, Adaptive Ensemble Strategy, Sampling Steps Disaster 

Summary: 
StableMotion is a new framework that utilizes pretrained large-scale image diffusion models to perform motion estimation for image rectification tasks like Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). It repurposes text-to-image Stable Diffusion (SD) models as an image-to-motion estimator and uses an Adaptive Ensemble Strategy (AES) to ensure high-quality results by consolidating multiple outputs. The framework leverages the concept of Sampling Steps Disaster (SSD) to achieve fast one-step inference and overcome inconsistencies in diffusion model outputs. StableMotion outperforms previous methods in image rectification tasks, demonstrating strong generalizability and a 200 times speedup. <div>
arXiv:2505.06668v1 Announce Type: new 
Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and content priors) from pretrained large-scale image diffusion models to perform motion estimation, solving single-image-based image rectification tasks such as Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD) models as backbone and repurposes it into an image-to-motion estimator. To mitigate inconsistent output produced by diffusion models, we propose Adaptive Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive, high-fidelity result. Additionally, we present the concept of Sampling Steps Disaster (SSD), the counterintuitive scenario where increasing the number of sampling steps can lead to poorer outcomes, which enables our framework to achieve one-step inference. StableMotion is verified on two image rectification tasks and delivers state-of-the-art performance in both, as well as showing strong generalizability. Supported by SSD, StableMotion offers a speedup of 200 times compared to previous diffusion model-based methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Dataset Condensation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06670</link>
<guid>https://arxiv.org/abs/2505.06670</guid>
<content:encoded><![CDATA[
<div> video dataset distillation, synthetic dataset, video diffusion model, VST-UNet, TAC-DT

Summary:
- Dataset distillation is used to generate compact synthetic datasets from large real datasets to save computational resources.
- Video dataset distillation is challenging due to limited performance and poor data quality.
- A video diffusion model is introduced alongside VST-UNet and TAC-DT to improve video dataset distillation.
- VST-UNet helps select diverse and informative subset of videos from the original dataset.
- TAC-DT efficiently selects representative videos without additional training overhead.
- Experimental results on benchmark datasets show up to 10.61% performance improvement over existing methods.
- The proposed method consistently outperforms other approaches, setting a new benchmark for video dataset distillation.

<br /><br />Summary: <div>
arXiv:2505.06670v1 Announce Type: new 
Abstract: In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance and poor data quality, particularly in the video domain. In this paper, we focus on video dataset distillation by employing a video diffusion model to generate high-quality synthetic videos. To enhance representativeness, we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking the Text-to-Video Generative Models</title>
<link>https://arxiv.org/abs/2505.06679</link>
<guid>https://arxiv.org/abs/2505.06679</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, generative models, jailbreak attack, safety concerns, optimization-based

<br /><br />Summary: This paper addresses the vulnerabilities of text-to-video generative models, which have advanced significantly through diffusion models like Pika, Luma, Kling, and Sora. While these models excel at generating content, they are susceptible to jailbreak attacks that can produce unsafe material such as pornography and violence. Previous efforts, like T2VSafetyBench, have evaluated these models against unsafe prompts but lacked systematic analysis of their vulnerabilities. The authors propose the first optimization-based jailbreak attack tailored for text-to-video models, framing prompt generation as an optimization problem with three objectives: maximizing semantic similarity between input and generated prompts, evading safety filters, and ensuring generated videos are semantically aligned with the original input. To improve the robustness of prompts, they introduce a mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on averaged scores. Extensive experiments conducted on various models, including Open-Sora, Pika, Luma, and Kling, show that their approach not only achieves a higher success rate in bypassing safety measures but also generates videos that maintain greater semantic relevance to the input prompts. <div>
arXiv:2505.06679v1 Announce Type: new 
Abstract: Text-to-video generative models have achieved significant progress, driven by the rapid advancements in diffusion models, with notable examples including Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their vulnerability to jailbreak attack, i.e. to generate unsafe content, including pornography, violence, and discrimination, raises serious safety concerns. Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for evaluating the safety of text-to-video models against unsafe prompts but lack systematic studies for exploiting their vulnerabilities effectively. In this paper, we propose the \textit{first} optimization-based jailbreak attack against text-to-video models, which is specifically designed. Our approach formulates the prompt generation task as an optimization problem with three key objectives: (1) maximizing the semantic similarity between the input and generated prompts, (2) ensuring that the generated prompts can evade the safety filter of the text-to-video model, and (3) maximizing the semantic similarity between the generated videos and the original input prompts. To further enhance the robustness of the generated prompts, we introduce a prompt mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on the averaged score. This strategy not only improves the attack success rate but also boosts the semantic relevance of the generated video. We conduct extensive experiments across multiple text-to-video models, including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our method not only achieves a higher attack success rate compared to baseline methods but also generates videos with greater semantic similarity to the original input prompts.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration</title>
<link>https://arxiv.org/abs/2505.06683</link>
<guid>https://arxiv.org/abs/2505.06683</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep unfolding networks, illumination degradation image restoration, UnfoldIR, reflectance-assisted illumination correction, illumination-guided reflectance enhancement

Summary:
UnfoldIR is a novel Deep Unfolding Network (DUN) method for illumination degradation image restoration (IDIR) tasks. It addresses limitations by introducing a new task-specific restoration model with dedicated regularization terms for illumination smoothing and texture enhancement. The iterative optimized solution is unfolded into a multistage network consisting of Reflectance-Assisted Illumination Correction (RAIC) and Illumination-Guided Reflectance Enhancement (IGRE) modules. RAIC enforces illumination smoothness using a visual state space (VSS), while IGRE globally aligns similar textures to enhance details in degraded regions. An inter-stage information consistent loss is proposed to ensure network stability in later stages and maintain structural preservation. Experiments validate the effectiveness of UnfoldIR across various IDIR tasks and downstream problems. <div>
arXiv:2505.06683v1 Announce Type: new 
Abstract: Deep unfolding networks (DUNs) are widely employed in illumination degradation image restoration (IDIR) to merge the interpretability of model-based approaches with the generalization of learning-based methods. However, the performance of DUN-based methods remains considerably inferior to that of state-of-the-art IDIR solvers. Our investigation indicates that this limitation does not stem from structural shortcomings of DUNs but rather from the limited exploration of the unfolding structure, particularly for (1) constructing task-specific restoration models, (2) integrating advanced network architectures, and (3) designing DUN-specific loss functions. To address these issues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR first introduces a new IDIR model with dedicated regularization terms for smoothing illumination and enhancing texture. We unfold the iterative optimized solution of this model into a multistage network, with each stage comprising a reflectance-assisted illumination correction (RAIC) module and an illumination-guided reflectance enhancement (IGRE) module. RAIC employs a visual state space (VSS) to extract non-local features, enforcing illumination smoothness, while IGRE introduces a frequency-aware VSS to globally align similar textures, enabling mildly degraded regions to guide the enhancement of details in more severely degraded areas. This suppresses noise while enhancing details. Furthermore, given the multistage structure, we propose an inter-stage information consistent loss to maintain network stability in the final stages. This loss contributes to structural preservation and sustains the model's performance even in unsupervised settings. Experiments verify our effectiveness across 5 IDIR tasks and 3 downstream problems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNBench: Benchmarking Robust Federated Learning against Noisy Labels</title>
<link>https://arxiv.org/abs/2505.06684</link>
<guid>https://arxiv.org/abs/2505.06684</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, label noise, benchmark study, robustness, regularization

Summary:
Federated learning faces the challenge of label noise within distributed datasets, affecting performance. The FNBench benchmark study evaluates state-of-the-art methods under unified settings, considering synthetic label noise, human-annotation errors, and systematic errors. This study sheds light on why noisy labels degrade federated learning performance. A representation-aware regularization method is proposed to enhance robustness against label noise. The open-sourced source code and observations provided in this study contribute to understanding and addressing the impact of label noise in federated learning. Future directions for research are also outlined to further advance the field. <div>
arXiv:2505.06684v1 Announce Type: new 
Abstract: Robustness to label noise within data is a significant challenge in federated learning (FL). From the data-centric perspective, the data quality of distributed datasets can not be guaranteed since annotations of different clients contain complicated label noise of varying degrees, which causes the performance degradation. There have been some early attempts to tackle noisy labels in FL. However, there exists a lack of benchmark studies on comprehensively evaluating their practical performance under unified settings. To this end, we propose the first benchmark study FNBench to provide an experimental investigation which considers three diverse label noise patterns covering synthetic label noise, imperfect human-annotation errors and systematic errors. Our evaluation incorporates eighteen state-of-the-art methods over five image recognition datasets and one text classification dataset. Meanwhile, we provide observations to understand why noisy labels impair FL, and additionally exploit a representation-aware regularization method to enhance the robustness of existing methods against noisy labels based on our observations. Finally, we discuss the limitations of this work and propose three-fold future directions. To facilitate related communities, our source code is open-sourced at https://github.com/Sprinter1999/FNBench.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search</title>
<link>https://arxiv.org/abs/2505.06694</link>
<guid>https://arxiv.org/abs/2505.06694</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater object detection, sonar imagery, Detection Transformer, Neural Architecture Search, NAS-DETR

Summary:
- Proposal of a Detection Transformer architecture optimized with a Neural Architecture Search approach for object detection in sonar images.
- Introduction of an improved Zero-shot Neural Architecture Search method based on the maximum entropy principle to identify a high-performance CNN-Transformer backbone for sonar image detection.
- Combination of the selected backbone with a Feature Pyramid Network and a deformable attention-based Transformer decoder to construct a complete network architecture for enhanced performance.
- Extensive experiments showcasing state-of-the-art performance on two representative datasets while maintaining real-time efficiency and minimal computational complexity.
- Correlation analysis between key parameters and differential entropy-based fitness function to enhance the interpretability of the proposed framework.

<br /><br />Summary: In this study, a novel approach to underwater object detection using sonar imagery is introduced. By combining a Detection Transformer architecture with a Neural Architecture Search method, the authors achieve state-of-the-art performance on representative datasets. The proposed method incorporates an improved Zero-shot Neural Architecture Search technique and advanced network components to enhance detection accuracy. Through extensive experiments, the effectiveness of the approach is demonstrated, highlighting its real-time efficiency and minimal computational complexity. Additionally, correlation analysis is performed to improve the interpretability of the framework, making this work a significant contribution to the field of sonar object detection. <div>
arXiv:2505.06694v1 Announce Type: new 
Abstract: Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</title>
<link>https://arxiv.org/abs/2505.06710</link>
<guid>https://arxiv.org/abs/2505.06710</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-instance learning, feature extractor, weakly-supervised learning, data augmentation, pathological images<br />
Summary: <br />
This paper introduces a novel approach to pre-train feature extractors for multi-instance learning (MIL) using weakly-supervised learning on whole-slide pathological images (WSI). The method focuses on learning instance-level representations by propagating weak bag-level labels to instances for supervised learning. It incorporates strong data augmentation, a non-linear prediction head, and a robust loss function to enhance feature learning for MIL. Experimental results demonstrate superior performance compared to other pre-training methods like ImageNet and self-supervised learning in various downstream tasks on large-scale WSI datasets. The proposed scheme is also scalable and compatible with fine-tuning pathological-specific models and pre-training on multiple merged datasets. This work, the first to emphasize representation learning for MIL, showcases the importance of optimizing feature extraction for improved performance in WSI analysis. <br /> <div>
arXiv:2505.06710v1 Announce Type: new 
Abstract: Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers</title>
<link>https://arxiv.org/abs/2505.06745</link>
<guid>https://arxiv.org/abs/2505.06745</guid>
<content:encoded><![CDATA[
<div> framework, symbolic rule extraction, Vision Transformers, sparse concept layer, logic programs <br />
 <br />Summary: 
This paper introduces a framework for extracting symbolic rules from Vision Transformers (ViTs). It addresses the challenge posed by ViTs' lack of modular concept detectors and reliance on global self-attention mechanisms by proposing a sparse concept layer inspired by Sparse Autoencoders. This layer learns a binarized representation of high-level visual concepts through a combination of sparsity, entropy minimization, and supervised contrastive loss. The resulting binarized concept activations are used as input to the FOLD-SE-M algorithm to generate logic programs, enabling symbolic reasoning. The method achieves higher classification accuracy than standard ViTs while providing interpretable and semantically meaningful rule-sets that serve as a logic-based decision layer operating directly on sparse concept representations. This work represents a significant advancement in neuro-symbolic AI by bridging the gap between transformer-based vision models and symbolic logic programming. <div>
arXiv:2505.06745v1 Announce Type: new 
Abstract: Recent neuro-symbolic approaches have successfully extracted symbolic rule-sets from CNN-based models to enhance interpretability. However, applying similar techniques to Vision Transformers (ViTs) remains challenging due to their lack of modular concept detectors and reliance on global self-attention mechanisms. We propose a framework for symbolic rule extraction from ViTs by introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This linear layer operates on attention-weighted patch representations and learns a disentangled, binarized representation in which individual neurons activate for high-level visual concepts. To encourage interpretability, we apply a combination of L1 sparsity, entropy minimization, and supervised contrastive loss. These binarized concept activations are used as input to the FOLD-SE-M algorithm, which generates a rule-set in the form of logic programs. Our method achieves a 5.14% better classification accuracy than the standard ViT while enabling symbolic reasoning. Crucially, the extracted rule-set is not merely post-hoc but acts as a logic-based decision layer that operates directly on the sparse concept representations. The resulting programs are concise and semantically meaningful. This work is the first to extract executable logic programs from ViTs using sparse symbolic representations. It bridges the gap between transformer-based vision models and symbolic logic programming, providing a step forward in interpretable and verifiable neuro-symbolic AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</title>
<link>https://arxiv.org/abs/2505.06796</link>
<guid>https://arxiv.org/abs/2505.06796</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Fake News Detection, Dataset, Shallow-Deep Multitask Learning, Image-text Generation, Deepfake Modeling

Summary:
The article introduces a new dataset called Multimodal Fake News Detection (MFND) to identify and localize authentic fake news that is easily manipulated by deepfake modeling attacks. A Shallow-Deep Multitask Learning (SDML) model is proposed to effectively detect fake news by leveraging unimodal and mutual modal features to capture the underlying semantics of news. The model employs momentum distillation-based contrastive learning for fine-grained image and text alignment and an adaptive cross-modal fusion module to enhance mutual modal features. It also incorporates a two-branch framework for image and text features to make four predictions through detection and localization projections. Experimental results show the model's superiority in detecting fake news on both mainstream and newly proposed datasets. The code and dataset are available for further research. 

<br /><br />Summary: 
- Introduction of Multimodal Fake News Detection dataset (MFND) to identify and localize authentic fake news affected by deepfake modeling attacks.
- Proposal of a Shallow-Deep Multitask Learning (SDML) model leveraging unimodal and mutual modal features for effective fake news detection.
- Utilization of momentum distillation-based contrastive learning and adaptive cross-modal fusion module for image-text alignment and feature enhancement.
- Implementation of a two-branch framework for image and text features to make four predictions through detection and localization projections.
- Superiority of the model demonstrated through experiments on both mainstream and newly proposed datasets. Availability of code and dataset for further research. <div>
arXiv:2505.06796v1 Announce Type: new 
Abstract: Multimodal news contains a wealth of information and is easily affected by deepfake modeling attacks. To combat the latest image and text generation methods, we present a new Multimodal Fake News Detection dataset (MFND) containing 11 manipulated types, designed to detect and localize highly authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning (SDML) model for fake news, which fully uses unimodal and mutual modal features to mine the intrinsic semantics of news. Under shallow inference, we propose the momentum distillation-based light punishment contrastive learning for fine-grained uniform spatial image and text semantic alignment, and an adaptive cross-modal fusion module to enhance mutual modal features. Under deep inference, we design a two-branch framework to augment the image and text unimodal features, respectively merging with mutual modalities features, for four predictions via dedicated detection and localization projections. Experiments on both mainstream and our proposed datasets demonstrate the superiority of the model. Codes and dataset are released at https://github.com/yunan-wang33/sdml.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
<link>https://arxiv.org/abs/2505.06814</link>
<guid>https://arxiv.org/abs/2505.06814</guid>
<content:encoded><![CDATA[
<div> Keywords: M4IVQA, multi-modal, multilingual, medical instructional videos, reasoning  

<br /><br />Summary:  
The M4IVQA challenge has been introduced following the successful CMIVQA and MMIVQA challenges. It aims to advance research in multi-modal, multilingual, and multi-hop medical instructional question answering systems, specifically focusing on medical instructional videos. The challenge features three tracks: M4TAGSV, which centers on Temporal Answer Grounding in a Single Video; M4VCR, focused on Video Corpus Retrieval; and M4TAGVC, which involves Temporal Answer Grounding in a Video Corpus. Participants are tasked with creating algorithms that can effectively process both video and text data, understand queries in multiple languages, and accurately respond to complex multi-hop medical questions. The challenge is expected to foster innovations in multimodal reasoning systems applicable to healthcare, enhancing both emergency response capabilities and medical education within multilingual communities. The official website for more information is https://cmivqa.github.io/. <div>
arXiv:2505.06814v1 Announce Type: new 
Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Multi-class Image Classification</title>
<link>https://arxiv.org/abs/2505.06825</link>
<guid>https://arxiv.org/abs/2505.06825</guid>
<content:encoded><![CDATA[
<div> active learning, image classification, CNN classifier, uncertainty metrics, training set size

Summary:
Active learning is proposed as a method to reduce the number of training examples needed for image classification tasks, particularly for CNN classifiers. By assigning values to image examples using different uncertainty metrics, high-value examples can be strategically selected in a smaller training set size. Results on digit recognition and fruit classification datasets demonstrate the effectiveness of active learning, with formal comparisons across four different uncertainty metrics. Active learning shows marked improvement over random sampling, especially for more difficult classification tasks. The study also indicates the viability of active learning for simpler binary classification tasks, showcasing its potential for a wide range of image classification problems. <div>
arXiv:2505.06825v1 Announce Type: new 
Abstract: A principle bottleneck in image classification is the large number of training examples needed to train a classifier. Using active learning, we can reduce the number of training examples to teach a CNN classifier by strategically selecting examples. Assigning values to image examples using different uncertainty metrics allows the model to identify and select high-value examples in a smaller training set size. We demonstrate results for digit recognition and fruit classification on the MNIST and Fruits360 data sets. We formally compare results for four different uncertainty metrics. Finally, we observe active learning is also effective on simpler (binary) classification tasks, but marked improvement from random sampling is more evident on more difficult tasks. We show active learning is a viable algorithm for image classification problems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification</title>
<link>https://arxiv.org/abs/2505.06831</link>
<guid>https://arxiv.org/abs/2505.06831</guid>
<content:encoded><![CDATA[
<div> Keywords: group-robust generalization, spurious correlations, Class-Conditional Distribution Balancing (CCDB), Bias Exploration via Overfitting (BEO), fine-grained variant

Summary: 
- Achieving group-robust generalization without bias annotations is challenging
- Spurious correlations often arise from mismatches in distributions of bias attributes
- CCDB addresses this issue through simple distribution matching but uses a single Gaussian for approximation
- BEO proposes modeling distributions as mixtures of latent groups for more detailed representation
- FG-CCDB, a fine-grained variant, performs precise distribution matching and balancing within each group
- FG-CCDB achieves stronger mitigation of spurious correlations through group-level reweighting
- BEO acts as a proxy for bias annotations and can be integrated with bias-supervised methods
- The combination of BEO and FG-CCDB performs comparably to bias-supervised approaches in binary classification and outperforms them in highly biased multi-class scenarios

<br /><br />Summary: <div>
arXiv:2505.06831v1 Announce Type: new 
Abstract: Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable. Recent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. However, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. To address this limitation, we propose a novel method called Bias Exploration via Overfitting (BEO), which captures each distribution in greater detail by modeling it as a mixture of latent groups. Building on these group-level descriptions, we introduce a fine-grained variant of CCDB, termed FG-CCDB, which performs more precise distribution matching and balancing within each group. Through group-level reweighting, FG-CCDB learns sample weights from a global perspective, achieving stronger mitigation of spurious correlations without incurring substantial storage or computational costs. Extensive experiments demonstrate that BEO serves as a strong proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods. Moreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Instruction Tuning with Chain of Region-of-Interest</title>
<link>https://arxiv.org/abs/2505.06840</link>
<guid>https://arxiv.org/abs/2505.06840</guid>
<content:encoded><![CDATA[
<div> Keywords: high-resolution images, multimodal large language models, Chain of Region-of-Interest, visual instruction tuning, computational efficiency 

Summary: 
The study introduces a novel method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning to address the computational challenges of high-resolution images in multimodal large language models. Inspired by the selective nature of the human visual system, CoRoI prioritizes informative regions in images to improve visual comprehension and recognition without processing lengthy high-resolution tokens. Extensive experiments across 11 benchmarks validate the effectiveness of CoRoI across various model sizes, consistently outperforming existing methods like LLaVA-NeXT and proprietary models like Gemini Pro 1.0 and GPT-4V on multiple benchmarks. The proposed method demonstrates superior performance, especially with a finetuned 34B model surpassing proprietary methods on six benchmarks and outperforming GPT-4V on specific tasks. Overall, CoRoI enhances multimodal visual comprehension while providing computational efficiency for large language models. 

<br /><br />Summary: <div>
arXiv:2505.06840v1 Announce Type: new 
Abstract: High-resolution (HR) images are pivotal for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs). However, directly increasing image resolution can significantly escalate computational demands. In this study, we propose a method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating the computational burden associated with high-resolution images for MLLMs. Drawing inspiration from the selective nature of the human visual system, we recognize that not all regions within high-resolution images carry equal importance. CoRoI seeks to identify and prioritize the most informative regions, thereby enhancing multimodal visual comprehension and recognition while circumventing the need for processing lengthy HR image tokens. Through extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI across varying sizes, ranging from 7B to 34B in parameters. Our models consistently demonstrate superior performance across diverse multimodal benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all benchmarks and our finetuned 34B model surpasses proprietary methods like Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB, SEED-I, and MME.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach</title>
<link>https://arxiv.org/abs/2505.06853</link>
<guid>https://arxiv.org/abs/2505.06853</guid>
<content:encoded><![CDATA[
<div> Pan American Health Organization, cancer cases, Latin America, osteosarcoma, surgical safety margins

Summary:
The Pan American Health Organization reports a rising trend in cancer cases in Latin America, with osteosarcoma being a common and lethal bone cancer affecting the youth. Detection of osteosarcoma poses challenges due to its unique characteristics. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while sparing healthy tissue. A novel approach is proposed in this study for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The method utilizes MRI and X-ray data, digital processing techniques, and unsupervised learning algorithms like k-means clustering to delineate tumor boundaries. Results from experiments demonstrate the potential of automated, patient-specific determination of safety margins, which could enhance the efficiency and accuracy of osteosarcoma surgery.<br /><br />Summary: <div>
arXiv:2505.06853v1 Announce Type: new 
Abstract: According to the Pan American Health Organization, the number of cancer cases in Latin America was estimated at 4.2 million in 2022 and is projected to rise to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone cancers affecting young people, is difficult to detect due to its unique texture and intensity. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while preserving healthy tissue. Therefore, this study proposes a method for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The proposed approach uses MRI and X-ray data from open-source repositories, digital processing techniques, and unsupervised learning algorithms (such as k-means clustering) to define tumor boundaries. Experimental results highlight the potential for automated, patient-specific determination of safety margins.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</title>
<link>https://arxiv.org/abs/2505.06855</link>
<guid>https://arxiv.org/abs/2505.06855</guid>
<content:encoded><![CDATA[
<div> Keywords: text recognition, self-supervised learning, masked image modeling, multi-masking strategy, real-world datasets

Summary:
The study focuses on improving text recognition methods by addressing the limitations of training on synthetic datasets. Existing techniques suffer from performance disparities when handling complex real-world images due to their inability to replicate real-world scenarios accurately. The researchers propose a Multi-Masking Strategy (MMS) that integrates different masking techniques, including random patch, blockwise, and span masking, into the Masked Image Modeling (MIM) framework. By combining low and high-level textual representations, MMS outperforms existing self-supervised methods after fine-tuning with real data. The approach enhances performance in various text-related tasks such as recognition, segmentation, and text-image super-resolution. This novel strategy aims to bridge the gap between synthetic and real-world data, leading to more accurate and robust text recognition systems.<br /><br />Summary: <div>
arXiv:2505.06855v1 Announce Type: new 
Abstract: Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuRN: Neuro-inspired Domain Generalization for Image Classification</title>
<link>https://arxiv.org/abs/2505.06881</link>
<guid>https://arxiv.org/abs/2505.06881</guid>
<content:encoded><![CDATA[
<div> Neural Response Normalization, domain generalization, image classification, deep learning architectures, neuro-inspired

Summary: 
Neural Response Normalization (NeuRN) layer is introduced to improve domain generalization in image classification tasks. Inspired by neurons in the visual cortex, NeuRN aims to enhance deep learning models' performance on unseen datasets. Experimenting with various deep learning architectures, including those from Neural Architecture Search and Vision Transformer, models integrated with NeuRN show improved performance compared to baseline models. A novel method that uses the Needleman-Wunsch algorithm is proposed to compute similarity between deep learning architectures, aiding in selecting models for experimentation. Results demonstrate the effectiveness of NeuRN in cross-domain image classification, paving the way for future neuro-inspired deep learning models. 

<br /><br />Summary: <div>
arXiv:2505.06881v1 Announce Type: new 
Abstract: Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.06886</link>
<guid>https://arxiv.org/abs/2505.06886</guid>
<content:encoded><![CDATA[
<div> Keywords: mouse, visual cortex, deep learning, NeuRN, neural representations  

<br /><br />Summary: The mouse serves as a vital model in systems neuroscience, particularly in understanding how its visual cortex responds to various natural scene stimuli. This study examines the functional alignment of the mouse visual cortex with deep learning models in object classification tasks. A novel representational learning strategy reveals a significant resemblance between the mouse's visual processing and top-performing deep learning models, observed at both population and single-cell levels. The research introduces a Neural Response Normalization (NeuRN) layer, inspired by the activation profiles of excitatory and inhibitory neurons, which enhances the representational similarity. Testing NeuRN within deep learning frameworks demonstrates marked improvements in robustness, especially against data shifts in domain generalization tasks. This work outlines a transformative approach for comparing the mouse visual cortex's architecture with advanced AI models, enabling a deeper understanding of neural representations. The findings suggest that deep learning models can benefit from insights gained from mouse vision, potentially leading to enhanced performance in real-world applications. Overall, this research holds significant implications for advancing AI models by integrating insights from biological systems. <div>
arXiv:2505.06886v1 Announce Type: new 
Abstract: The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.06894</link>
<guid>https://arxiv.org/abs/2505.06894</guid>
<content:encoded><![CDATA[
<div> NeuGen, brain-inspired normalization, NeRF architectures, generalization, image rendering

Summary:
Neural Radiance Fields (NeRF) have revolutionized novel view synthesis but face challenges in generalizing across diverse scenes. This study proposes integrating a brain-inspired normalization technique, NeuGen, into leading NeRF architectures like MVSNeRF and GeoNeRF. NeuGen extracts domain-invariant features to enhance models' generalization capabilities, improving accuracy and robustness in image rendering. Integration of NeuGen shows enhanced performance in benchmarks across diverse datasets, enabling better generalization across varied scenes. Comprehensive evaluations, both quantitative and qualitative, demonstrate that this approach outperforms existing models in generalizability and rendering quality. By merging neuroscientific principles with deep learning frameworks, this work sets a new standard for improved generalizability and efficiency in novel view synthesis. A demo showcasing the study is available at https://neugennerf.github.io.

<br /><br />Summary: <div>
arXiv:2505.06894v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2505.06898</link>
<guid>https://arxiv.org/abs/2505.06898</guid>
<content:encoded><![CDATA[
<div> Keywords: Generalist Medical AI, multi-modal explainability, prognostic capabilities, uncertainty quantification, clinician-centric  

<br /><br />Summary: This article introduces XMedGPT, a multi-modal AI assistant designed for clinical use, addressing limitations in existing Generalist Medical AI (GMAI) systems. XMedGPT enhances medical decision-making by integrating textual and visual interpretability, providing accurate diagnostic outputs while grounding anatomical references in medical images. The system introduces a reliability indexing mechanism to quantify uncertainty through interactive question-answering and produces validated results across four key areas: multi-modal interpretability, uncertainty quantification, prognostic modeling, and rigorous benchmarking. The model achieves a notable IoU of 0.703 across anatomical regions and a Kendall's tau-b of 0.479, reflecting strong alignment between visual rationales and clinical outcomes. It excels in uncertainty estimation with AUC scores of 0.862 for visual question answering and 0.764 for radiology report generation. In cancer prognosis, it outperforms prior models by 26.9% and GPT-4o by 25.0%. Availability of extensive benchmarking across 347 datasets and validation across four anatomical systems indicates the model's exceptional generalizability, evidenced by performance improvements of 20.7% in-domain and 16.7% on a large in-house dataset. XMedGPT signifies a notable advancement in clinician-centered AI integration for diverse healthcare applications. <div>
arXiv:2505.06898v1 Announce Type: new 
Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection</title>
<link>https://arxiv.org/abs/2505.06903</link>
<guid>https://arxiv.org/abs/2505.06903</guid>
<content:encoded><![CDATA[
arXiv:2505.06903v1 Announce Type: new 
Abstract: Temporal medical image analysis is essential for clinical decision-making, yet existing methods either align images and text at a coarse level - causing potential semantic mismatches - or depend solely on visual information, lacking medical semantic integration. We present CheXLearner, the first end-to-end framework that unifies anatomical region detection, Riemannian manifold-based structure alignment, and fine-grained regional semantic guidance. Our proposed Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to robustly align anatomical structures and capture pathologically meaningful discrepancies across temporal chest X-rays. By introducing regional progression descriptions as supervision, CheXLearner achieves enhanced cross-modal representation learning and supports dynamic low-level feature optimization. Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and 80.32% (+11.05%) F1-score on anatomical region progression detection - substantially outperforming state-of-the-art baselines, especially in structurally complex regions. Additionally, our model attains a 91.52% average AUC score in downstream disease classification, validating its superior feature representation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</title>
<link>https://arxiv.org/abs/2505.06905</link>
<guid>https://arxiv.org/abs/2505.06905</guid>
<content:encoded><![CDATA[
arXiv:2505.06905v1 Announce Type: new 
Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. Recently, models trained on synthetic data and refined through domain adaptation have shown remarkable performance in MHE, yet it remains unclear how these models make predictions or how reliable they truly are. In this paper, we investigate a state-of-the-art MHE model trained purely on synthetic data to explore where the model looks when making height predictions. Through systematic analyses, we find that the model relies heavily on shadow cues, a factor that can lead to overestimation or underestimation of heights when shadows deviate from expected norms. Furthermore, the inherent difficulty of evaluating regression tasks with the human eye underscores additional limitations of purely synthetic training. To address these issues, we propose a novel correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep-learning outputs to improve local accuracy and achieve spatially consistent corrections. Our method comprises two stages: pre-processing raw ICESat-2 data, followed by a random forest-based approach to densely refine height estimates. Experiments in three representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal substantial error reductions, with mean absolute error (MAE) decreased by 22.8\%, 6.9\%, and 4.9\%, respectively. These findings highlight the critical role of shadow awareness in synthetic data-driven models and demonstrate how fusing imperfect real-world LiDAR data can bolster the robustness of MHE, paving the way for more reliable and scalable 3D mapping solutions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI</title>
<link>https://arxiv.org/abs/2505.06912</link>
<guid>https://arxiv.org/abs/2505.06912</guid>
<content:encoded><![CDATA[
arXiv:2505.06912v1 Announce Type: new 
Abstract: Despite strong performance in medical question-answering, the clinical adoption of Large Language Models (LLMs) is critically hampered by their opaque 'black-box' reasoning, limiting clinician trust. This challenge is compounded by the predominant reliance of current medical LLMs on corpora from scientific literature or synthetic data, which often lack the granular expert validation and high clinical relevance essential for advancing their specialized medical capabilities. To address these critical gaps, we introduce a highly clinically relevant dataset with 31,247 medical question-answer pairs, each accompanied by expert-validated chain-of-thought (CoT) explanations. This resource, spanning multiple clinical domains, was curated via a scalable human-LLM hybrid pipeline: LLM-generated rationales were iteratively reviewed, scored, and refined by medical experts against a structured rubric, with substandard outputs revised through human effort or guided LLM regeneration until expert consensus. This publicly available dataset provides a vital source for the development of medical LLMs that capable of transparent and verifiable reasoning, thereby advancing safer and more interpretable AI in medicine.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion</title>
<link>https://arxiv.org/abs/2505.06920</link>
<guid>https://arxiv.org/abs/2505.06920</guid>
<content:encoded><![CDATA[
arXiv:2505.06920v1 Announce Type: new 
Abstract: Acquiring accurately aligned multi-modal image pairs is fundamental for achieving high-quality multi-modal image fusion. To address the lack of ground truth in current multi-modal image registration and fusion methods, we propose a novel self-supervised \textbf{B}i-directional \textbf{S}elf-\textbf{R}egistration framework (\textbf{B-SR}). Specifically, B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator (IPDG) to achieve self-supervised global-local registration. Visible-infrared image pairs with spatially misaligned differences are aligned to obtain global differences through the registration module. The same image pairs are processed by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain local differences. IPDG converts the obtained local differences into pseudo-global differences, which are used to perform global-local difference consistency with the global differences. Furthermore, aiming at eliminating the effect of modal gaps on the registration module, we design a neighborhood dynamic alignment loss to achieve cross-modal image edge alignment. Extensive experiments on misaligned multi-modal images demonstrate the effectiveness of the proposed method in multi-modal image alignment and fusion against the competing methods. Our code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network</title>
<link>https://arxiv.org/abs/2505.06937</link>
<guid>https://arxiv.org/abs/2505.06937</guid>
<content:encoded><![CDATA[
arXiv:2505.06937v1 Announce Type: new 
Abstract: In this paper, the dual-optical attention fusion crowd head point counting model (TAPNet) is proposed to address the problem of the difficulty of accurate counting in complex scenes such as crowd dense occlusion and low light in crowd counting tasks under UAV view. The model designs a dual-optical attention fusion module (DAFP) by introducing complementary information from infrared images to improve the accuracy and robustness of all-day crowd counting. In order to fully utilize different modal information and solve the problem of inaccurate localization caused by systematic misalignment between image pairs, this paper also proposes an adaptive two-optical feature decomposition fusion module (AFDF). In addition, we optimize the training strategy to improve the model robustness through spatial random offset data augmentation. Experiments on two challenging public datasets, DroneRGBT and GAIIC2, show that the proposed method outperforms existing techniques in terms of performance, especially in challenging dense low-light scenes. Code is available at https://github.com/zz-zik/TAPNet
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning for Class Distribution Mismatch</title>
<link>https://arxiv.org/abs/2505.06948</link>
<guid>https://arxiv.org/abs/2505.06948</guid>
<content:encoded><![CDATA[
arXiv:2505.06948v1 Announce Type: new 
Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an "other" category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDM's superiority over previous semi-supervised methods. Specifically, with a 60% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.06951</link>
<guid>https://arxiv.org/abs/2505.06951</guid>
<content:encoded><![CDATA[
arXiv:2505.06951v1 Announce Type: new 
Abstract: In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.06975</link>
<guid>https://arxiv.org/abs/2505.06975</guid>
<content:encoded><![CDATA[
arXiv:2505.06975v1 Announce Type: new 
Abstract: The primary challenge in accelerating image super-resolution lies in reducing computation while maintaining performance and adaptability. Motivated by the observation that high-frequency regions (e.g., edges and textures) are most critical for reconstruction, we propose a training-free adaptive masking module for acceleration that dynamically focuses computation on these challenging areas. Specifically, our method first extracts high-frequency components via Gaussian blur subtraction and adaptively generates binary masks using K-means clustering to identify regions requiring intensive processing. Our method can be easily integrated with both CNNs and Transformers. For CNN-based architectures, we replace standard $3 \times 3$ convolutions with an unfold operation followed by $1 \times 1$ convolutions, enabling pixel-wise sparse computation guided by the mask. For Transformer-based models, we partition the mask into non-overlapping windows and selectively process tokens based on their average values. During inference, unnecessary pixels or windows are pruned, significantly reducing computation. Moreover, our method supports dilation-based mask adjustment to control the processing scope without retraining, and is robust to unseen degradations (e.g., noise, compression). Extensive experiments on benchmarks demonstrate that our method reduces FLOPs by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving comparable or better quantitative metrics. The source code is available at https://github.com/shangwei5/AMSR
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition</title>
<link>https://arxiv.org/abs/2505.06982</link>
<guid>https://arxiv.org/abs/2505.06982</guid>
<content:encoded><![CDATA[
arXiv:2505.06982v1 Announce Type: new 
Abstract: Recent progress in image-based medical disease detection encounters challenges such as limited annotated data sets, inadequate spatial feature analysis, data security issues, and inefficient training frameworks. This study introduces a data-efficient image transformer (DeIT)-based approach that overcomes these challenges by utilizing multiscale patch embedding for better feature extraction and stratified weighted random sampling to address class imbalance. The model also incorporates a LoRA-enhanced transformer encoder, a distillation framework, and federated learning for decentralized training, improving both efficiency and data security. Consequently, it achieves state-of-the-art performance, with the highest AUC, F1 score, precision, minimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations improve interpretability by highlighting critical pathological regions, enhancing the model's clinical relevance. These results highlight the potential of this approach to advance AI-powered medical imaging and disease detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation</title>
<link>https://arxiv.org/abs/2505.06985</link>
<guid>https://arxiv.org/abs/2505.06985</guid>
<content:encoded><![CDATA[
arXiv:2505.06985v1 Announce Type: new 
Abstract: Both zero-shot and tuning-based customized text-to-image (CT2I) generation have made significant progress for storytelling content creation. In contrast, research on customized text-to-video (CT2V) generation remains relatively limited. Existing zero-shot CT2V methods suffer from poor generalization, while another line of work directly combining tuning-based T2I models with temporal motion modules often leads to the loss of structural and texture information. To bridge this gap, we propose an autoregressive structure and texture propagation module (STPM), which extracts key structural and texture features from the reference subject and injects them autoregressively into each video frame to enhance consistency. Additionally, we introduce a test-time reward optimization (TTRO) method to further refine fine-grained details. Quantitative and qualitative experiments validate the effectiveness of STPM and TTRO, demonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency metrics over the baseline, respectively.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</title>
<link>https://arxiv.org/abs/2505.06991</link>
<guid>https://arxiv.org/abs/2505.06991</guid>
<content:encoded><![CDATA[
arXiv:2505.06991v1 Announce Type: new 
Abstract: This report presents our semantic segmentation framework developed by team ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which focuses on parsing outdoor scenes into nine semantic categories under real-world conditions. Our method integrates a Swin Transformer backbone enhanced with Rotary Position Embedding (RoPE) for improved spatial generalization, alongside a Color Shift Estimation-and-Correction module designed to compensate for illumination inconsistencies in natural environments. To further improve training stability, we adopt a quantile-based denoising strategy that downweights the top 2.5\% of highest-error pixels, treating them as noise and suppressing their influence during optimization. Evaluated on the official GOOSE test set, our approach achieved a mean Intersection over Union (mIoU) of 0.848, demonstrating the effectiveness of combining color correction, positional encoding, and error-aware denoising in robust semantic segmentation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.06995</link>
<guid>https://arxiv.org/abs/2505.06995</guid>
<content:encoded><![CDATA[
arXiv:2505.06995v1 Announce Type: new 
Abstract: Recent advancements in text-to-image diffusion models are hindered by high computational demands, limiting accessibility and scalability. This paper introduces KDC-Diff, a novel stable diffusion framework that enhances efficiency while maintaining image quality. KDC-Diff features a streamlined U-Net architecture with nearly half the parameters of the original U-Net (482M), significantly reducing model complexity. We propose a dual-layered distillation strategy to ensure high-fidelity generation, transferring semantic and structural insights from a teacher to a compact student model while minimizing quality degradation. Additionally, replay-based continual learning is integrated to mitigate catastrophic forgetting, allowing the model to retain prior knowledge while adapting to new data. Despite operating under extremely low computational resources, KDC-Diff achieves state-of-the-art performance on the Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly reduces inference time compared to existing models. These results establish KDC-Diff as a highly efficient and adaptable solution for text-to-image generation, particularly in computationally constrained environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07001</link>
<guid>https://arxiv.org/abs/2505.07001</guid>
<content:encoded><![CDATA[
arXiv:2505.07001v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination--the tendency to generate descriptions that are inconsistent with the visual content--remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: https://github.com/bhattarailab/Hallucination-Aware-VLM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation</title>
<link>https://arxiv.org/abs/2505.07003</link>
<guid>https://arxiv.org/abs/2505.07003</guid>
<content:encoded><![CDATA[
arXiv:2505.07003v1 Announce Type: new 
Abstract: Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</title>
<link>https://arxiv.org/abs/2505.07007</link>
<guid>https://arxiv.org/abs/2505.07007</guid>
<content:encoded><![CDATA[
arXiv:2505.07007v1 Announce Type: new 
Abstract: Micro-expressions (MEs) are crucial psychological responses with significant potential for affective computing. However, current automatic micro-expression recognition (MER) research primarily focuses on discrete emotion classification, neglecting a convincing analysis of the subtle dynamic movements and inherent emotional cues. The rapid progress in multimodal large language models (MLLMs), known for their strong multimodal comprehension and language generation abilities, offers new possibilities. MLLMs have shown success in various vision-language tasks, indicating their potential to understand MEs comprehensively, including both fine-grained motion patterns and underlying emotional semantics. Nevertheless, challenges remain due to the subtle intensity and short duration of MEs, as existing MLLMs are not designed to capture such delicate frame-level facial dynamics. In this paper, we propose a novel Micro-Expression Large Language Model (MELLM), which incorporates a subtle facial motion perception strategy with the strong inference capabilities of MLLMs, representing the first exploration of MLLMs in the domain of ME analysis. Specifically, to explicitly guide the MLLM toward motion-sensitive regions, we construct an interpretable motion-enhanced color map by fusing onset-apex optical flow dynamics with the corresponding grayscale onset frame as the model input. Additionally, specialized fine-tuning strategies are incorporated to further enhance the model's visual perception of MEs. Furthermore, we construct an instruction-description dataset based on Facial Action Coding System (FACS) annotations and emotion labels to train our MELLM. Comprehensive evaluations across multiple benchmark datasets demonstrate that our model exhibits superior robustness and generalization capabilities in ME understanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization</title>
<link>https://arxiv.org/abs/2505.07013</link>
<guid>https://arxiv.org/abs/2505.07013</guid>
<content:encoded><![CDATA[
arXiv:2505.07013v1 Announce Type: new 
Abstract: Remote physiological sensing using camera-based technologies offers transformative potential for non-invasive vital sign monitoring across healthcare and human-computer interaction domains. Although deep learning approaches have advanced the extraction of physiological signals from video data, existing methods have not been sufficiently assessed for their robustness to domain shifts. These shifts in remote physiological sensing include variations in ambient conditions, camera specifications, head movements, facial poses, and physiological states which often impact real-world performance significantly. Cross-dataset evaluation provides an objective measure to assess generalization capabilities across these domain shifts. We introduce Target Signal Constrained Factorization module (TSFM), a novel multidimensional attention mechanism that explicitly incorporates physiological signal characteristics as factorization constraints, allowing more precise feature extraction. Building on this innovation, we present MMRPhys, an efficient dual-branch 3D-CNN architecture designed for simultaneous multitask estimation of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly outperforms state-of-the-art methods in generalization across domain shifts for rPPG and rRSP estimation, while maintaining a minimal inference latency suitable for real-time applications. Our approach establishes new benchmarks for robust multitask and multimodal physiological sensing and offers a computationally efficient framework for practical deployment in unconstrained environments. The web browser-based application featuring on-device real-time inference of MMRPhys model is available at https://physiologicailab.github.io/mmrphys-live
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision-Language Foundation Model for Leaf Disease Identification</title>
<link>https://arxiv.org/abs/2505.07019</link>
<guid>https://arxiv.org/abs/2505.07019</guid>
<content:encoded><![CDATA[
arXiv:2505.07019v1 Announce Type: new 
Abstract: Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD's effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at https://huggingface.co/enalis/scold
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MarkMatch: Same-Hand Stuffing Detection</title>
<link>https://arxiv.org/abs/2505.07032</link>
<guid>https://arxiv.org/abs/2505.07032</guid>
<content:encoded><![CDATA[
arXiv:2505.07032v1 Announce Type: new 
Abstract: We present MarkMatch, a retrieval system for detecting whether two paper ballot marks were filled by the same hand. Unlike the previous SOTA method BubbleSig, which used binary classification on isolated mark pairs, MarkMatch ranks stylistic similarity between a query mark and a mark in the database using contrastive learning. Our model is trained with a dense batch similarity matrix and a dual loss objective. Each sample is contrasted against many negatives within each batch, enabling the model to learn subtle handwriting difference and improve generalization under handwriting variation and visual noise, while diagonal supervision reinforces high confidence on true matches. The model achieves an F1 score of 0.943, surpassing BubbleSig's best performance. MarkMatch also integrates Segment Anything Model for flexible mark extraction via box- or point-based prompts. The system offers election auditors a practical tool for visual, non-biometric investigation of suspicious ballots.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection</title>
<link>https://arxiv.org/abs/2505.07040</link>
<guid>https://arxiv.org/abs/2505.07040</guid>
<content:encoded><![CDATA[
arXiv:2505.07040v1 Announce Type: new 
Abstract: Fabric defect detection confronts two fundamental challenges. First, conventional non-maximum suppression disrupts gradient flow, which hinders genuine end-to-end learning. Second, acquiring pixel-level annotations at industrial scale is prohibitively costly. Addressing these limitations, we propose a differentiable NMS framework for fabric defect detection that achieves superior localization precision through end-to-end optimization. We reformulate NMS as a differentiable bipartite matching problem solved through the Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow throughout the network. This approach specifically targets the irregular morphologies and ambiguous boundaries of fabric defects by integrating proposal quality, feature similarity, and spatial relationships. Our entropy-constrained mask refinement mechanism further enhances localization precision through principled uncertainty modeling. Extensive experiments on the Tianchi fabric defect dataset demonstrate significant performance improvements over existing methods while maintaining real-time speeds suitable for industrial deployment. The framework exhibits remarkable adaptability across different architectures and generalizes effectively to general object detection tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.07050</link>
<guid>https://arxiv.org/abs/2505.07050</guid>
<content:encoded><![CDATA[
arXiv:2505.07050v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation (UDA) aims to align source and target domain distributions to close the domain gap, but still struggles with obtaining the target data. Fortunately, Domain Generalization (DG) excels without the need for any target data. Recent works expose that depth maps contribute to improved generalized performance in the UDA tasks, but they ignore the noise and holes in depth maps due to device and environmental factors, failing to sufficiently and effectively learn domain-invariant representation. Although high-sensitivity region suppression has shown promising results in learning domain-invariant features, existing methods cannot be directly applicable to depth maps due to their unique characteristics. Hence, we propose a novel framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal stylization flow (DSSS), focusing on learning domain-invariant features from depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D inter-modal stylization flow to generate stylized depth maps for sensitivity detection, cleverly utilizing RGB information as the stylization source. Then, a class-wise soft spatial sensitivity suppression is designed to identify and emphasize non-sensitive depth features that contain more domain-invariant information. Furthermore, an RGB-D soft alignment loss is proposed to ensure that the stylized depth maps only align part of the RGB features while still retaining the unique depth information. To our best knowledge, our DSSS framework is the first work to integrate RGB and Depth information in the multi-class DG semantic segmentation task. Extensive experiments over multiple backbone networks show that our framework achieves remarkable performance improvement.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.07057</link>
<guid>https://arxiv.org/abs/2505.07057</guid>
<content:encoded><![CDATA[
arXiv:2505.07057v1 Announce Type: new 
Abstract: Video generation based on diffusion models presents a challenging multimodal task, with video editing emerging as a pivotal direction in this field. Recent video editing approaches primarily fall into two categories: training-required and training-free methods. While training-based methods incur high computational costs, training-free alternatives often yield suboptimal performance. To address these limitations, we propose DAPE, a high-quality yet cost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for video editing. In the first stage, we design an efficient norm-tuning method to enhance temporal consistency in generated videos. The second stage introduces a vision-friendly adapter to improve visual quality. Additionally, we identify critical shortcomings in existing benchmarks, including limited category diversity, imbalanced object distribution, and inconsistent frame counts. To mitigate these issues, we curate a large dataset benchmark comprising 232 videos with rich annotations and 6 editing prompts, enabling objective and comprehensive evaluation of advanced methods. Extensive experiments on existing datasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate that DAPE significantly improves temporal coherence and text-video alignment while outperforming previous state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed1.5-VL Technical Report</title>
<link>https://arxiv.org/abs/2505.07062</link>
<guid>https://arxiv.org/abs/2505.07062</guid>
<content:encoded><![CDATA[
arXiv:2505.07062v1 Announce Type: new 
Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.07071</link>
<guid>https://arxiv.org/abs/2505.07071</guid>
<content:encoded><![CDATA[
arXiv:2505.07071v1 Announce Type: new 
Abstract: Diffusion-based image super-resolution (SR) methods have demonstrated remarkable performance. Recent advancements have introduced deterministic sampling processes that reduce inference from 15 iterative steps to a single step, thereby significantly improving the inference speed of existing diffusion models. However, their efficiency remains limited when handling complex semantic regions due to the single-step inference. To address this limitation, we propose SAMSR, a semantic-guided diffusion framework that incorporates semantic segmentation masks into the sampling process. Specifically, we introduce the SAM-Noise Module, which refines Gaussian noise using segmentation masks to preserve spatial and semantic features. Furthermore, we develop a pixel-wise sampling strategy that dynamically adjusts the residual transfer rate and noise strength based on pixel-level semantic weights, prioritizing semantically rich regions during the diffusion process. To enhance model training, we also propose a semantic consistency loss, which aligns pixel-wise semantic weights between predictions and ground truth. Extensive experiments on both real-world and synthetic datasets demonstrate that SAMSR significantly improves perceptual quality and detail recovery, particularly in semantically complex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering</title>
<link>https://arxiv.org/abs/2505.07073</link>
<guid>https://arxiv.org/abs/2505.07073</guid>
<content:encoded><![CDATA[
arXiv:2505.07073v1 Announce Type: new 
Abstract: Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. Recently, the Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework, introduced by Varshney et al. (2025), attempts to identify concepts via dimension-wise traversal of the latent space of a Variational Autoencoder trained on counterfactual trajectories. Extending the CDCT framework, this work introduces Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC substantially reduces computational complexity by eliminating the exhaustive latent dimension traversal required in CDCT and enables the extraction of multidimensional semantic concepts encoded across the latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression</title>
<link>https://arxiv.org/abs/2505.07119</link>
<guid>https://arxiv.org/abs/2505.07119</guid>
<content:encoded><![CDATA[
arXiv:2505.07119v1 Announce Type: new 
Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where minimizing waste and operational costs is essential. Deploying deep learning models within Internet of Things (IoT) environments introduces specific challenges due to the limited computational power and bandwidth of edge devices. This study investigates how to perform VAD effectively under such constraints by leveraging compact and efficient processing strategies. We evaluate several data compression techniques, examining the trade-off between system latency and detection accuracy. Experiments on the MVTec AD benchmark demonstrate that significant compression can be achieved with minimal loss in anomaly detection performance compared to uncompressed data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework</title>
<link>https://arxiv.org/abs/2505.07165</link>
<guid>https://arxiv.org/abs/2505.07165</guid>
<content:encoded><![CDATA[
arXiv:2505.07165v1 Announce Type: new 
Abstract: Recently, numerous pancreas segmentation methods have achieved promising performance on local single-source datasets. However, these methods don't adequately account for generalizability issues, and hence typically show limited performance and low stability on test data from other sources. Considering the limited availability of distinct data sources, we seek to improve the generalization performance of a pancreas segmentation model trained with a single-source dataset, i.e., the single source generalization task. In particular, we propose a dual self-supervised learning model that incorporates both global and local anatomical contexts. Our model aims to fully exploit the anatomical features of the intra-pancreatic and extra-pancreatic regions, and hence enhance the characterization of the high-uncertainty regions for more robust generalization. Specifically, we first construct a global-feature contrastive self-supervised learning module that is guided by the pancreatic spatial structure. This module obtains complete and consistent pancreatic features through promoting intra-class cohesion, and also extracts more discriminative features for differentiating between pancreatic and non-pancreatic tissues through maximizing inter-class separation. It mitigates the influence of surrounding tissue on the segmentation outcomes in high-uncertainty regions. Subsequently, a local-image restoration self-supervised learning module is introduced to further enhance the characterization of the high uncertainty regions. In this module, informative anatomical contexts are actually learned to recover randomly corrupted appearance patterns in those regions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.07172</link>
<guid>https://arxiv.org/abs/2505.07172</guid>
<content:encoded><![CDATA[
arXiv:2505.07172v1 Announce Type: new 
Abstract: Despite significant advancements in multimodal reasoning tasks, existing Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded responses when interpreting associated images. In contrast, when humans embark on learning new knowledge, they often rely on a set of fundamental pre-study principles: reviewing outlines to grasp core concepts, summarizing key points to guide their focus and enhance understanding. However, such preparatory actions are notably absent in the current instruction tuning processes. This paper presents Re-Critic, an easily scalable rationale-augmented framework designed to incorporate fundamental rules and chain-of-thought (CoT) as a bridge to enhance reasoning abilities. Specifically, Re-Critic develops a visual rationale synthesizer that scalably augments raw instructions with rationale explanation. To probe more contextually grounded responses, Re-Critic employs an in-context self-critic mechanism to select response pairs for preference tuning. Experiments demonstrate that models fine-tuned with our rationale-augmented dataset yield gains that extend beyond hallucination-specific tasks to broader multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranking-aware Continual Learning for LiDAR Place Recognition</title>
<link>https://arxiv.org/abs/2505.07198</link>
<guid>https://arxiv.org/abs/2505.07198</guid>
<content:encoded><![CDATA[
arXiv:2505.07198v1 Announce Type: new 
Abstract: Place recognition plays a significant role in SLAM, robot navigation, and autonomous driving applications. Benefiting from deep learning, the performance of LiDAR place recognition (LPR) has been greatly improved. However, many existing learning-based LPR methods suffer from catastrophic forgetting, which severely harms the performance of LPR on previously trained places after training on a new environment. In this paper, we introduce a continual learning framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate forgetting. Inspired by the ranking process of place recognition retrieval, we present a ranking-aware knowledge distillation loss that encourages the network to preserve the high-level place recognition knowledge. We also introduce a knowledge fusion module to integrate the knowledge of old and new models for LiDAR place recognition. Our extensive experiments demonstrate that KDF can be applied to different networks to overcome catastrophic forgetting, surpassing the state-of-the-art methods in terms of mean Recall@1 and forgetting score.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2505.07209</link>
<guid>https://arxiv.org/abs/2505.07209</guid>
<content:encoded><![CDATA[
arXiv:2505.07209v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) try to make the decision-making process transparent by exploring an intermediate concept space between the input image and the output prediction. Existing CBMs just learn coarse-grained relations between the whole image and the concepts, less considering local image information, leading to two main drawbacks: i) they often produce spurious visual-concept relations, hence decreasing model reliability; and ii) though CBMs could explain the importance of every concept to the final prediction, it is still challenging to tell which visual region produces the prediction. To solve these problems, this paper proposes a Disentangled Optimal Transport CBM (DOT-CBM) framework to explore fine-grained visual-concept relations between local image patches and concepts. Specifically, we model the concept prediction process as a transportation problem between the patches and concepts, thereby achieving explicit fine-grained feature alignment. We also incorporate orthogonal projection losses within the modality to enhance local feature disentanglement. To further address the shortcut issues caused by statistical biases in the data, we utilize the visual saliency map and concept label statistics as transportation priors. Thus, DOT-CBM can visualize inversion heatmaps, provide more reliable concept predictions, and produce more accurate class predictions. Comprehensive experiments demonstrate that our proposed DOT-CBM achieves SOTA performance on several tasks, including image classification, local part detection and out-of-distribution generalization.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection</title>
<link>https://arxiv.org/abs/2505.07219</link>
<guid>https://arxiv.org/abs/2505.07219</guid>
<content:encoded><![CDATA[
arXiv:2505.07219v1 Announce Type: new 
Abstract: Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at https://github.com/qinhongda8/LDDS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dance Video Archives Challenge Computer Vision</title>
<link>https://arxiv.org/abs/2505.07249</link>
<guid>https://arxiv.org/abs/2505.07249</guid>
<content:encoded><![CDATA[
arXiv:2505.07249v1 Announce Type: new 
Abstract: The accuracy and efficiency of human body pose estimation depend on the quality of the data to be processed and of the particularities of these data. To demonstrate how dance videos can challenge pose estimation techniques, we proposed a new 3D human body pose estimation pipeline which combined up-to-date techniques and methods that had not been yet used in dance analysis. Second, we performed tests and extensive experimentations from dance video archives, and used visual analytic tools to evaluate the impact of several data parameters on human body pose. Our results are publicly available for research at https://www.couleur.org/articles/arXiv-1-2025/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incomplete In-context Learning</title>
<link>https://arxiv.org/abs/2505.07251</link>
<guid>https://arxiv.org/abs/2505.07251</guid>
<content:encoded><![CDATA[
arXiv:2505.07251v1 Announce Type: new 
Abstract: Large vision language models (LVLMs) achieve remarkable performance through Vision In-context Learning (VICL), a process that depends significantly on demonstrations retrieved from an extensive collection of annotated examples (retrieval database). Existing studies often assume that the retrieval database contains annotated examples for all labels. However, in real-world scenarios, delays in database updates or incomplete data annotation may result in the retrieval database containing labeled samples for only a subset of classes. We refer to this phenomenon as an \textbf{incomplete retrieval database} and define the in-context learning under this condition as \textbf{Incomplete In-context Learning (IICL)}. To address this challenge, we propose \textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage framework designed to mitigate the limitations of IICL. The Iterative Judgments Stage reformulates an \(\boldsymbol{m}\)-class classification problem into a series of \(\boldsymbol{m}\) binary classification tasks, effectively converting the IICL setting into a standard VICL scenario. The Integrated Prediction Stage further refines the classification process by leveraging both the input image and the predictions from the Iterative Judgments Stage to enhance overall classification accuracy. IJIP demonstrates considerable performance across two LVLMs and two datasets under three distinct conditions of label incompleteness, achieving the highest accuracy of 93.9\%. Notably, even in scenarios where labels are fully available, IJIP still achieves the best performance of all six baselines. Furthermore, IJIP can be directly applied to \textbf{Prompt Learning} and is adaptable to the \textbf{text domain}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2505.07254</link>
<guid>https://arxiv.org/abs/2505.07254</guid>
<content:encoded><![CDATA[
arXiv:2505.07254v1 Announce Type: new 
Abstract: This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and 0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\% in higher order tracking accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Similarity Search in Automotive Production</title>
<link>https://arxiv.org/abs/2505.07256</link>
<guid>https://arxiv.org/abs/2505.07256</guid>
<content:encoded><![CDATA[
arXiv:2505.07256v1 Announce Type: new 
Abstract: Visual quality inspection in automotive production is essential for ensuring the safety and reliability of vehicles. Computer vision (CV) has become a popular solution for these inspections due to its cost-effectiveness and reliability. However, CV models require large, annotated datasets, which are costly and time-consuming to collect. To reduce the need for extensive training data, we propose a novel image classification pipeline that combines similarity search using a vision-based foundation model with synthetic data. Our approach leverages a DINOv2 model to transform input images into feature vectors, which are then compared to pre-classified reference images using cosine distance measurements. By utilizing synthetic data instead of real images as references, our pipeline achieves high classification accuracy without relying on real data. We evaluate this approach in eight real-world inspection scenarios and demonstrate that it meets the high performance requirements of production environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.07263</link>
<guid>https://arxiv.org/abs/2505.07263</guid>
<content:encoded><![CDATA[
arXiv:2505.07263v1 Announce Type: new 
Abstract: We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers</title>
<link>https://arxiv.org/abs/2505.07300</link>
<guid>https://arxiv.org/abs/2505.07300</guid>
<content:encoded><![CDATA[
arXiv:2505.07300v1 Announce Type: new 
Abstract: Training-free Neural Architecture Search (NAS) efficiently identifies high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the need for model training, and (ii) interpretable, with proxy designs often theoretically grounded. Despite rapid developments in the field, current SOTA ZC proxies are typically constrained to well-established convolutional search spaces. With the rise of Large Language Models shaping the future of deep learning, this work extends ZC proxy applicability to Vision Transformers (ViTs). We present a new benchmark using the Autoformer search space evaluated on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients information (L-SWAG), a novel, generalizable metric that characterizes both convolutional and transformer architectures across 14 tasks. Additionally, previous works highlighted how different proxies contain complementary information, motivating the need for a ML model to identify useful combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low Information gain and Bias Re-Alignment), a method that strategically combines proxies to best represent a specific benchmark. Integrated into the NAS search, LIBRA-NAS outperforms evolution and gradient-based NAS techniques by identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1 GPU days.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
<link>https://arxiv.org/abs/2505.07301</link>
<guid>https://arxiv.org/abs/2505.07301</guid>
<content:encoded><![CDATA[
arXiv:2505.07301v1 Announce Type: new 
Abstract: In 3D Human Motion Prediction (HMP), conventional methods train HMP models with expensive motion capture data. However, the data collection cost of such motion capture data limits the data diversity, which leads to poor generalizability to unseen motions or subjects. To address this issue, this paper proposes to enhance HMP with additional learning using estimated poses from easily available videos. The 2D poses estimated from the monocular videos are carefully transformed into motion capture-style 3D motions through our pipeline. By additional learning with the obtained motions, the HMP model is adapted to the test domain. The experimental results demonstrate the quantitative and qualitative impact of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Privacy-Aware AI-Based Ergonomic Analysis</title>
<link>https://arxiv.org/abs/2505.07306</link>
<guid>https://arxiv.org/abs/2505.07306</guid>
<content:encoded><![CDATA[
arXiv:2505.07306v1 Announce Type: new 
Abstract: Musculoskeletal disorders (MSDs) are a leading cause of injury and productivity loss in the manufacturing industry, incurring substantial economic costs. Ergonomic assessments can mitigate these risks by identifying workplace adjustments that improve posture and reduce strain. Camera-based systems offer a non-intrusive, cost-effective method for continuous ergonomic tracking, but they also raise significant privacy concerns. To address this, we propose a privacy-aware ergonomic assessment framework utilizing machine learning techniques. Our approach employs adversarial training to develop a lightweight neural network that obfuscates video data, preserving only the essential information needed for human pose estimation. This obfuscation ensures compatibility with standard pose estimation algorithms, maintaining high accuracy while protecting privacy. The obfuscated video data is transmitted to a central server, where state-of-the-art keypoint detection algorithms extract body landmarks. Using multi-view integration, 3D keypoints are reconstructed and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system provides a secure, effective solution for ergonomic monitoring in industrial environments, addressing both privacy and workplace safety concerns.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning</title>
<link>https://arxiv.org/abs/2505.07322</link>
<guid>https://arxiv.org/abs/2505.07322</guid>
<content:encoded><![CDATA[
arXiv:2505.07322v1 Announce Type: new 
Abstract: High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming increasingly prevalent, intensifying the demand for converting Standard Dynamic Range (SDR) content to HDR. Existing methods primarily rely on fixed tone mapping operators, which are inadequate for handling SDR inputs with diverse styles commonly found in real-world scenarios. To address this challenge, we propose a generalized SDR-to-HDR method that handles diverse styles in real-world SDR content, termed Realistic Style Disentangled Representation Learning (RealRep). By disentangling luminance and chrominance, we analyze the intrinsic differences between contents with varying styles and propose a disentangled multi-view style representation learning method. This approach captures the guidance prior of true luminance and chrominance distributions across different styles, even when the SDR style distributions exhibit significant variations, thereby establishing a robust embedding space for inverse tone mapping. Motivated by the difficulty of directly utilizing degradation representation priors, we further introduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a two-stage framework that performs adaptive hierarchical mapping guided by a control-aware normalization mechanism. DDACMNet dynamically modulates the mapping process via degradation-conditioned hierarchical features, enabling robust adaptation across diverse degradation domains. Extensive experiments show that RealRep consistently outperforms state-of-the-art methods with superior generalization and perceptually faithful HDR color gamut reconstruction.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video</title>
<link>https://arxiv.org/abs/2505.07333</link>
<guid>https://arxiv.org/abs/2505.07333</guid>
<content:encoded><![CDATA[
arXiv:2505.07333v1 Announce Type: new 
Abstract: Fast 3D clothed human reconstruction from monocular video remains a significant challenge in computer vision, particularly in balancing computational efficiency with reconstruction quality. Current approaches are either focused on static image reconstruction but too computationally intensive, or achieve high quality through per-video optimization that requires minutes to hours of processing, making them unsuitable for real-time applications. To this end, we present TemPoFast3D, a novel method that leverages temporal coherency of human appearance to reduce redundant computation while maintaining reconstruction quality. Our approach is a "plug-and play" solution that uniquely transforms pixel-aligned reconstruction networks to handle continuous video streams by maintaining and refining a canonical appearance representation through efficient coordinate mapping. Extensive experiments demonstrate that TemPoFast3D matches or exceeds state-of-the-art methods across standard metrics while providing high-quality textured reconstruction across diverse pose and appearance, with a maximum speed of 12 FPS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction</title>
<link>https://arxiv.org/abs/2505.07336</link>
<guid>https://arxiv.org/abs/2505.07336</guid>
<content:encoded><![CDATA[
arXiv:2505.07336v1 Announce Type: new 
Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Pre-trained Autoregressive Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.07344</link>
<guid>https://arxiv.org/abs/2505.07344</guid>
<content:encoded><![CDATA[
arXiv:2505.07344v1 Announce Type: new 
Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography</title>
<link>https://arxiv.org/abs/2505.07347</link>
<guid>https://arxiv.org/abs/2505.07347</guid>
<content:encoded><![CDATA[
arXiv:2505.07347v1 Announce Type: new 
Abstract: Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practicality for timely diagnosis and monitoring of pulmonary hypertension progression. Here, we propose MePH, a multi-view, multi-modal vision-language model to accurately assess pulmonary hypertension progression using non-invasive echocardiography. We constructed a large dataset comprising paired standardized echocardiogram videos, spectral images and RHC data, covering 1,237 patient cases from 12 medical centers. For the first time, MePH precisely models the correlation between non-invasive multi-view, multi-modal echocardiography and the pressure and resistance obtained via RHC. We show that MePH significantly outperforms echocardiographers' assessments using echocardiography, reducing the mean absolute error in estimating mean pulmonary arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and 43.81%, respectively. In eight independent external hospitals, MePH achieved a mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an area under the curve of 0.921, surpassing echocardiographers (area under the curve of 0.842) in accurately predicting the severity of pulmonary hypertension, whether mild or severe. A prospective study demonstrated that MePH can predict treatment efficacy for patients. Our work provides pulmonary hypertension patients with a non-invasive and timely method for monitoring disease progression, improving the accuracy and efficiency of pulmonary hypertension management while enabling earlier interventions and more personalized treatment decisions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild</title>
<link>https://arxiv.org/abs/2505.07373</link>
<guid>https://arxiv.org/abs/2505.07373</guid>
<content:encoded><![CDATA[
arXiv:2505.07373v1 Announce Type: new 
Abstract: Neural implicit surface reconstruction using volume rendering techniques has recently achieved significant advancements in creating high-fidelity surfaces from multiple 2D images. However, current methods primarily target scenes with consistent illumination and struggle to accurately reconstruct 3D geometry in uncontrolled environments with transient occlusions or varying appearances. While some neural radiance field (NeRF)-based variants can better manage photometric variations and transient objects in complex scenes, they are designed for novel view synthesis rather than precise surface reconstruction due to limited surface constraints. To overcome this limitation, we introduce a novel approach that applies multiple geometric constraints to the implicit surface optimization process, enabling more accurate reconstructions from unconstrained image collections. First, we utilize sparse 3D points from structure-from-motion (SfM) to refine the signed distance function estimation for the reconstructed surface, with a displacement compensation to accommodate noise in the sparse points. Additionally, we employ robust normal priors derived from a normal predictor, enhanced by edge prior filtering and multi-view consistency constraints, to improve alignment with the actual surface geometry. Extensive testing on the Heritage-Recon benchmark and other datasets has shown that the proposed method can accurately reconstruct surfaces from in-the-wild images, yielding geometries with superior accuracy and granularity compared to existing techniques. Our approach enables high-quality 3D reconstruction of various landmarks, making it applicable to diverse scenarios such as digital preservation of cultural heritage sites.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.07375</link>
<guid>https://arxiv.org/abs/2505.07375</guid>
<content:encoded><![CDATA[
arXiv:2505.07375v1 Announce Type: new 
Abstract: Point cloud anomaly detection is essential for various industrial applications. The huge computation and storage costs caused by the increasing product classes limit the application of single-class unsupervised methods, necessitating the development of multi-class unsupervised methods. However, the feature similarity between normal and anomalous points from different class data leads to the feature confusion problem, which greatly hinders the performance of multi-class methods. Therefore, we introduce a multi-class point cloud anomaly detection method, named GLFM, leveraging global-local feature matching to progressively separate data that are prone to confusion across multiple classes. Specifically, GLFM is structured into three stages: Stage-I proposes an anomaly synthesis pipeline that stretches point clouds to create abundant anomaly data that are utilized to adapt the point cloud feature extractor for better feature representation. Stage-II establishes the global and local memory banks according to the global and local feature distributions of all the training data, weakening the impact of feature confusion on the establishment of the memory bank. Stage-III implements anomaly detection of test data leveraging its feature distance from global and local memory banks. Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts dataset showcase our proposed GLFM's superior point cloud anomaly detection performance. The code is available at https://github.com/hustCYQ/GLFM-Multi-class-3DAD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications</title>
<link>https://arxiv.org/abs/2505.07380</link>
<guid>https://arxiv.org/abs/2505.07380</guid>
<content:encoded><![CDATA[
arXiv:2505.07380v1 Announce Type: new 
Abstract: iPhone portrait-mode images contain a distinctive pattern in out-of-focus regions simulating the bokeh effect, which we term Apple's Synthetic Defocus Noise Pattern (SDNP). If overlooked, this pattern can interfere with blind forensic analyses, especially PRNU-based camera source verification, as noted in earlier works. Since Apple's SDNP remains underexplored, we provide a detailed characterization, proposing a method for its precise estimation, modeling its dependence on scene brightness, ISO settings, and other factors. Leveraging this characterization, we explore forensic applications of the SDNP, including traceability of portrait-mode images across iPhone models and iOS versions in open-set scenarios, assessing its robustness under post-processing. Furthermore, we show that masking SDNP-affected regions in PRNU-based camera source verification significantly reduces false positives, overcoming a critical limitation in camera attribution, and improving state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Semantic Encoding and Decoding for Video Surveillance</title>
<link>https://arxiv.org/abs/2505.07381</link>
<guid>https://arxiv.org/abs/2505.07381</guid>
<content:encoded><![CDATA[
arXiv:2505.07381v1 Announce Type: new 
Abstract: With the continuous increase in the number and resolution of video surveillance cameras, the burden of transmitting and storing surveillance video is growing. Traditional communication methods based on Shannon's theory are facing optimization bottlenecks. Semantic communication, as an emerging communication method, is expected to break through this bottleneck and reduce the storage and transmission consumption of video. Existing semantic decoding methods often require many samples to train the neural network for each scene, which is time-consuming and labor-intensive. In this study, a semantic encoding and decoding method for surveillance video is proposed. First, the sketch was extracted as semantic information, and a sketch compression method was proposed to reduce the bit rate of semantic information. Then, an image translation network was proposed to translate the sketch into a video frame with a reference frame. Finally, a few-shot sketch decoding network was proposed to reconstruct video from sketch. Experimental results showed that the proposed method achieved significantly better video reconstruction performance than baseline methods. The sketch compression method could effectively reduce the storage and transmission consumption of semantic information with little compromise on video quality. The proposed method provides a novel semantic encoding and decoding method that only needs a few training samples for each surveillance scene, thus improving the practicality of the semantic communication system.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Visualization in 3D Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2505.07387</link>
<guid>https://arxiv.org/abs/2505.07387</guid>
<content:encoded><![CDATA[
arXiv:2505.07387v1 Announce Type: new 
Abstract: Understanding the computations of convolutional neural networks requires effective visualization of their kernels. While maximal activation methods have proven successful in highlighting the preferred features of 2D convolutional kernels, directly applying these techniques to 3D convolutions often leads to uninterpretable results due to the higher dimensionality and complexity of 3D features. To address this challenge, we propose a novel visualization approach for 3D convolutional kernels that disentangles their texture and motion preferences. Our method begins with a data-driven decomposition of the optimal input that maximally activates a given kernel. We then introduce a two-stage optimization strategy to extract distinct texture and motion components from this input. Applying our approach to visualize kernels at various depths of several pre-trained models, we find that the resulting visualizations--particularly those capturing motion--clearly reveal the preferred dynamic patterns encoded by 3D kernels. These results demonstrate the effectiveness of our method in providing interpretable insights into 3D convolutional operations. Code is available at https://github.com/YatangLiLab/3DKernelVisualizer.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.07396</link>
<guid>https://arxiv.org/abs/2505.07396</guid>
<content:encoded><![CDATA[
arXiv:2505.07396v1 Announce Type: new 
Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.07398</link>
<guid>https://arxiv.org/abs/2505.07398</guid>
<content:encoded><![CDATA[
arXiv:2505.07398v1 Announce Type: new 
Abstract: State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture</title>
<link>https://arxiv.org/abs/2505.07444</link>
<guid>https://arxiv.org/abs/2505.07444</guid>
<content:encoded><![CDATA[
arXiv:2505.07444v1 Announce Type: new 
Abstract: Efficient crop-weed segmentation is critical for site-specific weed control in precision agriculture. Conventional CNN-based methods struggle to generalize and rely on RGB imagery, limiting performance under complex field conditions. To address these challenges, we propose a lightweight transformer-CNN hybrid. It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using specialized encoders and dynamic modality integration. Evaluated on the WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7 million parameters, the model offers high accuracy, computational efficiency, and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and edge devices, advancing precision weed management.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing degeneracies in latent interpolation for diffusion models</title>
<link>https://arxiv.org/abs/2505.07481</link>
<guid>https://arxiv.org/abs/2505.07481</guid>
<content:encoded><![CDATA[
arXiv:2505.07481v1 Announce Type: new 
Abstract: There is an increasing interest in using image-generating diffusion models for deep data augmentation and image morphing. In this context, it is useful to interpolate between latents produced by inverting a set of input images, in order to generate new images representing some mixture of the inputs. We observe that such interpolation can easily lead to degenerate results when the number of inputs is large. We analyze the cause of this effect theoretically and experimentally, and suggest a suitable remedy. The suggested approach is a relatively simple normalization scheme that is easy to use whenever interpolation between latents is needed. We measure image quality using FID and CLIP embedding distance and show experimentally that baseline interpolation methods lead to a drop in quality metrics long before the degeneration issue is clearly visible. In contrast, our method significantly reduces the degeneration effect and leads to improved quality metrics also in non-degenerate situations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocVXQA: Context-Aware Visual Explanations for Document Question Answering</title>
<link>https://arxiv.org/abs/2505.07496</link>
<guid>https://arxiv.org/abs/2505.07496</guid>
<content:encoded><![CDATA[
arXiv:2505.07496v1 Announce Type: new 
Abstract: We propose DocVXQA, a novel framework for visually self-explainable document question answering. The framework is designed not only to produce accurate answers to questions but also to learn visual heatmaps that highlight contextually critical regions, thereby offering interpretable justifications for the model's decisions. To integrate explanations into the learning process, we quantitatively formulate explainability principles as explicit learning objectives. Unlike conventional methods that emphasize only the regions pertinent to the answer, our framework delivers explanations that are \textit{contextually sufficient} while remaining \textit{representation-efficient}. This fosters user trust while achieving a balance between predictive performance and interpretability in DocVQA applications. Extensive experiments, including human evaluation, provide strong evidence supporting the effectiveness of our method. The code is available at https://github.com/dali92002/DocVXQA.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models</title>
<link>https://arxiv.org/abs/2505.07500</link>
<guid>https://arxiv.org/abs/2505.07500</guid>
<content:encoded><![CDATA[
arXiv:2505.07500v1 Announce Type: new 
Abstract: The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as "bring me a spoon", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAIS: Memory-Attention for Interactive Segmentation</title>
<link>https://arxiv.org/abs/2505.07511</link>
<guid>https://arxiv.org/abs/2505.07511</guid>
<content:encoded><![CDATA[
arXiv:2505.07511v1 Announce Type: new 
Abstract: Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images</title>
<link>https://arxiv.org/abs/2505.07530</link>
<guid>https://arxiv.org/abs/2505.07530</guid>
<content:encoded><![CDATA[
arXiv:2505.07530v1 Announce Type: new 
Abstract: Synthetic face datasets are increasingly used to overcome the limitations of real-world biometric data, including privacy concerns, demographic imbalance, and high collection costs. However, many existing methods lack fine-grained control over identity attributes and fail to produce paired, identity-consistent images under structured capture conditions. We introduce FLUXSynID, a framework for generating high-resolution synthetic face datasets with user-defined identity attribute distributions and paired document-style and trusted live capture images. The dataset generated using the FLUXSynID framework shows improved alignment with real-world identity distributions and greater inter-set diversity compared to prior work. The FLUXSynID framework for generating custom datasets, along with a dataset of 14,889 synthetic identities, is publicly released to support biometric research, including face recognition and morphing attack detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability</title>
<link>https://arxiv.org/abs/2505.07533</link>
<guid>https://arxiv.org/abs/2505.07533</guid>
<content:encoded><![CDATA[
arXiv:2505.07533v1 Announce Type: new 
Abstract: Monitoring and analyzing electrocardiogram (ECG) signals, even under varying physiological conditions, including those influenced by physical activity, drugs and stress, is crucial to accurately assess cardiac health. However, current AI-based methods often fail to account for how these factors interact and alter ECG patterns, ultimately limiting their applicability in real-world settings. This study introduces IKrNet, a novel neural network model, which identifies drug-specific patterns in ECGs amidst certain physiological conditions. IKrNet's architecture incorporates spatial and temporal dynamics by using a convolutional backbone with varying receptive field size to capture spatial features. A bi-directional Long Short-Term Memory module is also employed to model temporal dependencies. By treating heart rate variability as a surrogate for physiological fluctuations, we evaluated IKrNet's performance across diverse scenarios, including conditions with physical stress, drug intake alone, and a baseline without drug presence. Our assessment follows a clinical protocol in which 990 healthy volunteers were administered 80mg of Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art models' accuracy and stability in varying physiological conditions, underscoring its clinical viability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning</title>
<link>https://arxiv.org/abs/2505.07538</link>
<guid>https://arxiv.org/abs/2505.07538</guid>
<content:encoded><![CDATA[
arXiv:2505.07538v1 Announce Type: new 
Abstract: We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</title>
<link>https://arxiv.org/abs/2505.07539</link>
<guid>https://arxiv.org/abs/2505.07539</guid>
<content:encoded><![CDATA[
arXiv:2505.07539v1 Announce Type: new 
Abstract: Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynID: Passport Synthetic Dataset for Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2505.07540</link>
<guid>https://arxiv.org/abs/2505.07540</guid>
<content:encoded><![CDATA[
arXiv:2505.07540v1 Announce Type: new 
Abstract: The demand for Presentation Attack Detection (PAD) to identify fraudulent ID documents in remote verification systems has significantly risen in recent years. This increase is driven by several factors, including the rise of remote work, online purchasing, migration, and advancements in synthetic images. Additionally, we have noticed a surge in the number of attacks aimed at the enrolment process. Training a PAD to detect fake ID documents is very challenging because of the limited number of ID documents available due to privacy concerns. This work proposes a new passport dataset generated from a hybrid method that combines synthetic data and open-access information using the ICAO requirement to obtain realistic training and testing images.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies</title>
<link>https://arxiv.org/abs/2505.07552</link>
<guid>https://arxiv.org/abs/2505.07552</guid>
<content:encoded><![CDATA[
arXiv:2505.07552v1 Announce Type: new 
Abstract: Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs</title>
<link>https://arxiv.org/abs/2505.07556</link>
<guid>https://arxiv.org/abs/2505.07556</guid>
<content:encoded><![CDATA[
arXiv:2505.07556v1 Announce Type: new 
Abstract: Event cameras offer significant advantages over traditional frame-based sensors. These include microsecond temporal resolution, robustness under varying lighting conditions and low power consumption. Nevertheless, the effective processing of their sparse, asynchronous event streams remains challenging. Existing approaches to this problem can be categorised into two distinct groups. The first group involves the direct processing of event data with neural models, such as Spiking Neural Networks or Graph Convolutional Neural Networks. However, this approach is often accompanied by a compromise in terms of qualitative performance. The second group involves the conversion of events into dense representations with handcrafted aggregation functions, which can boost accuracy at the cost of temporal fidelity. This paper introduces a novel Self-Supervised Event Representation (SSER) method leveraging Gated Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event timestamps and polarities without temporal discretisation. The recurrent layers are trained in a self-supervised manner to maximise the fidelity of event-time encoding. The inference is performed with event representations generated asynchronously, thus ensuring compatibility with high-throughput sensors. The experimental validation demonstrates that SSER outperforms aggregation-based baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx object detection datasets. Furthermore, the paper presents the first hardware implementation of recurrent representation for event data on a System-on-Chip FPGA, achieving sub-microsecond latency and power consumption between 1-2 W, suitable for real-time, power-efficient applications. Code is available at https://github.com/vision-agh/RecRepEvent.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework</title>
<link>https://arxiv.org/abs/2505.07573</link>
<guid>https://arxiv.org/abs/2505.07573</guid>
<content:encoded><![CDATA[
arXiv:2505.07573v1 Announce Type: new 
Abstract: Kidney abnormality segmentation has important potential to enhance the clinical workflow, especially in settings requiring quantitative assessments. Kidney volume could serve as an important biomarker for renal diseases, with changes in volume correlating directly with kidney function. Currently, clinical practice often relies on subjective visual assessment for evaluating kidney size and abnormalities, including tumors and cysts, which are typically staged based on diameter, volume, and anatomical location. To support a more objective and reproducible approach, this research aims to develop a robust, thoroughly validated kidney abnormality segmentation algorithm, made publicly available for clinical and research use. We employ publicly available training datasets and leverage the state-of-the-art medical image segmentation framework nnU-Net. Validation is conducted using both proprietary and public test datasets, with segmentation performance quantified by Dice coefficient and the 95th percentile Hausdorff distance. Furthermore, we analyze robustness across subgroups based on patient sex, age, CT contrast phases, and tumor histologic subtypes. Our findings demonstrate that our segmentation algorithm, trained exclusively on publicly available data, generalizes effectively to external test sets and outperforms existing state-of-the-art models across all tested datasets. Subgroup analyses reveal consistent high performance, indicating strong robustness and reliability. The developed algorithm and associated code are publicly accessible at https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study</title>
<link>https://arxiv.org/abs/2505.07576</link>
<guid>https://arxiv.org/abs/2505.07576</guid>
<content:encoded><![CDATA[
arXiv:2505.07576v1 Announce Type: new 
Abstract: Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions</title>
<link>https://arxiv.org/abs/2505.07611</link>
<guid>https://arxiv.org/abs/2505.07611</guid>
<content:encoded><![CDATA[
arXiv:2505.07611v1 Announce Type: new 
Abstract: Traffic accident prediction and detection are critical for enhancing road safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged as a promising approach in the era of deep learning.This paper reviews 147 recent studies,focusing on the application of supervised,unsupervised,and hybrid deep learning models for accident prediction,alongside the use of real-world and synthetic datasets.Current methodologies are categorized into four key approaches: image and video feature-based prediction, spatiotemporal feature-based prediction, scene understanding,and multimodal data fusion.While these methods demonstrate significant potential,challenges such as data scarcity,limited generalization to complex scenarios,and real-time performance constraints remain prevalent. This review highlights opportunities for future research,including the integration of multimodal data fusion, self-supervised learning,and Transformer-based architectures to enhance prediction accuracy and scalability.By synthesizing existing advancements and identifying critical gaps, this paper provides a foundational reference for developing robust and adaptive Vision-TAA systems,contributing to road safety and traffic management.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Convolution Improves Neural Predictivity in the Retina</title>
<link>https://arxiv.org/abs/2505.07620</link>
<guid>https://arxiv.org/abs/2505.07620</guid>
<content:encoded><![CDATA[
arXiv:2505.07620v1 Announce Type: new 
Abstract: We present a novel approach to neural response prediction that incorporates higher-order operations directly within convolutional neural networks (CNNs). Our model extends traditional 3D CNNs by embedding higher-order operations within the convolutional operator itself, enabling direct modeling of multiplicative interactions between neighboring pixels across space and time. Our model increases the representational power of CNNs without increasing their depth, therefore addressing the architectural disparity between deep artificial networks and the relatively shallow processing hierarchy of biological visual systems. We evaluate our approach on two distinct datasets: salamander retinal ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC responses to controlled geometric transformations. Our higher-order CNN (HoCNN) achieves superior performance while requiring only half the training data compared to standard architectures, demonstrating correlation coefficients up to 0.75 with neural responses (against 0.80$\pm$0.02 retinal reliability). When integrated into state-of-the-art architectures, our approach consistently improves performance across different species and stimulus conditions. Analysis of the learned representations reveals that our network naturally encodes fundamental geometric transformations, particularly scaling parameters that characterize object expansion and contraction. This capability is especially relevant for specific cell types, such as transient OFF-alpha and transient ON cells, which are known to detect looming objects and object motion respectively, and where our model shows marked improvement in response prediction. The correlation coefficients for scaling parameters are more than twice as high in HoCNN (0.72) compared to baseline models (0.32).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios</title>
<link>https://arxiv.org/abs/2505.07622</link>
<guid>https://arxiv.org/abs/2505.07622</guid>
<content:encoded><![CDATA[
arXiv:2505.07622v1 Announce Type: new 
Abstract: Cross-view geo-localization is a promising solution for large-scale localization problems, requiring the sequential execution of retrieval and metric localization tasks to achieve fine-grained predictions. However, existing methods typically focus on designing standalone models for these two tasks, resulting in inefficient collaboration and increased training overhead. In this paper, we propose UnifyGeo, a novel unified hierarchical geo-localization framework that integrates retrieval and metric localization tasks into a single network. Specifically, we first employ a unified learning strategy with shared parameters to jointly learn multi-granularity representation, facilitating mutual reinforcement between these two tasks. Subsequently, we design a re-ranking mechanism guided by a dedicated loss function, which enhances geo-localization performance by improving both retrieval accuracy and metric localization references. Extensive experiments demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in both task-isolated and task-associated settings. Remarkably, on the challenging VIGOR benchmark, which supports fine-grained localization evaluation, the 1-meter-level localization recall rate improves from 1.53\% to 39.64\% and from 0.43\% to 25.58\% under same-area and cross-area evaluations, respectively. Code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.07652</link>
<guid>https://arxiv.org/abs/2505.07652</guid>
<content:encoded><![CDATA[
arXiv:2505.07652v1 Announce Type: new 
Abstract: Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token's effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in https://shotadapter.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomical Attention Alignment representation for Radiology Report Generation</title>
<link>https://arxiv.org/abs/2505.07689</link>
<guid>https://arxiv.org/abs/2505.07689</guid>
<content:encoded><![CDATA[
arXiv:2505.07689v1 Announce Type: new 
Abstract: Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologists' workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can limit the understanding of spatial structures and semantic relationships, often resulting in suboptimal text generation. To address this, we propose Anatomical Attention Alignment Network (A3Net), a framework that enhance visual-textual understanding by constructing hyper-visual representations. Our approach integrates a knowledge dictionary of anatomical structures with patch-level visual features, enabling the model to effectively associate image regions with their corresponding anatomical entities. This structured representation improves semantic reasoning, interpretability, and cross-modal alignment, ultimately enhancing the accuracy and clinical relevance of generated reports. Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net significantly improves both visual perception and text generation quality. Our code is available at \href{https://github.com/Vinh-AI/A3Net}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter for Continual Learning of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07690</link>
<guid>https://arxiv.org/abs/2505.07690</guid>
<content:encoded><![CDATA[
arXiv:2505.07690v1 Announce Type: new 
Abstract: This study aims to address the problem of multi-domain task incremental learning~(MTIL), which requires that vision-language models~(VLMs) continuously acquire new knowledge while maintaining their inherent zero-shot recognition capability. Existing paradigms delegate the testing of unseen-domain samples to the original CLIP, which only prevents the degradation of the model's zero-shot capability but fails to enhance the generalization of the VLM further. To this end, we propose a novel MTIL framework, named AFA, which comprises two core modules: (1) an against forward-forgetting adapter that learns task-invariant information for each dataset in the incremental tasks to enhance the zero-shot recognition ability of VLMs; (2) an against backward-forgetting adapter that strengthens the few-shot learning capability of VLMs while supporting incremental learning. Extensive experiments demonstrate that the AFA method significantly outperforms existing state-of-the-art approaches, especially in few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP in terms of transferability. The code is provided in the Supplementary Material.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.07691</link>
<guid>https://arxiv.org/abs/2505.07691</guid>
<content:encoded><![CDATA[
arXiv:2505.07691v1 Announce Type: new 
Abstract: Semi-supervised learning leverages unlabeled data to enhance model performance, addressing the limitations of fully supervised approaches. Among its strategies, pseudo-supervision has proven highly effective, typically relying on one or multiple teacher networks to refine pseudo-labels before training a student network. A common practice in pseudo-supervision is filtering pseudo-labels based on pre-defined confidence thresholds or entropy. However, selecting optimal thresholds requires large labeled datasets, which are often scarce in real-world semi-supervised scenarios. To overcome this challenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic feedback-driven thresholding strategy for pseudo-label selection. Instead of relying on static confidence thresholds, ENCORE estimates class-wise true-positive confidence within the unlabeled dataset and continuously adjusts thresholds based on the model's response to different levels of pseudo-label filtering. This feedback-driven mechanism ensures the retention of informative pseudo-labels while filtering unreliable ones, enhancing model training without manual threshold tuning. Our method seamlessly integrates into existing pseudo-supervision frameworks and significantly improves segmentation performance, particularly in data-scarce conditions. Extensive experiments demonstrate that integrating ENCORE with existing pseudo-supervision frameworks enhances performance across multiple datasets and network architectures, validating its effectiveness in semi-supervised learning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images</title>
<link>https://arxiv.org/abs/2505.07704</link>
<guid>https://arxiv.org/abs/2505.07704</guid>
<content:encoded><![CDATA[
arXiv:2505.07704v1 Announce Type: new 
Abstract: Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Spiking Vision Transformer for Object Detection with Event Cameras</title>
<link>https://arxiv.org/abs/2505.07715</link>
<guid>https://arxiv.org/abs/2505.07715</guid>
<content:encoded><![CDATA[
arXiv:2505.07715v1 Announce Type: new 
Abstract: Event-based object detection has gained increasing attention due to its advantages such as high temporal resolution, wide dynamic range, and asynchronous address-event representation. Leveraging these advantages, Spiking Neural Networks (SNNs) have emerged as a promising approach, offering low energy consumption and rich spatiotemporal dynamics. To further enhance the performance of event-based object detection, this study proposes a novel hybrid spike vision Transformer (HsVT) model. The HsVT model integrates a spatial feature extraction module to capture local and global features, and a temporal feature extraction module to model time dependencies and long-term patterns in event sequences. This combination enables HsVT to capture spatiotemporal features, improving its capability to handle complex event-based object detection tasks. To support research in this area, we developed and publicly released The Fall Detection Dataset as a benchmark for event-based object detection tasks. This dataset, captured using an event-based camera, ensures facial privacy protection and reduces memory usage due to the event representation format. We evaluated the HsVT model on GEN1 and Fall Detection datasets across various model sizes. Experimental results demonstrate that HsVT achieves significant performance improvements in event detection with fewer parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gameplay Highlights Generation</title>
<link>https://arxiv.org/abs/2505.07721</link>
<guid>https://arxiv.org/abs/2505.07721</guid>
<content:encoded><![CDATA[
arXiv:2505.07721v1 Announce Type: new 
Abstract: In this work, we enable gamers to share their gaming experience on social media by automatically generating eye-catching highlight reels from their gameplay session Our automation will save time for gamers while increasing audience engagement. We approach the highlight generation problem by first identifying intervals in the video where interesting events occur and then concatenate them. We developed an in-house gameplay event detection dataset containing interesting events annotated by humans using VIA video annotator. Traditional techniques for highlight detection such as game engine integration requires expensive collaboration with game developers. OCR techniques which detect patches of specific images or texts require expensive per game engineering and may not generalize across game UI and different language. We finetuned a multimodal general purpose video understanding model such as X-CLIP using our dataset which generalizes across multiple games in a genre without per game engineering. Prompt engineering was performed to improve the classification performance of this multimodal model. Our evaluation showed that such a finetuned model can detect interesting events in first person shooting games from unseen gameplay footage with more than 90% accuracy. Moreover, our model performed significantly better on low resource games (small dataset) when trained along with high resource games, showing signs of transfer learning. To make the model production ready, we used ONNX libraries to enable cross platform inference. These libraries also provide post training quantization tools to reduce model size and inference time for deployment. ONNX runtime libraries with DirectML backend were used to perform efficient inference on Windows OS. We show that natural language supervision in the X-CLIP model leads to data efficient and highly performant video recognition models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention</title>
<link>https://arxiv.org/abs/2505.07734</link>
<guid>https://arxiv.org/abs/2505.07734</guid>
<content:encoded><![CDATA[
arXiv:2505.07734v1 Announce Type: new 
Abstract: Detecting AI-synthetic faces presents a critical challenge: it is hard to capture consistent structural relationships between facial regions across diverse generation techniques. Current methods, which focus on specific artifacts rather than fundamental inconsistencies, often fail when confronted with novel generative models. To address this limitation, we introduce Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer designed for robust facial forgery detection. This model integrates distinct Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation (LAMM) components within each layer. RG-MHA utilizes facial landmarks to create regional attention masks, guiding the model to scrutinize architectural inconsistencies across different facial areas. Crucially, the separate LAMM module dynamically generates layer-specific parameters, including mask weights and gating values, based on network context. These parameters then modulate the behavior of RG-MHA, enabling adaptive adjustment of regional focus across network depths. This architecture facilitates the capture of subtle, hierarchical forgery cues ubiquitous among diverse generation techniques, such as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT demonstrates superior performance, achieving 94.09% mean ACC (a +5.45% improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results demonstrate LAMM-ViT's exceptional ability to generalize and its potential for reliable deployment against evolving synthetic media threats.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BodyGPS: Anatomical Positioning System</title>
<link>https://arxiv.org/abs/2505.07744</link>
<guid>https://arxiv.org/abs/2505.07744</guid>
<content:encoded><![CDATA[
arXiv:2505.07744v1 Announce Type: new 
Abstract: We introduce a new type of foundational model for parsing human anatomy in medical images that works for different modalities. It supports supervised or unsupervised training and can perform matching, registration, classification, or segmentation with or without user interaction. We achieve this by training a neural network estimator that maps query locations to atlas coordinates via regression. Efficiency is improved by sparsely sampling the input, enabling response times of less than 1 ms without additional accelerator hardware. We demonstrate the utility of the algorithm in both CT and MRI modalities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</title>
<link>https://arxiv.org/abs/2505.07747</link>
<guid>https://arxiv.org/abs/2505.07747</guid>
<content:encoded><![CDATA[
arXiv:2505.07747v1 Announce Type: new 
Abstract: While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Visual Autoregressive Generation via Score Maximization</title>
<link>https://arxiv.org/abs/2505.07812</link>
<guid>https://arxiv.org/abs/2505.07812</guid>
<content:encoded><![CDATA[
arXiv:2505.07812v1 Announce Type: new 
Abstract: Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
<link>https://arxiv.org/abs/2505.07818</link>
<guid>https://arxiv.org/abs/2505.07818</guid>
<content:encoded><![CDATA[
arXiv:2505.07818v1 Announce Type: new 
Abstract: Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion</title>
<link>https://arxiv.org/abs/2505.06250</link>
<guid>https://arxiv.org/abs/2505.06250</guid>
<content:encoded><![CDATA[
arXiv:2505.06250v1 Announce Type: cross 
Abstract: Digital Predistortion (DPD) is a popular technique to enhance signal quality in wideband RF power amplifiers (PAs). With increasing bandwidth and data rates, DPD faces significant energy consumption challenges during deployment, contrasting with its efficiency goals. State-of-the-art DPD models rely on recurrent neural networks (RNN), whose computational complexity hinders system efficiency. This paper introduces DeltaDPD, exploring the dynamic temporal sparsity of input signals and neuronal hidden states in RNNs for energy-efficient DPD, reducing arithmetic operations and memory accesses while preserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW 256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03 dBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square Error (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal sparsity, leading to a 1.8X reduction in estimated inference power. The DeltaDPD code will be released after formal publication at https://www.opendpd.com.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attonsecond Streaking Phase Retrieval Via Deep Learning Methods</title>
<link>https://arxiv.org/abs/2505.06275</link>
<guid>https://arxiv.org/abs/2505.06275</guid>
<content:encoded><![CDATA[
arXiv:2505.06275v1 Announce Type: cross 
Abstract: Attosecond streaking phase retrieval is essential for resolving electron dynamics on sub-femtosecond time scales yet traditional algorithms rely on iterative minimization and central momentum approximations that degrade accuracy for broadband pulses. In this work phase retrieval is reformulated as a supervised computer-vision problem and four neural architectures are systematically compared. A convolutional network demonstrates strong sensitivity to local streak edges but lacks global context; a vision transformer captures long-range delay-energy correlations at the expense of local inductive bias; a hybrid CNN-ViT model unites local feature extraction and full-graph attention; and a capsule network further enforces spatial pose agreement through dynamic routing. A theoretical analysis introduces local, global and positional sensitivity measures and derives surrogate error bounds that predict the strict ordering $CNN<Capsule$. Controlled experiments on synthetic streaking spectrograms confirm this hierarchy, with the capsule network achieving the highest retrieval fidelity. Looking forward, embedding the strong-field integral into physics-informed neural networks and exploring photonic hardware implementations promise pathways toward real-time attosecond pulse characterization under demanding experimental conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field</title>
<link>https://arxiv.org/abs/2505.06277</link>
<guid>https://arxiv.org/abs/2505.06277</guid>
<content:encoded><![CDATA[
arXiv:2505.06277v1 Announce Type: cross 
Abstract: Terahertz (THz) communication is a key enabler for 6G systems, offering ultra-wide bandwidth and unprecedented data rates. However, THz signal propagation differs significantly from lower-frequency bands due to severe free space path loss, minimal diffraction and specular reflection, and prominent scattering, making conventional channel modeling and pilot-based estimation approaches inefficient. In this work, we investigate the feasibility of applying radio radiance field (RRF) framework to the THz band. This method reconstructs a continuous RRF using visual-based geometry and sparse THz RF measurements, enabling efficient spatial channel state information (Spatial-CSI) modeling without dense sampling. We first build a fine simulated THz scenario, then we reconstruct the RRF and evaluate the performance in terms of both reconstruction quality and effectiveness in THz communication, showing that the reconstructed RRF captures key propagation paths with sparse training samples. Our findings demonstrate that RRF modeling remains effective in the THz regime and provides a promising direction for scalable, low-cost spatial channel reconstruction in future 6G networks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments</title>
<link>https://arxiv.org/abs/2505.06285</link>
<guid>https://arxiv.org/abs/2505.06285</guid>
<content:encoded><![CDATA[
arXiv:2505.06285v1 Announce Type: cross 
Abstract: Rolling bearings are critical components of rotating machinery, and their proper functioning is essential for industrial production. Most existing condition monitoring methods focus on extracting discriminative features from time-domain signals to assess bearing health status. However, under complex operating conditions, periodic impulsive characteristics related to fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn distinctive fault-related features in such scenarios. To address this issue, this paper proposes a novel CNN-based model named FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL) is introduced as an input denoising layer to enhance key features while filtering out irrelevant information. Subsequently, a Multiscale Time-Frequency Fusion (MSTFF) module is employed to extract fused time-frequency features, further improving the model robustness and nonlinear representation capability. Additionally, a distillation layer is incorporated to expand the receptive field. Based on these advancements, a novel deep lightweight CNN model, termed the Frequency-Enhanced Multiscale Network (FEMSN), is developed. The effectiveness of FEMSN and FADEL in machine health monitoring and stability assessment is validated through two case studies.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments</title>
<link>https://arxiv.org/abs/2505.06483</link>
<guid>https://arxiv.org/abs/2505.06483</guid>
<content:encoded><![CDATA[
arXiv:2505.06483v1 Announce Type: cross 
Abstract: Robot autonomy in unknown, GPS-denied, and complex underground environments requires real-time, robust, and accurate onboard pose estimation and mapping for reliable operations. This becomes particularly challenging in perception-degraded subterranean conditions under harsh environmental factors, including darkness, dust, and geometrically self-similar structures. This paper details CompSLAM, a highly resilient and hierarchical multi-modal localization and mapping framework designed to address these challenges. Its flexible architecture achieves resilience through redundancy by leveraging the complementary nature of pose estimates derived from diverse sensor modalities. Developed during the DARPA Subterranean Challenge, CompSLAM was successfully deployed on all aerial, legged, and wheeled robots of Team Cerberus during their competition-winning final run. Furthermore, it has proven to be a reliable odometry and mapping solution in various subsequent projects, with extensions enabling multi-robot map sharing for marsupial robotic deployments and collaborative mapping. This paper also introduces a comprehensive dataset acquired by a manually teleoperated quadrupedal robot, covering a significant portion of the DARPA Subterranean Challenge finals course. This dataset evaluates CompSLAM's robustness to sensor degradations as the robot traverses 740 meters in an environment characterized by highly variable geometries and demanding lighting conditions. The CompSLAM code and the DARPA SubT Finals dataset are made publicly available for the benefit of the robotics community
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
arXiv:2505.06502v1 Announce Type: cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities</title>
<link>https://arxiv.org/abs/2505.06507</link>
<guid>https://arxiv.org/abs/2505.06507</guid>
<content:encoded><![CDATA[
arXiv:2505.06507v1 Announce Type: cross 
Abstract: Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: https://github.com/Text-to-CadQuery/Text-to-CadQuery.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation</title>
<link>https://arxiv.org/abs/2505.06594</link>
<guid>https://arxiv.org/abs/2505.06594</guid>
<content:encoded><![CDATA[
arXiv:2505.06594v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Representation Transferring to Lightweight Models via Perception Coherence</title>
<link>https://arxiv.org/abs/2505.06595</link>
<guid>https://arxiv.org/abs/2505.06595</guid>
<content:encoded><![CDATA[
arXiv:2505.06595v1 Announce Type: cross 
Abstract: In this paper, we propose a method for transferring feature representation to lightweight student models from larger teacher models. We mathematically define a new notion called \textit{perception coherence}. Based on this notion, we propose a loss function, which takes into account the dissimilarities between data points in feature space through their ranking. At a high level, by minimizing this loss function, the student model learns to mimic how the teacher model \textit{perceives} inputs. More precisely, our method is motivated by the fact that the representational capacity of the student model is weaker than the teacher model. Hence, we aim to develop a new method allowing for a better relaxation. This means that, the student model does not need to preserve the absolute geometry of the teacher one, while preserving global coherence through dissimilarity ranking. Our theoretical insights provide a probabilistic perspective on the process of feature representation transfer. Our experiments results show that our method outperforms or achieves on-par performance compared to strong baseline methods for representation transferring.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models</title>
<link>https://arxiv.org/abs/2505.06621</link>
<guid>https://arxiv.org/abs/2505.06621</guid>
<content:encoded><![CDATA[
arXiv:2505.06621v1 Announce Type: cross 
Abstract: The distribution of child sexual abuse imagery (CSAI) is an ever-growing concern of our modern world; children who suffered from this heinous crime are revictimized, and the growing amount of illegal imagery distributed overwhelms law enforcement agents (LEAs) with the manual labor of categorization. To ease this burden researchers have explored methods for automating data triage and detection of CSAI, but the sensitive nature of the data imposes restricted access and minimal interaction between real data and learning algorithms, avoiding leaks at all costs. In observing how these restrictions have shaped the literature we formalize a definition of "Proxy Tasks", i.e., the substitute tasks used for training models for CSAI without making use of CSA data. Under this new terminology we review current literature and present a protocol for making conscious use of Proxy Tasks together with consistent input from LEAs to design better automation in this field. Finally, we apply this protocol to study -- for the first time -- the task of Few-shot Indoor Scene Classification on CSAI, showing a final model that achieves promising results on a real-world CSAI dataset whilst having no weights actually trained on sensitive data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification</title>
<link>https://arxiv.org/abs/2505.06646</link>
<guid>https://arxiv.org/abs/2505.06646</guid>
<content:encoded><![CDATA[
arXiv:2505.06646v1 Announce Type: cross 
Abstract: Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2505.06685</link>
<guid>https://arxiv.org/abs/2505.06685</guid>
<content:encoded><![CDATA[
arXiv:2505.06685v1 Announce Type: cross 
Abstract: Emotion understanding in videos aims to accurately recognize and interpret individuals' emotional states by integrating contextual, visual, textual, and auditory cues. While Large Multimodal Models (LMMs) have demonstrated significant progress in general vision-language (VL) tasks, their performance in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on emotion-related tasks often leads to catastrophic forgetting, hindering their ability to generalize across diverse tasks. To address these challenges, we present Emotion-Qwen, a tailored multimodal framework designed to enhance both emotion understanding and general VL reasoning. Emotion-Qwen incorporates a sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm, which dynamically routes inputs to balance emotion-specific and general-purpose processing. The model is pre-trained in a three-stage pipeline on large-scale general and emotional image datasets to support robust multimodal representations. Furthermore, we construct the Video Emotion Reasoning (VER) dataset, comprising more than 40K bilingual video clips with fine-grained descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning capability. Experimental results demonstrate that Emotion-Qwen achieves state-of-the-art performance on multiple emotion recognition benchmarks, while maintaining competitive results on general VL tasks. Code and models are available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark</title>
<link>https://arxiv.org/abs/2505.06746</link>
<guid>https://arxiv.org/abs/2505.06746</guid>
<content:encoded><![CDATA[
arXiv:2505.06746v1 Announce Type: cross 
Abstract: We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HistDiST: Histopathological Diffusion-based Stain Transfer</title>
<link>https://arxiv.org/abs/2505.06793</link>
<guid>https://arxiv.org/abs/2505.06793</guid>
<content:encoded><![CDATA[
arXiv:2505.06793v1 Announce Type: cross 
Abstract: Hematoxylin and Eosin (H&amp;E) staining is the cornerstone of histopathology but lacks molecular specificity. While Immunohistochemistry (IHC) provides molecular insights, it is costly and complex, motivating H&amp;E-to-IHC translation as a cost-effective alternative. Existing translation methods are mainly GAN-based, often struggling with training instability and limited structural fidelity, while diffusion-based approaches remain underexplored. We propose HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity H&amp;E-to-IHC translation. HistDiST introduces a dual-conditioning strategy, utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&amp;E representations to ensure pathology-relevant context and structural consistency. To overcome brightness biases, we incorporate a rescaled noise schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition at the final timestep. During inference, DDIM inversion preserves the morphological structure, while an eta-cosine noise schedule introduces controlled stochasticity, balancing structural consistency and molecular fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel pathology-aware metric leveraging GigaPath embeddings to assess molecular relevance. Extensive evaluations on MIST and BCI datasets demonstrate that HistDiST significantly outperforms existing methods, achieving a 28% improvement in MRA on the H&amp;E-to-Ki67 translation task, highlighting its effectiveness in capturing true IHC semantics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation</title>
<link>https://arxiv.org/abs/2505.06803</link>
<guid>https://arxiv.org/abs/2505.06803</guid>
<content:encoded><![CDATA[
arXiv:2505.06803v1 Announce Type: cross 
Abstract: Audio large language models (LLMs) are considered experts at recognizing sound objects, yet their performance relative to LLMs in other sensory modalities, such as visual or audio-visual LLMs, and to humans using their ears, eyes, or both remains unexplored. To investigate this, we systematically evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of different classes from audio-only, silent video, or sounded video inputs. We uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the sensory discrepancy between human ears and eyes. To reduce this gap, we introduce a cross-modal distillation framework, where an LLM in one modality serves as the teacher and another as the student, with knowledge transfer in sound classes predicted as more challenging to the student by a heuristic model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice versa, leads to notable improvements, particularly in challenging classes. This work highlights the sensory gap in LLMs from a human-aligned perspective and proposes a principled approach to enhancing modality-specific perception in multimodal LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods</title>
<link>https://arxiv.org/abs/2505.06811</link>
<guid>https://arxiv.org/abs/2505.06811</guid>
<content:encoded><![CDATA[
arXiv:2505.06811v1 Announce Type: cross 
Abstract: Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for non-invasive mapping of brain metabolites, providing critical insights into neurological conditions. However, its utility is often limited by missing or corrupted data due to motion artifacts, magnetic field inhomogeneities, or failed spectral fitting-especially in high resolution 3D acquisitions. To address this, we propose the first deep learning-based, mask-free framework for estimating missing data in MRSI metabolic maps. Unlike conventional restoration methods that rely on explicit masks to identify missing regions, our approach implicitly detects and estimates these areas using contextual spatial features through 2D and 3D U-Net architectures. We also introduce a progressive training strategy to enhance robustness under varying levels of data degradation. Our method is evaluated on both simulated and real patient datasets and consistently outperforms traditional interpolation techniques such as cubic and linear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97 with 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM of 0.98 with 15% missing voxels. Qualitative results show improved fidelity in estimating missing data, particularly in metabolically heterogeneous regions and ventricular regions. Importantly, our model generalizes well to real-world datasets without requiring retraining or mask input. These findings demonstrate the effectiveness and broad applicability of mask-free deep learning for MRSI restoration, with strong potential for clinical and research integration.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Robotic Policy Learning via Latent Space Backward Planning</title>
<link>https://arxiv.org/abs/2505.06861</link>
<guid>https://arxiv.org/abs/2505.06861</guid>
<content:encoded><![CDATA[
arXiv:2505.06861v1 Announce Type: cross 
Abstract: Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Classification Using a Diffusion Model as a Pre-Training Model</title>
<link>https://arxiv.org/abs/2505.06890</link>
<guid>https://arxiv.org/abs/2505.06890</guid>
<content:encoded><![CDATA[
arXiv:2505.06890v1 Announce Type: cross 
Abstract: In this paper, we propose a diffusion model that integrates a representation-conditioning mechanism, where the representations derived from a Vision Transformer (ViT) are used to condition the internal process of a Transformer-based diffusion model. This approach enables representation-conditioned data generation, addressing the challenge of requiring large-scale labeled datasets by leveraging self-supervised learning on unlabeled data. We evaluate our method through a zero-shot classification task for hematoma detection in brain imaging. Compared to the strong contrastive learning baseline, DINOv2, our method achieves a notable improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its effectiveness in image classification.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence</title>
<link>https://arxiv.org/abs/2505.06907</link>
<guid>https://arxiv.org/abs/2505.06907</guid>
<content:encoded><![CDATA[
arXiv:2505.06907v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-AIMS: AI-Powered Microscopy Image Analysis</title>
<link>https://arxiv.org/abs/2505.06918</link>
<guid>https://arxiv.org/abs/2505.06918</guid>
<content:encoded><![CDATA[
arXiv:2505.06918v1 Announce Type: cross 
Abstract: This paper presents a systematic solution for the intelligent recognition and automatic analysis of microscopy images. We developed a data engine that generates high-quality annotated datasets through a combination of the collection of diverse microscopy images from experiments, synthetic data generation and a human-in-the-loop annotation process. To address the unique challenges of microscopy images, we propose a segmentation model capable of robustly detecting both small and large objects. The model effectively identifies and separates thousands of closely situated targets, even in cluttered visual environments. Furthermore, our solution supports the precise automatic recognition of image scale bars, an essential feature in quantitative microscopic analysis. Building upon these components, we have constructed a comprehensive intelligent analysis platform and validated its effectiveness and practicality in real-world applications. This study not only advances automatic recognition in microscopy imaging but also ensures scalability and generalizability across multiple application domains, offering a powerful tool for automated microscopic analysis in interdisciplinary research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whitened CLIP as a Likelihood Surrogate of Images and Captions</title>
<link>https://arxiv.org/abs/2505.06934</link>
<guid>https://arxiv.org/abs/2505.06934</guid>
<content:encoded><![CDATA[
arXiv:2505.06934v1 Announce Type: cross 
Abstract: Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent space via an invertible linear operation. This transformation ensures that each feature in the embedding space has zero mean, unit standard deviation, and no correlation with all other features, resulting in an identity covariance matrix. We show that the whitened embeddings statistics can be well approximated as a standard normal distribution, thus, the log-likelihood is estimated simply by the square Euclidean norm in the whitened embedding space. The whitening procedure is completely training-free and performed using a pre-computed whitening matrix, hence, is very fast. We present several preliminary experiments demonstrating the properties and applicability of these likelihood scores to images and captions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing</title>
<link>https://arxiv.org/abs/2505.06963</link>
<guid>https://arxiv.org/abs/2505.06963</guid>
<content:encoded><![CDATA[
arXiv:2505.06963v1 Announce Type: cross 
Abstract: This paper introduces an innovative approach for the autonomous landing of Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera, therefore obviating the requirement for depth estimation cameras. Drawing on the inherent human estimating process, the proposed method reframes the landing task as an optimization problem. The UAV employs variations in the visual characteristics of a specially designed lenticular circle on the landing pad, where the perceived color and form provide critical information for estimating both altitude and depth. Reinforcement learning algorithms are utilized to approximate the functions governing these estimations, enabling the UAV to ascertain ideal landing settings via training. This method's efficacy is assessed by simulations and experiments, showcasing its potential for robust and accurate autonomous landing without dependence on complex sensor setups. This research contributes to the advancement of cost-effective and efficient UAV landing solutions, paving the way for wider applicability across various fields.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving</title>
<link>https://arxiv.org/abs/2505.06980</link>
<guid>https://arxiv.org/abs/2505.06980</guid>
<content:encoded><![CDATA[
arXiv:2505.06980v1 Announce Type: cross 
Abstract: Perception is a core capability of automated vehicles and has been significantly advanced through modern sensor technologies and artificial intelligence. However, perception systems still face challenges in complex real-world scenarios. To improve robustness against various external factors, multi-sensor fusion techniques are essential, combining the strengths of different sensor modalities. With recent developments in Vehicle-to-Everything (V2X communication, sensor fusion can now extend beyond a single vehicle to a cooperative multi-agent system involving Connected Automated Vehicle (CAV) and intelligent infrastructure. This paper presents VALISENS, an innovative multi-sensor system distributed across multiple agents. It integrates onboard and roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance situational awareness and support cooperative automated driving. The thermal camera adds critical redundancy for perceiving Vulnerable Road User (VRU), while fusion with roadside sensors mitigates visual occlusions and extends the perception range beyond the limits of individual vehicles. We introduce the corresponding perception module built on this sensor system, which includes object detection, tracking, motion forecasting, and high-level data fusion. The proposed system demonstrates the potential of cooperative perception in real-world test environments and lays the groundwork for future Cooperative Intelligent Transport Systems (C-ITS) applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Three-Phase Dynamics of Generalization Power of a DNN</title>
<link>https://arxiv.org/abs/2505.06993</link>
<guid>https://arxiv.org/abs/2505.06993</guid>
<content:encoded><![CDATA[
arXiv:2505.06993v1 Announce Type: cross 
Abstract: This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy of Groups in Dense Street Imagery</title>
<link>https://arxiv.org/abs/2505.07085</link>
<guid>https://arxiv.org/abs/2505.07085</guid>
<content:encoded><![CDATA[
arXiv:2505.07085v1 Announce Type: cross 
Abstract: Spatially and temporally dense street imagery (DSI) datasets have grown unbounded. In 2024, individual companies possessed around 3 trillion unique images of public streets. DSI data streams are only set to grow as companies like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze collisions. Academic researchers leverage DSI to explore novel approaches to urban analysis. Despite good-faith efforts by DSI providers to protect individual privacy through blurring faces and license plates, these measures fail to address broader privacy concerns. In this work, we find that increased data density and advancements in artificial intelligence enable harmful group membership inferences from supposedly anonymized data. We perform a penetration test to demonstrate how easily sensitive group affiliations can be inferred from obfuscated pedestrians in 25,232,608 dashcam images taken in New York City. We develop a typology of identifiable groups within DSI and analyze privacy implications through the lens of contextual integrity. Finally, we discuss actionable recommendations for researchers working with data from DSI providers.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</title>
<link>https://arxiv.org/abs/2505.07110</link>
<guid>https://arxiv.org/abs/2505.07110</guid>
<content:encoded><![CDATA[
arXiv:2505.07110v1 Announce Type: cross 
Abstract: Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skull stripping with purely synthetic data</title>
<link>https://arxiv.org/abs/2505.07159</link>
<guid>https://arxiv.org/abs/2505.07159</guid>
<content:encoded><![CDATA[
arXiv:2505.07159v1 Announce Type: cross 
Abstract: While many skull stripping algorithms have been developed for multi-modal and multi-species cases, there is still a lack of a fundamentally generalizable approach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain extrAction), a strategy to train a model for brain extraction with no real brain images or labels. Our results show that even without any real images or anatomical priors, the model achieves comparable accuracy in multi-modal, multi-species and pathological cases. This work presents a new direction of research for any generalizable medical image segmentation task.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metrics that matter: Evaluating image quality metrics for medical image generation</title>
<link>https://arxiv.org/abs/2505.07175</link>
<guid>https://arxiv.org/abs/2505.07175</guid>
<content:encoded><![CDATA[
arXiv:2505.07175v1 Announce Type: cross 
Abstract: Evaluating generative models for synthetic medical imaging is crucial yet challenging, especially given the high standards of fidelity, anatomical accuracy, and safety required for clinical applications. Standard evaluation of generated images often relies on no-reference image quality metrics when ground truth images are unavailable, but their reliability in this complex domain is not well established. This study comprehensively assesses commonly used no-reference image quality metrics using brain MRI data, including tumour and vascular images, providing a representative exemplar for the field. We systematically evaluate metric sensitivity to a range of challenges, including noise, distribution shifts, and, critically, localised morphological alterations designed to mimic clinically relevant inaccuracies. We then compare these metric scores against model performance on a relevant downstream segmentation task, analysing results across both controlled image perturbations and outputs from different generative model architectures. Our findings reveal significant limitations: many widely-used no-reference image quality metrics correlate poorly with downstream task suitability and exhibit a profound insensitivity to localised anatomical details crucial for clinical validity. Furthermore, these metrics can yield misleading scores regarding distribution shifts, e.g. data memorisation. This reveals the risk of misjudging model readiness, potentially leading to the deployment of flawed tools that could compromise patient safety. We conclude that ensuring generative models are truly fit for clinical purpose requires a multifaceted validation framework, integrating performance on relevant downstream tasks with the cautious interpretation of carefully selected no-reference image quality metrics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards user-centered interactive medical image segmentation in VR with an assistive AI agent</title>
<link>https://arxiv.org/abs/2505.07214</link>
<guid>https://arxiv.org/abs/2505.07214</guid>
<content:encoded><![CDATA[
arXiv:2505.07214v1 Announce Type: cross 
Abstract: Crucial in disease analysis and surgical planning, manual segmentation of volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and challenging to master, while fully automatic algorithms can benefit from user-feedback. Therefore, with the complementary power of the latest radiological AI foundation models and virtual reality (VR)'s intuitive data interaction, we propose SAMIRA, a novel conversational AI agent that assists users with localizing, segmenting, and visualizing 3D medical concepts in VR. Through speech-based interaction, the agent helps users understand radiological features, locate clinical targets, and generate segmentation masks that can be refined with just a few point prompts. The system also supports true-to-scale 3D visualization of segmented pathology to enhance patient-specific anatomical understanding. Furthermore, to determine the optimal interaction paradigm under near-far attention-switching for refining segmentation masks in an immersive, human-in-the-loop workflow, we compare VR controller pointing, head pointing, and eye tracking as input modes. With a user study, evaluations demonstrated a high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as strong support for the proposed VR system's guidance, training potential, and integration of AI in radiological segmentation tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data</title>
<link>https://arxiv.org/abs/2505.07349</link>
<guid>https://arxiv.org/abs/2505.07349</guid>
<content:encoded><![CDATA[
arXiv:2505.07349v1 Announce Type: cross 
Abstract: Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a critical task for healthcare professionals. The diverse nature of MRI acquisitions with varying contrasts and orientation introduce complexity in identifying hemorrhage using neural networks. For acquisitions with varying orientations, traditional methods often involve resampling images to a fixed plane, which can lead to information loss. To address this, we propose a 3D multi-plane vision transformer (MP-ViT) for hemorrhage classification with varying orientation data. It employs two separate transformer encoders for axial and sagittal contrasts, using cross-attention to integrate information across orientations. MP-ViT also includes a modality indication vector to provide missing contrast information to the model. The effectiveness of the proposed model is demonstrated with extensive experiments on real world clinical dataset consists of 10,084 training, 1,289 validation and 1,496 test subjects. MP-ViT achieved substantial improvement in area under the curve (AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based architectures by 1.8%. These results highlight the potential of MP-ViT in improving performance for hemorrhage detection when different orientation contrasts are needed.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.07411</link>
<guid>https://arxiv.org/abs/2505.07411</guid>
<content:encoded><![CDATA[
arXiv:2505.07411v1 Announce Type: cross 
Abstract: Pruning is a widely used method for compressing Deep Neural Networks (DNNs), where less relevant parameters are removed from a DNN model to reduce its size. However, removing parameters reduces model accuracy, so pruning is typically combined with fine-tuning, and sometimes other operations such as rewinding weights, to recover accuracy. A common approach is to repeatedly prune and then fine-tune, with increasing amounts of model parameters being removed in each step. While straightforward to implement, pruning pipelines that follow this approach are computationally expensive due to the need for repeated fine-tuning.
  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs that significantly decreases the time required for pruning by reducing the overall cost of fine-tuning, while maintaining a similar accuracy to existing pruning pipelines. ICE-Pruning is based on three main components: i) an automatic mechanism to determine after which pruning steps fine-tuning should be performed; ii) a freezing strategy for faster fine-tuning in each pruning step; and iii) a custom pruning-aware learning rate scheduler to further improve the accuracy of each pruning step and reduce the overall time consumption. We also propose an efficient auto-tuning stage for the hyperparameters (e.g., freezing percentage) introduced by the three components. We evaluate ICE-Pruning on several DNN models and datasets, showing that it can accelerate pruning by up to 9.61x. Code is available at https://github.com/gicLAB/ICE-Pruning
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Continuous Generative Models</title>
<link>https://arxiv.org/abs/2505.07447</link>
<guid>https://arxiv.org/abs/2505.07447</guid>
<content:encoded><![CDATA[
arXiv:2505.07447v1 Announce Type: cross 
Abstract: Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model</title>
<link>https://arxiv.org/abs/2505.07449</link>
<guid>https://arxiv.org/abs/2505.07449</guid>
<content:encoded><![CDATA[
arXiv:2505.07449v1 Announce Type: cross 
Abstract: In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at https://github.com/mar-cry/Ophora.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts</title>
<link>https://arxiv.org/abs/2505.07477</link>
<guid>https://arxiv.org/abs/2505.07477</guid>
<content:encoded><![CDATA[
arXiv:2505.07477v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by $\sim 90\%$ while maintaining superior performance. Code is available at https://github.com/deng-ai-lab/SDO.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Optimized Conditional Diffusion for Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.07548</link>
<guid>https://arxiv.org/abs/2505.07548</guid>
<content:encoded><![CDATA[
arXiv:2505.07548v1 Announce Type: cross 
Abstract: Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples (\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical alignment, causing DA failures. To address this challenge, we propose \textbf{N}oise \textbf{O}ptimized \textbf{C}onditional \textbf{D}iffusion for \textbf{D}omain \textbf{A}daptation (\textbf{NOCDDA}), which seamlessly integrates the generative capabilities of conditional diffusion models with the decision-making requirements of DA to achieve task-coupled optimization for efficient adaptation. For robust cross-domain consistency, we modify the DA classifier to align with the conditional diffusion classifier within a unified optimization framework, enabling forward training on noise-varying cross-domain samples. Furthermore, we argue that the conventional \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \) initialization in diffusion models often generates class-confused hcpl-tds, compromising discriminative DA. To resolve this, we introduce a class-aware noise optimization strategy that refines sampling regions for reverse class-specific hcpl-tds generation, effectively enhancing cross-domain alignment. Extensive experiments across 5 benchmark datasets and 29 DA tasks demonstrate significant performance gains of \textbf{NOCDDA} over 31 state-of-the-art methods, validating its robustness and effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding</title>
<link>https://arxiv.org/abs/2505.07600</link>
<guid>https://arxiv.org/abs/2505.07600</guid>
<content:encoded><![CDATA[
arXiv:2505.07600v1 Announce Type: cross 
Abstract: Manipulating clothes is challenging due to their complex dynamics, high deformability, and frequent self-occlusions. Garments exhibit a nearly infinite number of configurations, making explicit state representations difficult to define. In this paper, we analyze BiFold, a model that predicts language-conditioned pick-and-place actions from visual observations, while implicitly encoding garment state through end-to-end learning. To address scenarios such as crumpled garments or recovery from failed manipulations, BiFold leverages temporal context to improve state estimation. We examine the internal representations of the model and present evidence that its fine-tuning and temporal context enable effective alignment between text and image regions, as well as temporal consistency.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Brain: A Neuroscience-inspired Framework for Embodied Agents</title>
<link>https://arxiv.org/abs/2505.07634</link>
<guid>https://arxiv.org/abs/2505.07634</guid>
<content:encoded><![CDATA[
arXiv:2505.07634v1 Announce Type: cross 
Abstract: The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework</title>
<link>https://arxiv.org/abs/2505.07654</link>
<guid>https://arxiv.org/abs/2505.07654</guid>
<content:encoded><![CDATA[
arXiv:2505.07654v1 Announce Type: cross 
Abstract: Breast-conserving surgery (BCS) aims to completely remove malignant lesions while maximizing healthy tissue preservation. Intraoperative margin assessment is essential to achieve a balance between thorough cancer resection and tissue conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM) enables rapid acquisition of whole surface images (WSIs) for excised tissue, providing contrast between malignant and normal tissues. However, breast cancer classification with DUV WSIs is challenged by high resolutions and complex histopathological features. This study introduces a DUV WSI classification framework using a patch-level vision transformer (ViT) model, capturing local and global features. Grad-CAM++ saliency weighting highlights relevant spatial regions, enhances result interpretability, and improves diagnostic accuracy for benign and malignant tissue classification. A comprehensive 5-fold cross-validation demonstrates the proposed approach significantly outperforms conventional deep learning methods, achieving a classification accuracy of 98.33%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells</title>
<link>https://arxiv.org/abs/2505.07661</link>
<guid>https://arxiv.org/abs/2505.07661</guid>
<content:encoded><![CDATA[
arXiv:2505.07661v1 Announce Type: cross 
Abstract: We present SparseAttnNet, a new hierarchical attention-driven framework for efficient image classification that adaptively selects and processes only the most informative pixels from images. Traditional convolutional neural networks typically process the entire images regardless of information density, leading to computational inefficiency and potential focus on irrelevant features. Our approach leverages a dynamic selection mechanism that uses coarse attention distilled by fine multi-head attention from the downstream layers of the model, allowing the model to identify and extract the most salient k pixels, where k is adaptively learned during training based on loss convergence trends. Once the top-k pixels are selected, the model processes only these pixels, embedding them as words in a language model to capture their semantics, followed by multi-head attention to incorporate global context. For biological cell images, we demonstrate that SparseAttnNet can process approximately 15% of the pixels instead of the full image. Applied to cell classification tasks using white blood cells images from the following modalities: optical path difference (OPD) images from digital holography for stain-free cells, images from motion-sensitive (event) camera from stain-free cells, and brightfield microscopy images of stained cells, For all three imaging modalities, SparseAttnNet achieves competitive accuracy while drastically reducing computational requirements in terms of both parameters and floating-point operations per second, compared to traditional CNNs and Vision Transformers. Since the model focuses on biologically relevant regions, it also offers improved explainability. The adaptive and lightweight nature of SparseAttnNet makes it ideal for deployment in resource-constrained and high-throughput settings, including imaging flow cytometry.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization</title>
<link>https://arxiv.org/abs/2505.07675</link>
<guid>https://arxiv.org/abs/2505.07675</guid>
<content:encoded><![CDATA[
arXiv:2505.07675v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that $\texttt{DHO}$ consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation</title>
<link>https://arxiv.org/abs/2505.07687</link>
<guid>https://arxiv.org/abs/2505.07687</guid>
<content:encoded><![CDATA[
arXiv:2505.07687v1 Announce Type: cross 
Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba's selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2's image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at https://github.com/gatina-yone/ABS-Mamba
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeletonization of neuronal processes using Discrete Morse techniques from computational topology</title>
<link>https://arxiv.org/abs/2505.07754</link>
<guid>https://arxiv.org/abs/2505.07754</guid>
<content:encoded><![CDATA[
arXiv:2505.07754v1 Announce Type: cross 
Abstract: To understand biological intelligence we need to map neuronal networks in vertebrate brains. Mapping mesoscale neural circuitry is done using injections of tracers that label groups of neurons whose axons project to different brain regions. Since many neurons are labeled, it is difficult to follow individual axons. Previous approaches have instead quantified the regional projections using the total label intensity within a region. However, such a quantification is not biologically meaningful. We propose a new approach better connected to the underlying neurons by skeletonizing labeled axon fragments and then estimating a volumetric length density. Our approach uses a combination of deep nets and the Discrete Morse (DM) technique from computational topology. This technique takes into account nonlocal connectivity information and therefore provides noise-robustness. We demonstrate the utility and scalability of the approach on whole-brain tracer injected data. We also define and illustrate an information theoretic measure that quantifies the additional information obtained, compared to the skeletonized tracer injection fragments, when individual axon morphologies are available. Our approach is the first application of the DM technique to computational neuroanatomy. It can help bridge between single-axon skeletons and tracer injections, two important data types in mapping neural networks in vertebrates.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution</title>
<link>https://arxiv.org/abs/2505.07766</link>
<guid>https://arxiv.org/abs/2505.07766</guid>
<content:encoded><![CDATA[
arXiv:2505.07766v1 Announce Type: cross 
Abstract: User privacy is a crucial concern in robotic applications, especially when mobile service robots are deployed in personal or sensitive environments. However, many robotic downstream tasks require the use of cameras, which may raise privacy risks. To better understand user perceptions of privacy in relation to visual data, we conducted a user study investigating how different image modalities and image resolutions affect users' privacy concerns. The results show that depth images are broadly viewed as privacy-safe, and a similarly high proportion of respondents feel the same about semantic segmentation images. Additionally, the majority of participants consider 32*32 resolution RGB images to be almost sufficiently privacy-preserving, while most believe that 16*16 resolution can fully guarantee privacy protection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies</title>
<link>https://arxiv.org/abs/2505.07813</link>
<guid>https://arxiv.org/abs/2505.07813</guid>
<content:encoded><![CDATA[
arXiv:2505.07813v1 Announce Type: cross 
Abstract: Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous manipulation policies to generalize to novel environments, but acquiring such datasets presents many challenges. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DexWild-System, a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. This combination results in robust robot policies capable of generalizing to novel environments, tasks, and embodiments with minimal additional robot-specific data. Experimental results demonstrate that DexWild significantly improves performance, achieving a 68.5% success rate in unseen environments-nearly four times higher than policies trained with robot data only-and offering 5.8x better cross-embodiment generalization. Video results, codebases, and instructions at https://dexwild.github.io
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07815</link>
<guid>https://arxiv.org/abs/2505.07815</guid>
<content:encoded><![CDATA[
arXiv:2505.07815v1 Announce Type: cross 
Abstract: Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel Motion as Universal Representation for Robot Control</title>
<link>https://arxiv.org/abs/2505.07817</link>
<guid>https://arxiv.org/abs/2505.07817</guid>
<content:encoded><![CDATA[
arXiv:2505.07817v1 Announce Type: cross 
Abstract: We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo for visualizations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^{\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning</title>
<link>https://arxiv.org/abs/2505.07819</link>
<guid>https://arxiv.org/abs/2505.07819</guid>
<content:encoded><![CDATA[
arXiv:2505.07819v1 Announce Type: cross 
Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\textbf{Triply-Hierarchical Diffusion Policy}~(\textbf{H$^{\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\mathbf{+27.5\%}$ average relative improvement over baselines across $\mathbf{44}$ simulation tasks and achieves superior performance in $\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review helps learn better: Temporal Supervised Knowledge Distillation</title>
<link>https://arxiv.org/abs/2307.00811</link>
<guid>https://arxiv.org/abs/2307.00811</guid>
<content:encoded><![CDATA[
arXiv:2307.00811v3 Announce Type: replace 
Abstract: Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and advantages of our method over existing knowledge distillation methods, including various network architectures and different tasks (image classification and object detection) .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC</title>
<link>https://arxiv.org/abs/2310.02719</link>
<guid>https://arxiv.org/abs/2310.02719</guid>
<content:encoded><![CDATA[
arXiv:2310.02719v2 Announce Type: replace 
Abstract: In this paper, we introduce a general framework for analyzing the numerical conditioning of minimal problems in multiple view geometry, using tools from computational algebra and Riemannian geometry. Special motivation comes from the fact that relative pose estimation, based on standard 5-point or 7-point Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are present and there is enough data to support a hypothesis. We argue that these cases arise due to the intrinsic instability of the 5- and 7-point minimal problems. We apply our framework to characterize the instabilities, both in terms of the world scenes that lead to infinite condition number, and directly in terms of ill-conditioned image data. The approach produces computational tests for assessing the condition number before solving the minimal problem. Lastly, synthetic and real data experiments suggest that RANSAC serves not only to remove outliers, but in practice it also selects for well-conditioned image data, which is consistent with our theory.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic and Expressive Variation in Image Captions Across Languages</title>
<link>https://arxiv.org/abs/2310.14356</link>
<guid>https://arxiv.org/abs/2310.14356</guid>
<content:encoded><![CDATA[
arXiv:2310.14356v5 Announce Type: replace 
Abstract: Computer vision often treats human perception as homogeneous: an implicit assumption that visual stimuli are perceived similarly by everyone. This assumption is reflected in the way researchers collect datasets and train vision models. By contrast, literature in cross-cultural psychology and linguistics has provided evidence that people from different cultural backgrounds observe vastly different concepts even when viewing the same visual stimuli. In this paper, we study how these differences manifest themselves in vision-language datasets and models, using language as a proxy for culture. By comparing textual descriptions generated across 7 languages for the same images, we find significant differences in the semantic content and linguistic expression. When datasets are multilingual as opposed to monolingual, descriptions have higher semantic coverage on average, where coverage is measured using scene graphs, model embeddings, and linguistic taxonomies. For example, multilingual descriptions have on average 29.9% more objects, 24.5% more relations, and 46.0% more attributes than a set of monolingual captions. When prompted to describe images in different languages, popular models (e.g. LLaVA) inherit this bias and describe different parts of the image. Moreover, finetuning models on captions from one language performs best on corresponding test data from that language, while finetuning on multilingual data performs consistently well across all test data compositions. Our work points towards the need to account for and embrace the diversity of human perception in the computer vision community.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Feature-Guided Diffusion Models for Shadow Removal</title>
<link>https://arxiv.org/abs/2312.02156</link>
<guid>https://arxiv.org/abs/2312.02156</guid>
<content:encoded><![CDATA[
arXiv:2312.02156v2 Announce Type: replace 
Abstract: Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process. Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Habitat Information for Fine-grained Bird Identification</title>
<link>https://arxiv.org/abs/2312.14999</link>
<guid>https://arxiv.org/abs/2312.14999</guid>
<content:encoded><![CDATA[
arXiv:2312.14999v2 Announce Type: replace 
Abstract: Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: https://anonymous.4open.science/r/reasoning-8B7E/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction</title>
<link>https://arxiv.org/abs/2401.13174</link>
<guid>https://arxiv.org/abs/2401.13174</guid>
<content:encoded><![CDATA[
arXiv:2401.13174v4 Announce Type: replace 
Abstract: It has been revealed that small efficient dense image prediction (EDIP) models, trained using the knowledge distillation (KD) framework, encounter two key challenges, including maintaining boundary region completeness and preserving target region connectivity, despite their favorable capacity to recognize main object regions. In this work, we propose a complementary boundary and context distillation (BCD) method within the KD framework for EDIPs, which facilitates the targeted knowledge transfer from large accurate teacher models to compact efficient student models. Specifically, the boundary distillation component focuses on extracting explicit object-level semantic boundaries from the hierarchical feature maps of the backbone network to enhance the student model's mask quality in boundary regions. Concurrently, the context distillation component leverages self-relations as a bridge to transfer implicit pixel-level contexts from the teacher model to the student model, ensuring strong connectivity in target regions. Our proposed BCD method is specifically designed for EDIP tasks and is characterized by its simplicity and efficiency. Extensive experimental results across semantic segmentation, object detection, and instance segmentation on various representative datasets demonstrate that our method can outperform existing methods without requiring extra supervisions or incurring increased inference costs, resulting in well-defined object boundaries and smooth connecting regions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Msmsfnet: a multi-stream and multi-scale fusion net for edge detection</title>
<link>https://arxiv.org/abs/2404.04856</link>
<guid>https://arxiv.org/abs/2404.04856</guid>
<content:encoded><![CDATA[
arXiv:2404.04856v3 Announce Type: replace 
Abstract: Edge detection is a long-standing problem in computer vision. Despite the efficiency of existing algorithms, their performance, however, rely heavily on the pre-trained weights of the backbone network on the ImageNet dataset. The use of pre-trained weights in previous methods significantly increases the difficulty to design new models for edge detection without relying on existing well-trained ImageNet models, as pre-training the model on the ImageNet dataset is expensive and becomes compulsory to ensure the fairness of comparison. Besides, the pre-training and fine-tuning strategy is not always useful and sometimes even inaccessible. For instance, the pre-trained weights on the ImageNet dataset are unlikely to be helpful for edge detection in Synthetic Aperture Radar (SAR) images due to strong differences in the statistics between optical images and SAR images. Moreover, no dataset has comparable size to the ImageNet dataset for SAR image processing. In this work, we study the performance achievable by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi-scale fusion net (msmsfnet), for edge detection. We show in our experiments that by training all models from scratch, our model outperforms state-of-the-art edge detectors in three publicly available datasets. We also demonstrate the efficiency of our model for edge detection in SAR images, where no useful pre-trained weight is available. Finally, We show that our model is able to achieve competitive performance on the BSDS500 dataset when the pre-trained weights are used.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention</title>
<link>https://arxiv.org/abs/2404.17484</link>
<guid>https://arxiv.org/abs/2404.17484</guid>
<content:encoded><![CDATA[
arXiv:2404.17484v2 Announce Type: replace 
Abstract: Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging technique. The fundamental unit of ODT is the 1D depth-resolved trace named raw A-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a cross-sectional flow image via Doppler phase-subtraction of raw A-scans along B-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required currently, leading to prolonged scanning time and increased storage demands. Addressing this issue, we propose a novel sparse ODT reconstruction framework with an Alternative State Space Attention Network (ASSAN) that effectively reduces raw A-scans needed. Inspired by the distinct distributions of information along A-line and B-line, ASSAN applies 1D State Space Model (SSM) to each A-line to learn the intra-A-scan representation, while using 1D gated self-attention along B-line to capture the inter-A-scan features. In addition, an effective feedforward network based on sequential 1D convolutions along different axes is employed to enhance the local feature. In validation experiments on real animal data, ASSAN shows clear effectiveness in the reconstruction in comparison with state-of-the-art reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPL: Memory Augmentation and Pseudo-Labeling for Semi-Supervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2405.06198</link>
<guid>https://arxiv.org/abs/2405.06198</guid>
<content:encoded><![CDATA[
arXiv:2405.06198v3 Announce Type: replace 
Abstract: Large unlabeled data and difficult-to-identify anomalies are the urgent issues need to overcome in most industrial scene. In order to address this issue, a new meth-odology for detecting surface defects in in-dustrial settings is introduced, referred to as Memory Augmentation and Pseudo-Labeling(MAPL). The methodology first in-troduces an anomaly simulation strategy, which significantly improves the model's ability to recognize rare or unknown anom-aly types by generating simulated anomaly samples. To cope with the problem of the lack of labeling of anomalous simulated samples, a pseudo-labeler method based on a one-classifier ensemble was employed in this study, which enhances the robustness of the model in the case of limited labeling data by automatically selecting key pseudo-labeling hyperparameters. Meanwhile, a memory-enhanced learning mechanism is introduced to effectively predict abnormal regions by analyzing the difference be-tween the input samples and the normal samples in the memory pool. An end-to-end learning framework is employed by MAPL to identify the abnormal regions directly from the input data, which optimizes the ef-ficiency and real-time performance of de-tection. By conducting extensive trials on the recently developed BHAD dataset (in-cluding MVTec AD [1], Visa [2], and MDPP [3]), MAPL achieves an average im-age-level AUROC score of 86.2%, demon-strating a 5.1% enhancement compared to the original MemSeg [4] model. The source code is available at https://github.com/jzc777/MAPL.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2405.17456</link>
<guid>https://arxiv.org/abs/2405.17456</guid>
<content:encoded><![CDATA[
arXiv:2405.17456v3 Announce Type: replace 
Abstract: We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions</title>
<link>https://arxiv.org/abs/2407.01330</link>
<guid>https://arxiv.org/abs/2407.01330</guid>
<content:encoded><![CDATA[
arXiv:2407.01330v2 Announce Type: replace 
Abstract: Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large 3D shape datasets, which is costly and necessitates re-training for new datasets. This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns in localized regions, prompting us to develop a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. Despite being highly lightweight, with only 653 KB of trainable parameters and a modest-sized training dataset with 0.5 GB storage, our method enables efficient and robust surface reconstruction from point clouds without requiring for shape-specific training. Furthermore, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We conduct comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy. Notably, our lightweight framework offers rapid and reliable initialization for other unsupervised iterative approaches, improving both the efficiency and accuracy of their reconstructions. Our project and code are available at https://jbhu67.github.io/LoSF-UDF.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration</title>
<link>https://arxiv.org/abs/2407.13120</link>
<guid>https://arxiv.org/abs/2407.13120</guid>
<content:encoded><![CDATA[
arXiv:2407.13120v3 Announce Type: replace 
Abstract: Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert space and lacks accelerated variants. To address these issues, we propose a Halpern-type PPP (HPPP) algorithm, which leverages the strong convergence and acceleration properties of Halpern's iteration method. Moreover, we propose a novel algorithm for image restoration by combining HPPP with denoiser priors such as Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method. Finally, numerical experiments including several toy examples and image restoration validate the effectiveness of our proposed algorithms.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation</title>
<link>https://arxiv.org/abs/2407.18715</link>
<guid>https://arxiv.org/abs/2407.18715</guid>
<content:encoded><![CDATA[
arXiv:2407.18715v3 Announce Type: replace 
Abstract: Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency through end-to-end learning. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, which restricts effective information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization in a semantic-aligned space for SGG, enabling efficient and generalizable interaction between entities and predicates. Specifically, we introduce an end-to-end scene graph generation model, the Bidirectional Conditioning Transformer (BCTR), to implement this factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) performs multi-stage interactive feature augmentation between entities and predicates, enabling mutual enhancement between these predictions. Second, Random Feature Alignment (RFA) is present to regularize feature space by distilling multi-modal knowledge from pre-trained models. Within this regularized feature space, BCG is feasible to capture interaction patterns across diverse relationships during training, and the learned interaction patterns can generalize to unseen but semantically related relationships during inference. Extensive experiments on Visual Genome and Open Image V6 show that BCTR achieves state-of-the-art performance on both benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking</title>
<link>https://arxiv.org/abs/2408.12232</link>
<guid>https://arxiv.org/abs/2408.12232</guid>
<content:encoded><![CDATA[
arXiv:2408.12232v2 Announce Type: replace 
Abstract: Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camouflaged Object Tracking: A Benchmark</title>
<link>https://arxiv.org/abs/2408.13877</link>
<guid>https://arxiv.org/abs/2408.13877</guid>
<content:encoded><![CDATA[
arXiv:2408.13877v3 Announce Type: replace 
Abstract: Visual tracking has seen remarkable advancements, largely driven by the availability of large-scale training datasets that have enabled the development of highly accurate and robust algorithms. While significant progress has been made in tracking general objects, research on more challenging scenarios, such as tracking camouflaged objects, remains limited. Camouflaged objects, which blend seamlessly with their surroundings or other objects, present unique challenges for detection and tracking in complex environments. This challenge is particularly critical in applications such as military, security, agriculture, and marine monitoring, where precise tracking of camouflaged objects is essential. To address this gap, we introduce the Camouflaged Object Tracking Dataset (COTD), a specialized benchmark designed specifically for evaluating camouflaged object tracking methods. The COTD dataset comprises 200 sequences and approximately 80,000 frames, each annotated with detailed bounding boxes. Our evaluation of 20 existing tracking algorithms reveals significant deficiencies in their performance with camouflaged objects. To address these issues, we propose a novel tracking framework, HiPTrack-MLS, which demonstrates promising results in improving tracking performance for camouflaged objects. COTD and code are avialable at https://github.com/openat25/HIPTrack-MLS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning</title>
<link>https://arxiv.org/abs/2409.00020</link>
<guid>https://arxiv.org/abs/2409.00020</guid>
<content:encoded><![CDATA[
arXiv:2409.00020v2 Announce Type: replace 
Abstract: Crop phenology describes the physiological development stages of crops from planting to harvest which is valuable information for decision makers to plan and adapt agricultural management strategies. In the era of big Earth observation data ubiquity, attempts have been made to accurately detect crop phenology using Remote Sensing (RS) and high resolution weather data. However, most studies have focused on large scale predictions of phenology or developed methods which are not adequate to help crop modeler communities on leveraging Sentinel-1 and Sentinal-2 data and fusing them with high resolution climate data, using a novel framework. For this, we trained a Machine Learning (ML) LightGBM model to predict 13 phenological stages for eight major crops across Germany at 20 m scale. Observed phonologies were taken from German national phenology network (German Meteorological Service; DWD) between 2017 and 2021. We proposed a thorough feature selection analysis to find the best combination of RS and climate data to detect phenological stages. At national scale, predicted phenology resulted in a reasonable precision of R2 > 0.43 and a low Mean Absolute Error of 6 days, averaged over all phenological stages and crops. The spatio-temporal analysis of the model predictions demonstrates its transferability across different spatial and temporal context of Germany. The results indicated that combining radar sensors with climate data yields a very promising performance for a multitude of practical applications. Moreover, these improvements are expected to be useful to generate highly valuable input for crop model calibrations and evaluations, facilitate informed agricultural decisions, and contribute to sustainable food production to address the increasing global food demand.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGEV++: Iterative Multi-range Geometry Encoding Volumes for Stereo Matching</title>
<link>https://arxiv.org/abs/2409.00638</link>
<guid>https://arxiv.org/abs/2409.00638</guid>
<content:encoded><![CDATA[
arXiv:2409.00638v3 Announce Type: replace 
Abstract: Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23\% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9\% and 54.8\% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks. The code is publicly available at https://github.com/gangweix/IGEV and https://github.com/gangweix/IGEV-plusplus.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance</title>
<link>https://arxiv.org/abs/2409.06002</link>
<guid>https://arxiv.org/abs/2409.06002</guid>
<content:encoded><![CDATA[
arXiv:2409.06002v4 Announce Type: replace 
Abstract: Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this https URL}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation</title>
<link>https://arxiv.org/abs/2409.18653</link>
<guid>https://arxiv.org/abs/2409.18653</guid>
<content:encoded><![CDATA[
arXiv:2409.18653v2 Announce Type: replace 
Abstract: This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code is available at https://github.com/zhoustan/SAM2-VCOS
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction</title>
<link>https://arxiv.org/abs/2409.20223</link>
<guid>https://arxiv.org/abs/2409.20223</guid>
<content:encoded><![CDATA[
arXiv:2409.20223v2 Announce Type: replace 
Abstract: Understanding and predicting pedestrian crossing behavioral intention is crucial for the driving safety of autonomous vehicles. Nonetheless, challenges emerge when using promising images or environmental context masks to extract various factors for time-series network modeling, causing pre-processing errors or a loss of efficiency. Typically, pedestrian positions captured by onboard cameras are often distorted and do not accurately reflect their actual movements. To address these issues, GTransPDM -- a Graph-embedded Transformer with a Position Decoupling Module -- was developed for pedestrian crossing intention prediction by leveraging multi-modal features. First, a positional decoupling module was proposed to decompose pedestrian lateral motion and encode depth cues in the image view. Then, a graph-embedded Transformer was designed to capture the spatio-temporal dynamics of human pose skeletons, integrating essential factors such as position, skeleton, and ego-vehicle motion. Experimental results indicate that the proposed method achieves 92% accuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a processing speed of 0.05ms. It outperforms the state-of-the-art in comparison.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Large Motion Models with Million-Level Human Motions</title>
<link>https://arxiv.org/abs/2410.03311</link>
<guid>https://arxiv.org/abs/2410.03311</guid>
<content:encoded><![CDATA[
arXiv:2410.03311v2 Announce Type: replace 
Abstract: Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named Being-M0, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://github.com/BeingBeyond/Being-M0.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotating-star Pattern for Camera Calibration</title>
<link>https://arxiv.org/abs/2410.13371</link>
<guid>https://arxiv.org/abs/2410.13371</guid>
<content:encoded><![CDATA[
arXiv:2410.13371v3 Announce Type: replace 
Abstract: Camera calibration is fundamental to 3D vision, and the choice of calibration pattern greatly affects the accuracy. To address aberration issue, star-shaped pattern has been proposed as alternatives to traditional checkerboard. However, such pattern suffers from aliasing artifacts. In this paper, we present a novel solution by employing a series of checkerboard patterns rotated around a central point instead of a single star-shaped pattern. We further propose a complete feature extraction algorithm tailored for this design. Experimental results demonstrate that our approach offers improved accuracy over the conventional star-shaped pattern and achieves high stability across varying exposure levels.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2410.19794</link>
<guid>https://arxiv.org/abs/2410.19794</guid>
<content:encoded><![CDATA[
arXiv:2410.19794v2 Announce Type: replace 
Abstract: Deep Neural Networks (DNNs) are increasingly deployed across applications. However, ensuring their reliability remains a challenge, and in many situations, alternative models with similar functionality and accuracy are available. Traditional accuracy-based evaluations often fail to capture behavioral differences between models, especially with limited test datasets, making it difficult to select or combine models effectively. Differential testing addresses this by generating test inputs that expose discrepancies in DNN model behavior. However, existing approaches face significant limitations: many rely on model internals or are constrained by available seed inputs. To address these challenges, we propose DiffGAN, a black-box test image generation approach for differential testing of DNN models. DiffGAN leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to generate diverse and valid triggering inputs that reveal behavioral discrepancies between models. DiffGAN employs two custom fitness functions, focusing on diversity and divergence, to guide the exploration of the GAN input space and identify discrepancies between models' outputs. By strategically searching this space, DiffGAN generates inputs with specific features that trigger differences in model behavior. DiffGAN is black-box, making it applicable in more situations. We evaluate DiffGAN on eight DNN model pairs trained on widely used image datasets. Our results show DiffGAN significantly outperforms a SOTA baseline, generating four times more triggering inputs, with greater diversity and validity, within the same budget. Additionally, the generated inputs improve the accuracy of a machine learning-based model selection mechanism, which selects the best-performing model based on input characteristics and can serve as a smart output voting mechanism when using alternative models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veri-Car: Towards Open-world Vehicle Information Retrieval</title>
<link>https://arxiv.org/abs/2411.06864</link>
<guid>https://arxiv.org/abs/2411.06864</guid>
<content:encoded><![CDATA[
arXiv:2411.06864v4 Announce Type: replace 
Abstract: Many industrial and service sectors require tools to extract vehicle characteristics from images. This is a complex task not only by the variety of noise, and large number of classes, but also by the constant introduction of new vehicle models to the market. In this paper, we present Veri-Car, an information retrieval integrated approach designed to help on this task. It leverages supervised learning techniques to accurately identify the make, type, model, year, color, and license plate of cars. The approach also addresses the challenge of handling open-world problems, where new car models and variations frequently emerge, by employing a sophisticated combination of pre-trained models, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust performance, achieving high precision and accuracy in classifying both seen and unseen data. Additionally, it integrates an ensemble license plate detection, and an OCR model to extract license plate numbers with impressive accuracy.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning</title>
<link>https://arxiv.org/abs/2411.07742</link>
<guid>https://arxiv.org/abs/2411.07742</guid>
<content:encoded><![CDATA[
arXiv:2411.07742v4 Announce Type: replace 
Abstract: This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive point cloud sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of point cloud sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Without incurring additional computational overhead, we increase the number of point cloud sweeps from 10, a common practice, to as many as 40. Consequently, there is a significant enhancement in perception performance. For instance, in nuScenes 3D object detection and BEV map segmentation tasks, our pruning strategy improves several 3D perception baseline methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules</title>
<link>https://arxiv.org/abs/2411.11011</link>
<guid>https://arxiv.org/abs/2411.11011</guid>
<content:encoded><![CDATA[
arXiv:2411.11011v2 Announce Type: replace 
Abstract: Fire incidents in urban and forested areas pose serious threats,underscoring the need for more effective detection technologies. To address these challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted improvements for detecting small fires and smoke. The model integrates the CARAFE up-sampling operator and a context-guided module to reduce information loss during up-sampling and down-sampling, thereby retaining richer feature representations. Additionally, an inverted residual mobile block enhanced C2f module captures small targets and fine smoke patterns, a critical improvement over the original model's detection capacity.For validation, we introduce Web-Fire, a dataset curated for fire and smoke detection across diverse real-world scenarios. Experimental results indicate that CCi-YOLOv8n outperforms YOLOv8n in detection precision, confirming its effectiveness for robust fire detection tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transmission Line Defect Detection Based on UAV Patrol Images and Vision-language Pretraining</title>
<link>https://arxiv.org/abs/2411.11370</link>
<guid>https://arxiv.org/abs/2411.11370</guid>
<content:encoded><![CDATA[
arXiv:2411.11370v2 Announce Type: replace 
Abstract: Unmanned aerial vehicle (UAV) patrol inspection has emerged as a predominant approach in transmission line monitoring owing to its cost-effectiveness. Detecting defects in transmission lines is a critical task during UAV patrol inspection. However, due to imaging distance and shooting angles, UAV patrol images often suffer from insufficient defect-related visual information, which has an adverse effect on detection accuracy. In this article, we propose a novel method for detecting defects in UAV patrol images, which is based on vision-language pretraining for transmission line (VLP-TL) and a progressive transfer strategy (PTS). Specifically, VLP-TL contains two novel pretraining tasks tailored for the transmission line scenario, aimimg at pretraining an image encoder with abundant knowledge acquired from both visual and linguistic information. Transferring the pretrained image encoder to the defect detector as its backbone can effectively alleviate the insufficient visual information problem. In addition, the PTS further improves transfer performance by progressively bridging the gap between pretraining and downstream defection detection. Experimental results demonstrate that the proposed method significantly improves defect detection accuracy by jointly utilizing multimodal information, overcoming the limitations of insufficient defect-related visual information provided by UAV patrol images.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2411.11904</link>
<guid>https://arxiv.org/abs/2411.11904</guid>
<content:encoded><![CDATA[
arXiv:2411.11904v3 Announce Type: replace 
Abstract: Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph</title>
<link>https://arxiv.org/abs/2411.14494</link>
<guid>https://arxiv.org/abs/2411.14494</guid>
<content:encoded><![CDATA[
arXiv:2411.14494v3 Announce Type: replace 
Abstract: A facial morph is an image created by combining two face images pertaining to two distinct identities. Face demorphing inverts the process and tries to recover the original images constituting a facial morph. While morph attack detection (MAD) techniques can be used to flag morph images, they do not divulge any visual information about the faces used to create them. Demorphing helps address this problem. Existing demorphing techniques are either very restrictive (assume identities during testing) or produce feeble outputs (both outputs look very similar). In this paper, we overcome these issues by proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph images. Our method overcomes morph-replication and produces high quality reconstructions of the bonafide images used to create the morphs. Moreover, our method is highly generalizable across demorphing paradigms (differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and MorDiff datasets to showcase the efficacy of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opt-In Art: Learning Art Styles Only from Few Examples</title>
<link>https://arxiv.org/abs/2412.00176</link>
<guid>https://arxiv.org/abs/2412.00176</guid>
<content:encoded><![CDATA[
arXiv:2412.00176v2 Announce Type: replace 
Abstract: We explore whether pre-training on datasets with paintings is necessary for a model to learn an artistic style with only a few examples. To investigate this, we train a text-to-image model exclusively on photographs, without access to any painting-related content. We show that it is possible to adapt a model that is trained without paintings to an artistic style, given only few examples. User studies and automatic evaluations confirm that our model (post-adaptation) performs on par with state-of-the-art models trained on massive datasets that contain artistic content like paintings, drawings or illustrations. Finally, using data attribution techniques, we analyze how both artistic and non-artistic datasets contribute to generating artistic-style images. Surprisingly, our findings suggest that high-quality artistic outputs can be achieved without prior exposure to artistic data, indicating that artistic style generation can occur in a controlled, opt-in manner using only a limited, carefully selected set of training examples.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SerialGen: Personalized Image Generation by First Standardization Then Personalization</title>
<link>https://arxiv.org/abs/2412.01485</link>
<guid>https://arxiv.org/abs/2412.01485</guid>
<content:encoded><![CDATA[
arXiv:2412.01485v2 Announce Type: replace 
Abstract: In this work, we are interested in achieving both high text controllability and whole-body appearance consistency in the generation of personalized human characters. We propose a novel framework, named SerialGen, which is a serial generation method consisting of two stages: first, a standardization stage that standardizes reference images, and then a personalized generation stage based on the standardized reference. Furthermore, we introduce two modules aimed at enhancing the standardization process. Our experimental results validate the proposed framework's ability to produce personalized images that faithfully recover the reference image's whole-body appearance while accurately responding to a wide range of text prompts. Through thorough analysis, we highlight the critical contribution of the proposed serial generation method and standardization model, evidencing enhancements in appearance consistency between reference and output images and across serial outputs generated from diverse text prompts. The term "Serial" in this work carries a double meaning: it refers to the two-stage method and also underlines our ability to generate serial images with consistent appearance throughout.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs</title>
<link>https://arxiv.org/abs/2412.01818</link>
<guid>https://arxiv.org/abs/2412.01818</guid>
<content:encoded><![CDATA[
arXiv:2412.01818v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis</title>
<link>https://arxiv.org/abs/2412.09521</link>
<guid>https://arxiv.org/abs/2412.09521</guid>
<content:encoded><![CDATA[
arXiv:2412.09521v2 Announce Type: replace 
Abstract: Pathological diagnosis is vital for determining disease characteristics, guiding treatment, and assessing prognosis, relying heavily on detailed, multi-scale analysis of high-resolution whole slide images (WSI). However, traditional pure vision models face challenges of redundant feature extraction, whereas existing large vision-language models (LVLMs) are limited by input resolution constraints, hindering their efficiency and accuracy. To overcome these issues, we propose two innovative strategies: the mixed task-guided feature enhancement, which directs feature extraction toward lesion-related details across scales, and the prompt-guided detail feature completion, which integrates coarse- and fine-grained features from WSI based on specific prompts without compromising inference speed. Leveraging a comprehensive dataset of 490,000 samples from diverse pathology tasks-including cancer detection, grading, vascular and neural invasion identification, and so on-we trained the pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that this model significantly outperforms existing methods in diagnostic accuracy and efficiency, offering an interactive, clinically aligned approach for auxiliary diagnosis in a wide range of pathology applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-QuAD: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification</title>
<link>https://arxiv.org/abs/2412.14489</link>
<guid>https://arxiv.org/abs/2412.14489</guid>
<content:encoded><![CDATA[
arXiv:2412.14489v3 Announce Type: replace 
Abstract: Multimodal machine learning has achieved remarkable progress in many scenarios, but its reliability is undermined by varying sample quality. This paper finds that existing reliable multimodal classification methods not only fail to provide robust estimation of data quality, but also lack dynamic networks for sample-specific depth and parameters to achieve reliable inference. To this end, a novel framework for multimodal reliable classification termed \textit{Multi-level Quality-Adaptive Dynamic multimodal network} (Multi-QuAD) is proposed. Multi-QuAD first adopts a novel approach based on noise-free prototypes and a classifier-free design to reliably estimate the quality of each sample at both modality and feature levels. It then achieves sample-specific network depth via the \textbf{\textit{Global Confidence Normalized Depth (GCND)}} mechanism. By normalizing depth across modalities and samples, \textit{\textbf{GCND}} effectively mitigates the impact of challenging modality inputs on dynamic depth reliability. Furthermore, Multi-QuAD provides sample-adaptive network parameters via the \textbf{\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by feature-level quality. The cross-modality layer-wise greedy strategy in \textbf{\textit{LGP}} designs a reliable parameter prediction paradigm for multimodal networks with variable architecture for the first time. Experiments conducted on four datasets demonstrate that Multi-QuAD significantly outperforms state-of-the-art methods in classification performance and reliability, exhibiting strong adaptability to data with diverse quality.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</title>
<link>https://arxiv.org/abs/2501.00843</link>
<guid>https://arxiv.org/abs/2501.00843</guid>
<content:encoded><![CDATA[
arXiv:2501.00843v3 Announce Type: replace 
Abstract: In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Tropical Cyclone Forecasting With Video Diffusion Models</title>
<link>https://arxiv.org/abs/2501.16003</link>
<guid>https://arxiv.org/abs/2501.16003</guid>
<content:encoded><![CDATA[
arXiv:2501.16003v5 Announce Type: replace 
Abstract: Tropical cyclone (TC) forecasting is crucial for disaster preparedness and mitigation. While recent deep learning approaches have shown promise, existing methods often treat TC evolution as a series of independent frame-to-frame predictions, limiting their ability to capture long-term dynamics. We present a novel application of video diffusion models for TC forecasting that explicitly models temporal dependencies through additional temporal layers. Our approach enables the model to generate multiple frames simultaneously, better capturing cyclone evolution patterns. We introduce a two-stage training strategy that significantly improves individual-frame quality and performance in low-data regimes. Experimental results show our method outperforms the previous approach of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably, we extend the reliable forecasting horizon from 36 to 50 hours. Through comprehensive evaluation using both traditional metrics and Fr\'echet Video Distance (FVD), we demonstrate that our approach produces more temporally coherent forecasts while maintaining competitive single-frame quality. Code accessible at https://github.com/Ren-creater/forecast-video-diffmodels.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning</title>
<link>https://arxiv.org/abs/2502.00848</link>
<guid>https://arxiv.org/abs/2502.00848</guid>
<content:encoded><![CDATA[
arXiv:2502.00848v2 Announce Type: replace 
Abstract: Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title>
<link>https://arxiv.org/abs/2502.02283</link>
<guid>https://arxiv.org/abs/2502.02283</guid>
<content:encoded><![CDATA[
arXiv:2502.02283v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling</title>
<link>https://arxiv.org/abs/2502.02590</link>
<guid>https://arxiv.org/abs/2502.02590</guid>
<content:encoded><![CDATA[
arXiv:2502.02590v2 Announce Type: replace 
Abstract: 3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is https://articulate-anymesh.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</title>
<link>https://arxiv.org/abs/2502.04847</link>
<guid>https://arxiv.org/abs/2502.04847</guid>
<content:encoded><![CDATA[
arXiv:2502.04847v4 Announce Type: replace 
Abstract: Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.14908</link>
<guid>https://arxiv.org/abs/2502.14908</guid>
<content:encoded><![CDATA[
arXiv:2502.14908v2 Announce Type: replace 
Abstract: Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lacks systematic investigation of cross-modal knowledge conflicts. This research introduces \segsub, a framework for applying targeted image perturbations to investigate VLM resilience against knowledge conflicts. Our analysis reveals distinct vulnerability patterns: while VLMs are robust to parametric conflicts (20% adherence rates), they exhibit significant weaknesses in identifying counterfactual conditions (<30% accuracy) and resolving source conflicts (<1% accuracy). Correlations between contextual richness and hallucination rate (r = -0.368, p = 0.003) reveal the kinds of images that are likely to cause hallucinations. Through targeted fine-tuning on our benchmark dataset, we demonstrate improvements in VLM knowledge conflict detection, establishing a foundation for developing hallucination-resilient multimodal systems in information-sensitive environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical Inspired MRI Lesion Segmentation</title>
<link>https://arxiv.org/abs/2502.16032</link>
<guid>https://arxiv.org/abs/2502.16032</guid>
<content:encoded><![CDATA[
arXiv:2502.16032v2 Announce Type: replace 
Abstract: Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting pathological tissues in various diseases. Different MRI sequences have different contrast mechanisms and sensitivities for different types of lesions, which pose challenges to accurate and consistent lesion segmentation. In clinical practice, radiologists commonly use the sub-sequence feature, i.e. the difference between post contrast-enhanced T1-weighted (post) and pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we propose a residual fusion method to learn subsequence representation for MRI lesion segmentation. Specifically, we iteratively and adaptively fuse features from pre- and post-contrast sequences at multiple resolutions, using dynamic weights to achieve optimal fusion and address diverse lesion enhancement patterns. Our method achieves state-of-the-art performances on BraTS2023 dataset for brain tumor segmentation and our in-house breast MRI dataset for breast lesion segmentation. Our method is clinically inspired and has the potential to facilitate lesion segmentation in various applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs</title>
<link>https://arxiv.org/abs/2502.19159</link>
<guid>https://arxiv.org/abs/2502.19159</guid>
<content:encoded><![CDATA[
arXiv:2502.19159v2 Announce Type: replace 
Abstract: Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. However, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the ``Patch-like'' feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we propose a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</title>
<link>https://arxiv.org/abs/2503.01103</link>
<guid>https://arxiv.org/abs/2503.01103</guid>
<content:encoded><![CDATA[
arXiv:2503.01103v2 Announce Type: replace 
Abstract: While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection</title>
<link>https://arxiv.org/abs/2503.01234</link>
<guid>https://arxiv.org/abs/2503.01234</guid>
<content:encoded><![CDATA[
arXiv:2503.01234v3 Announce Type: replace 
Abstract: Metal defect detection is critical in industrial quality assurance, yet existing methods struggle with grayscale variations and complex defect states, limiting its robustness. To address these challenges, this paper proposes a Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced detection framework integrating a Dynamic Gamma Correction (GC) module to enhance grayscale representation and optimize feature extraction for precise defect reconstruction. A State-Space Search Management (SSM) architecture captures robust multi-scale features, effectively handling defects of varying shapes and scales. Focal Loss is employed to mitigate class imbalance and refine detection accuracy. Additionally, the CD5-DET dataset is introduced, specifically designed for port container maintenance, featuring significant grayscale variations and intricate defect patterns. Experimental results demonstrate that the proposed model achieves substantial improvements, with mAP@0.5 gains of 27.6\%, 6.6\%, and 2.6\% on the CD5-DET, NEU-DET, and GC10-DET datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals</title>
<link>https://arxiv.org/abs/2503.06473</link>
<guid>https://arxiv.org/abs/2503.06473</guid>
<content:encoded><![CDATA[
arXiv:2503.06473v4 Announce Type: replace 
Abstract: Growing evidence suggests that layer attention mechanisms, which enhance interaction among layers in deep neural networks, have significantly advanced network architectures. However, existing layer attention methods suffer from redundancy, as attention weights learned by adjacent layers often become highly similar. This redundancy causes multiple layers to extract nearly identical features, reducing the model's representational capacity and increasing training time. To address this issue, we propose a novel approach to quantify redundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent layers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM) method that accurately identifies and skips redundant layers, thereby maintaining model stability. Our proposed Efficient Layer Attention (ELA) architecture, improves both training efficiency and overall performance, achieving a 30% reduction in training time while enhancing performance in tasks such as image classification and object detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction</title>
<link>https://arxiv.org/abs/2503.06587</link>
<guid>https://arxiv.org/abs/2503.06587</guid>
<content:encoded><![CDATA[
arXiv:2503.06587v2 Announce Type: replace 
Abstract: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We found the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectified the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Referring to Any Person</title>
<link>https://arxiv.org/abs/2503.08507</link>
<guid>https://arxiv.org/abs/2503.08507</guid>
<content:encoded><![CDATA[
arXiv:2503.08507v2 Announce Type: replace 
Abstract: Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-camera orientation tracking method for anisotropic particles in particle-laden flows</title>
<link>https://arxiv.org/abs/2503.08694</link>
<guid>https://arxiv.org/abs/2503.08694</guid>
<content:encoded><![CDATA[
arXiv:2503.08694v2 Announce Type: replace 
Abstract: A method for particle orientation tracking is developed and demonstrated specifically for anisotropic particles. Using (high-speed) multi-camera recordings of anisotropic particles from different viewpoints, we reconstruct the 3D location and orientation of these particles using their known shape. This paper describes an algorithm which tracks the location and orientation of multiple anisotropic particles over time, enabling detailed investigations of location, orientation, and rotation statistics. The robustness and error of this method is quantified, and we explore the effects of noise, image size, the number of used cameras, and the camera arrangement by applying the algorithm to synthetic images. We showcase several use-cases of this method in several experiments (in both quiescent and turbulent fluids), demonstrating the effectiveness and broad applicability of the described tracking method. The proposed method is shown to work for widely different particle shapes, successfully tracks multiple particles simultaneously, and the method can distinguish between different types of particles.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network</title>
<link>https://arxiv.org/abs/2503.13179</link>
<guid>https://arxiv.org/abs/2503.13179</guid>
<content:encoded><![CDATA[
arXiv:2503.13179v2 Announce Type: replace 
Abstract: This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Improving Motion Diffusion Models with Sparse Keyframes</title>
<link>https://arxiv.org/abs/2503.13859</link>
<guid>https://arxiv.org/abs/2503.13859</guid>
<content:encoded><![CDATA[
arXiv:2503.13859v2 Announce Type: replace 
Abstract: Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis. However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames. The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks. Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes. Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps. Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps. We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection</title>
<link>https://arxiv.org/abs/2503.13903</link>
<guid>https://arxiv.org/abs/2503.13903</guid>
<content:encoded><![CDATA[
arXiv:2503.13903v2 Announce Type: replace 
Abstract: Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object detection methods solely rely on CNNs or ViTs to conduct feature aggregation, which hampers their capability to simultaneously leverage global and local information, thereby resulting in limited detection performance. In this paper, we propose a Transformer-GraphFormer Blender Network (TGBFormer) for video object detection, with three key technical improvements to fully exploit the advantages of transformers and graph convolutional networks while compensating for their limitations. First, we develop a spatial-temporal transformer module to aggregate global contextual information, constituting global representations with long-range feature dependencies. Second, we introduce a spatial-temporal GraphFormer module that utilizes local spatial and temporal relationships to aggregate features, generating new local representations that are complementary to the transformer outputs. Third, we design a global-local feature blender module to adaptively couple transformer-based global representations and GraphFormer-based local representations. Extensive experiments demonstrate that our TGBFormer establishes new state-of-the-art results on the ImageNet VID dataset. Particularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS on a single Tesla A100 GPU.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision Centric Remote Sensing Benchmark</title>
<link>https://arxiv.org/abs/2503.15816</link>
<guid>https://arxiv.org/abs/2503.15816</guid>
<content:encoded><![CDATA[
arXiv:2503.15816v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks but their remote sensing (RS) counterpart are relatively under explored. Unlike natural images, RS imagery presents unique challenges that current MLLMs struggle to handle, particularly in visual grounding and spatial reasoning. This study investigates the limitations of CLIP-based MLLMs in RS, highlighting their failure to differentiate visually distinct yet semantically similar RS images. To address this, we introduce a remote sensing multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models incorrectly assign high similarity scores to visually distinct RS images. Through a visual question answering (VQA) evaluation, we analyze the performance of state-of-the-art MLLMs, revealing significant limitations in RS specific representation learning. The results provide valuable insights into the weaknesses of CLIP-based visual encoding and offer a foundation for future research to develop more effective MLLMs tailored for remote sensing applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2503.15831</link>
<guid>https://arxiv.org/abs/2503.15831</guid>
<content:encoded><![CDATA[
arXiv:2503.15831v2 Announce Type: replace 
Abstract: Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2503.15970</link>
<guid>https://arxiv.org/abs/2503.15970</guid>
<content:encoded><![CDATA[
arXiv:2503.15970v2 Announce Type: replace 
Abstract: Facial Expression Recognition (FER) plays a crucial role in human affective analysis and has been widely applied in computer vision tasks such as human-computer interaction and psychological assessment. The 8th Affective Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions using the video-based Aff-Wild2 dataset. This challenge includes various tasks, including the video-based EXPR recognition track, which is our primary focus. In this paper, we demonstrate that addressing label ambiguity and class imbalance, which are known to cause performance degradation, can lead to meaningful performance improvements. Specifically, we propose Video-based Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to each frame in a clip to address label ambiguity and effectively capture temporal variations in facial expressions. Furthermore, we introduce a simple and effective augmentation strategy to reduce redundancy between consecutive frames, which is a primary cause of overfitting. Through extensive experiments, we validate the effectiveness of our approach, demonstrating significant improvements in video-based FER performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2503.16188</link>
<guid>https://arxiv.org/abs/2503.16188</guid>
<content:encoded><![CDATA[
arXiv:2503.16188v4 Announce Type: replace 
Abstract: This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM image classification, using verifiable rewards for fine-tuning. Experiments show CLS-RL significantly outperforms SFT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RL on 6 diverse tasks across different model sizes and types. Experimental results reveal three key findings: 1). Visual perception tasks do not require thinking during RFT, as No-Thinking-RL consistently outperforms or matches Thinking-based RFT across model sizes. 2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-based RFT less effective than No-Thinking-RL. 3). There are inconsistencies between the answers in the thinking and answer tags for some responses of thinking-based RFT, which show lower accuracy than the overall accuracy. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance. To test this hypothesis, we propose Think-After-Answer, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an Adaptive-Thinking method. Experiments show that it converges to a specific prompt depending on model capability and task complexity, achieving comparable or better performance than both Thinking and No-Thinking-RL. This suggests MLLMs can adaptively decide to think or not based on their capabilities and task complexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2504.04893</link>
<guid>https://arxiv.org/abs/2504.04893</guid>
<content:encoded><![CDATA[
arXiv:2504.04893v3 Announce Type: replace 
Abstract: Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FANeRV: Frequency Separation and Augmentation based Neural Representation for Video</title>
<link>https://arxiv.org/abs/2504.06755</link>
<guid>https://arxiv.org/abs/2504.06755</guid>
<content:encoded><![CDATA[
arXiv:2504.06755v3 Announce Type: replace 
Abstract: Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block. This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene</title>
<link>https://arxiv.org/abs/2504.09455</link>
<guid>https://arxiv.org/abs/2504.09455</guid>
<content:encoded><![CDATA[
arXiv:2504.09455v2 Announce Type: replace 
Abstract: A common dilemma while photographing a scene is whether to capture it at a wider angle, allowing more of the scene to be covered but in less detail or to click in a narrow angle that captures better details but leaves out portions of the scene. We propose a novel method in this paper that infuses wider shots with finer quality details that is usually associated with an image captured by the primary lens by capturing the same scene using both narrow and wide field of view (FoV) lenses. We do so by training a Generative Adversarial Network (GAN)-based model to learn to extract the visual quality parameters from a narrow-angle shot and to transfer these to the corresponding wide-angle image of the scene using residual connections and an attention-based fusion module. We have mentioned in details the proposed technique to isolate the visual essence of an image and to transfer it into another image. We have also elaborately discussed our implementation details and have presented the results of evaluation over several benchmark datasets and comparisons with contemporary advancements in the field.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocusedAD: Character-centric Movie Audio Description</title>
<link>https://arxiv.org/abs/2504.12157</link>
<guid>https://arxiv.org/abs/2504.12157</guid>
<content:encoded><![CDATA[
arXiv:2504.12157v3 Announce Type: replace 
Abstract: Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForgetMe: Evaluating Selective Forgetting in Generative Models</title>
<link>https://arxiv.org/abs/2504.12574</link>
<guid>https://arxiv.org/abs/2504.12574</guid>
<content:encoded><![CDATA[
arXiv:2504.12574v2 Announce Type: replace 
Abstract: The widespread adoption of diffusion models in image generation has increased the demand for privacy-compliant unlearning. However, due to the high-dimensional nature and complex feature representations of diffusion models, achieving selective unlearning remains challenging, as existing methods struggle to remove sensitive information while preserving the consistency of non-sensitive regions. To address this, we propose an Automatic Dataset Creation Framework based on prompt-based layered editing and training-free local feature removal, constructing the ForgetMe dataset and introducing the Entangled evaluation metric. The Entangled metric quantifies unlearning effectiveness by assessing the similarity and consistency between the target and background regions and supports both paired (Entangled-D) and unpaired (Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe dataset encompasses a diverse set of real and synthetic scenarios, including CUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We apply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on this dataset and validate the effectiveness of both the ForgetMe dataset and the Entangled metric, establishing them as benchmarks for selective unlearning. Our work provides a scalable and adaptable solution for advancing privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compile Scene Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13617</link>
<guid>https://arxiv.org/abs/2504.13617</guid>
<content:encoded><![CDATA[
arXiv:2504.13617v3 Announce Type: replace 
Abstract: Next-token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. We design a set of graph-centric rewards, including three recall-based variants -- Hard Recall, Hard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial alignment between predictions and ground truth at the object and relation levels. A format consistency reward further ensures that outputs follow the expected structural schema. Extensive experiments on the VG150 and PSG benchmarks show that R1-SGG substantially reduces failure rates and achieves strong performance in Recall and mean Recall, surpassing traditional SGG models and existing multimodal language models. Our code is available at https://github.com/gpt4vision/R1-SGG
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Sound Source Localization with Joint Slot Attention on Image and Audio</title>
<link>https://arxiv.org/abs/2504.15118</link>
<guid>https://arxiv.org/abs/2504.15118</guid>
<content:encoded><![CDATA[
arXiv:2504.15118v2 Announce Type: replace 
Abstract: Sound source localization (SSL) is the task of locating the source of sound within an image. Due to the lack of localization labels, the de facto standard in SSL has been to represent an image and audio as a single embedding vector each, and use them to learn SSL via contrastive learning. To this end, previous work samples one of local image features as the image embedding and aggregates all local audio features to obtain the audio embedding, which is far from optimal due to the presence of noise and background irrelevant to the actual target in the input. We present a novel SSL method that addresses this chronic issue by joint slot attention on image and audio. To be specific, two slots competitively attend image and audio features to decompose them into target and off-target representations, and only target representations of image and audio are used for contrastive learning. Also, we introduce cross-modal attention matching to further align local features of image and audio. Our method achieved the best in almost all settings on three public benchmarks for SSL, and substantially outperformed all the prior work in cross-modal retrieval.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs</title>
<link>https://arxiv.org/abs/2504.17040</link>
<guid>https://arxiv.org/abs/2504.17040</guid>
<content:encoded><![CDATA[
arXiv:2504.17040v2 Announce Type: replace 
Abstract: We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableCenterNet: A one-stage network for table structure recognition</title>
<link>https://arxiv.org/abs/2504.17522</link>
<guid>https://arxiv.org/abs/2504.17522</guid>
<content:encoded><![CDATA[
arXiv:2504.17522v2 Announce Type: replace 
Abstract: Table structure recognition aims to parse tables in unstructured data into machine-understandable formats. Recent methods address this problem through a two-stage process or optimized one-stage approaches. However, these methods either require multiple networks to be serially trained and perform more time-consuming sequential decoding, or rely on complex post-processing algorithms to parse the logical structure of tables. They struggle to balance cross-scenario adaptability, robustness, and computational efficiency. In this paper, we propose a one-stage end-to-end table structure parsing network called TableCenterNet. This network unifies the prediction of table spatial and logical structure into a parallel regression task for the first time, and implicitly learns the spatial-logical location mapping laws of cells through a synergistic architecture of shared feature extraction layers and task-specific decoding. Compared with two-stage methods, our method is easier to train and faster to infer. Experiments on benchmark datasets show that TableCenterNet can effectively parse table structures in diverse scenarios and achieve state-of-the-art performance on the TableGraph-24k dataset. Code is available at https://github.com/dreamy-xay/TableCenterNet.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</title>
<link>https://arxiv.org/abs/2504.18864</link>
<guid>https://arxiv.org/abs/2504.18864</guid>
<content:encoded><![CDATA[
arXiv:2504.18864v2 Announce Type: replace 
Abstract: The need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems</title>
<link>https://arxiv.org/abs/2311.11796</link>
<guid>https://arxiv.org/abs/2311.11796</guid>
<content:encoded><![CDATA[
arXiv:2311.11796v2 Announce Type: replace-cross 
Abstract: As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini: A Family of Highly Capable Multimodal Models</title>
<link>https://arxiv.org/abs/2312.11805</link>
<guid>https://arxiv.org/abs/2312.11805</guid>
<content:encoded><![CDATA[
arXiv:2312.11805v5 Announce Type: replace-cross 
Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning</title>
<link>https://arxiv.org/abs/2402.06223</link>
<guid>https://arxiv.org/abs/2402.06223</guid>
<content:encoded><![CDATA[
arXiv:2402.06223v2 Announce Type: replace-cross 
Abstract: Directed acyclic graphs (DAGs) are fundamental graph structures in causal modeling, but identifying the desired DAG from observational data often requires strong assumptions that may not hold in real-world scenarios, especially for latent causal models and complex multimodal data. This raises the question of whether we can relax or bypass the DAG assumption while maintaining practical utility. In this work, we propose a novel latent partial causal model for multimodal data, featuring two latent coupled variables, connected by an undirected edge, to represent the transfer of knowledge across modalities. Under specific statistical assumptions, we establish an identifiability result, demonstrating that representations learned by multimodal contrastive learning correspond to the latent coupled variables up to a trivial transformation. This result deepens our understanding of the why multimodal contrastive learning works, highlights its potential for disentanglement, and expands the utility of pre-trained models like CLIP. Synthetic experiments confirm the robustness of our findings, even when the assumptions are partially violated. Most importantly, experiments on a pre-trained CLIP model embodies disentangled representations, enabling few-shot learning and improving domain generalization across diverse real-world datasets. Together, these contributions push the boundaries of multimodal contrastive learning, both theoretically and, crucially, in practical applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples</title>
<link>https://arxiv.org/abs/2404.19460</link>
<guid>https://arxiv.org/abs/2404.19460</guid>
<content:encoded><![CDATA[
arXiv:2404.19460v3 Announce Type: replace-cross 
Abstract: Adversarial examples are typically optimized with gradient-based attacks. While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models. This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others. In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks. To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences. We then introduce our framework, which evaluates their effectiveness and efficiency. We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget. Our extensive experimental analysis compares more than $100$ attack implementations with a total of over $800$ different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches. Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all. We release AttackBench as a publicly-available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2408.14419</link>
<guid>https://arxiv.org/abs/2408.14419</guid>
<content:encoded><![CDATA[
arXiv:2408.14419v2 Announce Type: replace-cross 
Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance. We benchmark leading LLMs as of late 2024 - including GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset and found that our benchmark was challenging to all of them, suggesting room for future large language models to improve.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering</title>
<link>https://arxiv.org/abs/2409.02426</link>
<guid>https://arxiv.org/abs/2409.02426</guid>
<content:encoded><![CDATA[
arXiv:2409.02426v3 Announce Type: replace-cross 
Abstract: Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.13439</link>
<guid>https://arxiv.org/abs/2410.13439</guid>
<content:encoded><![CDATA[
arXiv:2410.13439v4 Announce Type: replace-cross 
Abstract: Supervised contrastive learning has achieved remarkable success by leveraging label information; however, determining positive samples in multi-label scenarios remains a critical challenge. In multi-label supervised contrastive learning (MSCL), multi-label relations are not yet fully defined, leading to ambiguity in identifying positive samples and formulating contrastive loss functions to construct the representation space. To address these challenges, we: (i) first define five distinct multi-label relations in MSCL to systematically identify positive samples, (ii) introduce a novel Similarity-Dissimilarity Loss that dynamically re-weights samples through computing the similarity and dissimilarity factors between positive samples and given anchors based on multi-label relations, and (iii) further provide theoretical grounded proofs for our method through rigorous mathematical analysis that supports the formulation and effectiveness of the proposed loss function. We conduct the experiments across both image and text modalities, and extend the evaluation to medical domain. The results demonstrate that our method consistently outperforms baselines in a comprehensive evaluation, confirming its effectiveness and robustness. Code is available at: https://github.com/guangminghuang/similarity-dissimilarity-loss.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2410.21000</link>
<guid>https://arxiv.org/abs/2410.21000</guid>
<content:encoded><![CDATA[
arXiv:2410.21000v3 Announce Type: replace-cross 
Abstract: Medical Visual Question Answering (MedVQA) has attracted growing interest at the intersection of medical image understanding and natural language processing for clinical applications. By interpreting medical images and providing precise answers to relevant clinical inquiries, MedVQA has the potential to support diagnostic decision-making and reduce workload across various fields like radiology. While recent approaches rely heavily on unified large pre-trained Visual-Language Models, research on more efficient fusion mechanisms remains relatively limited in this domain. In this paper, we introduce a fusion model, OMniBAN, that integrates Orthogonality loss, Multi-head attention, and a Bilinear Attention Network to achieve high computational efficiency as well as solid performance. We conduct comprehensive experiments and demonstrate how bilinear attention fusion can approximate the performance of larger fusion models like cross-modal Transformer. Our results show that OMniBAN requires fewer parameters (approximately 2/3 of Transformer-based Co-Attention) and substantially lower FLOPs (approximately 1/4), while achieving comparable overall performance and even slight improvements on closed-ended questions on two key MedVQA benchmarks. This balance between efficiency and accuracy suggests that OMniBAN could be a viable option for real-world medical image question answering, where computational resources are often constrained.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relationships between the degrees of freedom in the affine Gaussian derivative model for visual receptive fields and 2-D affine image transformations, with application to covariance properties of simple cells in the primary visual cortex</title>
<link>https://arxiv.org/abs/2411.05673</link>
<guid>https://arxiv.org/abs/2411.05673</guid>
<content:encoded><![CDATA[
arXiv:2411.05673v3 Announce Type: replace-cross 
Abstract: When observing the surface patterns of objects delimited by smooth surfaces, the projections of the surface patterns to the image domain will be subject to substantial variabilities, as induced by variabilities in the geometric viewing conditions, and as generated by either monocular or binocular imaging conditions, or by relative motions between the object and the observer over time. To first order of approximation, the image deformations of such projected surface patterns can be modelled as local linearizations in terms of local 2-D spatial affine transformations.
  This paper presents a theoretical analysis of relationships between the degrees of freedom in 2-D spatial affine image transformations and the degrees of freedom in the affine Gaussian derivative model for visual receptive fields. For this purpose, we first describe a canonical decomposition of 2-D affine transformations on a product form, closely related to a singular value decomposition, while in closed form, and which reveals the degrees of freedom in terms of (i) uniform scaling transformations, (ii) an overall amount of global rotation, (iii) a complementary non-uniform scaling transformation and (iv) a relative normalization to a preferred symmetry orientation in the image domain. Then, we show how these degrees of freedom relate to the degrees of freedom in the affine Gaussian derivative model.
  Finally, we use these theoretical results to consider whether we could regard the biological receptive fields in the primary visual cortex of higher mammals as being able to span the degrees of freedom of 2-D spatial affine transformations, based on interpretations of existing neurophysiological experimental results.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering</title>
<link>https://arxiv.org/abs/2412.00259</link>
<guid>https://arxiv.org/abs/2412.00259</guid>
<content:encoded><![CDATA[
arXiv:2412.00259v4 Announce Type: replace-cross 
Abstract: Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable programming to identify world models are incapable of jointly optimizing the geometry, appearance, and physical properties of the scene. In this work, we introduce a novel rigid object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based geometry representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of world model identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence. The code and additional videos are available at our project website: https://tianyi20.github.io/rigid-world-model.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application</title>
<link>https://arxiv.org/abs/2412.03887</link>
<guid>https://arxiv.org/abs/2412.03887</guid>
<content:encoded><![CDATA[
arXiv:2412.03887v4 Announce Type: replace-cross 
Abstract: Maritime environmental sensing requires overcoming challenges from complex conditions such as harsh weather, platform perturbations, large dynamic objects, and the requirement for long detection ranges. While cameras and LiDAR are commonly used in ground vehicle navigation, their applicability in maritime settings is limited by range constraints and hardware maintenance issues. Radar sensors, however, offer robust long-range detection capabilities and resilience to physical contamination from weather and saline conditions, making it a powerful sensor for maritime navigation. Among various radar types, X-band radar is widely employed for maritime vessel navigation, providing effective long-range detection essential for situational awareness and collision avoidance. Nevertheless, it exhibits limitations during berthing operations where near-field detection is critical. To address this shortcoming, we incorporate W-band radar, which excels in detecting nearby objects with a higher update rate. We present a comprehensive maritime sensor dataset featuring multi-range detection capabilities. This dataset integrates short-range LiDAR data, medium-range W-band radar data, and long-range X-band radar data into a unified framework. Additionally, it includes object labels for oceanic object detection usage, derived from radar and stereo camera images. The dataset comprises seven sequences collected from diverse regions with varying levels of \bl{navigation algorithm} estimation difficulty, ranging from easy to challenging, and includes common locations suitable for global localization tasks. This dataset serves as a valuable resource for advancing research in place recognition, odometry estimation, SLAM, object detection, and dynamic object elimination within maritime environments. Dataset can be found at https://sites.google.com/view/rpmmoana.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereo Hand-Object Reconstruction for Human-to-Robot Handover</title>
<link>https://arxiv.org/abs/2412.07487</link>
<guid>https://arxiv.org/abs/2412.07487</guid>
<content:encoded><![CDATA[
arXiv:2412.07487v3 Announce Type: replace-cross 
Abstract: Jointly estimating hand and object shape facilitates the grasping task in human-to-robot handovers. However, relying on hand-crafted prior knowledge about the geometric structure of the object fails when generalising to unseen objects, and depth sensors fail to detect transparent objects such as drinking glasses. In this work, we propose a stereo-based method for hand-object reconstruction that combines single-view reconstructions probabilistically to form a coherent stereo reconstruction. We learn 3D shape priors from a large synthetic hand-object dataset to ensure that our method is generalisable, and use RGB inputs to better capture transparent objects. We show that our method reduces the object Chamfer distance compared to existing RGB based hand-object reconstruction methods on single view and stereo settings. We process the reconstructed hand-object shape with a projection-based outlier removal step and use the output to guide a human-to-robot handover pipeline with wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a robot to successfully receive a diverse range of household objects from the human.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
<link>https://arxiv.org/abs/2501.19047</link>
<guid>https://arxiv.org/abs/2501.19047</guid>
<content:encoded><![CDATA[
arXiv:2501.19047v4 Announce Type: replace-cross 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation</title>
<link>https://arxiv.org/abs/2502.05485</link>
<guid>https://arxiv.org/abs/2502.05485</guid>
<content:encoded><![CDATA[
arXiv:2502.05485v4 Announce Type: replace-cross 
Abstract: Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[
arXiv:2503.18938v2 Announce Type: replace-cross 
Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging State Space Models in Long Range Genomics</title>
<link>https://arxiv.org/abs/2504.06304</link>
<guid>https://arxiv.org/abs/2504.06304</guid>
<content:encoded><![CDATA[
arXiv:2504.06304v2 Announce Type: replace-cross 
Abstract: Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoLa: B-Rep Generation using a Holistic Latent Representation</title>
<link>https://arxiv.org/abs/2504.14257</link>
<guid>https://arxiv.org/abs/2504.14257</guid>
<content:encoded><![CDATA[
arXiv:2504.14257v3 Announce Type: replace-cross 
Abstract: We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of $\textit{boundary representations}$ (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a $\textit{holistic latent}$ (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. $\approx$50%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniAudio: Generating Spatial Audio from 360-Degree Video</title>
<link>https://arxiv.org/abs/2504.14906</link>
<guid>https://arxiv.org/abs/2504.14906</guid>
<content:encoded><![CDATA[
arXiv:2504.14906v2 Announce Type: replace-cross 
Abstract: Traditional video-to-audio generation techniques primarily focus on field-of-view (FoV) video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and FoV video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo page is available at https://OmniAudio-360V2SA.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography</title>
<link>https://arxiv.org/abs/2504.19200</link>
<guid>https://arxiv.org/abs/2504.19200</guid>
<content:encoded><![CDATA[
arXiv:2504.19200v2 Announce Type: replace-cross 
Abstract: In situ synchrotron X-ray computed tomography enables dynamic material studies, but automated segmentation remains challenging due to complex imaging artefacts and limited training data. We present a methodology for deep learning-based segmentation by transforming high-quality ex situ laboratory data to train models for binary segmentation of in situ synchrotron data, demonstrated through copper oxide dissolution studies. Using a modified SegFormer architecture, our approach achieves high segmentation performance on unseen data while reducing processing time from hours to seconds per 3D dataset. The method maintains consistent performance over significant morphological changes during experiments, despite training only on static specimens. This methodology can be readily applied to diverse materials systems, accelerating the analysis of time-resolved tomographic data across scientific disciplines.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving</title>
<link>https://arxiv.org/abs/2505.05487</link>
<guid>https://arxiv.org/abs/2505.05487</guid>
<content:encoded><![CDATA[
<div> intersection detection, head pose estimation, object detection, naturalistic driving studies, automated video processing

Summary:
Intersection detection and driver head pose estimation were successfully carried out using a custom-developed algorithm on data collected from in-car recording systems. The algorithm accurately detected intersection signage and driving maneuvers in the majority of instances. The detection of vehicle entry into intersections had a small error margin and the overlap between ground truth and estimated intersection bounds was high. Object detection using YOLO models successfully identified traffic lights, stop signs, pedestrians, and other vehicles on the road. Turning maneuvers were independently detected using vehicle self-motion patterns, and stop lines on the road surface were identified through changing intensity patterns over time. The algorithm correctly inferred intersection type, maneuver, and bounds using scene videos and speed data. Overall, the automated video processing algorithm demonstrated high accuracy and reliability in characterizing driver behavior at intersections in naturalistic driving studies.<br /><br />Summary: <div>
arXiv:2505.05487v1 Announce Type: new 
Abstract: Naturalistic driving studies use devices in participants' own vehicles to record daily driving over many months. Due to diverse and extensive amounts of data recorded, automated processing is necessary. This report describes methods to extract and characterize driver head scans at intersections from data collected from an in-car recording system that logged vehicle speed, GPS location, scene videos, and cabin videos. Custom tools were developed to mark the intersections, synchronize location and video data, and clip the cabin and scene videos for +/-100 meters from the intersection location. A custom-developed head pose detection AI model for wide angle head turns was run on the cabin videos to estimate the driver head pose, from which head scans >20 deg were computed in the horizontal direction. The scene videos were processed using a YOLO object detection model to detect traffic lights, stop signs, pedestrians, and other vehicles on the road. Turning maneuvers were independently detected using vehicle self-motion patterns. Stop lines on the road surface were detected using changing intensity patterns over time as the vehicle moved. The information obtained from processing the scene videos, along with the speed data was used in a rule-based algorithm to infer the intersection type, maneuver, and bounds. We processed 190 intersections from 3 vehicles driven in cities and suburban areas from Massachusetts and California. The automated video processing algorithm correctly detected intersection signage and maneuvers in 100% and 94% of instances, respectively. The median [IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9] meters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and estimated intersection bounds was 0.88[0.82-0.93].
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Events to Enhancement: A Survey on Event-Based Imaging Technologies</title>
<link>https://arxiv.org/abs/2505.05488</link>
<guid>https://arxiv.org/abs/2505.05488</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, imaging tasks, image/video enhancement, light field estimation, challenges

Summary: 
Event cameras, with their high dynamic range and low latency capabilities, have become a disruptive technology in the field of imaging. However, a comprehensive study of recent advances and challenges in leveraging these benefits for various imaging tasks is still lacking. This survey aims to address this gap by first introducing a physical model and characteristics of different event sensors. The survey then delves into the interaction of image/video enhancement tasks with event cameras, highlighting advancements in this area. Furthermore, it explores advanced tasks such as light field estimation, multi-view generation, and photometric, which enable the capture of richer light information using event cameras. The survey concludes by discussing new challenges and open questions, providing a perspective on the rapidly evolving field of event imaging.  <br /><br />Summary: <div>
arXiv:2505.05488v1 Announce Type: new 
Abstract: Event cameras offering high dynamic range and low latency have emerged as disruptive technologies in imaging. Despite growing research on leveraging these benefits for different imaging tasks, a comprehensive study of recently advances and challenges are still lacking. This limits the broader understanding of how to utilize events in universal imaging applications. In this survey, we first introduce a physical model and the characteristics of different event sensors as the foundation. Following this, we highlight the advancement and interaction of image/video enhancement tasks with events. Additionally, we explore advanced tasks, which capture richer light information with events, \eg~light field estimation, multi-view generation, and photometric. Finally, we discuss new challenges and open questions offering a perspective for this rapidly evolving field. More continuously updated resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection</title>
<link>https://arxiv.org/abs/2505.05491</link>
<guid>https://arxiv.org/abs/2505.05491</guid>
<content:encoded><![CDATA[
<div> dynamic dual fusion network, object detection, traffic sign, feature extraction, MDDFNet 

Summary:
The article introduces a novel object detection network called Mamba-based Dynamic Dual Fusion Network (MDDFNet) designed for traffic sign detection. The network addresses two main challenges in detecting small objects such as traffic signs: singular feature extraction and difficulties in handling objects of varying sizes. The MDDFNet integrates a dynamic dual fusion module that utilizes multiple branches to enhance feature diversity and a Mamba-based backbone that combines global feature fusion and local feature interactions. Extensive experiments on the TT100K dataset show that MDDFNet outperforms state-of-the-art detectors in terms of performance, while maintaining real-time processing capabilities typical of single-stage models. These results confirm the effectiveness of MDDFNet in detecting small traffic signs. 

<br /><br />Summary: <div>
arXiv:2505.05491v1 Announce Type: new 
Abstract: The Detection of small objects, especially traffic signs, is a critical sub-task in object detection and autonomous driving. Despite signficant progress in previous research, two main challenges remain. First, the issue of feature extraction being too singular. Second, the detection process struggles to efectively handle objects of varying sizes or scales. These problems are also prevalent in general object detection tasks. To address these challenges, we propose a novel object detection network, Mamba-based Dynamic Dual Fusion Network (MDDFNet), for traffic sign detection. The network integrates a dynamic dual fusion module and a Mamba-based backbone to simultaneously tackle the aforementioned issues. Specifically, the dynamic dual fusion module utilizes multiple branches to consolidate various spatial and semantic information, thus enhancing feature diversity. The Mamba-based backbone leverages global feature fusion and local feature interaction, combining features in an adaptive manner to generate unique classification characteristics. Extensive experiments conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that MDDFNet outperforms other state-of-the-art detectors, maintaining real-time processing capabilities of single-stage models while achieving superior performance. This confirms the efectiveness of MDDFNet in detecting small traffic signs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision</title>
<link>https://arxiv.org/abs/2505.05492</link>
<guid>https://arxiv.org/abs/2505.05492</guid>
<content:encoded><![CDATA[
<div> Python, fairness, deep learning, vision classifiers, DebioxAI

Summary:
DetoxAI is a new Python library designed to address fairness in deep learning vision classifiers. Existing solutions for fairness in machine learning often focus on tabular data, leaving vision-based classification tasks overlooked. DetoxAI bridges this gap by implementing state-of-the-art debiasing algorithms, fairness metrics, and visualization tools specifically tailored for deep learning vision classifiers. The library supports interventions in internal representations for debiasing and includes attribution-based visualization tools and quantitative algorithmic fairness metrics for demonstrating bias mitigation. With a focus on improving fairness in vision classifiers, DetoxAI offers engineers and researchers valuable tools for assessing and improving the equity of their deep learning models. <div>
arXiv:2505.05492v1 Announce Type: new 
Abstract: While machine learning fairness has made significant progress in recent years, most existing solutions focus on tabular data and are poorly suited for vision-based classification tasks, which rely heavily on deep learning. To bridge this gap, we introduce DetoxAI, an open-source Python library for improving fairness in deep learning vision classifiers through post-hoc debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness metrics, and visualization tools. It supports debiasing via interventions in internal representations and includes attribution-based visualization tools and quantitative algorithmic fairness metrics to show how bias is mitigated. This paper presents the motivation, design, and use cases of DetoxAI, demonstrating its tangible value to engineers and researchers.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D Persistent Embodied World Models</title>
<link>https://arxiv.org/abs/2505.05495</link>
<guid>https://arxiv.org/abs/2505.05495</guid>
<content:encoded><![CDATA[
<div> video models, world model, simulation, memory, planning

Summary:
- The article introduces a new persistent embodied world model that incorporates memory of previously generated content, allowing for more consistent long-term simulation of future actions.
- A video diffusion model is used to predict RGB-D video of future observations, which is then aggregated into a 3D map of the environment.
- By conditioning the video model on this 3D spatial map, the model can simulate both seen and unseen parts of the world, enabling more accurate prediction of future outcomes.
- The proposed world model demonstrates efficacy in downstream applications such as planning and policy learning, showcasing its potential for enhancing decision-making processes in intelligent agents.
- The integration of memory into the world model addresses the limitations of existing myopic models, providing a more comprehensive and reliable framework for long-horizon planning in complex environments. 

<br /><br />Summary: <div>
arXiv:2505.05495v1 Announce Type: new 
Abstract: The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preliminary Explorations with GPT-4o(mni) Native Image Generation</title>
<link>https://arxiv.org/abs/2505.05501</link>
<guid>https://arxiv.org/abs/2505.05501</guid>
<content:encoded><![CDATA[
<div> image generation, multimodal comprehension, task taxonomy, qualitative test, model capabilities

Summary:
GPT-4o(mni) by OpenAI showcases exceptional visual generation capabilities with strong multimodal understanding. The study evaluates the model across various task categories including traditional image generation, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. Results indicate GPT-4o excels in general-purpose synthesis tasks like text-to-image generation and visual stylization but struggles with precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. The model also faces challenges with knowledge-intensive or domain-specific tasks, displaying hallucinations and factual errors. While GPT-4o represents a significant advancement in unified multimodal generation, there are limitations that need to be addressed before its application in professional or safety-critical domains.<br /><br />Summary: <div>
arXiv:2505.05501v1 Announce Type: new 
Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by OpenAI. It demonstrates a very remarkable generation capability with excellent multimodal condition understanding and varied task instructions. In this paper, we aim to explore the capabilities of GPT-4o across various tasks. Inspired by previous study, we constructed a task taxonomy along with a carefully curated set of test samples to conduct a comprehensive qualitative test. Benefiting from GPT-4o's powerful multimodal comprehension, its image-generation process demonstrates abilities surpassing those of traditional image-generation tasks. Thus, regarding the dimensions of model capabilities, we evaluate its performance across six task categories: traditional image generation tasks, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. These tasks not only assess the quality and conditional alignment of the model's outputs but also probe deeper into GPT-4o's understanding of real-world concepts. Our results reveal that GPT-4o performs impressively well in general-purpose synthesis tasks, showing strong capabilities in text-to-image generation, visual stylization, and low-level image processing. However, significant limitations remain in its ability to perform precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. Furthermore, when faced with knowledge-intensive or domain-specific scenarios, such as scientific illustrations or mathematical plots, the model often exhibits hallucinations, factual errors, or structural inconsistencies. These findings suggest that while GPT-4o marks a substantial advancement in unified multimodal generation, there is still a long way to go before it can be reliably applied to professional or safety-critical domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2505.05505</link>
<guid>https://arxiv.org/abs/2505.05505</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-3D models, attribute binding, hierarchical chain of generation, occluded object parts, semantic labels<br />
Summary:<br />
Recent advances in text-to-3D models have improved the rendering of high-quality assets but struggle with objects containing complex attributes. Existing approaches face challenges due to limited comprehension of long descriptions by text encoders, leading to incorrect attribute binding in generated results. Addressing these issues, a new automated method called Hierarchical Chain of Generation (HCoG) decomposes descriptions into parts and orders them based on occlusions, ensuring a disciplined generation process. HCoG generates components within blocks and binds attributes using target-region localization and 3D Gaussian kernel optimization. It introduces novel techniques like Gaussian Extension and Label Elimination for seamless part generation, resulting in structurally coherent 3D objects with complex attributes. Experimental results validate the effectiveness of HCoG in producing attribute-faithful objects. The code for HCoG is available at https://github.com/Wakals/GASCOL. <br /><br />Summary: <div>
arXiv:2505.05505v1 Announce Type: new 
Abstract: Recent text-to-3D models can render high-quality assets, yet they still stumble on objects with complex attributes. The key obstacles are: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Occluded object parts demand a disciplined generation order and explicit part disentanglement. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to decompose the long description into blocks representing different object parts, and orders them from inside out according to occlusions, forming a hierarchical chain. Within each block we first coarsely create components, then precisely bind attributes via target-region localization and corresponding 3D Gaussian kernel optimization. Between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments confirm that HCoG yields structurally coherent, attribute-faithful 3D objects with complex attributes. The code is available at https://github.com/Wakals/GASCOL .
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occupancy World Model for Robots</title>
<link>https://arxiv.org/abs/2505.05512</link>
<guid>https://arxiv.org/abs/2505.05512</guid>
<content:encoded><![CDATA[
<div> Keywords: scene evolutions, embodied agents, occupancy world model, indoor robotics, 3D occupancy scene evolution prediction

Summary:
This work introduces a new framework, RoboOccWorld, for learning and forecasting the scene evolutions of fine-grained occupancy in indoor robotic scenarios. The proposed model, based on a combination of spatio-temporal receptive field and guided autoregressive transformer, utilizes Conditional Causal State Attention (CCSA) to incorporate camera poses and Hybrid Spatio-Temporal Aggregation (HSTA) for multi-scale cue exploitation. A restructuring of the OccWorld-ScanNet benchmark facilitates the evaluation of indoor 3D occupancy scene evolution prediction. Experimental results show that RoboOccWorld outperforms existing methods in this task. The release of code for this framework is expected soon. 

<br /><br />Summary: <div>
arXiv:2505.05512v1 Announce Type: new 
Abstract: Understanding and forecasting the scene evolutions deeply affect the exploration and decision of embodied agents. While traditional methods simulate scene evolutions through trajectory prediction of potential instances, current works use the occupancy world model as a generative framework for describing fine-grained overall scene dynamics. However, existing methods cluster on the outdoor structured road scenes, while ignoring the exploration of forecasting 3D occupancy scene evolutions for robots in indoor scenes. In this work, we explore a new framework for learning the scene evolutions of observed fine-grained occupancy and propose an occupancy world model based on the combined spatio-temporal receptive field and guided autoregressive transformer to forecast the scene evolutions, called RoboOccWorld. We propose the Conditional Causal State Attention (CCSA), which utilizes camera poses of next state as conditions to guide the autoregressive transformer to adapt and understand the indoor robotics scenarios. In order to effectively exploit the spatio-temporal cues from historical observations, Hybrid Spatio-Temporal Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive field based on multi-scale spatio-temporal windows. In addition, we restructure the OccWorld-ScanNet benchmark based on local annotations to facilitate the evaluation of the indoor 3D occupancy scene evolution prediction task. Experimental results demonstrate that our RoboOccWorld outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction task. The code will be released soon.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach</title>
<link>https://arxiv.org/abs/2505.05513</link>
<guid>https://arxiv.org/abs/2505.05513</guid>
<content:encoded><![CDATA[
<div> Asia, rice, classification, convolutional neural network, quality check  
Summary:  
- Rice cultivation and utilization are essential for international trade and nutrition, with Asian countries like China, India, and Thailand leading in production.  
- Different rice varieties, including basmati and jasmine, cater to diverse culinary preferences and cultural traditions.
- Manual rice grain quality check is laborious and error-prone, necessitating an automatic classification solution.
- A convolutional neural network (CNN) framework was developed for accurate rice grain variety classification, achieving high performance metrics and minimal misclassifications.
- Explainability techniques like LIME and SHAP offered valuable insights into the model's decision-making process and feature importance in classification outcomes. 

<br /><br />Summary: <div>
arXiv:2505.05513v1 Announce Type: new 
Abstract: Rice is an essential staple food worldwide that is important in promoting international trade, economic growth, and nutrition. Asian countries such as China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their significant contribution to the cultivation and utilization of rice. These nations are also known for cultivating different rice grains, including short and long grains. These sizes are further classified as basmati, jasmine, kainat saila, ipsala, arborio, etc., catering to diverse culinary preferences and cultural traditions. For both local and international trade, inspecting and maintaining the quality of rice grains to satisfy customers and preserve a country's reputation is necessary. Manual quality check and classification is quite a laborious and time-consuming process. It is also highly prone to mistakes. Therefore, an automatic solution must be proposed for the effective and efficient classification of different varieties of rice grains. This research paper presents an automatic framework based on a convolutional neural network (CNN) for classifying different varieties of rice grains. We evaluated the proposed model based on performance metrics such as accuracy, recall, precision, and F1-Score. The CNN model underwent rigorous training and validation, achieving a remarkable accuracy rate and a perfect area under each class's Receiver Operating Characteristic (ROC) curve. The confusion matrix analysis confirmed the model's effectiveness in distinguishing between the different rice varieties, indicating minimal misclassifications. Additionally, the integration of explainability techniques such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided valuable insights into the model's decision-making process, revealing how specific features of the rice grains influenced classification outcomes.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions</title>
<link>https://arxiv.org/abs/2505.05517</link>
<guid>https://arxiv.org/abs/2505.05517</guid>
<content:encoded><![CDATA[
<div> Keywords: functional grasp, human hand-object interaction (HOI), multi-finger robot hands, web images, simulator-augmented data

Summary: 
The study focuses on training a functional grasping model for multi-finger robot hands using human grasp information extracted from web images. This approach eliminates the need for costly teleoperated demonstrations and allows for the training of the model on a diverse range of objects. By reconstructing human hand-object interaction 3D meshes from RGB images and aligning object meshes with their accurate 3D shapes, the model is trained on 10 object categories and evaluated on 9 unseen objects. The model achieves a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with significant improvements in functionality ratings compared to baselines. Simulator-augmented data further enhances performance, boosting the success rate to 83.4%. The sim-to-real transfer to the LEAP Hand demonstrates an 85% success rate, showcasing the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2505.05517v1 Announce Type: new 
Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands to manipulate objects effectively. However, most prior work either focuses on power grasping, which simply involves holding an object still, or relies on costly teleoperated robot demonstrations to teach robots how to grasp each object functionally. Instead, we propose extracting human grasp information from web images since they depict natural and functional object interactions, thereby bypassing the need for curated demonstrations. We reconstruct human hand-object interaction (HOI) 3D meshes from RGB images, retarget the human hand to multi-finger robot hands, and align the noisy object mesh with its accurate 3D shape. We show that these relatively low-quality HOI data from inexpensive web sources can effectively train a functional grasping model. To further expand the grasp dataset for seen and unseen objects, we use the initially-trained grasping policy with web data in the IsaacGym simulator to generate physically feasible grasps while preserving functionality. We train the grasping model on 10 object categories and evaluate it on 9 unseen objects, including challenging items such as syringes, pens, spray bottles, and tongs, which are underrepresented in existing datasets. The model trained on the web HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with a 6.7% improvement in success rate and a 1.8x increase in functionality ratings over baselines. Simulator-augmented data further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the LEAP Hand achieves a 85% success rate. Project website is at: https://webgrasp.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Privacy Preservation for Robot Visual Perception</title>
<link>https://arxiv.org/abs/2505.05519</link>
<guid>https://arxiv.org/abs/2505.05519</guid>
<content:encoded><![CDATA[
<div> method, privacy-constrained, video streaming, deep learning, detection model

Summary:
A new method for privacy-constrained video streaming (PCVS) has been developed to conceal privacy-sensitive objects in real-time video streams. The approach uses a logical specification to determine which objects to blur out, ensuring the concealment of sensitive information such as faces. A detection model evaluates the presence of these objects in each frame, with a conformal prediction approach establishing a theoretical lower bound on the probability of object existence. PCVS demonstrates a high specification satisfaction rate of over 95% across multiple datasets, outperforming other methods and consistently exceeding theoretical bounds. The method has been successfully deployed on robots in real-time operation, preserving privacy without compromising functionality. <div>
arXiv:2505.05519v1 Announce Type: new 
Abstract: Many robots (e.g., iRobot's Roomba) operate based on visual observations from live video streams, and such observations may inadvertently include privacy-sensitive objects, such as personal identifiers. Existing approaches for preserving privacy rely on deep learning models, differential privacy, or cryptography. They lack guarantees for the complete concealment of all sensitive objects. Guaranteeing concealment requires post-processing techniques and thus is inadequate for real-time video streams. We develop a method for privacy-constrained video streaming, PCVS, that conceals sensitive objects within real-time video streams. PCVS takes a logical specification constraining the existence of privacy-sensitive objects, e.g., never show faces when a person exists. It uses a detection model to evaluate the existence of these objects in each incoming frame. Then, it blurs out a subset of objects such that the existence of the remaining objects satisfies the specification. We then propose a conformal prediction approach to (i) establish a theoretical lower bound on the probability of the existence of these objects in a sequence of frames satisfying the specification and (ii) update the bound with the arrival of each subsequent frame. Quantitative evaluations show that PCVS achieves over 95 percent specification satisfaction rate in multiple datasets, significantly outperforming other methods. The satisfaction rate is consistently above the theoretical bounds across all datasets, indicating that the established bounds hold. Additionally, we deploy PCVS on robots in real-time operation and show that the robots operate normally without being compromised when PCVS conceals objects.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation</title>
<link>https://arxiv.org/abs/2505.05520</link>
<guid>https://arxiv.org/abs/2505.05520</guid>
<content:encoded><![CDATA[
<div> Keywords: Gliomas, deep learning, GaMNet, lesion segmentation, interpretability <br />
Summary: GaMNet is a novel approach for glioma lesion segmentation using deep learning. The proposed method combines the NMamba module for global modeling with a multi-scale CNN for efficient local feature extraction. By incorporating Gabor filters at multiple scales, GaMNet improves interpretability and mimics the human visual system. This approach achieves high segmentation accuracy while using fewer parameters and offering faster computation compared to existing methods. Extensive experiments demonstrate that GaMNet outperforms other models by reducing false positives and negatives, ultimately enhancing the reliability of clinical diagnosis. <div>
arXiv:2505.05520v1 Announce Type: new 
Abstract: Gliomas are aggressive brain tumors that pose serious health risks. Deep learning aids in lesion segmentation, but CNN and Transformer-based models often lack context modeling or demand heavy computation, limiting real-time use on mobile medical devices. We propose GaMNet, integrating the NMamba module for global modeling and a multi-scale CNN for efficient local feature extraction. To improve interpretability and mimic the human visual system, we apply Gabor filters at multiple scales. Our method achieves high segmentation accuracy with fewer parameters and faster computation. Extensive experiments show GaMNet outperforms existing methods, notably reducing false positives and negatives, which enhances the reliability of clinical diagnosis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
<div> attack, CLIP, adversarial perturbation, transferability, X-Transfer

Summary:
X-Transfer introduces a novel attack method targeting CLIP models, showcasing universal adversarial vulnerability. This vulnerability, termed as super transferability, allows for deceptive perturbations to be successful across various CLIP encoders and downstream VLMs. The method utilizes surrogate scaling, dynamically selecting suitable surrogates for efficient scaling instead of relying on fixed models. Through extensive evaluation, X-Transfer demonstrates superior performance compared to existing UAP methods, setting a new benchmark for adversarial transferability in CLIP models. The code for X-Transfer is openly accessible on the GitHub repository provided by the authors. <div>
arXiv:2505.05528v1 Announce Type: new 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours</title>
<link>https://arxiv.org/abs/2505.05531</link>
<guid>https://arxiv.org/abs/2505.05531</guid>
<content:encoded><![CDATA[
<div> Keywords: lip segmentation, attention UNet, multidimensional input, facial anomalies, fetal alcohol syndrome

Summary:<br />
- The proposed method integrates attention UNet and multidimensional input to improve lip segmentation accuracy.
- Local binary patterns are used to unravel micro-patterns in facial images for building multidimensional inputs.
- A mask generation method utilizing anatomical landmarks helps estimate complete lip contour, enhancing segmentation accuracy.
- The method achieved a mean dice score of 84.75% and a mean pixel accuracy of 99.77% in upper lip segmentation.
- Using a generative adversarial network (GAN), the method attained 98.55% accuracy in identifying fetal alcohol syndrome (FAS) based on lip-related facial anomalies.

<br /><br />Summary: <div>
arXiv:2505.05531v1 Announce Type: new 
Abstract: Lip segmentation plays a crucial role in various domains, such as lip synchronization, lipreading, and diagnostics. However, the effectiveness of supervised lip segmentation is constrained by the availability of lip contour in the training phase. A further challenge with lip segmentation is its reliance on image quality , lighting, and skin tone, leading to inaccuracies in the detected boundaries. To address these challenges, we propose a sequential lip segmentation method that integrates attention UNet and multidimensional input. We unravel the micro-patterns in facial images using local binary patterns to build multidimensional inputs. Subsequently, the multidimensional inputs are fed into sequential attention UNets, where the lip contour is reconstructed. We introduce a mask generation method that uses a few anatomical landmarks and estimates the complete lip contour to improve segmentation accuracy. This mask has been utilized in the training phase for lip segmentation. To evaluate the proposed method, we use facial images to segment the upper lips and subsequently assess lip-related facial anomalies in subjects with fetal alcohol syndrome (FAS). Using the proposed lip segmentation method, we achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in upper lip segmentation. To further evaluate the method, we implemented classifiers to identify those with FAS. Using a generative adversarial network (GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study populations. This method could be used to improve lip segmentation accuracy, especially around Cupid's bow, and shed light on distinct lip-related characteristics of FAS.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open Ended Action Environments</title>
<link>https://arxiv.org/abs/2505.05540</link>
<guid>https://arxiv.org/abs/2505.05540</guid>
<content:encoded><![CDATA[
<div> benchmark, VLM, VLA, zero-shot generalization, Procgen

Summary:<br />
(1) State-of-the-art vision-language-action models, including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST, were evaluated on diverse procedural tasks from the Procgen benchmark. (2) The models showed limitations in zero-shot generalization to out-of-distribution tasks, with performance influenced by factors like action representation and task complexity. (3) Vision-language-action models generally outperformed others due to their robust architecture. (4) Variants of vision-language models improved significantly when appropriately constrained, highlighting the impact of precise prompt engineering on performance. (5) The study emphasizes the importance of systematic evaluation and analysis of VLM and VLA models in diverse environments to enhance their generalization capabilities for real-world applications. 

<br /><br />Summary: <div>
arXiv:2505.05540v1 Announce Type: new 
Abstract: Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLM and VLA models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexit; (2) VLAs generally outperform other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05573</link>
<guid>https://arxiv.org/abs/2505.05573</guid>
<content:encoded><![CDATA[
<div> fine-tuning, medical images, text-to-image synthesis, MSDM, healthcare AI<br />
Summary:<br />
This paper explores text-to-image synthesis in the medical domain, comparing fine-tuning large pre-trained latent diffusion models (FLUX, Kandinsky) with training compact domain-specific models (MSDM). The newly proposed MSDM model integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to align medical text prompts with generated images efficiently. Evaluation on colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets shows that while large models achieve higher fidelity, MSDM delivers comparable quality with lower computational costs. Quantitative metrics and feedback from medical experts highlight the strengths and limitations of each approach. This study highlights the potential of text-to-image synthesis in addressing data scarcity challenges in healthcare AI while maintaining patient privacy. <br />Summary: <div>
arXiv:2505.05573v1 Announce Type: new 
Abstract: The generation of realistic medical images from text descriptions has significant potential to address data scarcity challenges in healthcare AI while preserving patient privacy. This paper presents a comprehensive study of text-to-image synthesis in the medical domain, comparing two distinct approaches: (1) fine-tuning large pre-trained latent diffusion models and (2) training small, domain-specific models. We introduce a novel model named MSDM, an optimized architecture based on Stable Diffusion that integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to better align medical text prompts with generated images. Our study compares two approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus training compact domain-specific models (MSDM). Evaluation across colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models achieve higher fidelity, our optimized MSDM delivers comparable quality with lower computational costs. Quantitative metrics and qualitative evaluations by medical experts reveal strengths and limitations of each approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steepest Descent Density Control for Compact 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.05587</link>
<guid>https://arxiv.org/abs/2505.05587</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D, rendering, optimization, efficiency
Summary:
The paper introduces a theoretical framework for improving density control in 3D Gaussian Splatting (3DGS). It addresses the issue of redundant point clouds generated by the densification algorithm, which leads to excessive memory usage and slower performance. The analysis reveals the importance of splitting in escaping saddle points and establishes necessary conditions for densification. It determines the minimal number of offspring Gaussians, identifies the optimal parameter update direction, and provides an analytical solution for normalizing offspring opacity. The proposed SteepGS approach incorporates steepest density control, achieving a significant reduction in Gaussian points while maintaining rendering quality. This results in a 50% decrease in points, enhancing efficiency and scalability of 3DGS. <br /><br />Summary: <div>
arXiv:2505.05587v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation</title>
<link>https://arxiv.org/abs/2505.05589</link>
<guid>https://arxiv.org/abs/2505.05589</guid>
<content:encoded><![CDATA[
<div> Keywords: Reactive dance generation, diffusion-based framework, multi-scale controllability, interaction fidelity, temporal consistency

Summary:
ReactDance introduces a novel diffusion-based framework for reactive dance generation that addresses limitations of existing methods by capturing fine-grained spatial interactions and localized temporal context. The framework utilizes Group Residual Finite Scalar Quantization (GRFSQ) to disentangle motion representation across multiple scales, allowing for accurate capture of interaction semantics. Additionally, a Blockwise Local Context (BLC) sampling strategy is employed to eliminate error accumulation in long sequence generation. The model, implemented with Layer-Decoupled Classifier-free Guidance (LDCFG), enables granular control over motion semantics at varying scales. Through extensive experiments on standard benchmarks, ReactDance showcases superior performance compared to existing methods, achieving state-of-the-art results in terms of interaction fidelity, synchronization, and temporal consistency.<br /><br />Summary: ReactDance provides a cutting-edge solution for reactive dance generation with enhanced spatial coordination and temporal coherence, surpassing current methods by incorporating multi-scale controllability, disentangled motion representation, local context sampling, and granular motion guidance. <div>
arXiv:2505.05589v1 Announce Type: new 
Abstract: Reactive dance generation (RDG) produces follower movements conditioned on guiding dancer and music while ensuring spatial coordination and temporal coherence. However, existing methods overemphasize global constraints and optimization, overlooking local information, such as fine-grained spatial interactions and localized temporal context. Therefore, we present ReactDance, a novel diffusion-based framework for high-fidelity RDG with long-term coherence and multi-scale controllability. Unlike existing methods that struggle with interaction fidelity, synchronization, and temporal consistency in duet synthesis, our approach introduces two key innovations: 1)Group Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion representation that captures interaction semantics from coarse body rhythms to fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling strategy eliminating error accumulation in long sequence generation via local block causal masking and periodic positional encoding. Built on the decoupled multi-scale GRFSQ representation, we implement a diffusion model withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control over motion semantics across scales. Extensive experiments on standard benchmarks demonstrate that ReactDance surpasses existing methods, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization</title>
<link>https://arxiv.org/abs/2505.05591</link>
<guid>https://arxiv.org/abs/2505.05591</guid>
<content:encoded><![CDATA[
<div> Keywords: surface reconstruction, computer vision, gaussian splatting, indoor scenes, data-driven optimization<br />
Summary: <br />
Surface reconstruction is a crucial aspect of computer vision and graphics, with applications in various fields such as 3D modeling and robotics. Existing approaches to surface reconstruction based on volumetric rendering have limitations in modeling under-observed or textureless regions. The QuickSplat method introduces data-driven priors to generate dense initializations for optimizing large-scale indoor scenes using 2D gaussian splatting. This accelerates the optimization process and enhances the geometry of flat wall structures. The method also includes a densifier network that predicts new Gaussians based on rendering gradients, eliminating the need for heuristics in densification. Extensive experiments show that this data-driven optimization significantly improves runtime efficiency by 8x and reduces depth errors by up to 48% compared to current state-of-the-art methods. <br /><br />Summary: <div>
arXiv:2505.05591v1 Announce Type: new 
Abstract: Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling</title>
<link>https://arxiv.org/abs/2505.05599</link>
<guid>https://arxiv.org/abs/2505.05599</guid>
<content:encoded><![CDATA[
<div> Keywords: satellite imagery, object localization, YOLO-DCAP, multi-scale features, attention-aided spatial pooling <br />
Summary: <br />
Object localization in satellite imagery is complex due to variability in objects, low resolution, and interference. This research focuses on GW, Bore, and OE datasets, each with unique challenges. YOLO-DCAP, an enhanced YOLOv5 variant, addresses these challenges with a MDRC block for multi-scale feature capture and an AaSP module for global spatial focus. YOLO-DCAP outperforms base model and state-of-the-art methods, with 20.95% and 32.23% mAP50 and IoU improvements over base model, and 7.35% and 9.84% over alternatives. The approach is robust and generalizable across all three datasets. Open-source code is available at the provided GitHub link. 
<br /> <div>
arXiv:2505.05599v1 Announce Type: new 
Abstract: Object localization in satellite imagery is particularly challenging due to the high variability of objects, low spatial resolution, and interference from noise and dominant features such as clouds and city lights. In this research, we focus on three satellite datasets: upper atmospheric Gravity Waves (GW), mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique challenges. These challenges include the variability in the scale and appearance of the main object patterns, where the size, shape, and feature extent of objects of interest can differ significantly. To address these challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed to improve object localization in these complex scenarios. YOLO-DCAP incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture multi-scale features at scale with varying dilation rates, and an Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant spatial regions, enhancing feature selection. These structural improvements help to better localize objects in satellite imagery. Experimental results demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model and state-of-the-art approaches, achieving an average improvement of 20.95% in mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively over state-of-the-art alternatives, consistently across all three satellite datasets. These consistent gains across all three satellite datasets highlight the robustness and generalizability of the proposed approach. Our code is open sourced at https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Preliminary Study for GPT-4o on Image Restoration</title>
<link>https://arxiv.org/abs/2505.05621</link>
<guid>https://arxiv.org/abs/2505.05621</guid>
<content:encoded><![CDATA[
<div> image restoration, GPT-4o, multi-modal inputs, autoregressive architecture, image generation<br />
Summary:<br />
OpenAI's GPT-4o model, incorporating multi-modal inputs and outputs in an autoregressive framework, has shown remarkable image generation capabilities. However, a systematic evaluation reveals issues with pixel-level fidelity in image restoration tasks, such as variations in proportions and object positions. Despite these challenges, GPT-4o's outputs can serve as potent visual priors in tasks like dehazing and derainning, bolstering existing networks' performance. This study provides guidance for integrating GPT-4o into image restoration pipelines, potentially driving innovation in the field. Released images and datasets will support further research in the broader realm of image generation.<br /> <div>
arXiv:2505.05621v1 Announce Type: new 
Abstract: OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models</title>
<link>https://arxiv.org/abs/2505.05626</link>
<guid>https://arxiv.org/abs/2505.05626</guid>
<content:encoded><![CDATA[
<div> vision, language, Multimodal Large Language Models, visual understanding, language generation

Summary:
- Multimodal Large Language Models (MLLMs) struggle to effectively combine vision and language, often relying too heavily on language priors.
- This study delves into how MLLMs internally process visual information from image regions.
- The researchers introduce techniques to enhance the model's grasp of visual content and ensure that this understanding shapes language generation.
- The resulting model shows superior multimodal comprehension, as evidenced by its ability to predict visually-relevant tokens and outperform on visually challenging tasks.
- Through a rigorous analysis, the model demonstrates a 10-point improvement on difficult multimodal tasks. 

<br /><br />Summary: <div>
arXiv:2505.05626v1 Announce Type: new 
Abstract: Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models</title>
<link>https://arxiv.org/abs/2505.05635</link>
<guid>https://arxiv.org/abs/2505.05635</guid>
<content:encoded><![CDATA[
<div> Keywords: Open-vocabulary recognition, bird species, multimodal vision language encoders, retrieval-augmented generation, biodiversity monitoring<br />
Summary:<br />
- The article addresses the challenge of open-vocabulary bird species recognition, focusing on classifying species without predefined taxonomic categories.<br />
- Traditional benchmarks are limited in real-world scenarios where novel species emerge, showing reduced performance under open-vocabulary settings.<br />
- The proposed framework integrates structured textual knowledge from Wikipedia articles of 11,202 bird species to improve recognition capabilities.<br />
- The Visual Re-ranking Retrieval-Augmented Generation (VR-RAG) framework uses visual similarities to rerank top candidates, enabling recognition of unseen species.<br />
- Extensive experiments across five classification benchmarks demonstrate the effectiveness of the approach, improving performance and surpassing conventional VLM-based methods.<br />
Summary: <div>
arXiv:2505.05635v1 Announce Type: new 
Abstract: Open-vocabulary recognition remains a challenging problem in computer vision, as it requires identifying objects from an unbounded set of categories. This is particularly relevant in nature, where new species are discovered every year. In this work, we focus on open-vocabulary bird species recognition, where the goal is to classify species based on their descriptions without being constrained to a predefined set of taxonomic categories. Traditional benchmarks like CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary paradigm, limiting their applicability to real-world scenarios where novel species continually emerge. We show that the performance of current systems when evaluated under settings closely aligned with open-vocabulary drops by a huge margin. To address this gap, we propose a scalable framework integrating structured textual knowledge from Wikipedia articles of 11,202 bird species distilled via GPT-4o into concise, discriminative summaries. We propose Visual Re-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented generation framework that uses visual similarities to rerank the top m candidates retrieved by a set of multimodal vision language encoders. This allows for the recognition of unseen taxa. Extensive experiments across five established classification benchmarks show that our approach is highly effective. By integrating VR-RAG, we improve the average performance of state-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five benchmarks. Our approach outperforms conventional VLM-based approaches, which struggle with unseen species. By bridging the gap between encyclopedic knowledge and visual recognition, our work advances open-vocabulary recognition, offering a flexible, scalable solution for biodiversity monitoring and ecological research.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Style Transfer for Enhancing Animal Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2505.05640</link>
<guid>https://arxiv.org/abs/2505.05640</guid>
<content:encoded><![CDATA[
<div> Style Transfer, Neural Networks, Facial Landmark Detection, Data Augmentation, Animal Detection<br />
<br />
Summary:<br />
Neural Style Transfer (NST) is explored for enhancing animal facial landmark detectors training. By using cropped facial images rather than full-body images, structural consistency is improved, enhancing image quality. Challenges arose with annotation misalignment when training on style-transferred images, but Supervised Style Transfer (SST) helped maintain up to 98% of baseline accuracy. Augmenting the dataset with style-transferred images proved more effective than traditional methods, boosting robustness. The study focused on cat facial landmarks but suggests the method can be applied to other species and landmark detection models. <div>
arXiv:2505.05640v1 Announce Type: new 
Abstract: Neural Style Transfer (NST) is a technique for applying the visual characteristics of one image onto another while preserving structural content. Traditionally used for artistic transformations, NST has recently been adapted, e.g., for domain adaptation and data augmentation. This study investigates the use of this technique for enhancing animal facial landmark detectors training. As a case study, we use a recently introduced Ensemble Landmark Detector for 48 anatomical cat facial landmarks and the CatFLW dataset it was trained on, making three main contributions. First, we demonstrate that applying style transfer to cropped facial images rather than full-body images enhances structural consistency, improving the quality of generated images. Secondly, replacing training images with style-transferred versions raised challenges of annotation misalignment, but Supervised Style Transfer (SST) - which selects style sources based on landmark accuracy - retained up to 98% of baseline accuracy. Finally, augmenting the dataset with style-transferred images further improved robustness, outperforming traditional augmentation methods. These findings establish semantic style transfer as an effective augmentation strategy for enhancing the performance of facial landmark detection models for animals and beyond. While this study focuses on cat facial landmarks, the proposed method can be generalized to other species and landmark detection models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction</title>
<link>https://arxiv.org/abs/2505.05644</link>
<guid>https://arxiv.org/abs/2505.05644</guid>
<content:encoded><![CDATA[
<div> transformer architecture, multimodal learning, reflectance parameter estimation, image-based 3D reconstruction, lunar images <br />
<br />
Summary: 
Multimodal learning, relatively unexplored in planetary science, is applied to the tasks of reflectance parameter estimation and image-based 3D reconstruction of lunar images. A unified transformer architecture is proposed and trained to learn shared representations from grayscale images, digital elevation models, surface normals, and albedo maps. The model can translate between different input and target modalities, enabling the simultaneous prediction of DEMs and albedo maps from grayscale images. This approach solves the 3D reconstruction problem while separating photometric parameters and height information. Results show that the model can learn meaningful relationships across multiple modalities, with potential for future enhancements like photometric normalization and co-registration by incorporating additional input sources. <div>
arXiv:2505.05644v1 Announce Type: new 
Abstract: Multimodal learning is an emerging research topic across multiple disciplines but has rarely been applied to planetary science. In this contribution, we identify that reflectance parameter estimation and image-based 3D reconstruction of lunar images can be formulated as a multimodal learning problem. We propose a single, unified transformer architecture trained to learn shared representations between multiple sources like grayscale images, digital elevation models, surface normals, and albedo maps. The architecture supports flexible translation from any input modality to any target modality. Predicting DEMs and albedo maps from grayscale images simultaneously solves the task of 3D reconstruction of planetary surfaces and disentangles photometric parameters and height information. Our results demonstrate that our foundation model learns physically plausible relations across these four modalities. Adding more input modalities in the future will enable tasks such as photometric normalization and co-registration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval</title>
<link>https://arxiv.org/abs/2505.05666</link>
<guid>https://arxiv.org/abs/2505.05666</guid>
<content:encoded><![CDATA[
<div> Vision-based RAG, OCR-based RAG, document quality, semantic answer evaluation benchmark, question-answering performance<br />
<br />
Summary:<br />
The study compares a vision-based RAG system (ColPali) and traditional OCR-based pipelines with Llama 3.2 and Nougat OCR. Vision-based RAG performs well on fine-tuned documents but struggles with generalization to unseen documents of varying quality. In contrast, OCR-based RAG shows better generalization capabilities. The study introduces a semantic answer evaluation benchmark to assess question-answering performance. It highlights the trade-offs between computational efficiency and semantic accuracy, providing practical guidance for RAG practitioners on choosing between OCR-dependent and vision-based document retrieval systems in production environments.<br /> <div>
arXiv:2505.05666v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for enhancing the reliability and utility of Large Language Models (LLMs) by grounding responses in external documents. Traditional RAG systems rely on Optical Character Recognition (OCR) to first process scanned documents into text. However, even state-of-the-art OCRs can introduce errors, especially in degraded or complex documents. Recent vision-language approaches, such as ColPali, propose direct visual embedding of documents, eliminating the need for OCR. This study presents a systematic comparison between a vision-based RAG system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2 (90B) and Nougat OCR across varying document qualities. Beyond conventional retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark to assess end-to-end question-answering performance. Our findings indicate that while vision-based RAG performs well on documents it has been fine-tuned on, OCR-based RAG is better able to generalize to unseen documents of varying quality. We highlight the key trade-offs between computational efficiency and semantic accuracy, offering practical guidance for RAG practitioners in selecting between OCR-dependent and vision-based document retrieval systems in production environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</title>
<link>https://arxiv.org/abs/2505.05672</link>
<guid>https://arxiv.org/abs/2505.05672</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D head avatars, photorealism, 3D Gaussian splatting, facial motion estimation, high-detail model

Summary: 
This paper introduces a new high-detail 3D head avatar model that improves on existing models by increasing the number of 3D Gaussians used for rendering at 4K resolution. The model is reconstructed from multiview input video and utilizes a mesh-based 3D morphable model for coarse deformation. 3D Gaussians, embedded within the UVD tangent space of the mesh, provide photoreal appearance and allow for densification where needed. A novel deformable Gaussian encoding and fitting procedure preserves appearance detail while capturing facial motion and high-frequency features like skin wrinkling. This approach addresses challenges such as inaccurate motion estimation and memory limitations, enhancing the fidelity and quality of photoreal avatars for applications in telepresence, extended reality, and entertainment. <div>
arXiv:2505.05672v1 Announce Type: new 
Abstract: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstanceGen: Image Generation with Instance-level Instructions</title>
<link>https://arxiv.org/abs/2505.05678</link>
<guid>https://arxiv.org/abs/2505.05678</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, text-to-image, structural constraints, instance-level attributes, spatial relations <br />
Summary: 
In this new arXiv announcement, the focus is on improving pretrained text-to-image models' ability to understand complex prompts involving multiple objects and attributes. The approach involves integrating structural constraints in the form of fine-grained structural initialization from contemporary image generation models. By combining this structural guidance with instance-level instructions, the proposed technique aims to generate images that accurately represent all aspects of the text prompt, including object counts, attributes, and spatial relations between instances. This advancement addresses the limitations faced by current generative models in capturing the semantics of complex prompts, offering a promising solution for more accurate and detailed image generation. <br /><br />Summary: <div>
arXiv:2505.05678v1 Announce Type: new 
Abstract: Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, %leveraging additional structural inputs typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible \emph{fine-grained} structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos</title>
<link>https://arxiv.org/abs/2505.05681</link>
<guid>https://arxiv.org/abs/2505.05681</guid>
<content:encoded><![CDATA[
<div> Keywords: Nonhuman primates, Capuchin monkeys, Video-text models, Fine-tuning, Behavioral analysis

Summary: 
The study focuses on developing computational models to assist researchers in extracting useful clips from videos of capuchin monkeys in their natural habitat. By fine-tuning pre-trained video-text models, the researchers aim to address the challenge of training models solely based on raw, unlabeled video footage and weak audio descriptions. They propose a two-folded approach involving an agentic data treatment pipeline and a fine-tuning process using Microsoft's X-CLIP model with Low-Rank Adaptation (LoRA). The results show a significant improvement in retrieval performance, with a 167% uplift in Hits@5 for the 16 frames model and a 114% uplift for the 8 frame model on domain data. Additionally, the model effectively ranks various behaviors based on NDCG@K results, outperforming raw pre-trained models in behavioral analysis tasks.<br /><br />Summary: <div>
arXiv:2505.05681v1 Announce Type: new 
Abstract: Video recordings of nonhuman primates in their natural habitat are a common source for studying their behavior in the wild. We fine-tune pre-trained video-text foundational models for the specific domain of capuchin monkeys, with the goal of developing useful computational models to help researchers to retrieve useful clips from videos. We focus on the challenging problem of training a model based solely on raw, unlabeled video footage, using weak audio descriptions sometimes provided by field collaborators. We leverage recent advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs) to address the extremely noisy nature of both video and audio content. Specifically, we propose a two-folded approach: an agentic data treatment pipeline and a fine-tuning process. The data processing pipeline automatically extracts clean and semantically aligned video-text pairs from the raw videos, which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of $167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame model on our domain data. Moreover, based on $NDCG@K$ results, our model is able to rank well most of the considered behaviors, while the tested raw pre-trained models are not able to rank them at all. The code will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder</title>
<link>https://arxiv.org/abs/2505.05710</link>
<guid>https://arxiv.org/abs/2505.05710</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, hyperspectral imagery, dual masking, spectral-spatial representations, transfer learning <br />
<br />
Summary: HyperspectralMAE is a Transformer-based model for hyperspectral data that uses dual masking during pre-training to learn robust spatial and spectral representations. It incorporates learnable harmonic Fourier positional embeddings based on wavelength and balances pixel-level accuracy and spectral-shape fidelity in reconstruction using mean-squared error and spectral angle mapper. With a large parameter capacity and 768-dimensional embeddings, it excels in transfer learning when fine-tuned for land-cover classification. Pre-trained on NASA Hyperion and DLR EnMAP datasets, HyperspectralMAE achieves state-of-the-art accuracy on the Indian Pines benchmark, showcasing the efficacy of dual masking and wavelength-aware embeddings in advancing hyperspectral image analysis. <br /><br />Summary: <div>
arXiv:2505.05710v1 Announce Type: new 
Abstract: Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation model for hyperspectral data that employs a \textit{dual masking} strategy: during pre-training we randomly occlude 50\% of spatial patches and 50\% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity.
  The resulting model contains about $1.8\times10^{8}$ parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra) and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer</title>
<link>https://arxiv.org/abs/2505.05711</link>
<guid>https://arxiv.org/abs/2505.05711</guid>
<content:encoded><![CDATA[
<div> Proposed Keywords: temporal action detection, transformer, multi-dilated gated encoder, central-adjacent region integrated decoder, state-of-the-art performance

Summary:
In this paper, the authors address limitations in query-based detectors for temporal action detection (TAD) by proposing a new model called DiGIT. The model tackles challenges specific to TAD by introducing a multi-dilated gated encoder to reduce redundant information and capture fine-grained temporal details. Additionally, a central-adjacent region integrated decoder is implemented to improve the sampling strategy for deformable cross-attention. Through extensive experiments on benchmark datasets like THUMOS14, ActivityNet v1.3, and HACS-Segment, DiGIT achieves state-of-the-art performance. This innovative approach outperforms existing models by effectively capturing long-range temporal context and essential information for accurate action detection. The code for DiGIT is publicly available on GitHub, providing a valuable resource for further research in the field. 

<br /><br />Summary: <div>
arXiv:2505.05711v1 Announce Type: new 
Abstract: In this paper, we examine a key limitation in query-based detectors for temporal action detection (TAD), which arises from their direct adaptation of originally designed architectures for object detection. Despite the effectiveness of the existing models, they struggle to fully address the unique challenges of TAD, such as the redundancy in multi-scale features and the limited ability to capture sufficient temporal context. To address these issues, we propose a multi-dilated gated encoder and central-adjacent region integrated decoder for temporal action detection transformer (DiGIT). Our approach replaces the existing encoder that consists of multi-scale deformable attention and feedforward network with our multi-dilated gated encoder. Our proposed encoder reduces the redundant information caused by multi-level features while maintaining the ability to capture fine-grained and long-range temporal information. Furthermore, we introduce a central-adjacent region integrated decoder that leverages a more comprehensive sampling strategy for deformable cross-attention to capture the essential information. Extensive experiments demonstrate that DiGIT achieves state-of-the-art performance on THUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at: https://github.com/Dotori-HJ/DiGIT
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Space-Intervened Diffusive Alignment for Visual Classification</title>
<link>https://arxiv.org/abs/2505.05721</link>
<guid>https://arxiv.org/abs/2505.05721</guid>
<content:encoded><![CDATA[
<div> alignment, cross-modal, semantic space, diffusive, classification

Summary:
The paper introduces a novel approach called Semantic-Space-Intervened Diffusive Alignment (SeDA) for improving visual classification through cross-modal alignment. SeDA utilizes a semantic space as a bridge in the visual-to-textual projection process, leveraging the shared class-level information between visual and textual features. A bi-stage diffusion framework is implemented in SeDA, with a Diffusion-Controlled Semantic Learner modeling the semantic feature space of visual features and a Diffusion-Controlled Semantic Translator focusing on learning the distribution of textual features from the semantic space. The Progressive Feature Interaction Network facilitates stepwise feature interactions to integrate textual information into mapped features. Experimental results demonstrate that SeDA outperforms existing methods in cross-modal feature alignment, leading to enhanced performance across various scenarios. <br /><br />Summary: <div>
arXiv:2505.05721v1 Announce Type: new 
Abstract: Cross-modal alignment is an effective approach to improving visual classification. Existing studies typically enforce a one-step mapping that uses deep neural networks to project the visual features to mimic the distribution of textual features. However, they typically face difficulties in finding such a projection due to the two modalities in both the distribution of class-wise samples and the range of their feature values. To address this issue, this paper proposes a novel Semantic-Space-Intervened Diffusive Alignment method, termed SeDA, models a semantic space as a bridge in the visual-to-textual projection, considering both types of features share the same class-level information in classification. More importantly, a bi-stage diffusion framework is developed to enable the progressive alignment between the two modalities. Specifically, SeDA first employs a Diffusion-Controlled Semantic Learner to model the semantic features space of visual features by constraining the interactive features of the diffusion model and the category centers of visual features. In the later stage of SeDA, the Diffusion-Controlled Semantic Translator focuses on learning the distribution of textual features from the semantic space. Meanwhile, the Progressive Feature Interaction Network introduces stepwise feature interactions at each alignment step, progressively integrating textual information into mapped features. Experimental results show that SeDA achieves stronger cross-modal feature alignment, leading to superior performance over existing methods across multiple scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation</title>
<link>https://arxiv.org/abs/2505.05722</link>
<guid>https://arxiv.org/abs/2505.05722</guid>
<content:encoded><![CDATA[
<div> Synthetic datasets, point tracking, surgical videos, domain shift, semi-supervised learning,<br />
<br />
Summary: SurgTracker is a framework designed to adapt synthetic-trained point trackers to surgical video environments. It utilizes filtered self-distillation to generate pseudo-labels and enforce geometric consistency during training. The approach improves tracking performance in high-shift domains such as surgery by utilizing only 80 unlabeled videos. SurgTracker addresses the challenges of domain shift and lack of labeled data in surgical videos, where scenes feature complex tissue deformation, occlusion, and lighting variation. This adaptation method provides stable supervision throughout training without the need for multiple teachers, making it computationally efficient. The results on the STIR benchmark demonstrate the potential of SurgTracker for robust tracking performance in data-scarce domains. <br /><br /> <div>
arXiv:2505.05722v1 Announce Type: new 
Abstract: Synthetic datasets have enabled significant progress in point tracking by providing large-scale, densely annotated supervision. However, deploying these models in real-world domains remains challenging due to domain shift and lack of labeled data-issues that are especially severe in surgical videos, where scenes exhibit complex tissue deformation, occlusion, and lighting variation. While recent approaches adapt synthetic-trained trackers to natural videos using teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their effectiveness in high-shift domains like surgery remains unexplored. This work presents SurgTracker, a semi-supervised framework for adapting synthetic-trained point trackers to surgical video using filtered self-distillation. Pseudo-labels are generated online by a fixed teacher-identical in architecture and initialization to the student-and are filtered using a cycle consistency constraint to discard temporally inconsistent trajectories. This simple yet effective design enforces geometric consistency and provides stable supervision throughout training, without the computational overhead of maintaining multiple teachers. Experiments on the STIR benchmark show that SurgTracker improves tracking performance using only 80 unlabeled videos, demonstrating its potential for robust adaptation in high-shift, data-scarce domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection</title>
<link>https://arxiv.org/abs/2505.05741</link>
<guid>https://arxiv.org/abs/2505.05741</guid>
<content:encoded><![CDATA[
<div> Keywords: Tiny object detection, Dome-DETR, DeFE, MWAS, PAQI

Summary:
Dome-DETR is a new framework designed for efficient tiny object detection in various applications such as drone surveillance and autonomous systems. It addresses challenges in feature redundancy and high computational costs by introducing innovative techniques. The Density-Focal Extractor (DeFE) generates compact foreground masks to reduce redundancies, while Masked Window Attention Sparsification (MWAS) prioritizes informative regions using sparse attention. Progressive Adaptive Query Initialization (PAQI) dynamically adjusts query density for improved allocation. Dome-DETR achieves state-of-the-art performance on AI-TOD-V2 and VisDrone datasets with a compact model size and low computational complexity. The proposed framework demonstrates superior results (+3.3 AP on AI-TOD-V2 and +2.5 AP on VisDrone) compared to existing methods. The code for Dome-DETR will be made available upon acceptance. 

<br /><br />Summary: Dome-DETR introduces novel techniques for efficient tiny object detection, including DeFE for generating compact foreground masks, MWAS for focusing on informative regions, and PAQI for adaptive query initialization. It achieves state-of-the-art performance on AI-TOD-V2 and VisDrone datasets while maintaining low computational complexity and a compact model size. The proposed framework addresses the limitations of existing methods and demonstrates significant improvements in accuracy and efficiency. <div>
arXiv:2505.05741v1 Announce Type: new 
Abstract: Tiny object detection plays a vital role in drone surveillance, remote sensing, and autonomous systems, enabling the identification of small targets across vast landscapes. However, existing methods suffer from inefficient feature leverage and high computational costs due to redundant feature processing and rigid query allocation. To address these challenges, we propose Dome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection. To reduce feature redundancies, we introduce a lightweight Density-Focal Extractor (DeFE) to produce clustered compact foreground masks. Leveraging these masks, we incorporate Masked Window Attention Sparsification (MWAS) to focus computational resources on the most informative regions via sparse attention. Besides, we propose Progressive Adaptive Query Initialization (PAQI), which adaptively modulates query density across spatial areas for better query allocation. Extensive experiments demonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on AI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational complexity and a compact model size. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>kFuse: A novel density based agglomerative clustering</title>
<link>https://arxiv.org/abs/2505.05748</link>
<guid>https://arxiv.org/abs/2505.05748</guid>
<content:encoded><![CDATA[
<div> agglomerative clustering, kFuse, sub-cluster partitioning, boundary connectivity, density similarity
Summary:<br />
The paper introduces a novel density-based agglomerative clustering method called kFuse. It addresses issues in existing clustering methods by utilizing natural neighbors for sub-cluster partitioning, determining boundary connectivity between sub-clusters based on adjacent samples and shortest distances, assessing density similarity through mean density and variance calculations, and establishing merging rules based on connectivity and density. kFuse only requires the number of clusters to be specified at the final merging stage, enhancing accuracy by considering adjacent samples, distances, and densities during merging. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of kFuse in improving clustering accuracy and identification capability. <br /> <div>
arXiv:2505.05748v1 Announce Type: new 
Abstract: Agglomerative clustering has emerged as a vital tool in data analysis due to its intuitive and flexible characteristics. However, existing agglomerative clustering methods often involve additional parameters for sub-cluster partitioning and inter-cluster similarity assessment. This necessitates different parameter settings across various datasets, which is undoubtedly challenging in the absence of prior knowledge. Moreover, existing agglomerative clustering techniques are constrained by the calculation method of connection distance, leading to unstable clustering results. To address these issues, this paper introduces a novel density-based agglomerative clustering method, termed kFuse. kFuse comprises four key components: (1) sub-cluster partitioning based on natural neighbors; (2) determination of boundary connectivity between sub-clusters through the computation of adjacent samples and shortest distances; (3) assessment of density similarity between sub-clusters via the calculation of mean density and variance; and (4) establishment of merging rules between sub-clusters based on boundary connectivity and density similarity. kFuse requires the specification of the number of clusters only at the final merging stage. Additionally, by comprehensively considering adjacent samples, distances, and densities among different sub-clusters, kFuse significantly enhances accuracy during the merging phase, thereby greatly improving its identification capability. Experimental results on both synthetic and real-world datasets validate the effectiveness of kFuse.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
<link>https://arxiv.org/abs/2505.05752</link>
<guid>https://arxiv.org/abs/2505.05752</guid>
<content:encoded><![CDATA[
<div> Keywords: automation, point cloud data, ADA compliance, deep learning, infrastructure surveying  

<br /><br />Summary: This paper discusses a framework for automating geometric measurements and compliance assessments in infrastructure surveying using point cloud data. The approach combines deep learning-based detection and segmentation with geometric and signal processing techniques to streamline surveying tasks. As a proof of concept, the authors apply this framework to assess curb ramps' compliance with the Americans with Disabilities Act (ADA), showcasing the effectiveness of point cloud data in automating surveys. A significant aspect of this work is the creation of a large annotated dataset of curb ramps, which is publicly accessible, aiding in robust model training and evaluation. The authors present experimental results comparing the proposed method with manual field measurements, demonstrating its accuracy and reliability. This method not only aims to reduce manual labor but also to enhance consistency in infrastructure assessments. Additionally, the framework lays the foundation for broader applications in infrastructure surveying and automated construction evaluation, encouraging further use of point cloud data in these fields. The associated annotated database, manual ramp survey data, and algorithms developed during this project are available on GitHub. <div>
arXiv:2505.05752v1 Announce Type: new 
Abstract: Automation can play a prominent role in improving efficiency, accuracy, and scalability in infrastructure surveying and assessing construction and compliance standards. This paper presents a framework for automation of geometric measurements and compliance assessment using point cloud data. The proposed approach integrates deep learning-based detection and segmentation, in conjunction with geometric and signal processing techniques, to automate surveying tasks. As a proof of concept, we apply this framework to automatically evaluate the compliance of curb ramps with the Americans with Disabilities Act (ADA), demonstrating the utility of point cloud data in survey automation. The method leverages a newly collected, large annotated dataset of curb ramps, made publicly available as part of this work, to facilitate robust model training and evaluation. Experimental results, including comparison with manual field measurements of several ramps, validate the accuracy and reliability of the proposed method, highlighting its potential to significantly reduce manual effort and improve consistency in infrastructure assessment. Beyond ADA compliance, the proposed framework lays the groundwork for broader applications in infrastructure surveying and automated construction evaluation, promoting wider adoption of point cloud data in these domains. The annotated database, manual ramp survey data, and developed algorithms are publicly available on the project's GitHub page: https://github.com/Soltanilara/SurveyAutomation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A review of advancements in low-light image enhancement using deep learning</title>
<link>https://arxiv.org/abs/2505.05759</link>
<guid>https://arxiv.org/abs/2505.05759</guid>
<content:encoded><![CDATA[
<div> Keywords: low-light, computer vision, deep learning, image enhancement, vision tasks 

Summary:
This review focuses on the challenges faced by computer vision algorithms in low-light environments and the advancements made in using deep learning for low-light image processing. It examines recent deep-learning-based methods for enhancing low-light images, discussing their operation, enhancement mechanisms, and impact on downstream vision tasks such as segmentation, detection, and classification. The review provides a comprehensive analysis of the strengths and limitations of different enhancement techniques, offering insights for optimizing vision task performance in low-light conditions. Through clear illustrations and detailed explanations, the review serves as a valuable reference for researchers and practitioners seeking to improve the performance of computer vision algorithms in low-light scenarios. Future research directions in the field of low-light image enhancement are also proposed. 

<br /><br />Summary: <div>
arXiv:2505.05759v1 Announce Type: new 
Abstract: In low-light environments, the performance of computer vision algorithms often deteriorates significantly, adversely affecting key vision tasks such as segmentation, detection, and classification. With the rapid advancement of deep learning, its application to low-light image processing has attracted widespread attention and seen significant progress in recent years. However, there remains a lack of comprehensive surveys that systematically examine how recent deep-learning-based low-light image enhancement methods function and evaluate their effectiveness in enhancing downstream vison tasks. To address this gap, this review provides a detailed elaboration on how various recent approaches (from 2020) operate and their enhancement mechanisms, supplemented with clear illustrations. It also investigates the impact of different enhancement techniques on subsequent vision tasks, critically analyzing their strengths and limitations. Additionally, it proposes future research directions. This review serves as a useful reference for determining low-light image enhancement techniques and optimizing vision task performance in low-light conditions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Describe Anything in Medical Images</title>
<link>https://arxiv.org/abs/2505.05804</link>
<guid>https://arxiv.org/abs/2505.05804</guid>
<content:encoded><![CDATA[
<div> Keywords: localized image captioning, medical imaging, vision-language models, region-specific captioning, clinical factuality<br />
Summary:<br />
Localized image captioning has seen advancements with models like DAM, but has not been extensively applied to medical imaging. To address this gap, MedDAM is proposed as the first framework utilizing large vision-language models for region-specific captioning in medical images. It incorporates medical expert-designed prompts and establishes a benchmark for evaluation focusing on clinical factuality through attribute-level verification tasks. MedDAM outperforms various leading models in the task, emphasizing the importance of region-level semantic alignment in medical image understanding. The framework shows promise for clinical vision-language integration. <div>
arXiv:2505.05804v1 Announce Type: new 
Abstract: Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAM's superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework</title>
<link>https://arxiv.org/abs/2505.05806</link>
<guid>https://arxiv.org/abs/2505.05806</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, variational models, deep learning, UNet, boundary preservation<br />
<br />
Summary: <br />
Traditional image segmentation methods, such as variational models based on PDEs, can struggle with parameter sensitivity and high computational costs. On the other hand, deep learning models like UNet are excellent at automatic feature extraction but lack interpretability and require extensive labeled data. To combine the strengths of both approaches, VM_TUNet integrates the fourth-order modified Cahn-Hilliard equation with UNet, offering both interpretability and adaptive feature learning. By introducing a data-driven operator and incorporating TFPM for boundary preservation, VM_TUNet outperforms existing methods in segmentation performance, particularly for precise boundary delineation. The experimental results on benchmark datasets validate the effectiveness of the proposed hybrid framework. <br /> <div>
arXiv:2505.05806v1 Announce Type: new 
Abstract: Traditional image segmentation methods, such as variational models based on partial differential equations (PDEs), offer strong mathematical interpretability and precise boundary modeling, but often suffer from sensitivity to parameter settings and high computational costs. In contrast, deep learning models such as UNet, which are relatively lightweight in parameters, excel in automatic feature extraction but lack theoretical interpretability and require extensive labeled data. To harness the complementary strengths of both paradigms, we propose Variational Model Based Tailored UNet (VM_TUNet), a novel hybrid framework that integrates the fourth-order modified Cahn-Hilliard equation with the deep learning backbone of UNet, which combines the interpretability and edge-preserving properties of variational methods with the adaptive feature learning of neural networks. Specifically, a data-driven operator is introduced to replace manual parameter tuning, and we incorporate the tailored finite point method (TFPM) to enforce high-precision boundary preservation. Experimental results on benchmark datasets demonstrate that VM_TUNet achieves superior segmentation performance compared to existing approaches, especially for fine boundary delineation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2505.05829</link>
<guid>https://arxiv.org/abs/2505.05829</guid>
<content:encoded><![CDATA[
<div> Diffusion transformer, image generation, caching, acceleration, calibration<br />
<br />
Summary: 
The paper introduces an increment-calibrated caching method for accelerating diffusion transformer (DiT) models in image generation. The method utilizes pre-trained models for calibration parameters and employs channel-aware Singular Value Decomposition (SVD) to enhance the calibration effect. Experimental results demonstrate superior performance compared to existing cache-based methods, reducing computation by over 45% while improving Inception Score (IS) by 12 with minimal increase in Fréchet Inception Distance (FID). The proposed method eliminates the need for training and achieves efficient acceleration of DiT models, making it a promising approach for improving the scalability and generative capabilities of diffusion models. The code for implementing the method is available on GitHub for further exploration. 

Summary:<br />
Diffusion transformer, image generation, caching, acceleration, calibration. The paper proposes increment-calibrated caching for diffusion transformer (DiT) models, using pre-trained models for calibration and channel-aware SVD for correction. Experimental results show superior performance in computation reduction and Inception Score improvement with minimal FID increase. The method offers training-free acceleration of DiT models, aiding scalability and generative capabilities. Code is available on GitHub for implementation. <div>
arXiv:2505.05829v1 Announce Type: new 
Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression</title>
<link>https://arxiv.org/abs/2505.05834</link>
<guid>https://arxiv.org/abs/2505.05834</guid>
<content:encoded><![CDATA[
<div> Keywords: ordinal regression, patch-level features, fuzzy learning, image classification, deep learning

Summary:
The paper introduces a novel framework called Dual-level Fuzzy Learning with Patch Guidance (DFPG) for image ordinal regression. DFPG aims to learn precise grading boundaries using patch-level supervision, despite having only image-level ordinal labels. It utilizes patch-labeling and filtering strategies to concentrate on patch-level features, and incorporates a dual-level fuzzy learning module that handles label ambiguity effectively. Through extensive experiments on various datasets, DFPG outperforms existing methods and excels in categorizing challenging samples. The proposed framework demonstrates the ability to discern samples from difficult-to-classify categories with accuracy, showcasing its potential in advancing image classification tasks. The code for DFPG is publicly available for further exploration and implementation. 

<br /><br />
Summary: <div>
arXiv:2505.05834v1 Announce Type: new 
Abstract: Ordinal regression bridges regression and classification by assigning objects to ordered classes. While human experts rely on discriminative patch-level features for decisions, current approaches are limited by the availability of only image-level ordinal labels, overlooking fine-grained patch-level characteristics. In this paper, we propose a Dual-level Fuzzy Learning with Patch Guidance framework, named DFPG that learns precise feature-based grading boundaries from ambiguous ordinal labels, with patch-level supervision. Specifically, we propose patch-labeling and filtering strategies to enable the model to focus on patch-level features exclusively with only image-level ordinal labels available. We further design a dual-level fuzzy learning module, which leverages fuzzy logic to quantitatively capture and handle label ambiguity from both patch-wise and channel-wise perspectives. Extensive experiments on various image ordinal regression datasets demonstrate the superiority of our proposed method, further confirming its ability in distinguishing samples from difficult-to-classify categories. The code is available at https://github.com/ZJUMAI/DFPG-ord.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
<link>https://arxiv.org/abs/2505.05845</link>
<guid>https://arxiv.org/abs/2505.05845</guid>
<content:encoded><![CDATA[
<div> keyword: knots, wood, detection, machine learning, automation
Summary: 
The paper introduces a novel automated pipeline for knot detection and pairing in wood, addressing the labor-intensive and inefficient manual annotation process. Utilizing high-resolution surface images of wooden boards and machine learning techniques, the pipeline achieves a high detection accuracy using YOLOv8l with an mAP@0.5 of 0.887. In the pairing stage, knots are analyzed and paired based on multidimensional feature extraction and a triplet neural network to map features into a latent space, enabling clustering algorithms to identify corresponding knots with a pairing accuracy of 0.85. The experiments highlight the importance of distances from knot start and end points to the bottom of the wooden board, as well as longitudinal coordinates, in achieving high pairing accuracy. The study showcases the potential of AI in advancing wood science and industry.<br /><br />Summary: <div>
arXiv:2505.05845v1 Announce Type: new 
Abstract: Knots in wood are critical to both aesthetics and structural integrity, making their detection and pairing essential in timber processing. However, traditional manual annotation was labor-intensive and inefficient, necessitating automation. This paper proposes a lightweight and fully automated pipeline for knot detection and pairing based on machine learning techniques. In the detection stage, high-resolution surface images of wooden boards were collected using industrial-grade cameras, and a large-scale dataset was manually annotated and preprocessed. After the transfer learning, the YOLOv8l achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were analyzed and paired based on multidimensional feature extraction. A triplet neural network was used to map the features into a latent space, enabling clustering algorithms to identify and pair corresponding knots. The triplet network with learnable weights achieved a pairing accuracy of 0.85. Further analysis revealed that he distances from the knot's start and end points to the bottom of the wooden board, and the longitudinal coordinates play crucial roles in achieving high pairing accuracy. Our experiments validate the effectiveness of the proposed solution, demonstrating the potential of AI in advancing wood science and industry.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects</title>
<link>https://arxiv.org/abs/2505.05848</link>
<guid>https://arxiv.org/abs/2505.05848</guid>
<content:encoded><![CDATA[
<div> dataset, refractive, reflective, 3D reconstruction, neural rendering

Summary:
The article introduces a new synthetic dataset called RefRef for reconstructing scenes with refractive and reflective objects from posed images. The dataset includes 50 objects of varying complexity placed in different background types, resulting in 150 scenes. An oracle method is proposed to calculate accurate light paths for neural rendering based on object geometry and refractive indices. The performance of several state-of-the-art methods is benchmarked against the oracle, showing significant lag in performance. The challenges of handling refractive and reflective materials in 3D reconstruction and novel view synthesis are highlighted by the dataset and benchmark results. <div>
arXiv:2505.05848v1 Announce Type: new 
Abstract: Modern 3D reconstruction and novel view synthesis approaches have demonstrated strong performance on scenes with opaque Lambertian objects. However, most assume straight light paths and therefore cannot properly handle refractive and reflective materials. Moreover, datasets specialized for these effects are limited, stymieing efforts to evaluate performance and develop suitable techniques. In this work, we introduce a synthetic RefRef dataset and benchmark for reconstructing scenes with refractive and reflective objects from posed images. Our dataset has 50 such objects of varying complexity, from single-material convex shapes to multi-material non-convex shapes, each placed in three different background types, resulting in 150 scenes. We also propose an oracle method that, given the object geometry and refractive indices, calculates accurate light paths for neural rendering, and an approach based on this that avoids these assumptions. We benchmark these against several state-of-the-art methods and show that all methods lag significantly behind the oracle, highlighting the challenges of the task and dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PICD: Versatile Perceptual Image Compression with Diffusion Rendering</title>
<link>https://arxiv.org/abs/2505.05853</link>
<guid>https://arxiv.org/abs/2505.05853</guid>
<content:encoded><![CDATA[
<div> compression, image, text, diffusion, rendering

Summary:
- The article introduces a new perceptual image compression method called PICD that works effectively for both screen and natural images.
- PICD utilizes a diffusion rendering approach where text and image are encoded separately and combined using a diffusion model.
- Three levels of conditional information integration are incorporated into the diffusion models: domain level, adaptor level, and instance level.
- By fine-tuning the base diffusion model with text content prompts and efficiently controlling the model using compressed image and text input, PICD achieves high text accuracy and perceptual quality.
- Additionally, the PICD codec can serve as a high-quality perceptual codec for natural images without the need for text conditions. 

<br /><br />Summary: <div>
arXiv:2505.05853v1 Announce Type: new 
Abstract: Recently, perceptual image compression has achieved significant advancements, delivering high visual quality at low bitrates for natural images. However, for screen content, existing methods often produce noticeable artifacts when compressing text. To tackle this challenge, we propose versatile perceptual screen image compression with diffusion rendering (PICD), a codec that works well for both screen and natural images. More specifically, we propose a compression framework that encodes the text and image separately, and renders them into one image using diffusion model. For this diffusion rendering, we integrate conditional information into diffusion models at three distinct levels: 1). Domain level: We fine-tune the base diffusion model using text content prompts with screen content. 2). Adaptor level: We develop an efficient adaptor to control the diffusion model using compressed image and text as input. 3). Instance level: We apply instance-wise guidance to further enhance the decoding process. Empirically, our PICD surpasses existing perceptual codecs in terms of both text accuracy and perceptual quality. Additionally, without text conditions, our approach serves effectively as a perceptual codec for natural images.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations</title>
<link>https://arxiv.org/abs/2505.05855</link>
<guid>https://arxiv.org/abs/2505.05855</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, multi-contrast, super-resolution, unpaired, neural representations

Summary:
The article introduces a Modular Multi-Contrast Super-Resolution (MCSR) framework for enhancing MRI quality. It addresses the challenge of cross-modal enhancement by decoupling the MCSR task into two stages: Unpaired Cross-Modal Synthesis (U-CMS) and Unsupervised Super-Resolution (U-SR). This framework eliminates the need for paired training data and supports arbitrary upscaling. The U-CMS stage translates high-resolution reference modalities into synthesized versions of the target contrast, while the U-SR stage reconstructs the final output using implicit neural representations conditioned on spatial coordinates. The proposed method achieves superior performance at 4x and 8x upscaling, with improved fidelity and anatomical consistency compared to existing methods. The framework shows promise for scalable, subject-specific, and data-efficient MCSR in clinical settings. 

<br /><br />Summary: <div>
arXiv:2505.05855v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is often limited by long acquisition times and low signal-to-noise ratios, especially in modalities like diffusion and functional MRI. The multi-contrast nature of MRI presents a valuable opportunity for cross-modal enhancement, where high-resolution (HR) modalities can serve as references to boost the quality of their low-resolution (LR) counterparts-motivating the development of Multi-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that leveraging complementary contrasts can improve SR performance; however, effective feature extraction and fusion across modalities with varying resolutions remains a major challenge. Moreover, existing MCSR methods often assume fixed resolution settings and all require large, perfectly paired training datasets-conditions rarely met in real-world clinical environments. To address these challenges, we propose a novel Modular Multi-Contrast Super-Resolution (MCSR) framework that eliminates the need for paired training data and supports arbitrary upscaling. Our method decouples the MCSR task into two stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a high-resolution reference modality into a synthesized version of the target contrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the final output using implicit neural representations (INRs) conditioned on spatial coordinates. This design enables scale-agnostic and anatomically faithful reconstruction by bridging un-paired cross-modal synthesis with unsupervised resolution enhancement. Experiments show that our method achieves superior performance at 4x and 8x upscaling, with improved fidelity and anatomical consistency over existing baselines. Our framework demonstrates strong potential for scalable, subject-specific, and data-efficient MCSR in real-world clinical settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Facial Image Compression with Consistency Preserving Diffusion Prior</title>
<link>https://arxiv.org/abs/2505.05870</link>
<guid>https://arxiv.org/abs/2505.05870</guid>
<content:encoded><![CDATA[
<div> Facial Image Compression, Diffusion Prior, High-Frequency Information, Visual Quality, Machine Vision Accuracy
Summary:<br /><br />Facial Image Compression with a Stable Diffusion Prior (FaSDiff) addresses the issue of unsatisfactory reconstructed image quality in existing learned face image compression methods at low bit rates. By using a high-frequency-sensitive compressor and a hybrid low-frequency enhancement module, FaSDiff captures fine image details and disentangles low-frequency facial semantics to improve visual quality and machine vision accuracy. The method preserves consistency through frequency enhancement, leveraging diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments demonstrate that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code for FaSDiff will be released after the paper is accepted. <div>
arXiv:2505.05870v1 Announce Type: new 
Abstract: With the widespread application of facial image data across various domains, the efficient storage and transmission of facial images has garnered significant attention. However, the existing learned face image compression methods often produce unsatisfactory reconstructed image quality at low bit rates. Simply adapting diffusion-based compression methods to facial compression tasks results in reconstructed images that perform poorly in downstream applications due to insufficient preservation of high-frequency information. To further explore the diffusion prior in facial image compression, we propose Facial Image Compression with a Stable Diffusion Prior (FaSDiff), a method that preserves consistency through frequency enhancement. FaSDiff employs a high-frequency-sensitive compressor in an end-to-end framework to capture fine image details and produce robust visual prompts. Additionally, we introduce a hybrid low-frequency enhancement module that disentangles low-frequency facial semantics and stably modulates the diffusion prior alongside visual prompts. The proposed modules allow FaSDiff to leverage diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments show that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code will be released after the paper is accepted.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Register and CLS tokens yield a decoupling of local and global features in large ViTs</title>
<link>https://arxiv.org/abs/2505.05892</link>
<guid>https://arxiv.org/abs/2505.05892</guid>
<content:encoded><![CDATA[
<div> Keywords: DINOv2 model, attention maps, register tokens, global image information, interpretability<br />
<br />
Summary: <br />
Recent research has uncovered issues with attention maps in the DINOv2 model, where artifacts hinder interpretability and performance. The problem arises from the model using patch tokens for global image information, resulting in inaccuracies. To rectify this, register tokens are introduced to store global information separately. However, it was found that this leads to a dominance of global information from register tokens, causing a disconnect between local and global features. Surprisingly, the CLS token also exhibits similar behavior in models lacking explicit register tokens. This study highlights the importance of careful attention map interpretation in large vision models and suggests a pathway towards more interpretable models by addressing the issues related to register and CLS tokens. <div>
arXiv:2505.05892v1 Announce Type: new 
Abstract: Recent work has shown that the attention maps of the widely popular DINOv2 model exhibit artifacts, which hurt both model interpretability and performance on dense image tasks. These artifacts emerge due to the model repurposing patch tokens with redundant local information for the storage of global image information. To address this problem, additional register tokens have been incorporated in which the model can store such information instead. We carefully examine the influence of these register tokens on the relationship between global and local image features, showing that while register tokens yield cleaner attention maps, these maps do not accurately reflect the integration of local image information in large models. Instead, global information is dominated by information extracted from register tokens, leading to a disconnect between local and global features. Inspired by these findings, we show that the CLS token itself, which can be interpreted as a register, leads to a very similar phenomenon in models without explicit register tokens. Our work shows that care must be taken when interpreting attention maps of large ViTs. Further, by clearly attributing the faulty behaviour to register and CLS tokens, we show a path towards more interpretable vision models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI</title>
<link>https://arxiv.org/abs/2505.05895</link>
<guid>https://arxiv.org/abs/2505.05895</guid>
<content:encoded><![CDATA[
<div> Dataset, vision-language framework, automotive infotainment systems, adaptation, Molmo-7B model

Summary:
Automotive infotainment systems require intelligent solutions for handling UI updates and design variations. A vision-language framework is introduced to understand and interact with these systems, along with the release of an open-source dataset called AutomotiveUI-Bench-4K. A synthetic data pipeline is presented for generating training data, and a Molmo-7B model is fine-tuned using Low-Rank Adaptation (LoRa). The resulting Evaluative Large Action Model (ELAM) achieves strong performance on the dataset, with a +5.2% improvement on ScreenSpot compared to the baseline model. Despite being trained for infotainment, ELAM achieves high accuracy on ScreenSpot, matching specialized models for other UI domains. This research showcases how data collection and fine-tuning can drive AI progress in automotive UI understanding, with a cost-efficient approach that allows deployment on consumer-grade GPUs.

Summary: <div>
arXiv:2505.05895v1 Announce Type: new 
Abstract: Modern automotive infotainment systems require intelligent and adaptive solutions to handle frequent User Interface (UI) updates and diverse design variations. We introduce a vision-language framework for understanding and interacting with automotive infotainment systems, enabling seamless adaptation across different UI designs. To further support research in this field, we release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208 annotations. Additionally, we present a synthetic data pipeline to generate training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation (LoRa) and incorporating reasoning generated by our pipeline, along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face) and demonstrating strong cross-domain generalization, including a +5.2% improvement on ScreenSpot over the baseline model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot, closely matching or even surpassing specialized models for desktop, mobile, and web, such as ShowUI, despite being trained for the infotainment domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven progress within automotive UI understanding and interaction. The applied method is cost-efficient and fine-tuned models can be deployed on consumer-grade GPUs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.05901</link>
<guid>https://arxiv.org/abs/2505.05901</guid>
<content:encoded><![CDATA[
<div> Anomaly detection, Mechanics Complementary framework, Corrective forces, Diverse Anomaly-Generation module, Corrective Force Prediction Network, hierarchical quality control strategy<br />
<br />
Summary: 
This paper introduces a new approach to anomaly detection that considers the underlying causes of anomalies and generates corrective forces for each point. The Mechanics Complementary framework for 3D anomaly detection (MC4AD) is proposed, along with a Diverse Anomaly-Generation module to simulate various anomalies. A Corrective Force Prediction Network (CFP-Net) is developed to predict internal and external corrective forces for each point. A combined loss function is introduced to properly constrain the corrective forces. The study also presents a hierarchical quality control strategy based on a three-way decision and introduces a new dataset, Anomaly-IntraVariance, for model evaluation. Nine state-of-the-art performers were achieved on the proposed and existing datasets with minimal parameters and fast inference speed. <div>
arXiv:2505.05901v1 Announce Type: new 
Abstract: In this paper, we go beyond identifying anomalies only in structural terms and think about better anomaly detection motivated by anomaly causes. Most anomalies are regarded as the result of unpredictable defective forces from internal and external sources, and their opposite forces are sought to correct the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly detection (MC4AD) to generate internal and external Corrective forces for each point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to simulate various anomalies. Then, we present a Corrective Force Prediction Network (CFP-Net) with complementary representations for point-level representation to simulate the different contributions of internal and external corrective forces. A combined loss was proposed, including a new symmetric loss and an overall loss, to constrain the corrective forces properly. As a highlight, we consider 3D anomaly detection in industry more comprehensively, creating a hierarchical quality control strategy based on a three-way decision and contributing a dataset named Anomaly-IntraVariance with intraclass variance to evaluate the model. On the proposed and existing five datasets, we obtained nine state-of-the-art performers with the minimum parameters and the fastest inference speed. The source is available at https://github.com/hzzzzzhappy/MC4AD
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFEN: Dual Feature Equalization Network for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.05913</link>
<guid>https://arxiv.org/abs/2505.05913</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, dual feature equalization network, Swin Transformer, convolutional neural network, state-of-the-art performance

Summary:
The paper introduces a novel method for medical image segmentation, the dual feature equalization network, which combines the advantages of Swin Transformer and Convolutional Neural Network. It addresses the issue of unequal contextual feature information at image boundaries and low-class pixel regions by proposing image-level and class-level feature equalization modules. These modules enhance pixel feature representations by equalizing contextual information within the image and aggregating regions of the same class. By utilizing Swin Transformer for encoding and decoding, the model effectively captures long-range dependencies and spatial correlations. Experimental results on various datasets show superior performance, achieving state-of-the-art results. The code for the proposed method is publicly available, showcasing the reproducibility and potential for further research and development in medical image segmentation. 
<br /><br />Summary: <div>
arXiv:2505.05913v1 Announce Type: new 
Abstract: Current methods for medical image segmentation primarily focus on extracting contextual feature information from the perspective of the whole image. While these methods have shown effective performance, none of them take into account the fact that pixels at the boundary and regions with a low number of class pixels capture more contextual feature information from other classes, leading to misclassification of pixels by unequal contextual feature information. In this paper, we propose a dual feature equalization network based on the hybrid architecture of Swin Transformer and Convolutional Neural Network, aiming to augment the pixel feature representations by image-level equalization feature information and class-level equalization feature information. Firstly, the image-level feature equalization module is designed to equalize the contextual information of pixels within the image. Secondly, we aggregate regions of the same class to equalize the pixel feature representations of the corresponding class by class-level feature equalization module. Finally, the pixel feature representations are enhanced by learning weights for image-level equalization feature information and class-level equalization feature information. In addition, Swin Transformer is utilized as both the encoder and decoder, thereby bolstering the ability of the model to capture long-range dependencies and spatial correlations. We conducted extensive experiments on Breast Ultrasound Images (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated Cardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental results demonstrate that our method have achieved state-of-the-art performance. Our code is publicly available at https://github.com/JianJianYin/DFEN.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking</title>
<link>https://arxiv.org/abs/2505.05936</link>
<guid>https://arxiv.org/abs/2505.05936</guid>
<content:encoded><![CDATA[
arXiv:2505.05936v1 Announce Type: new 
Abstract: Recent advancements in visual object tracking have markedly improved the capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical component in real-world robotics applications. While the integration of hierarchical lightweight networks has become a prevalent strategy for enhancing efficiency in UAV tracking, it often results in a significant drop in network capacity, which further exacerbates challenges in UAV scenarios, such as frequent occlusions and extreme changes in viewing angles. To address these issues, we introduce a novel family of UAV trackers, termed CGTrack, which combines explicit and implicit techniques to expand network capacity within a coarse-to-fine framework. Specifically, we first introduce a Hierarchical Feature Cascade (HFC) module that leverages the spirit of feature reuse to increase network capacity by integrating the deep semantic cues with the rich spatial information, incurring minimal computational costs while enhancing feature representation. Based on this, we design a novel Lightweight Gated Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented coordinates from previously expanded features, which contain dense local discriminative information. Extensive experiments on three challenging UAV tracking benchmarks demonstrate that CGTrack achieves state-of-the-art performance while running fast. Code will be available at https://github.com/Nightwatch-Fox11/CGTrack.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving 3D Attention via Triplet Squeeze and Excitation Block</title>
<link>https://arxiv.org/abs/2505.05943</link>
<guid>https://arxiv.org/abs/2505.05943</guid>
<content:encoded><![CDATA[
arXiv:2505.05943v1 Announce Type: new 
Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and structural suitability of CNN-based models for vision tasks, re-establishing them as key players in image classification in general, and in facial expression recognition (FER) in particular. In this paper, we propose a new set of models that build on these advancements by incorporating a new set of attention mechanisms that combines Triplet attention with Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the effectiveness of these variants by applying them to the ResNet18, DenseNet and ConvNext architectures to validate their versatility and impact. Our study shows that incorporating a TripSE block in these CNN models boosts their performances, particularly for the ConvNeXt architecture, indicating its utility. We evaluate the proposed mechanisms and associated models across four datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where ConvNext with TripSE achieves state-of-the-art results with an accuracy of \textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition</title>
<link>https://arxiv.org/abs/2505.06002</link>
<guid>https://arxiv.org/abs/2505.06002</guid>
<content:encoded><![CDATA[
arXiv:2505.06002v1 Announce Type: new 
Abstract: Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained image models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on 5 benchmarks consistently. The code is open-sourced at https://github.com/Jaulin-Bage/Task-Adapter-pp.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection</title>
<link>https://arxiv.org/abs/2505.06003</link>
<guid>https://arxiv.org/abs/2505.06003</guid>
<content:encoded><![CDATA[
arXiv:2505.06003v1 Announce Type: new 
Abstract: Understanding the decision-making process of machine learning models provides valuable insights into the task, the data, and the reasons behind a model's failures. In this work, we propose a method that performs inherently interpretable predictions through the instance-wise sparsification of input images. To align the sparsification with human perception, we learn the masking in the space of semantically meaningful pixel regions rather than on pixel-level. Additionally, we introduce an explicit way to dynamically determine the required level of sparsity for each instance. We show empirically on semi-synthetic and natural image datasets that our inherently interpretable classifier produces more meaningful, human-understandable predictions than state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Image Rectification Bases on Self-Adaptive Multitask Fusion</title>
<link>https://arxiv.org/abs/2505.06038</link>
<guid>https://arxiv.org/abs/2505.06038</guid>
<content:encoded><![CDATA[
arXiv:2505.06038v1 Announce Type: new 
Abstract: Deformed document image rectification is essential for real-world document understanding tasks, such as layout analysis and text recognition. However, current multi-task methods -- such as background removal, 3D coordinate prediction, and text line segmentation -- often overlook the complementary features between tasks and their interactions. To address this gap, we propose a self-adaptive learnable multi-task fusion rectification network named SalmRec. This network incorporates an inter-task feature aggregation module that adaptively improves the perception of geometric distortions, enhances feature complementarity, and reduces negative interference. We also introduce a gating mechanism to balance features both within global tasks and between local tasks effectively. Experimental results on two English benchmarks (DIR300 and DocUNet) and one Chinese benchmark (DocReal) demonstrate that our method significantly improves rectification performance. Ablation studies further highlight the positive impact of different tasks on dewarping and the effectiveness of our proposed module.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Cephalometric Landmark Detection with Diffusion Data Generation</title>
<link>https://arxiv.org/abs/2505.06055</link>
<guid>https://arxiv.org/abs/2505.06055</guid>
<content:encoded><![CDATA[
arXiv:2505.06055v1 Announce Type: new 
Abstract: Cephalometric landmark detection is essential for orthodontic diagnostics and treatment planning. Nevertheless, the scarcity of samples in data collection and the extensive effort required for manual annotation have significantly impeded the availability of diverse datasets. This limitation has restricted the effectiveness of deep learning-based detection methods, particularly those based on large-scale vision models. To address these challenges, we have developed an innovative data generation method capable of producing diverse cephalometric X-ray images along with corresponding annotations without human intervention. To achieve this, our approach initiates by constructing new cephalometric landmark annotations using anatomical priors. Then, we employ a diffusion-based generator to create realistic X-ray images that correspond closely with these annotations. To achieve precise control in producing samples with different attributes, we introduce a novel prompt cephalometric X-ray image dataset. This dataset includes real cephalometric X-ray images and detailed medical text prompts describing the images. By leveraging these detailed prompts, our method improves the generation process to control different styles and attributes. Facilitated by the large, diverse generated data, we introduce large-scale vision detection models into the cephalometric landmark detection task to improve accuracy. Experimental results demonstrate that training with the generated data substantially enhances the performance. Compared to methods without using the generated data, our approach improves the Success Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and data are available at: https://um-lab.github.io/cepha-generation
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation</title>
<link>https://arxiv.org/abs/2505.06068</link>
<guid>https://arxiv.org/abs/2505.06068</guid>
<content:encoded><![CDATA[
arXiv:2505.06068v1 Announce Type: new 
Abstract: Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at GitHub.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.06113</link>
<guid>https://arxiv.org/abs/2505.06113</guid>
<content:encoded><![CDATA[
arXiv:2505.06113v1 Announce Type: new 
Abstract: Autonomous vehicle perception systems have traditionally relied on costly LiDAR sensors to generate precise environmental representations. In this paper, we propose a camera-only perception framework that produces Bird's Eye View (BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines YOLOv11-based object detection with DepthAnythingV2 monocular depth estimation across multi-camera inputs to achieve comprehensive 360-degree scene understanding. We evaluate our approach on the OpenLane-V2 and NuScenes datasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle detection rates when compared against LiDAR ground truth, with average positional errors limited to 1.2 meters. These results highlight the potential of deep learning to extract rich spatial information using only camera inputs, enabling cost-efficient autonomous navigation without sacrificing accuracy.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation</title>
<link>https://arxiv.org/abs/2505.06117</link>
<guid>https://arxiv.org/abs/2505.06117</guid>
<content:encoded><![CDATA[
arXiv:2505.06117v1 Announce Type: new 
Abstract: Accurate defect detection of photovoltaic (PV) cells is critical for ensuring quality and efficiency in intelligent PV manufacturing systems. However, the scarcity of rich defect data poses substantial challenges for effective model training. While existing methods have explored generative models to augment datasets, they often suffer from instability, limited diversity, and domain shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image Generator based on Stable Diffusion (SD). PDIG leverages the strong priors learned from large-scale datasets to enhance generation quality under limited data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that incorporates text-conditioned priors to capture the relational concepts between defect types and their appearances. To further enrich the domain distribution, we design a Lightweight Industrial Style Adaptor (LISA), which injects industrial defect characteristics into the SD model through cross-disentangled attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC) module, enforcing the quality of generated images via positional consistency and spatial smoothing alignment. Extensive experiments demonstrate that PDIG achieves superior realism and diversity compared to state-of-the-art methods. Specifically, our approach improves Frechet Inception Distance (FID) by 19.16 points over the second-best method and significantly enhances the performance of downstream defect detection tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation</title>
<link>https://arxiv.org/abs/2505.06133</link>
<guid>https://arxiv.org/abs/2505.06133</guid>
<content:encoded><![CDATA[
arXiv:2505.06133v1 Announce Type: new 
Abstract: The segmentation of substantial brain lesions is a significant and challenging task in the field of medical image segmentation. Substantial brain lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries between lesion regions and normal brain tissue. Small lesions in single slices are difficult to identify, making the accurate and reproducible segmentation of abnormal regions, as well as their feature description, highly complex. Existing methods have the following limitations: 1) They rely solely on single-modal information for learning, neglecting the multi-modal information commonly used in diagnosis. This hampers the ability to comprehensively acquire brain lesion information from multiple perspectives and prevents the effective integration and utilization of multi-modal data inputs, thereby limiting a holistic understanding of lesions. 2) They are constrained by the amount of data available, leading to low sensitivity to small lesions and difficulty in detecting subtle pathological changes. 3) Current SAM-based models rely on external prompts, which cannot achieve automatic segmentation and, to some extent, affect diagnostic efficiency.To address these issues, we have developed a large-scale fully automated segmentation model specifically designed for brain lesion segmentation, named BrainSegDMLF. This model has the following features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and integrates multi-modal data during the encoding process, providing the SAM encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling Decoder, enabling the model to extract rich low-level and high-level features even with limited data, thereby detecting the presence of small lesions. 3) Automatic segmentation masks, allowing the model to generate lesion masks automatically without requiring manual prompts.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks</title>
<link>https://arxiv.org/abs/2505.06152</link>
<guid>https://arxiv.org/abs/2505.06152</guid>
<content:encoded><![CDATA[
arXiv:2505.06152v1 Announce Type: new 
Abstract: Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped, primarily due to less specialized text descriptions in current dermatology multimodal datasets. To address this issue, we propose MM-Skin, the first large-scale multimodal dermatology dataset that encompasses 3 imaging modalities, including clinical, dermoscopic, and pathological and nearly 10k high-quality image-text pairs collected from professional textbooks. In addition, we generate over 27k diverse, instruction-following vision question answering (VQA) samples (9 times the size of current largest dermatology VQA dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a dermatology-specific VLM designed for precise and nuanced skin disease interpretation. Comprehensive benchmark evaluations of SkinVL on VQA, supervised fine-tuning (SFT) and zero-shot classification tasks across 8 datasets, reveal its exceptional performance for skin diseases in comparison to both general and medical VLM models. The introduction of MM-Skin and SkinVL offers a meaningful contribution to advancing the development of clinical dermatology VLM assistants. MM-Skin is available at https://github.com/ZwQ803/MM-Skin
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06166</link>
<guid>https://arxiv.org/abs/2505.06166</guid>
<content:encoded><![CDATA[
arXiv:2505.06166v1 Announce Type: new 
Abstract: We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at https://radualexandru.github.io/difflocks/
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting a Segmentation Foundation Model for Medical Image Classification</title>
<link>https://arxiv.org/abs/2505.06217</link>
<guid>https://arxiv.org/abs/2505.06217</guid>
<content:encoded><![CDATA[
arXiv:2505.06217v1 Announce Type: new 
Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have shown strong performance in various vision tasks, particularly image segmentation, due to their impressive zero-shot segmentation capabilities. However, effectively adapting such models for medical image classification is still a less explored topic. In this paper, we introduce a new framework to adapt SAM for medical image classification. First, we utilize the SAM image encoder as a feature extractor to capture segmentation-based features that convey important spatial and contextual details of the image, while freezing its weights to avoid unnecessary overhead during training. Next, we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to compute spatially localized attention weights for the feature maps. The features extracted from SAM's image encoder are processed through SLCA to compute attention weights, which are then integrated into deep learning classification models to enhance their focus on spatially relevant or meaningful regions of the image, thus improving classification performance. Experimental results on three public medical image classification datasets demonstrate the effectiveness and data-efficiency of our approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction</title>
<link>https://arxiv.org/abs/2505.06219</link>
<guid>https://arxiv.org/abs/2505.06219</guid>
<content:encoded><![CDATA[
arXiv:2505.06219v1 Announce Type: new 
Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using minimal resources, time, or number of captures to enable efficient 3D reconstruction of a scene. Existing approaches often rely on prior scene knowledge or additional image captures and often develop policies that maximize coverage. Yet, for many real scenes with complex geometry and self-occlusions, coverage maximization does not lead to better reconstruction quality directly. In this paper, we propose the View Introspection Network (VIN), which is trained to predict the reconstruction quality improvement of views directly, and the VIN-NBV policy. A greedy sequential sampling-based policy, where at each acquisition step, we sample multiple query views and choose the one with the highest VIN predicted improvement score. We design the VIN to perform 3D-aware featurization of the reconstruction built from prior acquisitions, and for each query view create a feature that can be decoded into an improvement score. We then train the VIN using imitation learning to predict the reconstruction improvement score. We show that VIN-NBV improves reconstruction quality by ~30% over a coverage maximization baseline when operating with constraints on the number of acquisitions or the time in motion.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network</title>
<link>https://arxiv.org/abs/2505.05477</link>
<guid>https://arxiv.org/abs/2505.05477</guid>
<content:encoded><![CDATA[
arXiv:2505.05477v1 Announce Type: cross 
Abstract: Electrocardiogram (ECG) signals are frequently corrupted by noise, such as baseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which significantly degrade their diagnostic utility. To address this issue, we propose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a Double Recurrent Dense Network architecture. In contrast to traditional approaches, we introduce a double recurrent scheme to enhance information reuse from both ECG waveforms and the estimated clean image. For ECG waveform processing, our basic model employs LSTM layers cascaded with DenseNet blocks. The estimated clean ECG image, obtained by subtracting predicted noise components from the noisy input, is iteratively fed back into the model. This dual recurrent architecture enables comprehensive utilization of both temporal waveform features and spatial image details, leading to more effective noise suppression. Experimental results on the MIT-BIH dataset demonstrate that our method achieves superior performance compared to conventional image denoising methods in terms of PSNR and SSIM while also surpassing classical ECG denoising techniques in both SNR and RMSE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Restoration via Multi-domain Learning</title>
<link>https://arxiv.org/abs/2505.05504</link>
<guid>https://arxiv.org/abs/2505.05504</guid>
<content:encoded><![CDATA[
arXiv:2505.05504v1 Announce Type: cross 
Abstract: Due to adverse atmospheric and imaging conditions, natural images suffer from various degradation phenomena. Consequently, image restoration has emerged as a key solution and garnered substantial attention. Although recent Transformer architectures have demonstrated impressive success across various restoration tasks, their considerable model complexity poses significant challenges for both training and real-time deployment. Furthermore, instead of investigating the commonalities among different degradations, most existing restoration methods focus on modifying Transformer under limited restoration priors. In this work, we first review various degradation phenomena under multi-domain perspective, identifying common priors. Then, we introduce a novel restoration framework, which integrates multi-domain learning into Transformer. Specifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain structure that facilitates local-region-global multi-receptive field modeling to replace vanilla self-attention. Additionally, in Feed-Forward Network, we incorporate multi-scale learning to fuse multi-domain features at different resolutions. Comprehensive experimental results across ten restoration tasks, such as dehazing, desnowing, motion deblurring, defocus deblurring, rain streak/raindrop removal, cloud removal, shadow removal, underwater enhancement and low-light enhancement, demonstrate that our proposed model outperforms state-of-the-art methods and achieves a favorable trade-off among restoration performance, parameter size, computational cost and inference latency. The code is available at: https://github.com/deng-ai-lab/SWFormer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2505.05509</link>
<guid>https://arxiv.org/abs/2505.05509</guid>
<content:encoded><![CDATA[
arXiv:2505.05509v1 Announce Type: cross 
Abstract: Stereo image super-resolution (SSR) aims to enhance high-resolution details by leveraging information from stereo image pairs. However, existing stereo super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook cross-view geometric consistency and are limited to fixed-scale upsampling. The key issue is that previous upsampling methods use convolution to independently process deep features of different views, lacking cross-view and non-local information perception, making it difficult to select beneficial information from multi-view scenes adaptively. In this work, we propose Stereo Implicit Neural Representation (StereoINR), which innovatively models stereo image pairs as continuous implicit representations. This continuous representation breaks through the scale limitations, providing a unified solution for arbitrary-scale stereo super-resolution reconstruction of left-right views. Furthermore, by incorporating spatial warping and cross-attention mechanisms, StereoINR enables effective cross-view information fusion and achieves significant improvements in pixel-level geometric consistency. Extensive experiments across multiple datasets show that StereoINR outperforms out-of-training-distribution scale upsampling and matches state-of-the-art SSR methods within training-distribution scales.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train Your Metamorphic Deep Neural Network</title>
<link>https://arxiv.org/abs/2505.05510</link>
<guid>https://arxiv.org/abs/2505.05510</guid>
<content:encoded><![CDATA[
arXiv:2505.05510v1 Announce Type: cross 
Abstract: Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural networks of varying width and depth. Based on Implicit Neural Representation (INR), NeuMeta learns a continuous weight manifold, enabling the direct generation of compressed models, including those with configurations not seen during training. While promising, the original formulation of NeuMeta proves effective only for the final layers of the undelying model, limiting its broader applicability. In this work, we propose a training algorithm that extends the capabilities of NeuMeta to enable full-network metamorphosis with minimal accuracy degradation. Our approach follows a structured recipe comprising block-wise incremental training, INR initialization, and strategies for replacing batch normalization. The resulting metamorphic networks maintain competitive accuracy across a wide range of compression ratios, offering a scalable solution for adaptable and efficient deployment of deep models. The code is available at: https://github.com/TSommariva/HTTY_NeuMeta.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility</title>
<link>https://arxiv.org/abs/2505.05518</link>
<guid>https://arxiv.org/abs/2505.05518</guid>
<content:encoded><![CDATA[
arXiv:2505.05518v1 Announce Type: cross 
Abstract: Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control.
  A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage.
  Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy.
  Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive Anywhere with Model-Based Reannotation11</title>
<link>https://arxiv.org/abs/2505.05592</link>
<guid>https://arxiv.org/abs/2505.05592</guid>
<content:encoded><![CDATA[
arXiv:2505.05592v1 Announce Type: cross 
Abstract: Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Self-supervised MRI Denoising</title>
<link>https://arxiv.org/abs/2505.05631</link>
<guid>https://arxiv.org/abs/2505.05631</guid>
<content:encoded><![CDATA[
arXiv:2505.05631v1 Announce Type: cross 
Abstract: Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized denoising score matching (GDSM) loss, which extends denoising score matching to work directly with noisy observations by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes</title>
<link>https://arxiv.org/abs/2505.05643</link>
<guid>https://arxiv.org/abs/2505.05643</guid>
<content:encoded><![CDATA[
arXiv:2505.05643v1 Announce Type: cross 
Abstract: Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New k-Space Model for Non-Cartesian Fourier Imaging</title>
<link>https://arxiv.org/abs/2505.05647</link>
<guid>https://arxiv.org/abs/2505.05647</guid>
<content:encoded><![CDATA[
arXiv:2505.05647v1 Announce Type: cross 
Abstract: For the past several decades, it has been popular to reconstruct Fourier imaging data using model-based approaches that can easily incorporate physical constraints and advanced regularization/machine learning priors. The most common modeling approach is to represent the continuous image as a linear combination of shifted "voxel" basis functions. Although well-studied and widely-deployed, this voxel-based model is associated with longstanding limitations, including high computational costs, slow convergence, and a propensity for artifacts. In this work, we reexamine this model from a fresh perspective, identifying new issues that may have been previously overlooked (including undesirable approximation, periodicity, and nullspace characteristics). Our insights motivate us to propose a new model that is more resilient to the limitations (old and new) of the previous approach. Specifically, the new model is based on a Fourier-domain basis expansion rather than the standard image-domain voxel-based approach. Illustrative results, which are presented in the context of non-Cartesian MRI reconstruction, demonstrate that the new model enables improved image quality (reduced artifacts) and/or reduced computational complexity (faster computations and improved convergence).
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models</title>
<link>https://arxiv.org/abs/2505.05659</link>
<guid>https://arxiv.org/abs/2505.05659</guid>
<content:encoded><![CDATA[
arXiv:2505.05659v1 Announce Type: cross 
Abstract: EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for granted the inter-channel relationships. This paper introduces vector-valued EfficientNets (V-EfficientNets), a novel extension of EfficientNet designed to process arbitrary vector-valued data. The proposed models are evaluated on a medical image classification task, achieving an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency, significantly reducing parameters while outperforming state-of-the-art models, including the original EfficientNet. The source code is available at https://github.com/mevalle/v-nets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology</title>
<link>https://arxiv.org/abs/2505.05689</link>
<guid>https://arxiv.org/abs/2505.05689</guid>
<content:encoded><![CDATA[
arXiv:2505.05689v1 Announce Type: cross 
Abstract: Histopathology evaluation of tissue specimens through microscopic examination is essential for accurate disease diagnosis and prognosis. However, traditional manual analysis by specially trained pathologists is time-consuming, labor-intensive, cost-inefficient, and prone to inter-rater variability, potentially affecting diagnostic consistency and accuracy. As digital pathology images continue to proliferate, there is a pressing need for automated analysis to address these challenges. Recent advancements in artificial intelligence-based tools such as machine learning (ML) models, have significantly enhanced the precision and efficiency of analyzing histopathological slides. However, despite their impressive performance, ML models are invariant only to translation, lacking invariance to rotation and reflection. This limitation restricts their ability to generalize effectively, particularly in histopathology, where images intrinsically lack meaningful orientation. In this study, we develop robust, equivariant histopathological biomarkers through a novel symmetric convolutional kernel via unsupervised segmentation. The approach is validated using prostate tissue micro-array (TMA) images from 50 patients in the Gleason 2019 Challenge public dataset. The biomarkers extracted through this approach demonstrate enhanced robustness and generalizability against rotation compared to models using standard convolution kernels, holding promise for enhancing the accuracy, consistency, and robustness of ML models in digital pathology. Ultimately, this work aims to improve diagnostic and prognostic capabilities of histopathology beyond prostate cancer through equivariant imaging.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference</title>
<link>https://arxiv.org/abs/2505.05703</link>
<guid>https://arxiv.org/abs/2505.05703</guid>
<content:encoded><![CDATA[
arXiv:2505.05703v1 Announce Type: cross 
Abstract: Purpose: Deep learning has demonstrated strong potential for MRI reconstruction, but conventional supervised learning methods require high-quality reference images, which are often unavailable in practice. Self-supervised learning offers an alternative, yet its performance degrades at high acceleration rates. To overcome these limitations, we propose hybrid learning, a novel two-stage training framework that combines self-supervised and supervised learning for robust image reconstruction.
  Methods: Hybrid learning is implemented in two sequential stages. In the first stage, self-supervised learning is employed to generate improved images from noisy or undersampled reference data. These enhanced images then serve as pseudo-ground truths for the second stage, which uses supervised learning to refine reconstruction performance and support higher acceleration rates. We evaluated hybrid learning in two representative applications: (1) accelerated 0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of the brain without access to fully sampled ground truth.
  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image quality over both self-supervised and conventional supervised methods across different acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping, hybrid learning achieved superior T1 quantification accuracy across a wide dynamic range, outperforming self-supervised learning in all tested conditions.
  Conclusions: Hybrid learning provides a practical and effective solution for training deep MRI reconstruction networks when only low-quality or incomplete reference data are available. It enables improved image quality and accurate quantitative mapping across different applications and field strengths, representing a promising technique toward broader clinical deployment of deep learning-based MRI.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05732</link>
<guid>https://arxiv.org/abs/2505.05732</guid>
<content:encoded><![CDATA[
arXiv:2505.05732v1 Announce Type: cross 
Abstract: Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
<link>https://arxiv.org/abs/2505.05736</link>
<guid>https://arxiv.org/abs/2505.05736</guid>
<content:encoded><![CDATA[
arXiv:2505.05736v1 Announce Type: cross 
Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition</title>
<link>https://arxiv.org/abs/2505.05768</link>
<guid>https://arxiv.org/abs/2505.05768</guid>
<content:encoded><![CDATA[
arXiv:2505.05768v1 Announce Type: cross 
Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment in diabetic patients. Treatment responses to intravitreal therapies vary, highlighting the need for patient stratification to predict therapeutic benefits and enable personalized strategies. To our knowledge, this study is the first to explore pre-treatment stratification for predicting DME treatment responses. To advance this research, we organized the 2nd Asia-Pacific Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The competition focused on improving predictive accuracy for anti-VEGF therapy responses using ophthalmic OCT images. We provided a dataset containing tens of thousands of OCT images from 2,000 patients with labels across four sub-tasks. This paper details the competition's structure, dataset, leading methods, and evaluation metrics. The competition attracted strong scientific community participation, with 170 teams initially registering and 41 reaching the final round. The top-performing team achieved an AUC of 80.06%, highlighting the potential of AI in personalized DME treatment and clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes</title>
<link>https://arxiv.org/abs/2505.05798</link>
<guid>https://arxiv.org/abs/2505.05798</guid>
<content:encoded><![CDATA[
arXiv:2505.05798v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using univariate spline compositions without nonlinear activations. In this work, we integrate Error-Correcting Output Codes (ECOC) into the KAN framework to transform multi-class classification into multiple binary tasks, improving robustness via Hamming-distance decoding. Our proposed KAN with ECOC method outperforms vanilla KAN on a challenging blood cell classification dataset, achieving higher accuracy under diverse hyperparameter settings. Ablation studies further confirm that ECOC consistently enhances performance across FastKAN and FasterKAN variants. These results demonstrate that ECOC integration significantly boosts KAN generalizability in critical healthcare AI applications. To the best of our knowledge, this is the first integration of ECOC with KAN for enhancing multi-class medical image classification performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks</title>
<link>https://arxiv.org/abs/2505.05800</link>
<guid>https://arxiv.org/abs/2505.05800</guid>
<content:encoded><![CDATA[
arXiv:2505.05800v1 Announce Type: cross 
Abstract: Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1$\%$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising</title>
<link>https://arxiv.org/abs/2505.05812</link>
<guid>https://arxiv.org/abs/2505.05812</guid>
<content:encoded><![CDATA[
arXiv:2505.05812v1 Announce Type: cross 
Abstract: Breast cancer is the most frequently diagnosed human cancer in the United States at present. Early detection is crucial for its successful treatment. X-ray mammography and digital breast tomosynthesis are currently the main methods for breast cancer screening. However, both have known limitations in terms of their sensitivity and specificity to breast cancers, while also frequently causing patient discomfort due to the requirement for breast compression. Breast computed tomography is a promising alternative, however, to obtain high-quality images, the X-ray dose needs to be sufficiently high. As the breast is highly radiosensitive, dose reduction is particularly important. Phase-contrast computed tomography (PCT) has been shown to produce higher-quality images at lower doses and has no need for breast compression. It is demonstrated in the present study that, when imaging full fresh mastectomy samples with PCT, deep learning-based image denoising can further reduce the radiation dose by a factor of 16 or more, without any loss of image quality. The image quality has been assessed both in terms of objective metrics, such as spatial resolution and contrast-to-noise ratio, as well as in an observer study by experienced medical imaging specialists and radiologists. This work was carried out in preparation for live patient PCT breast cancer imaging, initially at specialized synchrotron facilities.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints</title>
<link>https://arxiv.org/abs/2505.05957</link>
<guid>https://arxiv.org/abs/2505.05957</guid>
<content:encoded><![CDATA[
arXiv:2505.05957v1 Announce Type: cross 
Abstract: While classical convolutional neural networks (CNNs) have revolutionized image classification, the emergence of quantum computing presents new opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs) leverage quantum mechanical properties and hold potential to outperform classical approaches. However, their implementation on current noisy intermediate-scale quantum (NISQ) devices remains challenging due to hardware limitations. In our research, we address this challenge by introducing an encoding scheme that significantly reduces the input dimensionality. We demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to directly process $28\times 28$ pixel MNIST images, eliminating the need for classical dimensionality reduction pre-processing. Additionally, we propose an automated framework based on expressibility, entanglement, and complexity characteristics to identify the building blocks of QCNNs, parameterized quantum circuits (PQCs). Our approach demonstrates advantages in accuracy and convergence speed with a similar parameter count compared to both hybrid QCNNs and classical CNNs. We validated our experiments on IBM's Heron r2 quantum processor, achieving $96.08\%$ classification accuracy, surpassing the $71.74\%$ benchmark of traditional approaches under identical training conditions. These results represent one of the first implementations of image classifications on real quantum hardware and validate the potential of quantum computing in this area.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[
arXiv:2505.06020v1 Announce Type: cross 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects</title>
<link>https://arxiv.org/abs/2505.06030</link>
<guid>https://arxiv.org/abs/2505.06030</guid>
<content:encoded><![CDATA[
arXiv:2505.06030v1 Announce Type: cross 
Abstract: Combining natural language and geometric shapes is an emerging research area with multiple applications in robotics and language-assisted design. A crucial task in this domain is object referent identification, which involves selecting a 3D object given a textual description of the target. Variability in language descriptions and spatial relationships of 3D objects makes this a complex task, increasing the need to better understand the behavior of neural network models in this domain. However, limited research has been conducted in this area. Specifically, when a model makes an incorrect prediction despite being provided with a seemingly correct object description, practitioners are left wondering: "Why is the model wrong?". In this work, we present a method answering this question by generating counterfactual examples. Our method takes a misclassified sample, which includes two objects and a text description, and generates an alternative yet similar formulation that would have resulted in a correct prediction by the model. We have evaluated our approach with data from the ShapeTalk dataset along with three distinct models. Our counterfactual examples maintain the structure of the original description, are semantically similar and meaningful. They reveal weaknesses in the description, model bias and enhance the understanding of the models behavior. Theses insights help practitioners to better interact with systems as well as engineers to improve models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations</title>
<link>https://arxiv.org/abs/2505.06079</link>
<guid>https://arxiv.org/abs/2505.06079</guid>
<content:encoded><![CDATA[
arXiv:2505.06079v1 Announce Type: cross 
Abstract: Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: https://shuaiyihuang.github.io/publications/TREND.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram</title>
<link>https://arxiv.org/abs/2505.06105</link>
<guid>https://arxiv.org/abs/2505.06105</guid>
<content:encoded><![CDATA[
arXiv:2505.06105v1 Announce Type: cross 
Abstract: Echocardiogram is the most commonly used imaging modality in cardiac assessment duo to its non-invasive nature, real-time capability, and cost-effectiveness. Despite its advantages, most clinical echocardiograms provide only two-dimensional views, limiting the ability to fully assess cardiac anatomy and function in three dimensions. While three-dimensional echocardiography exists, it often suffers from reduced resolution, limited availability, and higher acquisition costs. To overcome these challenges, we propose a deep learning framework S2MNet that reconstructs continuous and high-fidelity 3D heart models by integrating six slices of routinely acquired 2D echocardiogram views. Our method has three advantages. First, our method avoid the difficulties on training data acquasition by simulate six of 2D echocardiogram images from corresponding slices of a given 3D heart mesh. Second, we introduce a deformation field-based method, which avoid spatial discontinuities or structural artifacts in 3D echocardiogram reconstructions. We validate our method using clinically collected echocardiogram and demonstrate that our estimated left ventricular volume, a key clinical indicator of cardiac function, is strongly correlated with the doctor measured GLPS, a clinical measurement that should demonstrate a negative correlation with LVE in medical theory. This association confirms the reliability of our proposed 3D construction method.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review</title>
<link>https://arxiv.org/abs/2505.06118</link>
<guid>https://arxiv.org/abs/2505.06118</guid>
<content:encoded><![CDATA[
arXiv:2505.06118v1 Announce Type: cross 
Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena</title>
<link>https://arxiv.org/abs/2505.06123</link>
<guid>https://arxiv.org/abs/2505.06123</guid>
<content:encoded><![CDATA[
arXiv:2505.06123v1 Announce Type: cross 
Abstract: Wasserstein distances provide a powerful framework for comparing data distributions. They can be used to analyze processes over time or to detect inhomogeneities within data. However, simply calculating the Wasserstein distance or analyzing the corresponding transport map (or coupling) may not be sufficient for understanding what factors contribute to a high or low Wasserstein distance. In this work, we propose a novel solution based on Explainable AI that allows us to efficiently and accurately attribute Wasserstein distances to various data components, including data subgroups, input features, or interpretable subspaces. Our method achieves high accuracy across diverse datasets and Wasserstein distance specifications, and its practical utility is demonstrated in two use cases.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills</title>
<link>https://arxiv.org/abs/2505.06176</link>
<guid>https://arxiv.org/abs/2505.06176</guid>
<content:encoded><![CDATA[
arXiv:2505.06176v1 Announce Type: cross 
Abstract: Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet</title>
<link>https://arxiv.org/abs/2505.06185</link>
<guid>https://arxiv.org/abs/2505.06185</guid>
<content:encoded><![CDATA[
arXiv:2505.06185v1 Announce Type: cross 
Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using transformers for classification and semantic segmentation. For spurious-correlation problems, this method allows us to enhance the image representation with two other image representations: representation obtained by semantic segmentation and representation obtained by image reconstruction. In our experiments, the proposed method outperformed in F-value measure than other classifiers when the test data included slices from the same patient (no covariate shift). Similarly, when the test data did not include slices from the same patient (covariate shift setting), the proposed method outperformed in AUC measure.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Concepts</title>
<link>https://arxiv.org/abs/2505.06191</link>
<guid>https://arxiv.org/abs/2505.06191</guid>
<content:encoded><![CDATA[
arXiv:2505.06191v1 Announce Type: cross 
Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation</title>
<link>https://arxiv.org/abs/2505.06210</link>
<guid>https://arxiv.org/abs/2505.06210</guid>
<content:encoded><![CDATA[
arXiv:2505.06210v1 Announce Type: cross 
Abstract: Convolutional neural network (CNN) and Transformer-based architectures are two dominant deep learning models for polyp segmentation. However, CNNs have limited capability for modeling long-range dependencies, while Transformers incur quadratic computational complexity. Recently, State Space Models such as Mamba have been recognized as a promising approach for polyp segmentation because they not only model long-range interactions effectively but also maintain linear computational complexity. However, Mamba-based architectures still struggle to capture topological features (e.g., connected components, loops, voids), leading to inaccurate boundary delineation and polyp segmentation. To address these limitations, we propose a new approach called Topo-VM-UNetV2, which encodes topological features into the Mamba-based state-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of two stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for the training and test images, which are then used to compute topology attention maps. Specifically, we first compute persistence diagrams of the PMs, then we generate persistence score maps by assigning persistence values (i.e., the difference between death and birth times) of each topological feature to its birth location, finally we transform persistence scores into attention weights using the sigmoid function. Stage 2: These topology attention maps are integrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to form a topology-guided semantics and detail infusion (Topo-SDI) module for enhancing the segmentation results. Extensive experiments on five public polyp segmentation datasets demonstrate the effectiveness of our proposed method. The code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Humanoids Hike! Integrative Skill Development on Complex Trails</title>
<link>https://arxiv.org/abs/2505.06218</link>
<guid>https://arxiv.org/abs/2505.06218</guid>
<content:encoded><![CDATA[
arXiv:2505.06218v1 Announce Type: cross 
Abstract: Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anymate: A Dataset and Baselines for Learning 3D Object Rigging</title>
<link>https://arxiv.org/abs/2505.06227</link>
<guid>https://arxiv.org/abs/2505.06227</guid>
<content:encoded><![CDATA[
arXiv:2505.06227v1 Announce Type: cross 
Abstract: Rigging and skinning are essential steps to create realistic 3D animations, often requiring significant expertise and manual effort. Traditional attempts at automating these processes rely heavily on geometric heuristics and often struggle with objects of complex geometry. Recent data-driven approaches show potential for better generality, but are often constrained by limited training data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information -- 70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction. We systematically design and experiment with various architectures as baselines for each module and conduct comprehensive evaluations on our dataset to compare their performance. Our models significantly outperform existing methods, providing a foundation for comparing future methods in automated rigging and skinning. Code and dataset can be found at https://anymate3d.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation</title>
<link>https://arxiv.org/abs/2303.17051</link>
<guid>https://arxiv.org/abs/2303.17051</guid>
<content:encoded><![CDATA[
arXiv:2303.17051v4 Announce Type: replace 
Abstract: The recent popularity of foundation models and the pre-train-and-adapt paradigm, where a large-scale model is transferred to downstream tasks, is gaining attention for volumetric medical image segmentation. However, current transfer learning strategies devoted to full fine-tuning for transfer learning may require significant resources and yield sub-optimal results when the labeled data of the target task is scarce. This makes its applicability in real clinical settings challenging since these institutions are usually constrained on data and computational resources to develop proprietary solutions. To address this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a novel and realistic scenario for adapting medical image segmentation foundation models. This setting considers the key role of both data- and parameter-efficiency during adaptation. Building on a foundation model pre-trained on open-access CT organ segmentation sources, we propose leveraging Parameter-Efficient Fine-Tuning and black-box Adapters to address such challenges. Furthermore, novel efficient adaptation methodologies are introduced in this work, which include Spatial black-box Adapters that are more appropriate for dense prediction tasks and constrained transductive inference, leveraging task-specific prior knowledge. Our comprehensive transfer learning experiments confirm the suitability of foundation models in medical image segmentation and unveil the limitations of popular fine-tuning strategies in few-shot scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2306.14070</link>
<guid>https://arxiv.org/abs/2306.14070</guid>
<content:encoded><![CDATA[
arXiv:2306.14070v2 Announce Type: replace 
Abstract: Super-resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench, the first benchmark dataset featuring high-resolution datasets, including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will help to advance SR methods for science.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image space formalism of convolutional neural networks for k-space interpolation</title>
<link>https://arxiv.org/abs/2402.17410</link>
<guid>https://arxiv.org/abs/2402.17410</guid>
<content:encoded><![CDATA[
arXiv:2402.17410v2 Announce Type: replace 
Abstract: Purpose: Noise resilience in image reconstructions by scan-specific robust artificial neural networks for k-space interpolation (RAKI) is linked to nonlinear activations in k-space. To gain a deeper understanding of this relationship, an image space formalism of RAKI is introduced for analyzing noise propagation analytically, identifying and characterizing image reconstruction features and to describe the role of nonlinear activations in a human readable manner. Methods: The image space formalism for RAKI inference is employed by expressing nonlinear activations in k-space as element-wise multiplications with activation masks, which transform into convolutions in image space. Jacobians of the de-aliased, coil-combined image relative to the aliased coil images can be expressed algebraically, and thus, the noise amplification is quantified analytically (g-factor maps). We analyze the role of nonlinearity for noise resilience by controlling the degree of nonlinearity in the reconstruction model via the negative slope parameter in leaky ReLU. Results: The analytical g-factor maps correspond with those obtained from Monte Carlo simulations and from an auto differentiation approach for in vivo brain images. Apparent blurring and contrast loss artifacts are identified as implications of enhanced noise resilience. These residual artifacts can be traded against noise resilience by adjusting the degree of nonlinearity in the model (Tikhonov-like regularization) in case of limited training data. The inspection of image space activations reveals an autocorrelation pattern leading to a potential center artifact. Conclusion: The image space formalism of RAKI provides the means for analytical quantitative noisepropagation analysis and human-readable visualization of the effects of the nonlinear activation functions in k-space.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations</title>
<link>https://arxiv.org/abs/2403.07887</link>
<guid>https://arxiv.org/abs/2403.07887</guid>
<content:encoded><![CDATA[
arXiv:2403.07887v4 Announce Type: replace 
Abstract: Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable concepts, ground them in our environment, and reason over these grounded entities. This trifecta of human thought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations extracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the Neural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is a nested schema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. Then, the NSI metric learns to ground primitives into slots through a structured contrastive learning objective that reasons over the intermodal alignment. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and interpretability of correspondences learned by NSI. From a scene representation standpoint, we find that emergent NSI slots that move beyond the image grid by binding to spatial objects facilitate improved visual grounding compared to conventional bounding-box-based approaches. From a data efficiency standpoint, we empirically validate that NSI learns more generalizable representations from a fixed amount of annotation data than the traditional approach. We also show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene complexity. Finally, we investigate the downstream efficacy of the grounded slots. Vision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on challenging few-shot classification tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model</title>
<link>https://arxiv.org/abs/2404.09957</link>
<guid>https://arxiv.org/abs/2404.09957</guid>
<content:encoded><![CDATA[
arXiv:2404.09957v3 Announce Type: replace 
Abstract: Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or "best-practice" guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer</title>
<link>https://arxiv.org/abs/2404.12734</link>
<guid>https://arxiv.org/abs/2404.12734</guid>
<content:encoded><![CDATA[
arXiv:2404.12734v4 Announce Type: replace 
Abstract: With the rapid development of OCR technology, mixed-scene text recognition has become a key technical challenge. Although deep learning models have achieved significant results in specific scenarios, their generality and stability still need improvement, and the high demand for computing resources affects flexibility. To address these issues, this paper proposes DLoRA-TrOCR, a parameter-efficient hybrid text spotting method based on a pre-trained OCR Transformer. By embedding a weight-decomposed DoRA module in the image encoder and a LoRA module in the text decoder, this method can be efficiently fine-tuned on various downstream tasks. Our method requires no more than 0.7\% trainable parameters, not only accelerating the training efficiency but also significantly improving the recognition accuracy and cross-dataset generalization performance of the OCR system in mixed text scenes. Experiments show that our proposed DLoRA-TrOCR outperforms other parameter-efficient fine-tuning methods in recognizing complex scenes with mixed handwritten, printed, and street text, achieving a CER of 4.02 on the IAM dataset, a F1 score of 94.29 on the SROIE dataset, and a WAR of 86.70 on the STR Benchmark, reaching state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</title>
<link>https://arxiv.org/abs/2407.06188</link>
<guid>https://arxiv.org/abs/2407.06188</guid>
<content:encoded><![CDATA[
arXiv:2407.06188v2 Announce Type: replace 
Abstract: While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning and Identity Adversarial Training for Facial Behavior Understanding</title>
<link>https://arxiv.org/abs/2407.11243</link>
<guid>https://arxiv.org/abs/2407.11243</guid>
<content:encoded><![CDATA[
arXiv:2407.11243v2 Announce Type: replace 
Abstract: Facial Action Unit (AU) detection has gained significant attention as it enables the breakdown of complex facial expressions into individual muscle movements. In this paper, we revisit two fundamental factors in AU detection: diverse and large-scale data and subject identity regularization. Motivated by recent advances in foundation models, we highlight the importance of data and introduce Face9M, a diverse dataset comprising 9 million facial images from multiple public sources. Pretraining a masked autoencoder on Face9M yields strong performance in AU detection and facial expression tasks. More importantly, we emphasize that the Identity Adversarial Training (IAT) has not been well explored in AU tasks. To fill this gap, we first show that subject identity in AU datasets creates shortcut learning for the model and leads to sub-optimal solutions to AU predictions. Secondly, we demonstrate that strong IAT regularization is necessary to learn identity-invariant features. Finally, we elucidate the design space of IAT and empirically show that IAT circumvents the identity-based shortcut learning and results in a better solution. Our proposed methods, Facial Masked Autoencoder (FMAE) and IAT, are simple, generic and effective. Remarkably, the proposed FMAE-IAT approach achieves new state-of-the-art F1 scores on BP4D (67.1\%), BP4D+ (66.8\%), and DISFA (70.1\%) databases, significantly outperforming previous work. We release the code and model at https://github.com/forever208/FMAE-IAT.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAPTRv2: Attention-based Position Update Improves Tracking Any Point</title>
<link>https://arxiv.org/abs/2407.16291</link>
<guid>https://arxiv.org/abs/2407.16291</guid>
<content:encoded><![CDATA[
arXiv:2407.16291v2 Announce Type: replace 
Abstract: In this paper, we present TAPTRv2, a Transformer-based approach built upon TAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from DEtection TRansformer (DETR) and formulates each tracking point as a point query, making it possible to leverage well-studied operations in DETR-like algorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its reliance on cost-volume,which contaminates the point query\'s content feature and negatively impacts both visibility prediction and cost-volume computation. In TAPTRv2, we propose a novel attention-based position update (APU) operation and use key-aware deformable attention to realize. For each query, this operation uses key-aware attention weights to combine their corresponding deformable sampling positions to predict a new query position. This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. By introducing this new operation, TAPTRv2 not only removes the extra burden of cost-volume computation, but also leads to a substantial performance improvement. TAPTRv2 surpasses TAPTR and achieves state-of-the-art performance on many challenging datasets, demonstrating the superiority
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention</title>
<link>https://arxiv.org/abs/2409.08840</link>
<guid>https://arxiv.org/abs/2409.08840</guid>
<content:encoded><![CDATA[
arXiv:2409.08840v3 Announce Type: replace 
Abstract: Collaborative perception (CP) leverages visual data from connected and autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV). Despite recent progress, current CP methods expand the ego vehicle's 360-degree perceptual range almost equally, which faces two key challenges. Firstly, in areas with uneven traffic distribution, focusing on directions with little traffic offers limited benefits. Secondly, under limited communication budgets, allocating excessive bandwidth to less critical directions lowers the perception accuracy in more vital areas. To address these issues, we propose Direct-CP, a proactive and direction-aware CP system aiming at improving CP in specific directions. Our key idea is to enable an ego vehicle to proactively signal its interested directions and readjust its attention to enhance local directional CP performance. To achieve this, we first propose an RSU-aided direction masking mechanism that assists an ego vehicle in identifying vital directions. Additionally, we design a direction-aware selective attention module to wisely aggregate pertinent features based on ego vehicle's directional priorities, communication budget, and the positional data of CAVs. Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture the divergence between directional CP outcomes and the ground truth, facilitating effective model training. Extensive experiments on the V2X-Sim 2.0 dataset demonstrate that our approach achieves 19.8\% higher local perception accuracy in interested directions and 2.5\% higher overall perception accuracy than the state-of-the-art methods in collaborative 3D object detection tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker</title>
<link>https://arxiv.org/abs/2410.01966</link>
<guid>https://arxiv.org/abs/2410.01966</guid>
<content:encoded><![CDATA[
arXiv:2410.01966v3 Announce Type: replace 
Abstract: Being able to accurately monitor the screen exposure of young children is important for research on phenomena linked to screen use such as childhood obesity, physical activity, and social interaction. Most existing studies rely upon self-report or manual measures from bulky wearable sensors, thus lacking efficiency and accuracy in capturing quantitative screen exposure data. In this work, we developed a novel sensor informatics framework that utilizes egocentric images from a wearable sensor, termed the screen time tracker (STT), and a vision language model (VLM). In particular, we devised a multi-view VLM that takes multiple views from egocentric image sequences and interprets screen exposure dynamically. We validated our approach by using a dataset of children's free-living activities, demonstrating significant improvement over existing methods in plain vision language models and object detection models. Results supported the promise of this monitoring approach, which could optimize behavioral research on screen exposure in children's naturalistic settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Egocentric and Exocentric Methods: A Short Survey</title>
<link>https://arxiv.org/abs/2410.20621</link>
<guid>https://arxiv.org/abs/2410.20621</guid>
<content:encoded><![CDATA[
arXiv:2410.20621v2 Announce Type: replace 
Abstract: Egocentric vision captures the scene from the point of view of the camera wearer, while exocentric vision captures the overall scene context. Jointly modeling ego and exo views is crucial to developing next-generation AI agents. The community has regained interest in the field of egocentric vision. While the third-person view and first-person have been thoroughly investigated, very few works aim to study both synchronously. Exocentric videos contain many relevant signals that are transferrable to egocentric videos. This paper provides a timely overview of works combining egocentric and exocentric visions, a very new but promising research topic. We describe in detail the datasets and present a survey of the key applications of ego-exo joint learning, where we identify the most recent advances. With the presentation of the current status of the progress, we believe this short but timely survey will be valuable to the broad video-understanding community, particularly when multi-view modeling is critical.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VladVA: Discriminative Fine-tuning of LVLMs</title>
<link>https://arxiv.org/abs/2412.04378</link>
<guid>https://arxiv.org/abs/2412.04378</guid>
<content:encoded><![CDATA[
arXiv:2412.04378v3 Announce Type: replace 
Abstract: Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown to be capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.
  In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.
  Our contributions include (1) a carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components; (2) a parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters; (3) significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</title>
<link>https://arxiv.org/abs/2412.14123</link>
<guid>https://arxiv.org/abs/2412.14123</guid>
<content:encoded><![CDATA[
arXiv:2412.14123v3 Announce Type: replace 
Abstract: Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and $11$ distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned or probed, we reach state-of-the-art results on the test sets of GeoPlex and for 6 external datasets across various environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, climate type classification, and segmentation of flood, burn scar, and deforestation. The code and models are available at https://github.com/gastruc/AnySat.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking</title>
<link>https://arxiv.org/abs/2412.20002</link>
<guid>https://arxiv.org/abs/2412.20002</guid>
<content:encoded><![CDATA[
arXiv:2412.20002v2 Announce Type: replace 
Abstract: Visual tracking has made significant strides due to the adoption of transformer-based models. Most state-of-the-art trackers struggle to meet real-time processing demands on mobile platforms with constrained computing resources, particularly for real-time unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we introduce AVTrack, an adaptive computation framework designed to selectively activate transformer blocks for real-time UAV tracking. The proposed Activation Module (AM) dynamically optimizes the ViT architecture by selectively engaging relevant components, thereby enhancing inference efficiency without significant compromise to tracking performance. Furthermore, to tackle the challenges posed by extreme changes in viewing angles often encountered in UAV tracking, the proposed method enhances ViTs' effectiveness by learning view-invariant representations through mutual information (MI) maximization. Two effective design principles are proposed in the AVTrack. Building on it, we propose an improved tracker, dubbed AVTrack-MD, which introduces the novel MI maximization-based multi-teacher knowledge distillation (MD) framework. It harnesses the benefits of multiple teachers, specifically the off-the-shelf tracking models from the AVTrack, by integrating and refining their outputs, thereby guiding the learning process of the compact student network. Specifically, we maximize the MI between the softened feature representations from the multi-teacher models and the student model, leading to improved generalization and performance of the student model, particularly in noisy conditions. Extensive experiments on multiple UAV tracking benchmarks demonstrate that AVTrack-MD not only achieves performance comparable to the AVTrack baseline but also reduces model complexity, resulting in a significant 17\% increase in average tracking speed.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Class Discovery in Instance Segmentation</title>
<link>https://arxiv.org/abs/2502.08149</link>
<guid>https://arxiv.org/abs/2502.08149</guid>
<content:encoded><![CDATA[
arXiv:2502.08149v2 Announce Type: replace 
Abstract: This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Pretraining for Fine-Grained Plankton Recognition</title>
<link>https://arxiv.org/abs/2503.11341</link>
<guid>https://arxiv.org/abs/2503.11341</guid>
<content:encoded><![CDATA[
arXiv:2503.11341v2 Announce Type: replace 
Abstract: Plankton recognition is an important computer vision problem due to plankton's essential role in ocean food webs and carbon capture, highlighting the need for species-level monitoring. However, this task is challenging due to its fine-grained nature and dataset shifts caused by different imaging instruments and varying species distributions. As new plankton image datasets are collected at an increasing pace, there is a need for general plankton recognition models that require minimal expert effort for data labeling. In this work, we study large-scale self-supervised pretraining for fine-grained plankton recognition. We first employ masked autoencoding and a large volume of diverse plankton image data to pretrain a general-purpose plankton image encoder. Then we utilize fine-tuning to obtain accurate plankton recognition models for new datasets with a very limited number of labeled training images. Our experiments show that self-supervised pretraining with diverse plankton data clearly increases plankton recognition accuracy compared to standard ImageNet pretraining when the amount of training data is limited. Moreover, the accuracy can be further improved when unlabeled target data is available and utilized during the pretraining.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion</title>
<link>https://arxiv.org/abs/2503.20698</link>
<guid>https://arxiv.org/abs/2503.20698</guid>
<content:encoded><![CDATA[
arXiv:2503.20698v4 Announce Type: replace 
Abstract: Videos inherently contain multiple modalities, including visual events, text overlays, sounds, and speech, all of which are important for retrieval. However, state-of-the-art multimodal language models like VAST and LanguageBind are built on vision-language models (VLMs), and thus overly prioritize visual signals. Retrieval benchmarks further reinforce this bias by focusing on visual queries and neglecting other modalities. We create a search system MMMORRF that extracts text and features from both visual and audio modalities and integrates them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is both effective and efficient, demonstrating practicality in searching videos based on users' information needs instead of visual descriptive queries. We evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed for more targeted information needs, and find that it improves nDCG@20 by 81% over leading multimodal encoders and 37% over single-modality retrieval, demonstrating the value of integrating diverse modalities.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</title>
<link>https://arxiv.org/abs/2503.22976</link>
<guid>https://arxiv.org/abs/2503.22976</guid>
<content:encoded><![CDATA[
arXiv:2503.22976v3 Announce Type: replace 
Abstract: Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models For Seismic Data Processing: An Extensive Review</title>
<link>https://arxiv.org/abs/2503.24166</link>
<guid>https://arxiv.org/abs/2503.24166</guid>
<content:encoded><![CDATA[
arXiv:2503.24166v2 Announce Type: replace 
Abstract: Seismic processing plays a crucial role in transforming raw data into high-quality subsurface images, pivotal for various geoscience applications. Despite its importance, traditional seismic processing techniques face challenges such as noisy and damaged data and the reliance on manual, time-consuming workflows. The emergence of deep learning approaches has introduced effective and user-friendly alternatives, yet many of these deep learning approaches rely on synthetic datasets and specialized neural networks. Recently, foundation models have gained traction in the seismic domain, due to their success in the natural image domain. Therefore, we investigate the application of natural image foundation models on the three seismic processing tasks: demultiple, interpolation, and denoising. We evaluate the impact of different model characteristics, such as pre-training technique and neural network architecture, on performance and efficiency. Rather than proposing a single seismic foundation model, we critically examine various natural image foundation models and suggest some promising candidates for future exploration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualization of a multidimensional point cloud as a 3D swarm of avatars</title>
<link>https://arxiv.org/abs/2504.06751</link>
<guid>https://arxiv.org/abs/2504.06751</guid>
<content:encoded><![CDATA[
arXiv:2504.06751v2 Announce Type: replace 
Abstract: The article presents an innovative approach to the visualization of multidimensional data, using icons inspired by Chernoff faces. The approach merges classical projection techniques with the assignment of particular data dimensions to mimic features, capitalizing on the natural ability of the human brain to interpret facial expressions. We introduce a semantic division of data dimensions into intuitive and technical categories, assigning the former to avatar features and projecting the latter into a hyperspace of four, or potentially more dimensions. The technique is implemented as a plugin to the dpVision open-source image handling platform. The plugin allows the data to be interactively explored in the form of a swarm of avatars whose position in hyperspace as well as facial features represent various aspects of the data. Sample visualizations, based on synthetic test data as well as the 12-dimensional database on Portuguese Vinho Verde wines, confirm the usefulness of our approach to the analysis of complex data structures.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery</title>
<link>https://arxiv.org/abs/2504.08049</link>
<guid>https://arxiv.org/abs/2504.08049</guid>
<content:encoded><![CDATA[
arXiv:2504.08049v2 Announce Type: replace 
Abstract: This work presents a new approach to anomaly detection and localization in synthetic aperture radar imagery (SAR), expanding upon the existing patch distribution modeling framework (PaDiM). We introduce the adaptive cosine estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at inference, an unbounded metric. ACE instead uses the cosine similarity metric, providing bounded anomaly detection scores. The proposed method is evaluated across multiple SAR datasets, with performance metrics including the area under the receiver operating curve (AUROC) at the image and pixel level, aiming for increased performance in anomaly detection and localization of SAR imagery. The code is publicly available: https://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.13580</link>
<guid>https://arxiv.org/abs/2504.13580</guid>
<content:encoded><![CDATA[
arXiv:2504.13580v2 Announce Type: replace 
Abstract: High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Perception-Inspired Grain Segmentation Refinement Using Conditional Random Fields</title>
<link>https://arxiv.org/abs/2312.09968</link>
<guid>https://arxiv.org/abs/2312.09968</guid>
<content:encoded><![CDATA[
arXiv:2312.09968v2 Announce Type: replace-cross 
Abstract: Automated detection of grain boundaries in electron microscope images of polycrystalline materials could help accelerate the nanoscale characterization of myriad engineering materials and novel materials under scientific research. Accurate segmentation of interconnected line networks, such as grain boundaries in polycrystalline material microstructures, poses a significant challenge due to the fragmented masks produced by conventional computer vision algorithms, including convolutional neural networks. These algorithms struggle with thin masks, often necessitating post-processing for effective contour closure and continuity. Previous approaches in this domain have typically relied on custom post-processing techniques that are problem-specific and heavily dependent on the quality of the mask obtained from a computer vision algorithm. Addressing this issue, this paper introduces a fast, high-fidelity post-processing technique that is universally applicable to segmentation masks of interconnected line networks. Leveraging domain knowledge about grain boundary connectivity, this method employs conditional random fields and perceptual grouping rules to refine segmentation masks of any image with a discernible grain structure. This approach significantly enhances segmentation mask accuracy, achieving a 79% segment identification accuracy in validation with a U-Net model on electron microscopy images of a polycrystalline oxide. Additionally, a novel grain alignment metric is introduced, showing a 51% improvement in grain alignment. This method not only enables rapid and accurate segmentation but also facilitates an unprecedented level of data analysis, significantly improving the statistical representation of grain boundary networks, making it suitable for a range of disciplines where precise segmentation of interconnected line networks is essential.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer</title>
<link>https://arxiv.org/abs/2408.08456</link>
<guid>https://arxiv.org/abs/2408.08456</guid>
<content:encoded><![CDATA[
arXiv:2408.08456v2 Announce Type: replace-cross 
Abstract: Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect the prediction results of machine learning models. However, current methods have limitations in detecting drift, for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a pre-trained Vision Transformer model to extract relevant features, using mammography as a case study, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 99.1%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction</title>
<link>https://arxiv.org/abs/2502.04521</link>
<guid>https://arxiv.org/abs/2502.04521</guid>
<content:encoded><![CDATA[
arXiv:2502.04521v2 Announce Type: replace-cross 
Abstract: Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL aggregates locally trained model weights into a global model, inherently constraining all sites to use a homogeneous model architecture. This rigidity forces sites to compromise on architectures tailored to their compute resources and application-specific needs, making conventional FL unsuitable for model-heterogeneous settings where each site may prefer a distinct architecture. To overcome this limitation, we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that learns the distribution of multi-site MR images. For high-fidelity synthesis, we propose a novel site-prompted GAT prior that controllably synthesizes realistic MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its own reconstruction model -- using an architecture of its choice -- on a hybrid dataset augmenting its local MRI dataset with GAT-generated synthetic MR images emulating datasets from other sites. This hybrid training strategy enables site-specific reconstruction models to generalize more effectively across diverse data distributions while preserving data privacy. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT enables flexible, model-heterogeneous collaborations and achieves superior within-site and cross-site reconstruction performance compared to state-of-the-art FL baselines.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-preserving contrastive learning for spatial time series</title>
<link>https://arxiv.org/abs/2502.06380</link>
<guid>https://arxiv.org/abs/2502.06380</guid>
<content:encoded><![CDATA[
arXiv:2502.06380v3 Announce Type: replace-cross 
Abstract: The effectiveness of neural network models largely relies on learning meaningful latent patterns from data, where self-supervised learning of informative representations can enhance model performance and generalisability. However, self-supervised representation learning for spatially characterised time series, which are ubiquitous in transportation domain, poses unique challenges due to the necessity of maintaining fine-grained spatio-temporal similarities in the latent space. In this study, we introduce two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance the contrastive learning objective and the need for structure preservation, we propose a dynamic weighting mechanism that adaptively manages this trade-off and stabilises training. We validate the proposed method through extensive experiments, including multivariate time series classification to demonstrate its general applicability, as well as macroscopic and microscopic traffic prediction to highlight its particular usefulness in encoding traffic interactions. Across all tasks, our method preserves the similarity structures more effectively and improves state-of-the-art task performances. This method can be integrated with an arbitrary neural network model and is particularly beneficial for time series data with spatial or geographical features. Furthermore, our findings suggest that well-preserved similarity structures in the latent space indicate more informative and useful representations. This provides insights to design more effective neural networks for data-driven transportation research. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RS2AD: End-to-End Autonomous Driving Data Generation from Roadside Sensor Observations</title>
<link>https://arxiv.org/abs/2503.07085</link>
<guid>https://arxiv.org/abs/2503.07085</guid>
<content:encoded><![CDATA[
arXiv:2503.07085v3 Announce Type: replace-cross 
Abstract: End-to-end autonomous driving solutions, which process multi-modal sensory data to directly generate refined control commands, have become a dominant paradigm in autonomous driving research. However, these approaches predominantly depend on single-vehicle data collection for model training and optimization, resulting in significant challenges such as high data acquisition and annotation costs, the scarcity of critical driving scenarios, and fragmented datasets that impede model generalization. To mitigate these limitations, we introduce RS2AD, a novel framework for reconstructing and synthesizing vehicle-mounted LiDAR data from roadside sensor observations. Specifically, our method transforms roadside LiDAR point clouds into the vehicle-mounted LiDAR coordinate system by leveraging the target vehicle's relative pose. Subsequently, high-fidelity vehicle-mounted LiDAR data is synthesized through virtual LiDAR modeling, point cloud classification, and resampling techniques. To the best of our knowledge, this is the first approach to reconstruct vehicle-mounted LiDAR data from roadside sensor inputs. Extensive experimental evaluations demonstrate that incorporating the data generated by the RS2AD method (the RS2V-L dataset) into model training as a supplement to the KITTI dataset can significantly enhance the accuracy of 3D object detection and greatly improve the efficiency of end-to-end autonomous driving data generation. These findings strongly validate the effectiveness of the proposed method and underscore its potential in reducing dependence on costly vehicle-mounted data collection while improving the robustness of autonomous driving models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage</title>
<link>https://arxiv.org/abs/2504.20007</link>
<guid>https://arxiv.org/abs/2504.20007</guid>
<content:encoded><![CDATA[
arXiv:2504.20007v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating video, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. We present our methodology, computational techniques, and findings, outlining a practical approach for law enforcement while advancing the frontiers of knowledge discovery from police BWC data.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&amp;E Whole Slide Images of Cutaneous Squamous Cell Carcinoma</title>
<link>https://arxiv.org/abs/2505.04672</link>
<guid>https://arxiv.org/abs/2505.04672</guid>
<content:encoded><![CDATA[
<div> Keywords: digital pathology, Whole-Slide Images, cutaneous squamous cell carcinoma, Histo-Miner, tumor segmentation  

<br /><br />Summary: Recent advancements in digital pathology facilitate the comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, but there is a scarcity of labeled datasets and open-source pipelines specific to skin tissue analysis. To address this gap, we introduce Histo-Miner, a deep learning-based pipeline specifically designed for analyzing skin WSIs, particularly focusing on patients with cutaneous squamous cell carcinoma (cSCC), a prevalent form of non-melanoma skin cancer. We created two datasets featuring 47,392 annotated cell nuclei and 144 tumor-segmented WSIs from cSCC patients. Histo-Miner utilizes convolutional neural networks and vision transformers for nucleus segmentation, classification, and tumor region segmentation, achieving competitive performance metrics. The results include a multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, a macro-averaged F1 score of 0.832 for nucleus classification, and a mean Intersection over Union (mIoU) of 0.884 for tumor segmentation. Furthermore, Histo-Miner forecasts patient responses to immunotherapy using WSI analysis, identifying key immune features that predict therapy outcomes, thus demonstrating its clinical relevance and interpretative power for underlying biological insights. <div>
arXiv:2505.04672v1 Announce Type: new 
Abstract: Recent advancements in digital pathology have enabled comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution microscopy and computational capabilities. Despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based pipeline for analysis of skin WSIs and generate two datasets with labeled nuclei and tumor regions. We develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients, Histo-Miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. Performance of trained models positively compares to state of the art with multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, macro-averaged F1 of 0.832 for nucleus classification and mean Intersection over Union (mIoU) of 0.884 for tumor region segmentation. From these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. Here, we use Histo-Miner to predict cSCC patient response to immunotherapy based on pre-treatment WSIs from 45 patients. Histo-Miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. This highlights the applicability of Histo-Miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Visual Trackers for Biomechanical Analysis of Running</title>
<link>https://arxiv.org/abs/2505.04713</link>
<guid>https://arxiv.org/abs/2505.04713</guid>
<content:encoded><![CDATA[
<div> Keywords: human pose estimation, deep learning, biomechanical analysis, joint trackers, root mean squared errors 

<br /><br />Summary: This work explores the advancements in human pose estimation, particularly in the context of biomechanics for sprinting. It evaluates the performance of six different trackers, including two point trackers and four joint trackers, by comparing their outputs against manual annotations from biomechanical experts across 5870 frames. The study focuses on forty sprints from five professional runners and emphasizes three crucial angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. To enhance accuracy, a post-processing module is introduced for outlier detection and fusion prediction of joint angles. The results indicate that the joint-based models achieved root mean squared errors ranging from 11.41° to 4.37°. When utilizing the post-processing modules, accuracy improves further, with errors reducing to 6.99° and 3.88°, respectively. The findings suggest that human pose tracking is a promising tool for biomechanical analysis in running. Still, the study identifies opportunities for improvement, particularly for applications demanding high precision. This research highlights the potential of integrating advanced pose estimation techniques into sports analysis and performance evaluation. <div>
arXiv:2505.04713v1 Announce Type: new 
Abstract: Human pose estimation has witnessed significant advancements in recent years, mainly due to the integration of deep learning models, the availability of a vast amount of data, and large computational resources. These developments have led to highly accurate body tracking systems, which have direct applications in sports analysis and performance evaluation.
  This work analyzes the performance of six trackers: two point trackers and four joint trackers for biomechanical analysis in sprints. The proposed framework compares the results obtained from these pose trackers with the manual annotations of biomechanical experts for more than 5870 frames. The experimental framework employs forty sprints from five professional runners, focusing on three key angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. We propose a post-processing module for outlier detection and fusion prediction in the joint angles.
  The experimental results demonstrate that using joint-based models yields root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. When integrated with the post-processing modules, these errors can be reduced to 6.99{\deg} and 3.88{\deg}, respectively. The experimental findings suggest that human pose tracking approaches can be valuable resources for the biomechanical analysis of running. However, there is still room for improvement in applications where high accuracy is required.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.04718</link>
<guid>https://arxiv.org/abs/2505.04718</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-layout, open-vocabulary, diffusion Transformer, scene generation, image editing  

<br /><br />Summary: Lay-Your-Scene (LayouSyn) is a novel pipeline designed for generating layouts of natural scenes from text prompts. The primary limitation of previous scene layout generation methods is their reliance on closed-vocabulary approaches or proprietary large language models, which hampers their flexibility and applicability in generating controllable images. LayouSyn addresses this by utilizing lightweight, open-source language models to extract scene elements from the prompts and implementing a new aspect-aware diffusion Transformer architecture for layout generation in an open-vocabulary format. Extensive experiments have shown that LayouSyn outperforms existing methodologies, achieving state-of-the-art results on complex spatial and numerical reasoning benchmarks. The article also presents two key applications for LayouSyn: Firstly, the method can integrate coarse initializations from larger language models, leading to improved outcomes. Secondly, it introduces a pipeline for seamlessly adding objects to existing images, underscoring LayouSyn's potential in image editing tasks. This combination of open-vocabulary capabilities and innovative architecture positions LayouSyn as a versatile tool in text-to-layout generation and editing applications. <div>
arXiv:2505.04718v1 Announce Type: new 
Abstract: We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims</title>
<link>https://arxiv.org/abs/2505.04720</link>
<guid>https://arxiv.org/abs/2505.04720</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, AI, performance comparisons, false claims, Bayesian approach

<br /><br />Summary: This paper investigates the reliability of performance claims in medical imaging AI research, emphasizing that many studies assert superiority based on empirical mean performance without adequate validation. Analyzing a representative cohort of medical imaging papers, the authors apply a Bayesian framework to assess the probability of false claims regarding outperformance. They find that over 80% of studies claim to outperform existing methods, with a particularly high likelihood of false claims—over 5%—in 86% of classification papers and 53% of segmentation papers. This suggests that the majority of claims about new methods exceeding the state of the art may be unwarranted and often result purely from chance rather than genuine improvements. This investigation sheds light on a significant flaw in current benchmarking practices within the field, indicating that many assertions of advancement could mislead researchers and misdirect future investigations. Overall, the results call for more rigorous validation and transparent reporting to enhance the credibility of performance comparisons in medical imaging AI. <div>
arXiv:2505.04720v1 Announce Type: new 
Abstract: Performance comparisons are fundamental in medical imaging Artificial Intelligence (AI) research, often driving claims of superiority based on relative improvements in common performance metrics. However, such claims frequently rely solely on empirical mean performance. In this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. We quantify the probability of false claims based on a Bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. According to our results, the majority (>80%) of papers claims outperformance when introducing a new method. Our analysis further revealed a high probability (>5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. These findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging AI are frequently unsubstantiated, posing a risk of misdirecting future research efforts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer</title>
<link>https://arxiv.org/abs/2505.04740</link>
<guid>https://arxiv.org/abs/2505.04740</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Layer Perceptrons, Vision Transformers, Wavelet Functions, Hybrid Kolmogorov-Arnold Network, ImageNet-1K

<br /><br />Summary: This article presents a new framework called Hybrid Kolmogorov-Arnold Network-Vision Transformer (Hyb-KAN ViT) aimed at addressing the limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs). It highlights the importance of leveraging the prebuilt modularity of the ViT architecture and integrating edge detection capabilities through wavelet functions. The framework proposes two innovative modules: Efficient-KAN (Eff-KAN), which substitutes MLP layers with spline functions, and Wavelet-KAN (Wav-KAN), which utilizes orthogonal wavelet transforms for multi-resolution feature extraction. These modules are effectively incorporated into ViT encoder layers and classification heads to improve spatial-frequency modeling while reducing computational constraints. Experimental evaluations on datasets such as ImageNet-1K, COCO, and ADE20K indicate that Hyb-KAN ViT achieves state-of-the-art performance across various tasks, including image recognition, object detection, and semantic segmentation. Additionally, ablation studies confirm the effectiveness of wavelet-driven spectral priors in enhancing segmentation and the efficiency of spline functions for detection tasks. Overall, the proposed framework establishes a new standard for optimizing parameter efficiency while facilitating multi-scale representation in vision architectures. <div>
arXiv:2505.04740v1 Announce Type: new 
Abstract: This study addresses the inherent limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold Network (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates wavelet-based spectral decomposition and spline-optimized activation functions, prior work has failed to focus on the prebuilt modularity of the ViT architecture and integration of edge detection capabilities of Wavelet functions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces MLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging orthogonal wavelet transforms for multi-resolution feature extraction. These modules are systematically integrated in ViT encoder layers and classification heads to enhance spatial-frequency modeling while mitigating computational bottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object Detection and Instance Segmentation), and ADE20K (Semantic Segmentation) demonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies validate the efficacy of wavelet-driven spectral priors in segmentation and spline-based efficiency in detection tasks. The framework establishes a new paradigm for balancing parameter efficiency and multi-scale representation in vision architectures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective</title>
<link>https://arxiv.org/abs/2505.04758</link>
<guid>https://arxiv.org/abs/2505.04758</guid>
<content:encoded><![CDATA[
<div> Keywords: RGB-D, SATNet, lightweight, depth quality, feature representation

<br /><br />Summary: The authors propose a novel Speed-Accuracy Tradeoff Network (SATNet) specifically designed for Lightweight RGB-D Salient Object Detection (SOD). Recognizing the typical trade-offs between accuracy and efficiency in existing models, SATNet addresses three core areas: depth quality, modality fusion, and feature representation. First, to improve depth quality, a Depth Anything Model is introduced to generate high-quality depth maps, which helps bridge the gaps present in current datasets. Regarding modality fusion, the Decoupled Attention Module (DAM) is developed to enhance the consistency of features within and between different modalities, effectively separating them into dual-view feature vectors to boost discriminative capabilities. For feature representation, a Dual Information Representation Module (DIRM) employs a bi-directional inverted framework to expand the feature space generated by lightweight backbones, capturing both texture and saliency features. Additionally, the model incorporates two-way prediction heads to optimize parameters through bi-directional backpropagation. Finally, a Dual Feature Aggregation Module (DFAM) is designed for effective integration of texture and saliency features in the decoder. Experimental results demonstrate that SATNet outperforms state-of-the-art heavyweight models while maintaining a lightweight structure with only 5.2 million parameters and 415 frames per second (FPS). <div>
arXiv:2505.04758v1 Announce Type: new 
Abstract: Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. For modality fusion, we propose a Decoupled Attention Module (DAM) to explore the consistency within and between modalities. Here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. For feature representation, we develop a Dual Information Representation Module (DIRM) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. DIRM models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. Finally, we design a Dual Feature Aggregation Module (DFAM) in the decoder to aggregate texture and saliency features. Extensive experiments on five public RGB-D SOD datasets indicate that the proposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and achieves a lightweight framework with 5.2 M parameters and 415 FPS.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</title>
<link>https://arxiv.org/abs/2505.04769</link>
<guid>https://arxiv.org/abs/2505.04769</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, Agentic AI, AI Agents, Vision-language Models, robotics  

<br /><br />Summary: This foundational review synthesizes recent advancements in Vision-Language-Action (VLA) models, emphasizing their role in unifying perception, natural language understanding, and embodied actions. It begins by establishing the conceptual foundations of VLA systems, tracing their development from cross-modal learning to integrated systems combining vision-language models, action planners, and hierarchical controllers. The methodology employs a robust literature review of over 80 VLA models published in the last three years. Key advancements are highlighted in architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. Various application domains are discussed, including humanoid robotics, autonomous vehicles, medical robotics, precision agriculture, and augmented reality navigation. The review also confronts major challenges such as real-time control, multimodal action representation, and generalization to new tasks, while assessing ethical deployment risks. Proposed solutions involve agentic AI adaptation and cross-embodiment generalization. Finally, the article outlines a forward-looking roadmap for the convergence of VLA models and agentic AI, aiming to create socially aligned, adaptive, and general-purpose embodied agents. This work serves as a crucial reference for advancing intelligent robotics and artificial general intelligence. <div>
arXiv:2505.04769v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay</title>
<link>https://arxiv.org/abs/2505.04787</link>
<guid>https://arxiv.org/abs/2505.04787</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Learning, Catastrophic Forgetting, generative replay, uncertainty-driven, synthetic labeled data

<br /><br />Summary: The article introduces a new framework for Continual Learning, termed "Replay to Remember (R2R)," aimed at addressing the challenge of Catastrophic Forgetting in neural networks. R2R utilizes a novel uncertainty-driven approach combined with Generative Replay to effectively assimilate new knowledge while preserving previous learning. The framework distinguishes itself by operating without the need for pre-trained models and pseudo-labels, relying instead on visual features extracted from unlabeled data. It employs a cluster-level uncertainty feedback mechanism, enhanced by dynamic thresholding, to adapt and improve continuously. Additionally, the architecture incorporates a generative replay mechanism, powered by a DeepSeek-R1 and CLIP VLM combination, to synthesize labeled data that reflects past experiences. This method mimics biological visual cognition, facilitating memory replay for enhanced performance in novel tasks. Extensive experiments conducted across benchmarks such as CIFAR-10, CIFAR-100, CINIC-10, SVHN, and TinyImageNet demonstrate the effectiveness of R2R, achieving unprecedented knowledge retention rates of 98.13%, 73.06%, 93.41%, 95.18%, and 59.74%, respectively, surpassing prior state-of-the-art results by over 4.36%. <div>
arXiv:2505.04787v1 Announce Type: new 
Abstract: Continual Learning entails progressively acquiring knowledge from new data while retaining previously acquired knowledge, thereby mitigating ``Catastrophic Forgetting'' in neural networks. Our work presents a novel uncertainty-driven Unsupervised Continual Learning framework using Generative Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture efficiently uses unlabelled and synthetic labelled data in a balanced proportion using a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module. Unlike traditional memory-buffer methods that depend on pretrained models and pseudo-labels, our R2R framework operates without any prior training. It leverages visual features from unlabeled data and adapts continuously using clustering-based uncertainty estimation coupled with dynamic thresholding. Concurrently, a generative replay mechanism along with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data representative of past experiences, resembling biological visual thinking that replays memory to remember and act in new, unseen tasks. Extensive experimental analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and TinyImageNet datasets. Our proposed R2R approach improves knowledge retention, achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%, 59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World</title>
<link>https://arxiv.org/abs/2505.04788</link>
<guid>https://arxiv.org/abs/2505.04788</guid>
<content:encoded><![CDATA[
<div> Keywords: vanishing points, Manhattan world, convex relaxation, GlobustVP, semidefinite programming  

<br /><br />Summary: The paper addresses the task of determining vanishing points (VPs) in a Manhattan world, which is crucial for various 3D vision applications. It highlights the limitations of existing methods, which are either sub-optimal or computationally intensive in pursuit of global optimality. To overcome these drawbacks, the authors introduce convex relaxation techniques to jointly infer line-VP associations and VP locations. They propose a "soft" association scheme using a truncated multi-selection error, leading to a primal problem reformulated into a quadratically constrained quadratic programming (QCQP) problem. This problem is further relaxed into a convex semidefinite programming (SDP) problem. The key innovation is the development of a globally optimal outlier-robust iterative solver called GlobustVP, which updates one VP at a time while treating others as outliers. This iterative process is complemented by a local refinement that enforces the mutual orthogonality of the three VPs inherent in a Manhattan world. Comprehensive experiments on synthetic and real-world data validate that GlobustVP strikes a commendable balance between efficiency, robustness, and global optimality in comparison to existing methods. The code for this approach is publicly available. <div>
arXiv:2505.04788v1 Announce Type: new 
Abstract: Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a ``soft'' association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs' locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called \textbf{GlobustVP}), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that \textbf{GlobustVP} achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. The code is publicly available at https://github.com/WU-CVGL/GlobustVP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition</title>
<link>https://arxiv.org/abs/2505.04793</link>
<guid>https://arxiv.org/abs/2505.04793</guid>
<content:encoded><![CDATA[
<div> Keywords: Person Reidentification, DetReIDX, dataset, aerial-ground, real-world conditions

<br /><br />Summary: Person reidentification (ReID) technology has shown promise in controlled, ground-level environments but struggles in real-world scenarios due to variability in data such as resolution, viewpoint changes, and occlusions. Existing public datasets do not adequately represent these conditions, hindering progress in the field. In response, this paper introduces DetReIDX, a large-scale aerial-ground person dataset designed to stress-test ReID under realistic circumstances. Comprising over 13 million bounding boxes from 509 individuals, the dataset was collected across seven university campuses spanning three continents, at drone altitudes between 5.8 and 120 meters. A key feature of DetReIDX is that subjects were recorded over multiple sessions on different days, capturing variations in clothing, daylight, and locations, making it suitable for evaluating long-term ReID. Additionally, the dataset includes annotations for 16 soft biometric attributes and multitask labels, encompassing human detection, tracking, ReID, and action recognition. Empirical evidence demonstrates that state-of-the-art methods experience significant performance degradation—up to 80% in detection accuracy and over 70% in Rank-1 ReID—when tested with DetReIDX, underscoring the dataset's relevance. The dataset and evaluation protocols are available at https://www.it.ubi.pt/DetReIDX/. <div>
arXiv:2505.04793v1 Announce Type: new 
Abstract: Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?</title>
<link>https://arxiv.org/abs/2505.04835</link>
<guid>https://arxiv.org/abs/2505.04835</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, robustness, synthetic corruptions, semantic segmentation, benchmarking

<br /><br />Summary: Deep learning (DL) models are widely utilized in practical applications but show susceptibility to distribution shifts, particularly due to variations in weather and lighting. Collecting diverse real-world data for testing DL models' robustness can be resource-intensive. Therefore, synthetic corruptions serve as an appealing alternative for evaluating robustness. This study aims to determine whether synthetic corruptions are reliable proxies for real-world corruptions. We conduct the largest benchmarking study focused on semantic segmentation models, comparing their performance on datasets with real-world and synthetic corruptions. Our findings demonstrate a strong correlation in mean performance across both types of corruptions, suggesting that synthetic corruptions can be effectively used for robustness evaluation. Additionally, we examine corruption-specific correlations, which provide important insights into the circumstances under which synthetic corruptions accurately represent real-world scenarios. Overall, our research supports the viability of synthetic corruptions for testing the resilience of DL models against various adverse conditions, paving the way for enhanced model evaluation techniques. The study includes open-source code to facilitate further exploration in this area, available at the provided GitHub link. <div>
arXiv:2505.04835v1 Announce Type: new 
Abstract: Deep learning (DL) models are widely used in real-world applications but remain vulnerable to distribution shifts, especially due to weather and lighting changes. Collecting diverse real-world data for testing the robustness of DL models is resource-intensive, making synthetic corruptions an attractive alternative for robustness testing. However, are synthetic corruptions a reliable proxy for real-world corruptions? To answer this, we conduct the largest benchmarking study on semantic segmentation models, comparing performance on real-world corruptions and synthetic corruptions datasets. Our results reveal a strong correlation in mean performance, supporting the use of synthetic corruptions for robustness evaluation. We further analyze corruption-specific correlations, providing key insights to understand when synthetic corruptions succeed in representing real-world corruptions. Open-source Code: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Cells Clearly: Evaluating Machine Vision Strategies for Microglia Centroid Detection in 3D Images</title>
<link>https://arxiv.org/abs/2505.04838</link>
<guid>https://arxiv.org/abs/2505.04838</guid>
<content:encoded><![CDATA[
<div> Keywords: microglia, 3D images, ilastik, 3D Morph, Omnipose<br /><br />Summary: Microglia are crucial cells in the brain, and their morphology can provide insights into brain health. In this study, the author evaluates three different tools designed to identify the center points of microglia in 3D microscope images. The tools assessed include ilastik, 3D Morph, and Omnipose, each with unique methodologies for cell detection. The project aims to determine the effectiveness of each tool in accurately locating microglial cells and to compare the outcomes produced by these different software. Through the analysis, it becomes evident that each tool perceives the cells in distinct ways, leading to variations in the information extracted from the images. This highlights the importance of choosing the appropriate tool for specific research needs in neuroimaging, as the results can significantly influence our understanding of microglial function and overall brain health. Ultimately, this project contributes to the ongoing discourse on the best practices for analyzing microglia morphology in neurobiology research. <div>
arXiv:2505.04838v1 Announce Type: new 
Abstract: Microglia are important cells in the brain, and their shape can tell us a lot about brain health. In this project, I test three different tools for finding the center points of microglia in 3D microscope images. The tools include ilastik, 3D Morph, and Omnipose. I look at how well each one finds the cells and how their results compare. My findings show that each tool sees the cells in its own way, and this can affect the kind of information we get from the images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORXE: Orchestrating Experts for Dynamically Configurable Efficiency</title>
<link>https://arxiv.org/abs/2505.04850</link>
<guid>https://arxiv.org/abs/2505.04850</guid>
<content:encoded><![CDATA[
<div> Keywords: ORXE, AI models, adaptability, inference pathways, image classification

<br /><br />Summary: This paper introduces ORXE, a modular and adaptable framework designed for real-time configurable efficiency in AI models. ORXE utilizes a collection of pre-trained experts, each with varying computational costs and performance, allowing it to dynamically adjust inference pathways based on the complexity of input samples. The system stands out from conventional approaches due to its avoidance of complex metamodel training, thus simplifying the development process while achieving high efficiency and flexibility. A confidence-based gating mechanism is employed to optimize the allocation of computational resources for each input, providing a balance between inference cost and prediction performance, which can be adjusted during runtime. The authors implemented a training-free ORXE system specifically for image classification tasks, rigorously evaluating its efficiency and accuracy across different devices. Results indicate that ORXE consistently outperforms individual experts and other dynamic models in most scenarios. The proposed framework not only excels in image classification but also has the potential to be expanded to various applications, making it a scalable solution suitable for diverse real-world deployment contexts. <div>
arXiv:2505.04850v1 Announce Type: new 
Abstract: This paper presents ORXE, a modular and adaptable framework for achieving real-time configurable efficiency in AI models. By leveraging a collection of pre-trained experts with diverse computational costs and performance levels, ORXE dynamically adjusts inference pathways based on the complexity of input samples. Unlike conventional approaches that require complex metamodel training, ORXE achieves high efficiency and flexibility without complicating the development process. The proposed system utilizes a confidence-based gating mechanism to allocate appropriate computational resources for each input. ORXE also supports adjustments to the preference between inference cost and prediction performance across a wide range during runtime. We implemented a training-free ORXE system for image classification tasks, evaluating its efficiency and accuracy across various devices. The results demonstrate that ORXE achieves superior performance compared to individual experts and other dynamic models in most cases. This approach can be extended to other applications, providing a scalable solution for diverse real-world deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model</title>
<link>https://arxiv.org/abs/2505.04861</link>
<guid>https://arxiv.org/abs/2505.04861</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, Post-Training Quantization, mixed-precision, Integer Quadratic Programming, bit-width allocation  

<br /><br />Summary: The Segment Anything Model (SAM) faces challenges in deployment on resource-constrained devices due to its high computational and memory requirements. Post-Training Quantization (PTQ) is a common solution for alleviating these demands, but traditional methods typically use fixed bit-width quantization, which can compromise accuracy and efficiency. The proposed solution, Mix-QSAM, introduces a mixed-precision PTQ framework tailored for SAM. It first establishes a layer-wise importance score using Kullback-Leibler (KL) divergence to measure each layer's output contribution. A novel metric, cross-layer synergy, is introduced based on causal mutual information to track dependencies between adjacent layers, ensuring consistent bit-width allocations and enhancing numerical stability. An Integer Quadratic Programming (IQP) formulation is employed to optimize bit-width assignments while adhering to model size and bit-operation constraints, allocating higher precision to essential layers and reducing bit-width for less critical ones. Experimental findings reveal that Mix-QSAM outperforms existing PTQ methods across instance segmentation and object detection tasks, achieving up to 20% higher average precision in mixed-precision settings of 6-bit and 4-bit, all while maintaining computational efficiency. <div>
arXiv:2505.04861v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) is a popular vision foundation model; however, its high computational and memory demands make deployment on resource-constrained devices challenging. While Post-Training Quantization (PTQ) is a practical approach for reducing computational overhead, existing PTQ methods rely on fixed bit-width quantization, leading to suboptimal accuracy and efficiency. To address this limitation, we propose Mix-QSAM, a mixed-precision PTQ framework for SAM. First, we introduce a layer-wise importance score, derived using Kullback-Leibler (KL) divergence, to quantify each layer's contribution to the model's output. Second, we introduce cross-layer synergy, a novel metric based on causal mutual information, to capture dependencies between adjacent layers. This ensures that highly interdependent layers maintain similar bit-widths, preventing abrupt precision mismatches that degrade feature propagation and numerical stability. Using these metrics, we formulate an Integer Quadratic Programming (IQP) problem to determine optimal bit-width allocation under model size and bit-operation constraints, assigning higher precision to critical layers while minimizing bit-width in less influential layers. Experimental results demonstrate that Mix-QSAM consistently outperforms existing PTQ methods on instance segmentation and object detection tasks, achieving up to 20% higher average precision under 6-bit and 4-bit mixed-precision settings, while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-regressive transformation for image alignment</title>
<link>https://arxiv.org/abs/2505.04864</link>
<guid>https://arxiv.org/abs/2505.04864</guid>
<content:encoded><![CDATA[
<div> Keywords: image alignment, transformation field, multi-scale features, cross-attention, Auto-Regressive Transformation (ART)  

<br /><br />Summary: Existing image alignment methods face challenges in feature-sparse regions, large deformation, and extreme scale differences, leading to suboptimal accuracy. The iterative refinement of transformation fields is crucial for enhancing robustness against these issues. To address this, the authors propose a new method called Auto-Regressive Transformation (ART), which estimates transformations from coarse to fine within an auto-regressive framework. ART utilizes hierarchical multi-scale features and refines transformations by randomly sampling points at different scales. Additionally, the incorporation of a cross-attention layer allows the model to emphasize critical regions, promoting accurate alignment even under difficult conditions with limited features. Extensive experiments on various datasets indicate that ART significantly outperforms current state-of-the-art image alignment methods. As a result, ART is established as a powerful and innovative approach for achieving precise image alignment, demonstrating its broad applicability in diverse scenarios. This approach not only enhances accuracy but also addresses the inherent challenges faced in the field, making it a significant advancement in image processing techniques. <div>
arXiv:2505.04864v1 Announce Type: new 
Abstract: Existing methods for image alignment struggle in cases involving feature-sparse regions, extreme scale and field-of-view differences, and large deformations, often resulting in suboptimal accuracy. Robustness to these challenges improves through iterative refinement of the transformation field while focusing on critical regions in multi-scale image representations. We thus propose Auto-Regressive Transformation (ART), a novel method that iteratively estimates the coarse-to-fine transformations within an auto-regressive framework. Leveraging hierarchical multi-scale features, our network refines the transformations using randomly sampled points at each scale. By incorporating guidance from the cross-attention layer, the model focuses on critical regions, ensuring accurate alignment even in challenging, feature-limited conditions. Extensive experiments across diverse datasets demonstrate that ART significantly outperforms state-of-the-art methods, establishing it as a powerful new method for precise image alignment with broad applicability.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning</title>
<link>https://arxiv.org/abs/2505.04877</link>
<guid>https://arxiv.org/abs/2505.04877</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed Precision Quantization, optimization, quantization policies, CIFAR10, ImageNet

<br /><br />Summary: Mixed Precision Quantization (MPQ) is a critical strategy for optimizing neural networks by finding the ideal bitwidth for each layer. Traditional MPQ methods are hindered by their reliance on costly searches for quantization policies using large-scale datasets. To address this challenge, the proposed method first explores quantization strategies on smaller datasets and subsequently generalizes these findings to larger datasets, thus streamlining the process. The technique reduces the need for exhaustive fine-tuning on large-scale data by allowing for adjustments merely to the model weights. Key components of this approach include: employing sharpness-aware minimization to improve generalization in quantization, using implicit gradient direction alignment to manage conflicts in gradient objectives, and implementing an adaptive perturbation radius to speed up optimization. Theoretical evaluations and practical experiments support the effectiveness of the approach. When utilizing the CIFAR10 dataset, which is significantly smaller than ImageNet's training data, the method achieved comparable accuracy to models trained directly on ImageNet while demonstrating a computational cost reduction and improving efficiency by up to 150% compared to existing baselines. <div>
arXiv:2505.04877v1 Announce Type: new 
Abstract: Mixed Precision Quantization (MPQ) has become an essential technique for optimizing neural network by determining the optimal bitwidth per layer. Existing MPQ methods, however, face a major hurdle: they require a computationally expensive search for quantization policies on large-scale datasets. To resolve this issue, we introduce a novel approach that first searches for quantization policies on small datasets and then generalizes them to large-scale datasets. This approach simplifies the process, eliminating the need for large-scale quantization fine-tuning and only necessitating model weight adjustment. Our method is characterized by three key techniques: sharpness-aware minimization for enhanced quantization generalization, implicit gradient direction alignment to handle gradient conflicts among different optimization objectives, and an adaptive perturbation radius to accelerate optimization. Both theoretical analysis and experimental results validate our approach. Using the CIFAR10 dataset (just 0.5\% the size of ImageNet training data) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a significantly lower computational cost, while improving efficiency by up to 150% over the baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.04888</link>
<guid>https://arxiv.org/abs/2505.04888</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake, generative AI, detection, feature disentanglement, multimedia content

<br /><br />Summary: Remarkable advancements in generative AI have led to new deepfake categories that exhibit unprecedented realism, posing challenges to law enforcement and public trust. These face deepfakes have created significant confusion and deception, undermining societal faith in multimedia content. Existing deepfake detection methods are struggling to keep pace with the rapid evolution of deepfake technologies, primarily due to their reliance on specific forgery artifacts that limit generalization capabilities. To address the issue of malicious face deepfakes, this paper introduces a novel strategy that leverages coarse-to-fine spatial and semantic information, ensuring feature distinctiveness and reducing redundancy. A key innovation is the implementation of a feature orthogonality-based disentanglement strategy that facilitates branch-level and cross-branch feature disentanglement. This approach enables the integration of various feature vectors while maintaining simplicity and enhancing generalization. Experimental results on three public benchmarks—FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC)—demonstrate that this method outperforms current state-of-the-art techniques, achieving improvements of 5% on Celeb-DF and 7% on DFDC in cross-dataset evaluations. <div>
arXiv:2505.04888v1 Announce Type: new 
Abstract: Remarkable advancements in generative AI technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. In particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. This is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. To combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. A novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. Comprehensive experiments on three public benchmarks: FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a cross-dataset evaluation setting.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
<link>https://arxiv.org/abs/2505.04899</link>
<guid>https://arxiv.org/abs/2505.04899</guid>
<content:encoded><![CDATA[
<div> Keywords: representation learning, organ-wise tokenization, interpretability, medical imaging, segmentation

<br /><br />Summary: Recent advancements in representation learning have highlighted the limitations of holistic embeddings, which entangle multiple semantic components, specifically in the realm of medical imaging. To overcome these challenges, the authors propose an Organ-Wise Tokenization (OWT) framework, complemented by a Token Group-based Reconstruction (TGR) training paradigm. OWT distinctively separates an image into token groups, each corresponding to specific organs or semantic entities, thereby ensuring that individual token groups capture organ-specific information. This structural separation enhances interpretability, generalization, and efficiency while allowing for precise control in various downstream tasks. Experimental results utilizing CT and MRI datasets indicate that OWT significantly improves image reconstruction and segmentation performance. Furthermore, OWT facilitates innovative semantic-level generation and retrieval applications that traditional holistic embedding methods cannot achieve. These insights emphasize the promise of OWT as a foundational framework for semantically disentangled representation learning. This approach not only showcases broad scalability but also substantial applicability to practical medical imaging scenarios and beyond, paving the way for enhanced methodologies in the field. <div>
arXiv:2505.04899v1 Announce Type: new 
Abstract: Recent advances in representation learning often rely on holistic, black-box embeddings that entangle multiple semantic components, limiting interpretability and generalization. These issues are especially critical in medical imaging. To address these limitations, we propose an Organ-Wise Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR) training paradigm. Unlike conventional approaches that produce holistic features, OWT explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. Our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while allowing fine-grained control in downstream tasks. Experiments on CT and MRI datasets demonstrate the effectiveness of OWT in not only achieving strong image reconstruction and segmentation performance, but also enabling novel semantic-level generation and retrieval applications that are out of reach for standard holistic embedding methods. These findings underscore the potential of OWT as a foundational framework for semantically disentangled representation learning, offering broad scalability and applicability to real-world medical imaging scenarios and beyond.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization</title>
<link>https://arxiv.org/abs/2505.04905</link>
<guid>https://arxiv.org/abs/2505.04905</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly Supervised Object Localization, Class Activation Map, Segment Anything Model, fine-grained segmentation, mask prompt  

<br /><br />Summary: Weakly Supervised Object Localization (WSOL) seeks to identify objects using only image-level labels, minimizing annotation costs. Traditional methods like Class Activation Maps (CAM) and self-attention maps struggle to capture fine-grained pixel-level information, limiting WSOL's effectiveness. To overcome this challenge, the authors leverage the zero-shot generalization and fine-grained segmentation capabilities of the Segment Anything Model (SAM) to enhance activation of complete object regions. They introduce a novel mask prompt for the SAM, called the Pro2SAM network, which utilizes grid points instead of relying solely on single point prompts to address semantic ambiguity. A Global Token Transformer (GTFormer) generates a coarse-grained foreground map that acts as a flexible mask prompt, incorporating patch tokens and global tokens to capture foreground semantics. The authors then input dense grid points into SAM to maximize the likelihood of accurate foreground masks. They also propose a pixel-level similarity metric to match mask prompts with SAM, selecting the mask with the highest score as the final localization output. Experimental results demonstrate that Pro2SAM achieves state-of-the-art performance on the CUB-200-2011 and ILSVRC datasets, achieving Top-1 Localization rates of 84.03% and 66.85%, respectively. <div>
arXiv:2505.04905v1 Announce Type: new 
Abstract: Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\% and 66.85\% Top-1 Loc, respectively.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.04911</link>
<guid>https://arxiv.org/abs/2505.04911</guid>
<content:encoded><![CDATA[
<div> Keywords: SpatialPrompting, zero-shot spatial reasoning, multimodal, keyframe-driven, benchmark datasets<br /><br />Summary: This study presents SpatialPrompting, a new framework that leverages the reasoning capabilities of multimodal large language models to facilitate zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike traditional approaches that require costly 3D-specific fine-tuning and specialized inputs such as point clouds or voxel representations, SpatialPrompting employs a keyframe-driven prompt generation strategy. The method selects a diverse and informative set of keyframes from image sequences using metrics including vision-language similarity, Mahalanobis distance, field of view, and image sharpness. These keyframes are integrated with camera pose data to effectively abstract spatial relationships and infer complex 3D structures. This innovative framework establishes a new flexible paradigm for spatial reasoning by utilizing intuitive visual and positional cues. Moreover, it achieves state-of-the-art zero-shot performance on benchmark datasets like ScanQA and SQA3D across multiple metrics. Ultimately, SpatialPrompting eliminates the need for specialized 3D inputs and fine-tuning, providing a simpler and more scalable alternative to conventional methods, thus paving the way for advancements in spatial reasoning tasks. <div>
arXiv:2505.04911v1 Announce Type: new 
Abstract: This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing</title>
<link>https://arxiv.org/abs/2505.04915</link>
<guid>https://arxiv.org/abs/2505.04915</guid>
<content:encoded><![CDATA[
<div> Keywords: scene text editing, diffusion-based methods, glyph encoder, stroke-level precision, multi-scale features

<br /><br />Summary: Scene text editing involves modifying text in images while maintaining style and visual coherence. Diffusion-based methods have potential in text generation; however, they often generate distorted characters, particularly complex ones like Chinese. Characters consist of intricate stroke patterns that require precise maintenance in these systems. To address this issue, the authors introduce GlyphMastero, a glyph encoder designed to enhance the capability of latent diffusion models for generating text with stroke-level accuracy. Existing methods often overlook the hierarchical nature of text structures, leading to inefficiencies. GlyphMastero addresses this gap by capturing cross-level interactions between individual characters and text lines using a novel glyph attention module. Additionally, the model employs a feature pyramid network to integrate multi-scale features from an OCR backbone. This combination allows for detailed glyph-aware guidance, resulting in improved control over scene text generation. The proposed method demonstrates a significant 18.02% increase in sentence accuracy compared to the leading multi-lingual scene text editing methods and also reduces the text-region Fréchet inception distance by 53.28%, highlighting its effectiveness in generating high-quality scene text. <div>
arXiv:2505.04915v1 Announce Type: new 
Abstract: Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region Fr\'echet inception distance by 53.28\%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Detector with Frame Dynamics is a Strong Tracker</title>
<link>https://arxiv.org/abs/2505.04917</link>
<guid>https://arxiv.org/abs/2505.04917</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared tracking, Anti-UAV, motion-aware learning, frame dynamics, trajectory constraint filtering  

<br /><br />Summary:  
Infrared object tracking is essential for Anti-Unmanned Aerial Vehicle (Anti-UAV) applications, yet existing trackers often face challenges with tiny targets due to reliance on cropped templates and limited motion capabilities. This paper proposes a novel infrared tiny-object tracker that significantly improves tracking performance by integrating global detection with motion-aware learning and temporal priors. The method introduces two key innovations to enhance effectiveness. First, it utilizes frame dynamics, employing frame differences and optical flow to better encode prior target features and motion characteristics, thereby improving target-background distinction. Second, it implements a trajectory constraint filtering strategy in the post-processing phase, which capitalizes on spatio-temporal priors to reduce false positives and bolster tracking robustness. Comprehensive experiments demonstrate that this approach consistently outperforms existing methods across various metrics in challenging infrared UAV tracking scenarios. The method has achieved notable success, earning state-of-the-art performance during the 4th Anti-UAV Challenge, where it secured 1st place in Track 1 and 2nd place in Track 2. <div>
arXiv:2505.04917v1 Announce Type: new 
Abstract: Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Reasoning, Large Multimodal Reasoning Models, Chain-of-Thought, Omni-modal Generalization, Agentic Behavior

<br /><br />Summary: The article discusses the importance of reasoning in artificial intelligence, particularly within Large Multimodal Reasoning Models (LMRMs) that integrate various modalities such as text, images, audio, and video. It highlights the evolution of multimodal reasoning from early, modular approaches to unified frameworks that enhance cross-modal understanding. Specifically, the authors review initial efforts where reasoning was implicitly included in task-specific modules, transitioning to contemporary models that incorporate Multimodal Chain-of-Thought (MCoT) and reinforcement learning to facilitate more structured reasoning processes. The survey addresses ongoing challenges, including omni-modal generalization, the depth of reasoning, and the need for agentic behavior in these systems. Furthermore, it presents a developmental roadmap to guide future research toward the conceptualization of native large multimodal reasoning models (N-LMRMs), which are designed to support scalable, adaptive reasoning and planning capabilities in complex, real-world environments. The authors aim to spark discussions on how to effectively advance multimodal reasoning research to create more robust AI systems capable of effective decision-making and problem-solving in diverse settings. <div>
arXiv:2505.04921v1 Announce Type: new 
Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training</title>
<link>https://arxiv.org/abs/2505.04922</link>
<guid>https://arxiv.org/abs/2505.04922</guid>
<content:encoded><![CDATA[
<div> Keywords: palmprint recognition, Canny2Palm, synthetic data, Pix2Pix, large-scale pre-training

<br /><br />Summary: Palmprint recognition offers a secure and privacy-friendly biometric identification method, but it faces challenges due to limited palmprint data. Recent research focuses on synthesizing virtual palmprints for extensive pre-training. This paper presents a novel synthesis method called Canny2Palm, which utilizes the Canny edge detector to extract palm textures, conditioning a Pix2Pix network for realistic palmprint generation. By reassembling textures from diverse identities, new identities can be created by introducing new assemblies to the generator. The Canny2Palm method not only generates realistic data that aligns with the distribution of actual palmprints but also facilitates controllable diversity, allowing for the creation of a large-scale variety of new identities. In open-set palmprint recognition benchmarks, models pre-trained with data from Canny2Palm demonstrate improved performance, achieving up to 7.2% higher identification accuracy compared to state-of-the-art approaches. Furthermore, models pre-trained with Canny2Palm continue to show improvement as synthetic IDs increase to 10,000, while performances of those using existing methods tend to plateau, highlighting the effectiveness and potential of Canny2Palm for large-scale pre-training. <div>
arXiv:2505.04922v1 Announce Type: new 
Abstract: Palmprint recognition is a secure and privacy-friendly method of biometric identification. One of the major challenges to improve palmprint recognition accuracy is the scarcity of palmprint data. Recently, a popular line of research revolves around the synthesis of virtual palmprints for large-scale pre-training purposes. In this paper, we propose a novel synthesis method named Canny2Palm that extracts palm textures with Canny edge detector and uses them to condition a Pix2Pix network for realistic palmprint generation. By re-assembling palmprint textures from different identities, we are able to create new identities by seeding the generator with new assemblies. Canny2Palm not only synthesizes realistic data following the distribution of real palmprints but also enables controllable diversity to generate large-scale new identities. On open-set palmprint recognition benchmarks, models pre-trained with Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2% higher identification accuracy. Moreover, the performance of models pre-trained with Canny2Palm continues to improve given 10,000 synthetic IDs while those with existing methods already saturate, demonstrating the potential of our method for large-scale pre-training.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration</title>
<link>https://arxiv.org/abs/2505.04938</link>
<guid>https://arxiv.org/abs/2505.04938</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, pyramid registration network, Residual Feature Fusion Module, Residual Deformation Field Fusion Module, registration accuracy  

<br /><br />Summary: In recent years, deformable medical image registration techniques have significantly advanced, yet existing models still face challenges in efficiently extracting both coarse and fine-grained features in parallel. To address these limitations, a new pyramid registration network has been constructed, named FF-PNet, which leverages two innovative modules. For coarse-grained feature extraction, a Residual Feature Fusion Module (RFFM) is designed, while a Residual Deformation Field Fusion Module (RDFFM) is introduced for capturing fine-grained image deformations. The parallel operation of these two modules allows the model to effectively manage complex image deformations. Notably, the encoding stage of FF-PNet relies solely on traditional convolutional neural networks, eschewing attention mechanisms or multilayer perceptrons, yet still achieves significant improvements in registration accuracy. This emphasizes the enhanced feature decoding capabilities provided by RFFM and RDFFM. Comprehensive experiments conducted on the LPBA and OASIS datasets demonstrate that the proposed network consistently outperforms established methods in key evaluation metrics, such as the Dice Similarity Coefficient, confirming its efficacy in the domain of medical image registration. <div>
arXiv:2505.04938v1 Announce Type: new 
Abstract: In recent years, deformable medical image registration techniques have made significant progress. However, existing models still lack efficiency in parallel extraction of coarse and fine-grained features. To address this, we construct a new pyramid registration network based on feature and deformation field (FF-PNet). For coarse-grained feature extraction, we design a Residual Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a Residual Deformation Field Fusion Module (RDFFM). Through the parallel operation of these two modules, the model can effectively handle complex image deformations. It is worth emphasizing that the encoding stage of FF-PNet only employs traditional convolutional neural networks without any attention mechanisms or multilayer perceptrons, yet it still achieves remarkable improvements in registration accuracy, fully demonstrating the superior feature decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on the LPBA and OASIS datasets. The results show our network consistently outperforms popular methods in metrics like the Dice Similarity Coefficient.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping</title>
<link>https://arxiv.org/abs/2505.04941</link>
<guid>https://arxiv.org/abs/2505.04941</guid>
<content:encoded><![CDATA[
<div> Keywords: building damage assessment, remote sensing images, pseudo-label learning, multi-model fusion, change detection  

<br /><br />Summary: This study focuses on accurately assessing building damage using bi-temporal multi-modal remote sensing images, which is vital for disaster response and recovery. It introduces a Building-Guided Pseudo-Label Learning Framework to tackle the challenges posed by the mapping of building damage from pre-disaster optical and post-disaster SAR images. Initially, the research trains multiple building extraction models using pre-disaster optical images coupled with building labels. To improve building segmentation, strategies like multi-model fusion and test-time augmentation are applied to create pseudo-probabilities, culminating in a low-uncertainty pseudo-label training method for refinement. Subsequently, a change detection model is developed, using bi-temporal cross-modal images and damaged building labels. To enhance damage classification accuracy, a building-guided low-uncertainty pseudo-label refinement approach is introduced, utilizing prior knowledge of buildings to inform pseudo-label generation for damaged structures. The efficacy of this framework is evidenced by experimental results from the 2025 IEEE GRSS Data Fusion Contest dataset, where it achieved the highest mean Intersection over Union (mIoU) score of 54.28% and secured the top position in the competition. <div>
arXiv:2505.04941v1 Announce Type: new 
Abstract: Accurate building damage assessment using bi-temporal multi-modal remote sensing images is essential for effective disaster response and recovery planning. This study proposes a novel Building-Guided Pseudo-Label Learning Framework to address the challenges of mapping building damage from pre-disaster optical and post-disaster SAR images. First, we train a series of building extraction models using pre-disaster optical images and building labels. To enhance building segmentation, we employ multi-model fusion and test-time augmentation strategies to generate pseudo-probabilities, followed by a low-uncertainty pseudo-label training method for further refinement. Next, a change detection model is trained on bi-temporal cross-modal images and damaged building labels. To improve damage classification accuracy, we introduce a building-guided low-uncertainty pseudo-label refinement strategy, which leverages building priors from the previous step to guide pseudo-label generation for damaged buildings, reducing uncertainty and enhancing reliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest dataset demonstrate the effectiveness of our approach, which achieved the highest mIoU score (54.28%) and secured first place in the competition.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models</title>
<link>https://arxiv.org/abs/2505.04946</link>
<guid>https://arxiv.org/abs/2505.04946</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, benchmark, fidelity, evaluation, synthesis  

<br /><br />Summary: Recent advancements in scalable deep architectures and large-scale pretraining have significantly improved text-to-video generation, enabling high-fidelity content across diverse styles, with applications in advertising, entertainment, and education. However, these models face challenges in rendering precise on-screen text, such as captions and mathematical formulas, which is crucial for applications requiring textual accuracy. To address this gap, the authors introduce T2VTextBench, the first human-evaluation benchmark specifically designed to assess on-screen text fidelity and temporal consistency in text-to-video models. The benchmark includes a suite of prompts with complex text strings and dynamic scene changes, testing each model's ability to maintain detailed instructions throughout the video frames. The researchers evaluate ten state-of-the-art text-to-video systems, spanning both open-source and commercial offerings. Their findings reveal that most of the models struggle to generate legible and consistent text, highlighting a significant shortcoming in current video generation technology. This research underscores the necessity for further advancements in textual manipulation within video synthesis to improve the overall quality and utility of text-to-video applications. <div>
arXiv:2505.04946v1 Announce Type: new 
Abstract: Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects</title>
<link>https://arxiv.org/abs/2505.04962</link>
<guid>https://arxiv.org/abs/2505.04962</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous picking, pose estimation, cuboidal objects, local registration, linear time approach

<br /><br />Summary: This paper introduces a solution for the autonomous picking of cuboidal objects from both organized and unorganized piles, emphasizing high precision. The focus is on an efficient method for accurate pose estimation of cuboid-shaped objects to minimize errors in target pose while being time-efficient. Traditional pose estimation techniques, particularly global point cloud registrations, often encounter minor pose errors, necessitating the use of local registration algorithms to enhance accuracy. However, these local methods introduce execution time overhead and uncertainty related to the final pose accuracy. To address these concerns, the authors propose an alternative approach that operates in linear time for pose error estimation and correction. The paper provides an overview of the entire solution and subsequently details the individual modules that constitute the proposed algorithm, presenting a comprehensive framework designed to improve the reliability and efficiency of pose estimation in robotic applications for picking tasks. The proposed method aims to achieve both precision and speed, which are critical for effective automation in various industrial settings. <div>
arXiv:2505.04962v1 Announce Type: new 
Abstract: The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis</title>
<link>https://arxiv.org/abs/2505.04963</link>
<guid>https://arxiv.org/abs/2505.04963</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, ViCTr, liver cirrhosis, pathologies, image synthesis  

<br /><br />Summary: Synthesizing medical images, particularly for conditions like liver cirrhosis, poses significant challenges due to limited annotated data and domain gaps. Existing methodologies often struggle with maintaining anatomical fidelity and addressing diffuse pathologies. To tackle these issues, this work introduces ViCTr (Vital Consistency Transfer), a two-stage framework that integrates a rectified flow trajectory with a Tweedie-corrected diffusion process. The first stage involves pretraining ViCTr on the ATLAS-8k dataset utilizing Elastic Weight Consolidation (EWC) to preserve anatomy. In the second stage, the model undergoes adversarial fine-tuning with Low-Rank Adaptation (LoRA) to enable precise control over the severity of pathologies. Notably, ViCTr reformulates Tweedie's formula, allowing for one-step sampling that trims inference time from 50 to just 4 steps while maintaining anatomical realism. Evaluation on BTCV, AMOS, and CirrMRI600+ datasets reveals state-of-the-art performance, with a Medical Frechet Inception Distance (MFID) score that is 28% lower than previous techniques. Additionally, the model improves nnUNet segmentation by 3.8% mDSC when used for data augmentation. Radiologist reviews affirm the clinical indistinguishability of ViCTr-generated MRIs from actual scans, marking a significant advancement in AI-driven medical imaging research. <div>
arXiv:2505.04963v1 Announce Type: new 
Abstract: Synthesizing medical images remains challenging due to limited annotated pathological data, modality domain gaps, and the complexity of representing diffuse pathologies such as liver cirrhosis. Existing methods often struggle to maintain anatomical fidelity while accurately modeling pathological features, frequently relying on priors derived from natural images or inefficient multi-step sampling. In this work, we introduce ViCTr (Vital Consistency Transfer), a novel two-stage framework that combines a rectified flow trajectory with a Tweedie-corrected diffusion process to achieve high-fidelity, pathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k dataset using Elastic Weight Consolidation (EWC) to preserve critical anatomical structures. We then fine-tune the model adversarially with Low-Rank Adaptation (LoRA) modules for precise control over pathology severity. By reformulating Tweedie's formula within a linear trajectory framework, ViCTr supports one-step sampling, reducing inference from 50 steps to just 4, without sacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and CirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art performance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for cirrhosis synthesis 28% lower than existing approaches and improving nnUNet segmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews indicate that ViCTr-generated liver cirrhosis MRIs are clinically indistinguishable from real scans. To our knowledge, ViCTr is the first method to provide fine-grained, pathology-aware MRI synthesis with graded severity control, closing a critical gap in AI-driven medical imaging research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems</title>
<link>https://arxiv.org/abs/2505.04964</link>
<guid>https://arxiv.org/abs/2505.04964</guid>
<content:encoded><![CDATA[
<div> Keywords: coronary angiography, AI, VLMs, decision support, key-frame detection  

<br /><br />Summary: Coronary angiography (CAG) is recognized as the gold-standard imaging technique for assessing coronary artery disease, but its interpretation is primarily dependent on expert cardiologists. To enhance decision-making with AI, the authors present a two-stage, physician-curated workflow alongside a bilingual CAG image-report dataset (Japanese/English). In the first stage, they sampled 14,686 frames from 539 exams and annotated them for key-frame detection and laterality classification. A ConvNeXt-Base CNN trained on this dataset demonstrated an impressive 0.96 F1 score for laterality classification, even with low-contrast frames. In the second stage, this CNN was applied to 243 independent exams to extract 1,114 key frames, which were paired with their associated pre-procedure reports and validated diagnostic and treatment summaries, forming a parallel corpus. The authors subsequently fine-tuned three open-source Vision-Language Models (VLMs)—PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3—utilizing Low-Rank Adaptation (LoRA), evaluating them against VLScore and cardiologist assessments. Although PaliGemma2 with LoRA received the highest VLScore, Gemma3 with LoRA garnered the best clinician rating, thus designating it as the optimal model, CAG-VLM, indicating its potential to aid cardiologists in generating clinical reports and treatment recommendations from CAG images. <div>
arXiv:2505.04964v1 Announce Type: new 
Abstract: Coronary angiography (CAG) is the gold-standard imaging modality for evaluating coronary artery disease, but its interpretation and subsequent treatment planning rely heavily on expert cardiologists. To enable AI-based decision support, we introduce a two-stage, physician-curated pipeline and a bilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686 frames from 539 exams and annotate them for key-frame detection and left/right laterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on laterality classification, even on low-contrast frames. Second, we apply the CNN to 243 independent exams, extract 1,114 key frames, and pair each with its pre-procedure report and expert-validated diagnostic and treatment summary, yielding a parallel corpus. We then fine-tune three open-source VLMs (PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate them using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains the highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean 7.20/10); we designate this best-performing model as CAG-VLM. These results demonstrate that specialized, fine-tuned VLMs can effectively assist cardiologists in generating clinical reports and treatment recommendations from CAG images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2505.04965</link>
<guid>https://arxiv.org/abs/2505.04965</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D visual grounding, DenseGrounding, ego-centric, language models, semantic enhancement  

<br /><br />Summary: Enabling intelligent agents to effectively comprehend and interact with 3D environments using natural language is essential for advancements in robotics and human-computer interaction. A primary challenge in this domain is ego-centric 3D visual grounding, where agents must identify target objects based on verbal descriptions. This task faces two significant hurdles: the loss of fine-grained visual semantics during the fusion of point clouds and ego-centric multi-view images, and the constraints imposed by arbitrary language descriptions on textual context. To tackle these challenges, we introduce DenseGrounding, which enhances both visual and textual semantics. Our approach includes a Hierarchical Scene Semantic Enhancer that captures global scene features while maintaining fine-grained details, thereby facilitating better cross-modal alignment. Additionally, we employ a Language Semantic Enhancer that utilizes large language models to generate enriched and contextually diverse textual descriptions during training. Extensive experiments demonstrate that DenseGrounding outperforms existing methods by 5.81% and 7.56% in accuracy on comprehensive and smaller datasets, respectively, setting a new state-of-the-art in ego-centric 3D visual grounding. Furthermore, our method won 1st place and received the Innovation Award at the CVPR 2024 Autonomous Grand Challenge, underscoring its effectiveness and robustness. <div>
arXiv:2505.04965v1 Announce Type: new 
Abstract: Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding. Our method also achieves 1st place and receives the Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</title>
<link>https://arxiv.org/abs/2505.04974</link>
<guid>https://arxiv.org/abs/2505.04974</guid>
<content:encoded><![CDATA[
<div> Keywords: Bilingual, Motion Generation, Dataset, Diffusion Model, Alignment

<br /><br />Summary: Bilingual text-to-motion generation synthesizes 3D human motions from bilingual text inputs and has significant potential in fields like gaming, film, and robotics. However, it faces challenges including the lack of bilingual motion-language datasets and alignment issues in diffusion models, which can result in semantically inconsistent or low-quality motions. To tackle these problems, the authors introduce BiHumanML3D, a novel bilingual human motion dataset, establishing an important benchmark for bilingual text-to-motion generation models. Additionally, they present the Bilingual Motion Diffusion model (BiMD), which utilizes cross-lingual aligned representations to capture semantic meanings, facilitating a unified bilingual approach. The study further introduces the Reward-guided sampling Alignment (ReAlign) method, featuring a step-aware reward model that evaluates alignment quality during the sampling process. This method employs a reward-guided strategy to steer the diffusion process toward better alignment. The reward model incorporates step-aware tokens along with a text-aligned module for semantic consistency and a motion-aligned module for realism, refining motions at each timestep. Experimental results indicate that the proposed approach considerably enhances text-motion alignment and motion quality when compared to existing state-of-the-art methods. <div>
arXiv:2505.04974v1 Announce Type: new 
Abstract: Bilingual text-to-motion generation, which synthesizes 3D human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. However, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. To address these challenges, we propose BiHumanML3D, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. Building upon this, we propose Reward-guided sampling Alignment (ReAlign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. Project page: https://wengwanjiang.github.io/ReAlign-page/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization</title>
<link>https://arxiv.org/abs/2505.04979</link>
<guid>https://arxiv.org/abs/2505.04979</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, attribute bias, deconfounding, debiasing, causal graph

<br /><br />Summary:  
Attribute bias in federated learning (FL) can degrade model performance by promoting non-causal associations. Current solutions either use data augmentation to enhance sample diversity or knowledge distillation for invariant representations, but they fall short of comprehensively analyzing inference paths and suffer from confounding factors. To overcome these challenges, the authors introduce the FedDDL method, which constructs a structured causal graph to analyze the inference process and utilizes backdoor adjustment to remove confounding paths. It includes an intra-client deconfounding learning module tailored for computer vision tasks, effectively decoupling backgrounds from objects and generating counterfactual samples that prevent the model from incorrectly using background information to infer labels. Additionally, it features an inter-client debiasing learning module that forms causal prototypes tailored to minimize background presence in prototypes. This innovative approach enhances the alignment of heterogeneous representations through causal prototypical regularization. Extensive experiments conducted on two benchmark datasets reveal that FedDDL significantly improves the model’s ability to focus on primary objects in unseen data, yielding an average increase of 4.5% in Top-1 Accuracy compared to nine existing state-of-the-art methods. <div>
arXiv:2505.04979v1 Announce Type: new 
Abstract: Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \underline{Fed}erated \underline{D}econfounding and \underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</title>
<link>https://arxiv.org/abs/2505.05001</link>
<guid>https://arxiv.org/abs/2505.05001</guid>
<content:encoded><![CDATA[
<div> Keywords: video stitching, warping shake, temporal stabilization, unsupervised learning, StabStitch++

<br /><br />Summary: The article addresses a new issue in video stitching known as warping shake, which results in undesirable temporal shakes due to sequentially unsmooth warps, even when input videos are stable. To tackle this issue, the authors propose a novel framework called StabStitch++, which integrates spatial stitching and temporal stabilization using unsupervised learning. Instead of the traditional approach that warps one image to align with another, StabStitch++ introduces a virtual midplane for projection, utilizing a differentiable bidirectional decomposition module to disentangle homography transformations. This evenly distributes alignment burdens and distortions across image views. The framework derives stitching trajectories by combining spatial and temporal warps inspired by video stabilization techniques. A warp smoothing model is introduced to create stable stitched videos, supported by a hybrid loss function that promotes content alignment, trajectory smoothness, and collaborative online processing. StabStitch++ improves upon its predecessor, StabStitch, by optimizing both alignment and stabilization simultaneously, particularly in real-time applications. The authors also establish a benchmarking dataset with diverse camera motions and scenes to evaluate their framework, demonstrating superior performance and efficiency in video stitching. <div>
arXiv:2505.05001v1 Announce Type: new 
Abstract: We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort</title>
<link>https://arxiv.org/abs/2505.05004</link>
<guid>https://arxiv.org/abs/2505.05004</guid>
<content:encoded><![CDATA[
<div> Keywords: thoracolumbar, stump ribs, deep-learning, segmentation, morphology

<br /><br />Summary: This study focuses on thoracolumbar stump ribs, which are critical indicators of thoracolumbar transitional vertebrae or enumeration anomalies. Unlike previous research that qualitatively describes these anomalies through manual assessment, this work automates the detection of stump ribs and quantitatively analyzes their morphology. A high-resolution deep-learning model for rib segmentation was successfully trained, achieving a significant improvement in accuracy (Dice score of 0.997 compared to 0.779, p-value < 0.01). Additionally, an iterative algorithm and piece-wise linear interpolation were employed to assess rib length, resulting in a success rate of 98.2%. Morphological analysis revealed that stump ribs articulate more posteriorly at the vertebrae (-19.2 ± 3.8 vs. -13.8 ± 2.5, p-value < 0.01), are thinner (260.6 ± 103.4 vs. 563.6 ± 127.1, p-value < 0.01), and demonstrate a more downward and sideways orientation in the initial centimeters compared to full-length ribs. The study also achieved an F1-score of 0.84 in distinguishing stump ribs from regular ribs, even with partial visibility. The model weights and masks developed are made publicly available for further research. <div>
arXiv:2505.05004v1 Announce Type: new 
Abstract: Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. While some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. To this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. When analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value < 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. We show that with partially visible ribs, these features can achieve an F1-score of 0.84 in differentiating stump ribs from regular ones. We publish the model weights and masks for public use.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition</title>
<link>https://arxiv.org/abs/2505.05007</link>
<guid>https://arxiv.org/abs/2505.05007</guid>
<content:encoded><![CDATA[
<div> Keywords: map matching, Hidden Markov Model, lane markings, scenario recognition, multilevel road

<br /><br />Summary: Accurate online map matching is vital for vehicle navigation and intelligent driving features. Existing methods struggle in complex road networks, particularly in multilevel areas. This study introduces an online Standard Definition (SD) map matching technique that employs a Hidden Markov Model (HMM) enhanced by multiple probability factors. The method accurately matches maps by utilizing lane markings and scenario recognition. Initially, lane markings are generated using a multi-lane tracking approach and linked to the SD map through HMM, creating an enriched SD map. Vehicles can then re-localize by applying Iterative Closest Point (ICP) registration based on these lane markings. The probability factor for lane marking detection is determined through association probabilities between adjacent lanes and roads. Additionally, a driving scenario recognition model yields an emission probability factor, significantly enhancing map matching on elevated roads and the urban roads below them. Extensive road tests in Europe and China validate the method, revealing superior accuracy compared to existing techniques, notably in multilevel road regions. The proposed method achieves F1 scores of 98.04% on the Zenseact Open Dataset and 94.60% on Shanghai multilevel area data, outperforming benchmark methods. Implementation details are available at https://github.com/TRV-Lab/LMSR-OMM. <div>
arXiv:2505.05007v1 Announce Type: new 
Abstract: Accurate online map matching is fundamental to vehicle navigation and the activation of intelligent driving functions. Current online map matching methods are prone to errors in complex road networks, especially in multilevel road area. To address this challenge, we propose an online Standard Definition (SD) map matching method by constructing a Hidden Markov Model (HMM) with multiple probability factors. Our proposed method can achieve accurate map matching even in complex road networks by carefully leveraging lane markings and scenario recognition in the designing of the probability factors. First, the lane markings are generated by a multi-lane tracking method and associated with the SD map using HMM to build an enriched SD map. In areas covered by the enriched SD map, the vehicle can re-localize itself by performing Iterative Closest Point (ICP) registration for the lane markings. Then, the probability factor accounting for the lane marking detection can be obtained using the association probability between adjacent lanes and roads. Second, the driving scenario recognition model is applied to generate the emission probability factor of scenario recognition, which improves the performance of map matching on elevated roads and ordinary urban roads underneath them. We validate our method through extensive road tests in Europe and China, and the experimental results show that our proposed method effectively improves the online map matching accuracy as compared to other existing methods, especially in multilevel road area. Specifically, the experiments show that our proposed method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset and test data of multilevel road areas in Shanghai respectively, significantly outperforming benchmark methods. The implementation is available at https://github.com/TRV-Lab/LMSR-OMM.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Contextual Embedding for Robust Far-View Borehole Detection</title>
<link>https://arxiv.org/abs/2505.05008</link>
<guid>https://arxiv.org/abs/2505.05008</guid>
<content:encoded><![CDATA[
<div> Keywords: controlled blasting, detection method, adaptive augmentation, embedding stabilization, contextual refinement  

<br /><br />Summary:  
In controlled blasting operations, the ability to detect densely packed small boreholes from far-view imagery is vital for ensuring safety and operational efficiency. Existing methods struggle due to the minute scale, dense arrangements, and the lack of distinctive visual features of these boreholes. To overcome these limitations, the authors propose a novel adaptive detection approach that enhances existing architectures like YOLO by employing consistent embedding representations through exponential moving average (EMA)-based statistical updates. The method introduces three key components: first, adaptive augmentation that utilizes dynamically updated image statistics to effectively manage variations in illumination and texture; second, embedding stabilization to guarantee consistent and reliable feature extraction; and third, contextual refinement that uses spatial context to enhance detection accuracy. The use of EMA is particularly beneficial given the challenges of limited visual complexity and the small scale of boreholes, leading to stable and robust representation learning. Experimental results on a challenging proprietary quarry-site dataset demonstrate significant improvements over baseline YOLO-based architectures, underscoring the proposed method's effectiveness under realistic and complex industrial conditions. <div>
arXiv:2505.05008v1 Announce Type: new 
Abstract: In controlled blasting operations, accurately detecting densely distributed tiny boreholes from far-view imagery is critical for operational safety and efficiency. However, existing detection methods often struggle due to small object scales, highly dense arrangements, and limited distinctive visual features of boreholes. To address these challenges, we propose an adaptive detection approach that builds upon existing architectures (e.g., YOLO) by explicitly leveraging consistent embedding representations derived through exponential moving average (EMA)-based statistical updates.
  Our method introduces three synergistic components: (1) adaptive augmentation utilizing dynamically updated image statistics to robustly handle illumination and texture variations; (2) embedding stabilization to ensure consistent and reliable feature extraction; and (3) contextual refinement leveraging spatial context for improved detection accuracy. The pervasive use of EMA in our method is particularly advantageous given the limited visual complexity and small scale of boreholes, allowing stable and robust representation learning even under challenging visual conditions. Experiments on a challenging proprietary quarry-site dataset demonstrate substantial improvements over baseline YOLO-based architectures, highlighting our method's effectiveness in realistic and complex industrial scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOAP: Style-Omniscient Animatable Portraits</title>
<link>https://arxiv.org/abs/2505.05022</link>
<guid>https://arxiv.org/abs/2505.05022</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D avatars, diffusion models, topology-consistent, animation controls, soap

<br /><br />Summary: Creating animatable 3D avatars from a single image is a complex task, particularly due to style limitations such as realistic, cartoon, or anime representations, as well as challenges in handling accessories and hairstyles. Traditional 3D diffusion models have made strides in reconstructing single-view images but often lack necessary animation controls and can produce artifacts due to the domain gap. To address these issues, we introduce SOAP, a style-omniscient framework designed to generate rigged, topology-consistent avatars from any portrait. Our approach utilizes a multiview diffusion model trained on a dataset of 24,000 3D heads across various styles and employs an adaptive optimization pipeline to deform the FLAME mesh while preserving its topology and rigging through differentiable rendering techniques. The outcome is textured avatars that support FACS-based animation and include realistic features such as eyeballs, teeth, and intricate hairstyles or accessories. Extensive experiments validate the effectiveness of our method, demonstrating its superiority over existing state-of-the-art techniques in both single-view head modeling and diffusion-based 3D generation from images. Our implementation and dataset are publicly accessible for research at https://github.com/TingtingLiao/soap. <div>
arXiv:2505.05022v1 Announce Type: new 
Abstract: Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at https://github.com/TingtingLiao/soap.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Split Matching for Inductive Zero-shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.05023</link>
<guid>https://arxiv.org/abs/2505.05023</guid>
<content:encoded><![CDATA[
<div> Keywords: Zero-shot Semantic Segmentation, query-based segmentation, Split Matching, Hungarian matching, Multi-scale Feature Enhancement

<br /><br />Summary: 
This article focuses on Zero-shot Semantic Segmentation (ZSS), which involves segmenting categories not annotated during training. Traditional fine-tuning of vision-language models faces challenges, primarily overfitting to seen categories due to the absence of supervision for unseen classes. It highlights query-based segmentation as a promising alternative that excels in object localization without explicit labels. A major issue with conventional Hungarian matching in this context is its reliance on full supervision, often misclassifying unseen categories as background. To tackle this, the authors propose a novel Split Matching (SM) strategy that separates Hungarian matching into two components: one for annotated regions (seen classes) and another for unannotated regions (unseen candidates). They cluster CLIP dense features to create pseudo masks and derive region-level embeddings. Matching occurs independently for these two groups, focusing on class-level similarity and mask-level consistency. Additionally, they introduce a Multi-scale Feature Enhancement (MFE) module, which enhances decoder features through multi-scale aggregation for improved spatial detail capture. The work presents SM as the first decoupled Hungarian matching approach under the inductive ZSS setting, achieving state-of-the-art results on two standard benchmarks. <div>
arXiv:2505.05023v1 Announce Type: new 
Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not annotated during training. While fine-tuning vision-language models has achieved promising results, these models often overfit to seen categories due to the lack of supervision for unseen classes. As an alternative to fully supervised approaches, query-based segmentation has shown great latent in ZSS, as it enables object localization without relying on explicit labels. However, conventional Hungarian matching, a core component in query-based frameworks, needs full supervision and often misclassifies unseen categories as background in the setting of ZSS. To address this issue, we propose Split Matching (SM), a novel assignment strategy that decouples Hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). Specifically, we partition the queries into seen and candidate groups, enabling each to be optimized independently according to its available supervision. To discover unseen candidates, we cluster CLIP dense features to generate pseudo masks and extract region-level embeddings using CLS tokens. Matching is then conducted separately for the two groups based on both class-level similarity and mask-level consistency. Additionally, we introduce a Multi-scale Feature Enhancement (MFE) module that refines decoder features through residual multi-scale aggregation, improving the model's ability to capture spatial details across resolutions. SM is the first to introduce decoupled Hungarian matching under the inductive ZSS setting, and achieves state-of-the-art performance on two standard benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition</title>
<link>https://arxiv.org/abs/2505.05043</link>
<guid>https://arxiv.org/abs/2505.05043</guid>
<content:encoded><![CDATA[
<div> Keywords: facial affect, emotion recognition, xTrace, video datasets, dimensional emotions

<br /><br />Summary: The paper addresses the challenges of recognizing expressive behaviors in face videos, focusing on two primary issues: the lack of large-scale labeled datasets and the difficulty of extracting meaningful facial features. To tackle these, the authors introduce xTrace, a tool designed for analyzing facial expressive behavior and predicting continuous values of emotions, specifically valence and arousal. xTrace is trained on an extensive dataset comprising approximately 450,000 videos that encapsulate a wide range of emotional expressions, enhancing its versatility in real-world applications. It employs facial affect descriptors that ensure explainability while maintaining high accuracy and low computational demands. The performance of xTrace is benchmarked against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox. In validation using a set of 50,000 videos, xTrace achieved a mean Concordance Correlation Coefficient (CCC) of 0.86 and a mean absolute error of 0.13. A thorough error analysis reveals its high accuracy in recognizing emotions, robustness to non-frontal head poses, and a strong relationship between uncertainty estimates and accuracy, validating its effectiveness in the domain of Affective Computing. <div>
arXiv:2505.05043v1 Announce Type: new 
Abstract: Recognising expressive behaviours in face videos is a long-standing challenge in Affective Computing. Despite significant advancements in recent years, it still remains a challenge to build a robust and reliable system for naturalistic and in-the-wild facial expressive behaviour analysis in real time. This paper addresses two key challenges in building such a system: (1). The paucity of large-scale labelled facial affect video datasets with extensive coverage of the 2D emotion space, and (2). The difficulty of extracting facial video features that are discriminative, interpretable, robust, and computationally efficient. Toward addressing these challenges, we introduce xTrace, a robust tool for facial expressive behaviour analysis and predicting continuous values of dimensional emotions, namely valence and arousal, from in-the-wild face videos.
  To address challenge (1), our affect recognition model is trained on the largest facial affect video data set, containing ~450k videos that cover most emotion zones in the dimensional emotion space, making xTrace highly versatile in analysing a wide spectrum of naturalistic expressive behaviours. To address challenge (2), xTrace uses facial affect descriptors that are not only explainable, but can also achieve a high degree of accuracy and robustness with low computational complexity. The key components of xTrace are benchmarked against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox. On an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86 mean CCC and 0.13 mean absolute error values. We present a detailed error analysis of affect predictions from xTrace, illustrating (a). its ability to recognise emotions with high accuracy across most bins in the 2D emotion space, (b). its robustness to non-frontal head pose angles, and (c). a strong correlation between its uncertainty estimates and its accuracy.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
<link>https://arxiv.org/abs/2505.05049</link>
<guid>https://arxiv.org/abs/2505.05049</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, uncertainty quantification, Bayesian entropy, semi-supervised, predictive capabilities  

<br /><br />Summary:  
The introduction of the Segment Anything Model (SAM) has significantly impacted semantic segmentation applications, raising the importance of quantifying uncertainty in its outputs. The class-agnostic nature of SAM presents challenges for existing uncertainty quantification (UQ) methods. In response, this paper introduces a novel UQ model grounded in a Bayesian entropy framework that incorporates aleatoric, epistemic, and a newly defined task uncertainty. This theoretical foundation leads to the creation of USAM, a lightweight post-hoc method for uncertainty quantification. The model identifies the sources of uncertainty as stemming from under-parameterised models, poor prompts, or ambiguities within images. Demonstrating remarkable predictive capabilities, USAM is applied across several datasets, including SA-V, MOSE, ADE20k, DAVIS, and COCO, outperforming other methods. Furthermore, it offers a computationally efficient and user-friendly UQ solution capable of supporting user-prompting and enhancing semi-supervised learning pipelines. USAM allows users to effectively manage the tradeoff between accuracy and cost efficiency in their applications, making it a valuable tool in the realm of semantic segmentation. <div>
arXiv:2505.05049v1 Announce Type: new 
Abstract: The introduction of the Segment Anything Model (SAM) has paved the way for numerous semantic segmentation applications. For several tasks, quantifying the uncertainty of SAM is of particular interest. However, the ambiguous nature of the class-agnostic foundation model SAM challenges current uncertainty quantification (UQ) approaches. This paper presents a theoretically motivated uncertainty quantification model based on a Bayesian entropy formulation jointly respecting aleatoric, epistemic, and the newly introduced task uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ method. Our model traces the root of uncertainty back to under-parameterised models, insufficient prompts or image ambiguities. Our proposed deterministic USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ alternative that can support user-prompting, enhance semi-supervised pipelines, or balance the tradeoff between accuracy and cost efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.05062</link>
<guid>https://arxiv.org/abs/2505.05062</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-Tailed Semi-Supervised Learning, large-scale visual foundation models, Unbiased Lightweight Fine-tuning, pseudo-labels, model performance  

<br /><br />Summary: This paper investigates the effects of large-scale visual foundation models, such as CLIP, on Long-Tailed Semi-Supervised Learning (LTSSL) using three strategies: Linear Probing (LP), Lightweight Fine-Tuning (LFT), and Full Fine-Tuning (FFT). The research reveals key insights: i) FFT leads to decreased model performance compared to LTSSL algorithms trained from scratch. Although LP and LFT enhance overall performance, they provide minimal benefits for tail classes. ii) LP generates numerous false pseudo-labels due to underlearned data. In contrast, while LFT reduces these false labels, it becomes overconfident, leading to biased fitting and exacerbating inherent pseudo-labeled and classifier biases in LTSSL, ultimately hindering performance in tail classes. To address these issues, the authors propose ULFine, an Unbiased Lightweight Fine-tuning strategy. ULFine mitigates overconfidence through confidence-aware adaptive fitting of textual prototypes and counters biases with complementary fusion of dual logits. The extensive experiments demonstrate that ULFine significantly decreases training costs by over ten times while substantially improving prediction accuracies compared to state-of-the-art methods. <div>
arXiv:2505.05062v1 Announce Type: new 
Abstract: Based on the success of large-scale visual foundation models like CLIP in various downstream tasks, this paper initially attempts to explore their impact on Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation model with three strategies: Linear Probing (LP), Lightweight Fine-Tuning (LFT), and Full Fine-Tuning (FFT). Our analysis presents the following insights: i) Compared to LTSSL algorithms trained from scratch, FFT results in a decline in model performance, whereas LP and LFT, although boosting overall model performance, exhibit negligible benefits to tail classes. ii) LP produces numerous false pseudo-labels due to \textit{underlearned} training data, while LFT can reduce the number of these false labels but becomes overconfident about them owing to \textit{biased fitting} training data. This exacerbates the pseudo-labeled and classifier biases inherent in LTSSL, limiting performance improvement in the tail classes. With these insights, we propose a Unbiased Lightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates the overconfidence via confidence-aware adaptive fitting of textual prototypes and counteracts the pseudo-labeled and classifier biases via complementary fusion of dual logits. Extensive experiments demonstrate that ULFine markedly decreases training costs by over ten times and substantially increases prediction accuracies compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FG-CLIP: Fine-Grained Visual and Textual Alignment</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
<div> Keywords: FG-CLIP, fine-grained understanding, multimodal tasks, long captions, negative samples  

<br /><br />Summary:  
Contrastive Language-Image Pre-training (CLIP) has shown impressive performance in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its reliance on coarse-grained short captions. To improve this, the authors introduce Fine-Grained CLIP (FG-CLIP) featuring three key innovations. First, FG-CLIP utilizes large multimodal models to generate 1.6 billion long caption-image pairs, enhancing the capture of global semantic details. Second, a high-quality dataset is constructed containing 12 million images with 40 million region-specific bounding boxes aligned with detailed captions, promoting precise and context-rich representations. Third, the inclusion of 10 million hard fine-grained negative samples helps the model to better distinguish subtle semantic differences. Corresponding training methodologies are carefully designed to leverage this data. Extensive experiments confirm that FG-CLIP exceeds the performance of the original CLIP and other leading methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These findings underscore FG-CLIP's efficacy in capturing intricate image details and enhancing overall model performance. The related data, code, and models are publicly available at https://github.com/360CVGroup/FG-CLIP. <div>
arXiv:2505.05071v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Affordances: Enabling Robots to Understand Object Functionality</title>
<link>https://arxiv.org/abs/2505.05074</link>
<guid>https://arxiv.org/abs/2505.05074</guid>
<content:encoded><![CDATA[
<div> Keywords: human-robot interaction, affordance prediction, reproducibility, Affordance Sheet, visual perception  

<br /><br />Summary: This work addresses the critical issue of reproducibility in human-robot interaction for assistive technologies, specifically focusing on the prediction of affordances, or the actions a robot can perform on objects. It critiques the varying formulations used for tasks like grasping detection and affordance classification, which can lead to unfair and unreliable benchmarks. To tackle this, a unified approach to visual affordance prediction is proposed. The authors conduct a comprehensive review of previous research, outlining strengths and weaknesses of existing methods and datasets, and identifying factors that hinder reproducibility. In pursuit of enhancing transparency, they introduce the Affordance Sheet, a documentation tool detailing solutions, datasets, and validation processes. Furthermore, they present a framework that links visual affordance prediction to the physical properties of objects, emphasizing that these properties influence robot interactions. By exploring the example of estimating object mass, the paper illustrates how this factor can significantly affect affordance predictions. Ultimately, this approach aims to connect affordance perception and robot actuation, synthesizing all relevant information about objects and their interactions to improve task accomplishment. <div>
arXiv:2505.05074v1 Announce Type: new 
Abstract: Human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. Predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. In this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. To address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. To favour transparency, we introduce the Affordance Sheet, a document to detail the proposed solution, the datasets, and the validation. As the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. Using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. Our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIDiff: Image Customization for Personalized Identities with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05081</link>
<guid>https://arxiv.org/abs/2505.05081</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generation, PIDiff, identity information, diffusion model, StyleGAN

<br /><br />Summary: This paper introduces PIDiff, a novel fine-tuning-based diffusion model for generating personalized identities from text prompts and identity images. It addresses limitations in previous works that struggle with disentangling identity and background information, which often leads to diminished identity characteristics and reduced image diversity. Prior attempts combined the W+ space from StyleGAN with diffusion models to enhance identity representation but suffered from semantic interference during training due to entanglement. PIDiff overcomes these challenges by employing a fine-tuning strategy that leverages the W+ space for accurate feature extraction and localization, thereby avoiding semantic entanglement. Furthermore, PIDiff enables style editing by preserving the identity features across varying levels of detail. It also integrates a cross-attention block and an optimized parameter strategy to maintain both identity preservation and generative capabilities of pre-trained models during inference for in-the-wild images. Experimental results demonstrate the effectiveness of PIDiff in enhancing personalized identity image generation, showing improved accuracy and diversity while accurately reflecting identity characteristics in generated images. <div>
arXiv:2505.05081v1 Announce Type: new 
Abstract: Text-to-image generation for personalized identities aims at incorporating the specific identity into images using a text prompt and an identity image. Based on the powerful generative capabilities of DDPMs, many previous works adopt additional prompts, such as text embeddings and CLIP image embeddings, to represent the identity information, while they fail to disentangle the identity information and background information. As a result, the generated images not only lose key identity characteristics but also suffer from significantly reduced diversity. To address this issue, previous works have combined the W+ space from StyleGAN with diffusion models, leveraging this space to provide a more accurate and comprehensive representation of identity features through multi-level feature extraction. However, the entanglement of identity and background information in in-the-wild images during training prevents accurate identity localization, resulting in severe semantic interference between identity and background. In this paper, we propose a novel fine-tuning-based diffusion model for personalized identities text-to-image generation, named PIDiff, which leverages the W+ space and an identity-tailored fine-tuning strategy to avoid semantic entanglement and achieves accurate feature extraction and localization. Style editing can also be achieved by PIDiff through preserving the characteristics of identity features in the W+ space, which vary from coarse to fine. Through the combination of the proposed cross-attention block and parameter optimization strategy, PIDiff preserves the identity information and maintains the generation capability for in-the-wild images of the pre-trained model during inference. Our experimental results validate the effectiveness of our method in this task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow</title>
<link>https://arxiv.org/abs/2505.05089</link>
<guid>https://arxiv.org/abs/2505.05089</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, optical flow, spatio-temporal, nonlinear motion, unsupervised learning  

<br /><br />Summary: Event cameras capture continuous motion information, making them ideal for optical flow estimation. Existing learning-based methods often employ frame-based techniques, overlooking the spatio-temporal characteristics of events. These methods typically assume linear motion between consecutive events, leading to increased errors in long-time sequences. This study highlights the importance of spatio-temporal information and the need for accurate nonlinear motion modeling for effective optical flow estimation. To address these challenges, we introduce E-NMSTFlow, an unsupervised event-based optical flow network designed for long sequences. Key components of our approach include the Spatio-Temporal Motion Feature Aware (STMFA) module and the Adaptive Motion Feature Enhancement (AMFE) module, both of which leverage rich spatio-temporal data for better associations. Furthermore, we present a nonlinear motion compensation loss that enhances unsupervised learning by accounting for the nonlinear motion among events. Our extensive experiments underscore the effectiveness of E-NMSTFlow, demonstrating its superiority over existing methods. Notably, it achieves the top ranking among unsupervised learning approaches on the MVSEC and DSEC-Flow datasets. For further details, visit our project page at https://wynelio.github.io/E-NMSTFlow. <div>
arXiv:2505.05089v1 Announce Type: new 
Abstract: Event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. However, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. Additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. In this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel unsupervised event-based optical flow network focusing on long-time sequences. We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. Meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. Extensive experiments demonstrate the effectiveness and superiority of our method. Remarkably, our method ranks first among unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project page is available at https://wynelio.github.io/E-NMSTFlow.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions</title>
<link>https://arxiv.org/abs/2505.05091</link>
<guid>https://arxiv.org/abs/2505.05091</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Disparity Estimation, Robustness, Benchmarking, Adversarial Attacks<br /><br />Summary: Deep learning (DL) has outperformed human capabilities in various benchmarks, particularly in disparity estimation—an essential task for applications like medical surgeries and autonomous navigation. However, reliance on DL methods for disparity estimation raises concerns due to their vulnerability to distribution shifts and adversarial attacks, which could undermine their reliability and generalization. Despite these challenges, a standardized benchmark to evaluate the robustness of disparity estimation techniques is currently lacking, impeding advancements in this area. To fill this void, we present DispBench, a comprehensive benchmarking tool designed to systematically assess the reliability of disparity estimation methods. DispBench tests robustness against synthetic image corruptions, including adversarial attacks and out-of-distribution shifts stemming from 2D Common Corruptions. This initiates the most extensive performance and robustness analysis of disparity estimation methods conducted to date, revealing significant correlations among accuracy, reliability, and generalization. The open-source code for DispBench is available at https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation. <div>
arXiv:2505.05091v1 Announce Type: new 
Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. One such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. However, DL-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. Despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.
  To address this gap, we introduce DispBench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. DispBench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2D Common Corruptions across multiple datasets and diverse corruption scenarios. We conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. Open-source code for DispBench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05101</link>
<guid>https://arxiv.org/abs/2505.05101</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-object editing, localization, attribute-object mismatch, MDE-Edit, optimization

<br /><br />Summary: The paper addresses the challenges of multi-object editing in complex scenes, particularly when objects overlap or interact. Two main issues are presented: inaccurate localization of target objects leading to incomplete edits and attribute-object mismatches causing semantic conflicts, such as color bleeding. Current methods struggle with these due to attention alignment problems and feature entanglement. To improve upon existing techniques, the authors propose MDE-Edit, a training-free optimization method that operates at the inference stage. This approach focuses on precise localized image manipulation and introduces two key components: Object Alignment Loss (OAL), which aligns multi-layer cross-attention with segmentation masks for better object positioning, and Color Consistency Loss (CCL), which enhances attribute attention for target regions while minimizing leakage to adjacent areas. This dual-loss framework ensures coherent and accurate edits across multiple objects. Experiments showcase that MDE-Edit achieves superior editing accuracy and visual quality compared to state-of-the-art methods, establishing it as an effective solution for complex multi-object image manipulation tasks. <div>
arXiv:2505.05101v1 Announce Type: new 
Abstract: Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation</title>
<link>https://arxiv.org/abs/2505.05136</link>
<guid>https://arxiv.org/abs/2505.05136</guid>
<content:encoded><![CDATA[
<div> Keywords: subglottic stenosis, bronchoscopy, automated evaluation, 3D model, dataset  

<br /><br />Summary: Subglottic stenosis involves the narrowing of the airway between the vocal cords and the trachea, typically assessed by estimating the percentage of obstruction through CT data or expert visual inspections, which can be subjective. This study proposes a novel automated pipeline for estimating subglottic stenosis severity during bronchoscopy without requiring the physician to navigate through the stenosed region. The method utilizes the physical effect of illumination decline in endoscopy to segment and track the airway lumen, generating a 3D model from a single frame to measure airway narrowing. Notably, this pipeline is the first of its kind for automated evaluation of subglottic stenosis severity using bronchoscopy images. Results demonstrate high consistency with ground-truth estimations from CT scans and expert assessments while showing reliable repeatability in multiple estimations for the same patient. Evaluation is performed using a newly created Subglottic Stenosis Dataset comprising real bronchoscopy procedures. This approach aims to facilitate quicker diagnoses and monitoring, minimizing radiation exposure for patients by eliminating the need for CT scans, and introduces the first public benchmark for assessing subglottic stenosis severity. <div>
arXiv:2505.05136v1 Announce Type: new 
Abstract: Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the airway between the vocal cords and the trachea. Its severity is typically evaluated by estimating the percentage of obstructed airway. This estimation can be obtained from CT data or through visual inspection by experts exploring the region. However, visual inspections are inherently subjective, leading to less consistent and robust diagnoses. No public methods or datasets are currently available for automated evaluation of this condition from bronchoscopy video.
  Methods: We propose a pipeline for automated subglottic stenosis severity estimation during the bronchoscopy exploration, without requiring the physician to traverse the stenosed region. Our approach exploits the physical effect of illumination decline in endoscopy to segment and track the lumen and obtain a 3D model of the airway. This 3D model is obtained from a single frame and is used to measure the airway narrowing.
  Results: Our pipeline is the first to enable automated and robust subglottic stenosis severity measurement using bronchoscopy images. The results show consistency with ground-truth estimations from CT scans and expert estimations, and reliable repeatability across multiple estimations on the same patient. Our evaluation is performed on our new Subglottic Stenosis Dataset of real bronchoscopy procedures data.
  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis severity using only bronchoscopy. Our approach can assist with and shorten diagnosis and monitoring procedures, with automated and repeatable estimations and less exploration time, and save radiation exposure to patients as no CT is required. Additionally, we release the first public benchmark for subglottic stenosis severity assessment.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models</title>
<link>https://arxiv.org/abs/2505.05163</link>
<guid>https://arxiv.org/abs/2505.05163</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, probabilistic embeddings, Gaussian Process, uncertainty calibration, cross-modal retrieval

<br /><br />Summary: Vision-Language Models (VLMs) aim to create joint representations by linking images and text within a shared latent space. However, traditional VLMs often face challenges in capturing the uncertainties related to ambiguities in visual and textual descriptions. These challenges arise from the multiple possible connections between images and texts. Current methods that address this issue typically require extensive datasets and do not fully utilize the robust representations learned by existing large-scale VLMs like CLIP. To overcome these limitations, this paper introduces GroVE, a novel post-hoc method for deriving probabilistic embeddings from fixed VLMs. GroVE employs a Gaussian Process Latent Variable Model (GPLVM) to construct a coherent low-dimensional latent space, effectively mapping image and text inputs into a unified representation. The optimization is based on objectives focused on single-modal embedding reconstruction and cross-modal alignment. After training, the resulting Gaussian Process model is capable of generating uncertainty-aware probabilistic embeddings. Evaluation results demonstrate that GroVE achieves state-of-the-art uncertainty calibration performance across various downstream tasks, such as cross-modal retrieval, visual question answering, and active learning. <div>
arXiv:2505.05163v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) learn joint representations by mapping images and text into a shared latent space. However, recent research highlights that deterministic embeddings from standard VLMs often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. Existing approaches tackle this by learning probabilistic embeddings during VLM training, which demands large datasets and does not leverage the powerful representations already learned by large-scale VLMs like CLIP. In this paper, we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model (GPLVM) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. Once trained, the Gaussian Process model generates uncertainty-aware probabilistic embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting</title>
<link>https://arxiv.org/abs/2505.05183</link>
<guid>https://arxiv.org/abs/2505.05183</guid>
<content:encoded><![CDATA[
arXiv:2505.05183v1 Announce Type: new 
Abstract: The safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving Teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). While previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. In this research, we unveil PaniCar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. This vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. In addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3, "manufacturer C", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors (YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. We also evaluate four SOTA flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. To mitigate this risk, we propose Caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster RCNN, Caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. In addition, Caracetamol is capable of processing frames at a rate of between 30-50 FPS, enabling real-time ADAS car detection.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.05189</link>
<guid>https://arxiv.org/abs/2505.05189</guid>
<content:encoded><![CDATA[
arXiv:2505.05189v1 Announce Type: new 
Abstract: Prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (VLMs) to the biomedical image classification tasks in few shot scenarios. However, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. In this work, we propose Biomed-DPT, a knowledge-enhanced dual modality prompt tuning technique. In designing the text prompt, Biomed-DPT constructs a dual prompt including the template-driven clinical prompts and the large language model (LLM)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. In designing the vision prompt, Biomed-DPT introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. Biomed-DPT achieves an average classification accuracy of 66.14\% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06\% in base classes and 75.97\% in novel classes, surpassing the Context Optimization (CoOp) method by 6.20\%, 3.78\%, and 8.04\%, respectively. Our code are available at \underline{https://github.com/Kanyooo/Biomed-DPT}.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution</title>
<link>https://arxiv.org/abs/2505.05209</link>
<guid>https://arxiv.org/abs/2505.05209</guid>
<content:encoded><![CDATA[
arXiv:2505.05209v1 Announce Type: new 
Abstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HQC-NBV: A Hybrid Quantum-Classical View Planning Approach</title>
<link>https://arxiv.org/abs/2505.05212</link>
<guid>https://arxiv.org/abs/2505.05212</guid>
<content:encoded><![CDATA[
arXiv:2505.05212v1 Announce Type: new 
Abstract: Efficient view planning is a fundamental challenge in computer vision and robotic perception, critical for tasks ranging from search and rescue operations to autonomous navigation. While classical approaches, including sampling-based and deterministic methods, have shown promise in planning camera viewpoints for scene exploration, they often struggle with computational scalability and solution optimality in complex settings. This study introduces HQC-NBV, a hybrid quantum-classical framework for view planning that leverages quantum properties to efficiently explore the parameter space while maintaining robustness and scalability. We propose a specific Hamiltonian formulation with multi-component cost terms and a parameter-centric variational ansatz with bidirectional alternating entanglement patterns that capture the hierarchical dependencies between viewpoint parameters. Comprehensive experiments demonstrate that quantum-specific components provide measurable performance advantages. Compared to the classical methods, our approach achieves up to 49.2% higher exploration efficiency across diverse environments. Our analysis of entanglement architecture and coherence-preserving terms provides insights into the mechanisms of quantum advantage in robotic exploration tasks. This work represents a significant advancement in integrating quantum computing into robotic perception systems, offering a paradigm-shifting solution for various robot vision tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model Quantization: A Review</title>
<link>https://arxiv.org/abs/2505.05215</link>
<guid>https://arxiv.org/abs/2505.05215</guid>
<content:encoded><![CDATA[
arXiv:2505.05215v1 Announce Type: new 
Abstract: Recent success of large text-to-image models has empirically underscored the exceptional performance of diffusion models in generative tasks. To facilitate their efficient deployment on resource-constrained edge devices, model quantization has emerged as a pivotal technique for both compression and acceleration. This survey offers a thorough review of the latest advancements in diffusion model quantization, encapsulating and analyzing the current state of the art in this rapidly advancing domain. First, we provide an overview of the key challenges encountered in the quantization of diffusion models, including those based on U-Net architectures and Diffusion Transformers (DiT). We then present a comprehensive taxonomy of prevalent quantization techniques, engaging in an in-depth discussion of their underlying principles. Subsequently, we perform a meticulous analysis of representative diffusion model quantization schemes from both qualitative and quantitative perspectives. From a quantitative standpoint, we rigorously benchmark a variety of methods using widely recognized datasets, delivering an extensive evaluation of the most recent and impactful research in the field. From a qualitative standpoint, we categorize and synthesize the effects of quantization errors, elucidating these impacts through both visual analysis and trajectory examination. In conclusion, we outline prospective avenues for future research, proposing novel directions for the quantization of generative models in practical applications. The list of related papers, corresponding codes, pre-trained models and comparison results are publicly available at the survey project homepage https://github.com/TaylorJocelyn/Diffusion-Model-Quantization.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does CLIP perceive art the same way we do?</title>
<link>https://arxiv.org/abs/2505.05229</link>
<guid>https://arxiv.org/abs/2505.05229</guid>
<content:encoded><![CDATA[
arXiv:2505.05229v1 Announce Type: new 
Abstract: CLIP has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it "see" the same way humans do - especially when interpreting artworks? In this paper, we investigate CLIP's ability to extract high-level semantic and stylistic information from paintings, including both human-created and AI-generated imagery. We evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. By designing targeted probing tasks and comparing CLIP's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. Our findings reveal both strengths and limitations in CLIP's visual representations, particularly in relation to aesthetic cues and artistic intent. We further discuss the implications of these insights for using CLIP as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. Our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADriver: Towards Personalized Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.05240</link>
<guid>https://arxiv.org/abs/2505.05240</guid>
<content:encoded><![CDATA[
arXiv:2505.05240v1 Announce Type: new 
Abstract: In this paper, we propose PADriver, a novel closed-loop framework for personalized autonomous driving (PAD). Built upon Multi-modal Large Language Model (MLLM), PADriver takes streaming frames and personalized textual prompts as inputs. It autoaggressively performs scene understanding, danger level estimation and action decision. The predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. Moreover, we construct a closed-loop benchmark named PAD-Highway based on Highway-Env simulator to comprehensively evaluate the decision performance under traffic rules. The dataset contains 250 hours videos with high-quality annotation to facilitate the development of PAD behavior analysis. Experimental results on the constructed benchmark show that PADriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes</title>
<link>https://arxiv.org/abs/2505.05288</link>
<guid>https://arxiv.org/abs/2505.05288</guid>
<content:encoded><![CDATA[
arXiv:2505.05288v1 Announce Type: new 
Abstract: We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining</title>
<link>https://arxiv.org/abs/2505.05307</link>
<guid>https://arxiv.org/abs/2505.05307</guid>
<content:encoded><![CDATA[
arXiv:2505.05307v1 Announce Type: new 
Abstract: Event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. Existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. In this paper, we propose PRE-Mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. Our framework introduces a 4D event cloud representation that integrates dual temporal scales to preserve high temporal precision, a Spatio-Temporal Decoupling and Fusion module (STDF) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a Multi-Scale State Space Model (MS3M) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. Enhanced by frequency-domain regularization, PRE-Mamba achieves superior performance (0.95 SR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a comprehensive dataset with labeled synthetic and real-world sequences. Moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects</title>
<link>https://arxiv.org/abs/2505.05318</link>
<guid>https://arxiv.org/abs/2505.05318</guid>
<content:encoded><![CDATA[
arXiv:2505.05318v1 Announce Type: new 
Abstract: The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery</title>
<link>https://arxiv.org/abs/2505.05321</link>
<guid>https://arxiv.org/abs/2505.05321</guid>
<content:encoded><![CDATA[
arXiv:2505.05321v1 Announce Type: new 
Abstract: Accurate building segmentation from high-resolution RGB imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. In this study, we present a comprehensive deep learning framework for multiscale building segmentation using RGB aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including Principal Component Analysis (PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index (MBI), and Sobel edge filters from RGB channels. These features guide a Res-U-Net architecture in learning complex spatial patterns more effectively. We also propose training policies incorporating layer freezing, cyclical learning rates, and SuperConvergence to reduce training time and resource usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of 0.80, outperforming existing RGB-based benchmarks. This study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aesthetics Without Semantics</title>
<link>https://arxiv.org/abs/2505.05331</link>
<guid>https://arxiv.org/abs/2505.05331</guid>
<content:encoded><![CDATA[
arXiv:2505.05331v1 Announce Type: new 
Abstract: While it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. Furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. We address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. The resulting Minimum Semantic Content (MSC) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. We next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. Taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors</title>
<link>https://arxiv.org/abs/2505.05336</link>
<guid>https://arxiv.org/abs/2505.05336</guid>
<content:encoded><![CDATA[
arXiv:2505.05336v1 Announce Type: new 
Abstract: The motion capture system that supports full-body virtual representation is of key significance for virtual reality. Compared to vision-based systems, full-body pose estimation from sparse tracking signals is not limited by environmental conditions or recording range. However, previous works either face the challenge of wearing additional sensors on the pelvis and lower-body or rely on external visual sensors to obtain global positions of key joints. To improve the practicality of the technology for virtual reality applications, we estimate full-body poses using only inertial data obtained from three Inertial Measurement Unit (IMU) sensors worn on the head and wrists, thereby reducing the complexity of the hardware system. In this work, we propose a method called Progressive Inertial Poser (ProgIP) for human pose estimation, which combines neural network estimation with a human dynamics model, considers the hierarchical structure of the kinematic chain, and employs a multi-stage progressive network estimation with increased depth to reconstruct full-body motion in real time. The encoder combines Transformer Encoder and bidirectional LSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial sequence, while the decoder based on multi-layer perceptrons (MLPs) transforms high-dimensional features and accurately projects them onto Skinned Multi-Person Linear (SMPL) model parameters. Quantitative and qualitative experimental results on multiple public datasets show that our method outperforms state-of-the-art methods with the same inputs, and is comparable to recent works using six IMU sensors.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization</title>
<link>https://arxiv.org/abs/2505.05343</link>
<guid>https://arxiv.org/abs/2505.05343</guid>
<content:encoded><![CDATA[
arXiv:2505.05343v1 Announce Type: new 
Abstract: Large-scale vision-language models demonstrate strong multimodal alignment and generalization across diverse tasks. Among them, CLIP stands out as one of the most successful approaches. In this work, we extend the application of CLIP to sound source localization, proposing a self-supervised method operates without explicit text input. We introduce a framework that maps audios into tokens compatible with CLIP's text encoder, producing audio-driven embeddings. These embeddings are used to generate sounding region masks, from which visual features are extracted and aligned with the audio embeddings through a contrastive audio-visual correspondence objective. Our findings show that alignment knowledge of pre-trained multimodal foundation model enables our method to generate more complete and compact localization for sounding objects. We further propose an LLM-guided extension that distills object-aware audio-visual scene understanding into the model during training to enhance alignment. Extensive experiments across five diverse tasks demonstrate that our method, in all variants, outperforms state-of-the-art approaches and achieves strong generalization in zero-shot settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt</title>
<link>https://arxiv.org/abs/2505.05367</link>
<guid>https://arxiv.org/abs/2505.05367</guid>
<content:encoded><![CDATA[
arXiv:2505.05367v1 Announce Type: new 
Abstract: We propose a novel joint framework by integrating super-resolution and segmentation, called JointSeg, which enables the generation of 1-meter ISA maps directly from freely available Sentinel-2 imagery. JointSeg was trained on multimodal cross-resolution inputs, offering a scalable and affordable alternative to traditional approaches. This synergistic design enables gradual resolution enhancement from 10m to 1m while preserving fine-grained spatial textures, and ensures high classification fidelity through effective cross-scale feature fusion. This method has been successfully applied to the Yangtze River Economic Belt (YREB), a region characterized by complex urban-rural patterns and diverse topography. As a result, a comprehensive ISA mapping product for 2021, referred to as ISA-1, was generated, covering an area of over 2.2 million square kilometers. Quantitative comparisons against the 10m ESA WorldCover and other benchmark products reveal that ISA-1 achieves an F1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by 9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized areas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through improved discrimination of green spaces and water bodies. Conversely, in mountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more ISA due to its enhanced ability to detect fragmented anthropogenic features such as rural roads and sparse settlements, demonstrating its robustness across diverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023, capturing spatiotemporal urbanization dynamics across representative cities. The results highlight distinct regional growth patterns: rapid expansion in upstream cities, moderate growth in midstream regions, and saturation in downstream metropolitan areas.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.05375</link>
<guid>https://arxiv.org/abs/2505.05375</guid>
<content:encoded><![CDATA[
arXiv:2505.05375v1 Announce Type: new 
Abstract: Recently, spiking neural networks (SNNs), deployed on neuromorphic chips, provide highly efficient solutions on edge devices in different scenarios. However, their ability to adapt to distribution shifts after deployment has become a crucial challenge. Online test-time adaptation (OTTA) offers a promising solution by enabling models to dynamically adjust to new data distributions without requiring source data or labeled target samples. Nevertheless, existing OTTA methods are largely designed for traditional artificial neural networks and are not well-suited for SNNs. To address this gap, we propose a low-power, neuromorphic chip-friendly online test-time adaptation framework, aiming to enhance model generalization under distribution shifts. The proposed approach is called Threshold Modulation (TM), which dynamically adjusts the firing threshold through neuronal dynamics-inspired normalization, being more compatible with neuromorphic hardware. Experimental results on benchmark datasets demonstrate the effectiveness of this method in improving the robustness of SNNs against distribution shifts while maintaining low computational cost. The proposed method offers a practical solution for online test-time adaptation of SNNs, providing inspiration for the design of future neuromorphic chips. The demo code is available at github.com/NneurotransmitterR/TM-OTTA-SNN.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans</title>
<link>https://arxiv.org/abs/2505.05376</link>
<guid>https://arxiv.org/abs/2505.05376</guid>
<content:encoded><![CDATA[
arXiv:2505.05376v1 Announce Type: new 
Abstract: We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDmamba: A Simple yet Effective Event Denoising Method with State Space Model</title>
<link>https://arxiv.org/abs/2505.05391</link>
<guid>https://arxiv.org/abs/2505.05391</guid>
<content:encoded><![CDATA[
arXiv:2505.05391v1 Announce Type: new 
Abstract: Event cameras excel in high-speed vision due to their high temporal resolution, high dynamic range, and low power consumption. However, as dynamic vision sensors, their output is inherently noisy, making efficient denoising essential to preserve their ultra-low latency and real-time processing capabilities. Existing event denoising methods struggle with a critical dilemma: computationally intensive approaches compromise the sensor's high-speed advantage, while lightweight methods often lack robustness across varying noise levels. To address this, we propose a novel event denoising framework based on State Space Models (SSMs). Our approach represents events as 4D event clouds and includes a Coarse Feature Extraction (CFE) module that extracts embedding features from both geometric and polarity-aware subspaces. The model is further composed of two essential components: A Spatial Mamba (S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM) that captures global temporal dynamics, efficiently propagating spatiotemporal features across events. Experiments demonstrate that our method achieves state-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per 100K events inference time, and a 0.982 accuracy score, outperforming Transformer-based methods by 2.08% in denoising accuracy and 36X faster.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model</title>
<link>https://arxiv.org/abs/2505.05397</link>
<guid>https://arxiv.org/abs/2505.05397</guid>
<content:encoded><![CDATA[
arXiv:2505.05397v1 Announce Type: new 
Abstract: Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation</title>
<link>https://arxiv.org/abs/2505.05422</link>
<guid>https://arxiv.org/abs/2505.05422</guid>
<content:encoded><![CDATA[
arXiv:2505.05422v1 Announce Type: new 
Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at https://github.com/TencentARC/TokLIP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding</title>
<link>https://arxiv.org/abs/2505.05446</link>
<guid>https://arxiv.org/abs/2505.05446</guid>
<content:encoded><![CDATA[
arXiv:2505.05446v1 Announce Type: new 
Abstract: Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following. Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. Our code and models are released at https://github. com/Euphoria16/DocMark.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SITE: towards Spatial Intelligence Thorough Evaluation</title>
<link>https://arxiv.org/abs/2505.05456</link>
<guid>https://arxiv.org/abs/2505.05456</guid>
<content:encoded><![CDATA[
arXiv:2505.05456v1 Announce Type: new 
Abstract: Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</title>
<link>https://arxiv.org/abs/2505.05467</link>
<guid>https://arxiv.org/abs/2505.05467</guid>
<content:encoded><![CDATA[
arXiv:2505.05467v1 Announce Type: new 
Abstract: We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
<link>https://arxiv.org/abs/2505.05469</link>
<guid>https://arxiv.org/abs/2505.05469</guid>
<content:encoded><![CDATA[
arXiv:2505.05469v1 Announce Type: new 
Abstract: We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-GRPO: Training Flow Matching Models via Online RL</title>
<link>https://arxiv.org/abs/2505.05470</link>
<guid>https://arxiv.org/abs/2505.05470</guid>
<content:encoded><![CDATA[
arXiv:2505.05470v1 Announce Type: new 
Abstract: We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\%$ to $95\%$. In visual text rendering, its accuracy improves from $59\%$ to $92\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
<link>https://arxiv.org/abs/2505.05472</link>
<guid>https://arxiv.org/abs/2505.05472</guid>
<content:encoded><![CDATA[
arXiv:2505.05472v1 Announce Type: new 
Abstract: Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified framework that advances this paradigm by enabling interleaved multi-modal generation through a causal approach. Mogao integrates a set of key technical improvements in architecture design, including a deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, which allow it to harness the strengths of both autoregressive models for text generation and diffusion models for high-quality image synthesis. These practical improvements also make Mogao particularly effective to process interleaved sequences of text and images arbitrarily. To further unlock the potential of unified models, we introduce an efficient training strategy on a large-scale, in-house dataset specifically curated for joint text and image generation. Extensive experiments show that Mogao not only achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, but also excels in producing high-quality, coherent interleaved outputs. Its emergent capabilities in zero-shot image editing and compositional generation highlight Mogao as a practical omni-modal foundation model, paving the way for future development and scaling the unified multi-modal systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion</title>
<link>https://arxiv.org/abs/2505.05473</link>
<guid>https://arxiv.org/abs/2505.05473</guid>
<content:encoded><![CDATA[
arXiv:2505.05473v1 Announce Type: new 
Abstract: Current Structure-from-Motion (SfM) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. In contrast, we propose a data-driven multi-view reasoning approach that directly infers 3D scene geometry and camera poses from multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. To address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. We empirically validate DiffusionSfM on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Scene Generation: A Survey</title>
<link>https://arxiv.org/abs/2505.05474</link>
<guid>https://arxiv.org/abs/2505.05474</guid>
<content:encoded><![CDATA[
arXiv:2505.05474v1 Announce Type: new 
Abstract: 3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</title>
<link>https://arxiv.org/abs/2505.05475</link>
<guid>https://arxiv.org/abs/2505.05475</guid>
<content:encoded><![CDATA[
arXiv:2505.05475v1 Announce Type: new 
Abstract: Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization</title>
<link>https://arxiv.org/abs/2505.04647</link>
<guid>https://arxiv.org/abs/2505.04647</guid>
<content:encoded><![CDATA[
arXiv:2505.04647v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) achieve state-of-the-art performance in many vision tasks, yet understanding their internal behavior remains challenging, particularly how different layers and activation channels contribute to class separability. We introduce ChannelExplorer, an interactive visual analytics tool for analyzing image-based outputs across model layers, emphasizing data-driven insights over architecture analysis for exploring class separability. ChannelExplorer summarizes activations across layers and visualizes them using three primary coordinated views: a Scatterplot View to reveal inter- and intra-class confusion, a Jaccard Similarity View to quantify activation overlap, and a Heatmap View to inspect activation channel patterns. Our technique supports diverse model architectures, including CNNs, GANs, ResNet and Stable Diffusion models. We demonstrate the capabilities of ChannelExplorer through four use-case scenarios: (1) generating class hierarchy in ImageNet, (2) finding mislabeled images, (3) identifying activation channel contributions, and(4) locating latent states' position in Stable Diffusion model. Finally, we evaluate the tool with expert users.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Boundary Detection in Deep Learning-Based Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.04652</link>
<guid>https://arxiv.org/abs/2505.04652</guid>
<content:encoded><![CDATA[
arXiv:2505.04652v1 Announce Type: cross 
Abstract: Medical image segmentation is a pivotal task within the realms of medical image analysis and computer vision. While current methods have shown promise in accurately segmenting major regions of interest, the precise segmentation of boundary areas remains challenging. In this study, we propose a novel network architecture named CTO, which combines Convolutional Neural Networks (CNNs), Vision Transformer (ViT) models, and explicit edge detection operators to tackle this challenge. CTO surpasses existing methods in terms of segmentation accuracy and strikes a better balance between accuracy and efficiency, without the need for additional data inputs or label injections. Specifically, CTO adheres to the canonical encoder-decoder network paradigm, with a dual-stream encoder network comprising a mainstream CNN stream for capturing local features and an auxiliary StitchViT stream for integrating long-range dependencies. Furthermore, to enhance the model's ability to learn boundary areas, we introduce a boundary-guided decoder network that employs binary boundary masks generated by dedicated edge detection operators to provide explicit guidance during the decoding process. We validate the performance of CTO through extensive experiments conducted on seven challenging medical image segmentation datasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our experimental results unequivocally demonstrate that CTO achieves state-of-the-art accuracy on these datasets while maintaining competitive model complexity. The codes have been released at: https://github.com/xiaofang007/CTO.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Conversational Diagnostic AI with Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.04653</link>
<guid>https://arxiv.org/abs/2505.04653</guid>
<content:encoded><![CDATA[
arXiv:2505.04653v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection</title>
<link>https://arxiv.org/abs/2505.04660</link>
<guid>https://arxiv.org/abs/2505.04660</guid>
<content:encoded><![CDATA[
arXiv:2505.04660v1 Announce Type: cross 
Abstract: Training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. To address this, we explore the potential of Large Language Models (LLMs) for generating synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall scenarios. We generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a Long Short-Term Memory (LSTM) model. Additionally, we compare LLM-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. Results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with LLM-generated data performing best in low-frequency settings (e.g., 20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. Diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. An ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. These findings provide insights into optimizing synthetic data generation for fall detection models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2505.04664</link>
<guid>https://arxiv.org/abs/2505.04664</guid>
<content:encoded><![CDATA[
arXiv:2505.04664v1 Announce Type: cross 
Abstract: Our study presents PNN-UNet as a method for constructing deep neural networks that replicate the planarian neural network (PNN) structure in the context of 3D medical image data. Planarians typically have a cerebral structure comprising two neural cords, where the cerebrum acts as a coordinator, and the neural cords serve slightly different purposes within the organism's neurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a Wide-UNet as the nerve cords, with a densely connected autoencoder performing the role of the brain. This distinct architecture offers advantages over both monolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D MRI hippocampus dataset, with and without data augmentation, demonstrate that PNN-UNet outperforms the baseline UNet and several other UNet variants in image segmentation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</title>
<link>https://arxiv.org/abs/2505.04813</link>
<guid>https://arxiv.org/abs/2505.04813</guid>
<content:encoded><![CDATA[
arXiv:2505.04813v1 Announce Type: cross 
Abstract: We present WIR3D, a technique for abstracting 3D shapes through a sparse set of visually meaningful curves in 3D. We optimize the parameters of Bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. We leverage the intermediate activations of a pre-trained foundation model (CLIP) to guide our optimization process. We divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. Our second phase supervision is spatially guided by a novel localized keypoint loss. This spatial guidance enables user control over abstracted features. We ensure fidelity to the original surface through a neural SDF loss, which allows the curves to be used as intuitive deformation handles. We successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Image Reconstruction and Target Recognition based on Deep Learning Technique</title>
<link>https://arxiv.org/abs/2505.04836</link>
<guid>https://arxiv.org/abs/2505.04836</guid>
<content:encoded><![CDATA[
arXiv:2505.04836v1 Announce Type: cross 
Abstract: Computational microwave imaging (CMI) has gained attention as an alternative technique for conventional microwave imaging techniques, addressing their limitations such as hardware-intensive physical layer and slow data collection acquisition speed to name a few. Despite these advantages, CMI still encounters notable computational bottlenecks, especially during the image reconstruction stage. In this setting, both image recovery and object classification present significant processing demands. To address these challenges, our previous work introduced ClassiGAN, which is a generative deep learning model designed to simultaneously reconstruct images and classify targets using only back-scattered signals. In this study, we build upon that framework by incorporating attention gate modules into ClassiGAN. These modules are intended to refine feature extraction and improve the identification of relevant information. By dynamically focusing on important features and suppressing irrelevant ones, the attention mechanism enhances the overall model performance. The proposed architecture, named Att-ClassiGAN, significantly reduces the reconstruction time compared to traditional CMI approaches. Furthermore, it outperforms current advanced methods, delivering improved Normalized Mean Squared Error (NMSE), higher Structural Similarity Index (SSIM), and better classification outcomes for the reconstructed targets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.04851</link>
<guid>https://arxiv.org/abs/2505.04851</guid>
<content:encoded><![CDATA[
arXiv:2505.04851v1 Announce Type: cross 
Abstract: Despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. This is due to the content of existing large training datasets collected on the Internet, which are predominantly based on Western European or American popular culture. Meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. In an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. We propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the Russian one. We explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the Kandinsky 3.1 text-to-image model. Human evaluation results demonstrate an increase in the level of awareness of Russian culture in the model.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</title>
<link>https://arxiv.org/abs/2505.04860</link>
<guid>https://arxiv.org/abs/2505.04860</guid>
<content:encoded><![CDATA[
arXiv:2505.04860v1 Announce Type: cross 
Abstract: Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our project website is at: https://dcodaaug.github.io/D-CODA/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy</title>
<link>https://arxiv.org/abs/2505.04913</link>
<guid>https://arxiv.org/abs/2505.04913</guid>
<content:encoded><![CDATA[
arXiv:2505.04913v1 Announce Type: cross 
Abstract: This paper introduces an innovative approach to silicon and glass via inspection, which combines hybrid field microscopy with photometric stereo. Conventional optical microscopy techniques are generally limited to superficial inspections and struggle to effectively visualize the internal structures of silicon and glass vias. By utilizing various lighting conditions for 3D reconstruction, the proposed method surpasses these limitations. By integrating photometric stereo to the traditional optical microscopy, the proposed method not only enhances the capability to detect micro-scale defects but also provides a detailed visualization of depth and edge abnormality, which are typically not visible with conventional optical microscopy inspection. The experimental results demonstrated that the proposed method effectively captures intricate surface details and internal structures. Quantitative comparisons between the reconstructed models and actual measurements present the capability of the proposed method to significantly improve silicon and glass via inspection process. As a result, the proposed method achieves enhanced cost-effectiveness while maintaining high accuracy and repeatability, suggesting substantial advancements in silicon and glass via inspection techniques
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation</title>
<link>https://arxiv.org/abs/2505.04959</link>
<guid>https://arxiv.org/abs/2505.04959</guid>
<content:encoded><![CDATA[
arXiv:2505.04959v1 Announce Type: cross 
Abstract: This study presents an unsupervised, motion-resolved reconstruction framework for high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI), utilizing a three-dimensional Gaussian representation (3DGS). The proposed method leverages 3DGS to address the challenges of motion-resolved 3D isotropic pulmonary MRI reconstruction by enabling data smoothing between voxels for continuous spatial representation. Pulmonary MRI data acquisition is performed using a golden-angle radial sampling trajectory, with respiratory motion signals extracted from the center of k-space in each radial spoke. Based on the estimated motion signal, the k-space data is sorted into multiple respiratory phases. A 3DGS framework is then applied to reconstruct a reference image volume from the first motion state. Subsequently, a patient-specific convolutional neural network is trained to estimate the deformation vector fields (DVFs), which are used to generate the remaining motion states through spatial transformation of the reference volume. The proposed reconstruction pipeline is evaluated on six datasets from six subjects and bench-marked against three state-of-the-art reconstruction methods. The experimental findings demonstrate that the proposed reconstruction framework effectively reconstructs high-resolution, motion-resolved pulmonary MR images. Compared with existing approaches, it achieves superior image quality, reflected by higher signal-to-noise ratio and contrast-to-noise ratio. The proposed unsupervised 3DGS-based reconstruction method enables accurate motion-resolved pulmonary MRI with isotropic spatial resolution. Its superior performance in image quality metrics over state-of-the-art methods highlights its potential as a robust solution for clinical pulmonary MR imaging.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators</title>
<link>https://arxiv.org/abs/2505.04961</link>
<guid>https://arxiv.org/abs/2505.04961</guid>
<content:encoded><![CDATA[
arXiv:2505.04961v1 Announce Type: cross 
Abstract: Multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. Existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. The performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. These limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. Such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. To bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. The proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. We demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. Results are best visualized through https://youtu.be/rz8BYCE9E2w.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Transform: A Unified Framework for Adaptive Transform to Enhance Representations</title>
<link>https://arxiv.org/abs/2505.04969</link>
<guid>https://arxiv.org/abs/2505.04969</guid>
<content:encoded><![CDATA[
arXiv:2505.04969v1 Announce Type: cross 
Abstract: Discrete transforms, such as the discrete Fourier transform, are widely used in machine learning to improve model performance by extracting meaningful features. However, with numerous transforms available, selecting an appropriate one often depends on understanding the dataset's properties, making the approach less effective when such knowledge is unavailable. In this work, we propose General Transform (GT), an adaptive transform-based representation designed for machine learning applications. Unlike conventional transforms, GT learns data-driven mapping tailored to the dataset and task of interest. Here, we demonstrate that models incorporating GT outperform conventional transform-based approaches across computer vision and natural language processing tasks, highlighting its effectiveness in diverse learning scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments</title>
<link>https://arxiv.org/abs/2505.04972</link>
<guid>https://arxiv.org/abs/2505.04972</guid>
<content:encoded><![CDATA[
arXiv:2505.04972v1 Announce Type: cross 
Abstract: The miniaturisation of sensors and processors, the advancements in connected edge intelligence, and the exponential interest in Artificial Intelligence are boosting the affirmation of autonomous nano-size drones in the Internet of Robotic Things ecosystem. However, achieving safe autonomous navigation and high-level tasks such as exploration and surveillance with these tiny platforms is extremely challenging due to their limited resources. This work focuses on enabling the safe and autonomous flight of a pocket-size, 30-gram platform called Crazyflie 2.1 in a partially known environment. We propose a novel AI-aided, vision-based reactive planning method for obstacle avoidance under the ambit of Integrated Sensing, Computing and Communication paradigm. We deal with the constraints of the nano-drone by splitting the navigation task into two parts: a deep learning-based object detector runs on the edge (external hardware) while the planning algorithm is executed onboard. The results show the ability to command the drone at $\sim8$ frames-per-second and a model performance reaching a COCO mean-average-precision of $60.8$. Field experiments demonstrate the feasibility of the solution with the drone flying at a top speed of $1$ m/s while steering away from an obstacle placed in an unknown position and reaching the target destination. The outcome highlights the compatibility of the communication delay and the model performance with the requirements of the real-time navigation task. We provide a feasible alternative to a fully onboard implementation that can be extended to autonomous exploration with nano-drones.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication</title>
<link>https://arxiv.org/abs/2505.04996</link>
<guid>https://arxiv.org/abs/2505.04996</guid>
<content:encoded><![CDATA[
arXiv:2505.04996v1 Announce Type: cross 
Abstract: Full-body gestures play a pivotal role in natural interactions and are crucial for achieving effective communication. Nevertheless, most existing studies primarily focus on the gesture generation of speakers, overlooking the vital role of listeners in the interaction process and failing to fully explore the dynamic interaction between them. This paper innovatively proposes an Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication. For the first time, we integrate the full-body gestures of listeners into the generation framework. By devising a novel inter-diffusion mechanism, this model can accurately capture the complex interaction patterns between speakers and listeners during communication. In the model construction process, based on the advanced diffusion model architecture, we innovatively introduce interaction conditions and the GAN model to increase the denoising step size. As a result, when generating gesture sequences, the model can not only dynamically generate based on the speaker's speech information but also respond in realtime to the listener's feedback, enabling synergistic interaction between the two. Abundant experimental results demonstrate that compared with the current state-of-the-art gesture generation methods, the model we proposed has achieved remarkable improvements in the naturalness, coherence, and speech-gesture synchronization of the generated gestures. In the subjective evaluation experiments, users highly praised the generated interaction scenarios, believing that they are closer to real life human communication situations. Objective index evaluations also show that our model outperforms the baseline methods in multiple key indicators, providing more powerful support for effective communication.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Text Relation Prediction for Multilingual Tweets</title>
<link>https://arxiv.org/abs/2505.05040</link>
<guid>https://arxiv.org/abs/2505.05040</guid>
<content:encoded><![CDATA[
arXiv:2505.05040v1 Announce Type: cross 
Abstract: Various social networks have been allowing media uploads for over a decade now. Still, it has not always been clear what is their relation with the posted text or even if there is any at all. In this work, we explore how multilingual vision-language models tackle the task of image-text relation prediction in different languages, and construct a dedicated balanced benchmark data set from Twitter posts in Latvian along with their manual translations into English. We compare our results to previous work and show that the more recently released vision-language model checkpoints are becoming increasingly capable at this task, but there is still much room for further improvement.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization</title>
<link>https://arxiv.org/abs/2505.05041</link>
<guid>https://arxiv.org/abs/2505.05041</guid>
<content:encoded><![CDATA[
arXiv:2505.05041v1 Announce Type: cross 
Abstract: Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by amyloid-beta plaques and tau neurofibrillary tangles, which serve as key histopathological features. The identification and segmentation of these lesions are crucial for understanding AD progression but remain challenging due to the lack of large-scale annotated datasets and the impact of staining variations on automated image analysis. Deep learning has emerged as a powerful tool for pathology image segmentation; however, model performance is significantly influenced by variations in staining characteristics, necessitating effective stain normalization and enhancement techniques. In this study, we address these challenges by introducing an open-source dataset (ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of dystrophic tau-positive neurites) in human brain whole slide images. We establish a comprehensive benchmark by evaluating five widely adopted deep learning models across four stain normalization techniques, providing deeper insights into their influence on neuritic plaque segmentation. Additionally, we propose a novel image enhancement method that improves segmentation accuracy, particularly in complex tissue structures, by enhancing structural details and mitigating staining inconsistencies. Our experimental results demonstrate that this enhancement strategy significantly boosts model generalization and segmentation accuracy. All datasets and code are open-source, ensuring transparency and reproducibility while enabling further advancements in the field.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction</title>
<link>https://arxiv.org/abs/2505.05054</link>
<guid>https://arxiv.org/abs/2505.05054</guid>
<content:encoded><![CDATA[
arXiv:2505.05054v1 Announce Type: cross 
Abstract: The computational imaging technique of Fourier Ptychographic Microscopy (FPM) enables high-resolution imaging with a wide field of view and can serve as an extremely valuable tool, e.g. in the classification of cells in medical applications. However, reconstructing a high-resolution image from tens or even hundreds of measurements is computationally expensive, particularly for a wide field of view. Therefore, in this paper, we investigate the idea of classifying the image content in the FPM measurements directly without performing a reconstruction step first. We show that Convolutional Neural Networks (CNN) can extract meaningful information from measurement sequences, significantly outperforming the classification on a single band-limited image (up to 12 %) while being significantly more efficient than a reconstruction of a high-resolution image. Furthermore, we demonstrate that a learned multiplexing of several raw measurements allows maintaining the classification accuracy while reducing the amount of data (and consequently also the acquisition time) significantly.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepSNet: A Nucleus Instance Segmentation model based on Boundary Regression and Structural Re-parameterization</title>
<link>https://arxiv.org/abs/2505.05073</link>
<guid>https://arxiv.org/abs/2505.05073</guid>
<content:encoded><![CDATA[
arXiv:2505.05073v1 Announce Type: cross 
Abstract: Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus instance segmentation is a key step in digital pathology analysis and pathological diagnosis. However, the computational efficiency of the model and the treatment of overlapping targets are the major challenges in the studies of this problem. To this end, a neural network model RepSNet was designed based on a nucleus boundary regression and a structural re-parameterization scheme for segmenting and classifying the nuclei in H\&amp;E-stained histopathological images. First, RepSNet estimates the boundary position information (BPI) of the parent nucleus for each pixel. The BPI estimation incorporates the local information of the pixel and the contextual information of the parent nucleus. Then, the nucleus boundary is estimated by aggregating the BPIs from a series of pixels using a proposed boundary voting mechanism (BVM), and the instance segmentation results are computed from the estimated nucleus boundary using a connected component analysis procedure. The BVM intrinsically achieves a kind of synergistic belief enhancement among the BPIs from various pixels. Therefore, different from the methods available in literature that obtain nucleus boundaries based on a direct pixel recognition scheme, RepSNet computes its boundary decisions based on some guidances from macroscopic information using an integration mechanism. In addition, RepSNet employs a re-parametrizable encoder-decoder structure. This model can not only aggregate features from some receptive fields with various scales which helps segmentation accuracy improvement, but also reduce the parameter amount and computational burdens in the model inference phase through the structural re-parameterization technique. Extensive experiments demonstrated the superiorities of RepSNet compared to several typical benchmark models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The City that Never Settles: Simulation-based LiDAR Dataset for Long-Term Place Recognition Under Extreme Structural Changes</title>
<link>https://arxiv.org/abs/2505.05076</link>
<guid>https://arxiv.org/abs/2505.05076</guid>
<content:encoded><![CDATA[
arXiv:2505.05076v1 Announce Type: cross 
Abstract: Large-scale construction and demolition significantly challenge long-term place recognition (PR) by drastically reshaping urban and suburban environments. Existing datasets predominantly reflect limited or indoor-focused changes, failing to adequately represent extensive outdoor transformations. To bridge this gap, we introduce the City that Never Settles (CNS) dataset, a simulation-based dataset created using the CARLA simulator, capturing major structural changes-such as building construction and demolition-across diverse maps and sequences. Additionally, we propose TCR_sym, a symmetric version of the original TCR metric, enabling consistent measurement of structural changes irrespective of source-target ordering. Quantitative comparisons demonstrate that CNS encompasses more extensive transformations than current real-world benchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS reveal substantial performance degradation, underscoring the need for robust algorithms capable of handling significant environmental changes. Our dataset is available at https://github.com/Hyunho111/CNS_dataset.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal</title>
<link>https://arxiv.org/abs/2505.05088</link>
<guid>https://arxiv.org/abs/2505.05088</guid>
<content:encoded><![CDATA[
arXiv:2505.05088v1 Announce Type: cross 
Abstract: Visible watermark removal is challenging due to its inherent complexities and the noise carried within images. Existing methods primarily rely on supervised learning approaches that require paired datasets of watermarked and watermark-free images, which are often impractical to obtain in real-world scenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and Hybrid Network specifically designed for noisy image watermark removal. SSH-Net synthesizes reference watermark-free images using the watermark distribution in a self-supervised manner and adopts a dual-network design to address the task. The upper network, focused on the simpler task of noise removal, employs a lightweight CNN-based architecture, while the lower network, designed to handle the more complex task of simultaneously removing watermarks and noise, incorporates Transformer blocks to model long-range dependencies and capture intricate image features. To enhance the model's effectiveness, a shared CNN-based feature encoder is introduced before dual networks to extract common features that both networks can leverage. Our code will be available at https://github.com/wenyang001/SSH-Net.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Driver: Explainable Autonomous Driving with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.05098</link>
<guid>https://arxiv.org/abs/2505.05098</guid>
<content:encoded><![CDATA[
arXiv:2505.05098v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. However, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. In this paper, we introduce X-Driver, a unified multi-modal large language models(MLLMs) framework designed for closed-loop autonomous driving, leveraging Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and decision-making. We validate X-Driver across multiple autonomous driving tasks using public benchmarks in CARLA simulation environment, including Bench2Drive[6]. Our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(SOTA) while improving the interpretability of driving decisions. These findings underscore the importance of structured reasoning in end-to-end driving and establish X-Driver as a strong baseline for future research in closed-loop autonomous driving.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising</title>
<link>https://arxiv.org/abs/2505.05112</link>
<guid>https://arxiv.org/abs/2505.05112</guid>
<content:encoded><![CDATA[
arXiv:2505.05112v1 Announce Type: cross 
Abstract: Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single low-dose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Contour Model for Silhouette Vectorization using B\'ezier Curves</title>
<link>https://arxiv.org/abs/2505.05132</link>
<guid>https://arxiv.org/abs/2505.05132</guid>
<content:encoded><![CDATA[
arXiv:2505.05132v1 Announce Type: cross 
Abstract: In this paper, we propose an active contour model for silhouette vectorization using cubic B\'ezier curves. Among the end points of the B\'ezier curves, we distinguish between corner and regular points where the orientation of the tangent vector is prescribed. By minimizing the distance of the B\'ezier curves to the silhouette boundary, the active contour model optimizes the location of the B\'ezier curves end points, the orientation of the tangent vectors in the regular points, and the estimation of the B\'ezier curve parameters. This active contour model can use the silhouette vectorization obtained by any method as an initial guess. The proposed method significantly reduces the average distance between the silhouette boundary and its vectorization obtained by the world-class graphic software Inkscape, Adobe Illustrator, and a curvature-based vectorization method, which we introduce for comparison. Our method also allows us to impose additional regularity on the B\'ezier curves by reducing their lengths.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Anomaly Detection Methods Based on Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05137</link>
<guid>https://arxiv.org/abs/2505.05137</guid>
<content:encoded><![CDATA[
arXiv:2505.05137v1 Announce Type: cross 
Abstract: Anomaly detection is a fundamental task in machine learning and data mining, with significant applications in cybersecurity, industrial fault diagnosis, and clinical disease monitoring. Traditional methods, such as statistical modeling and machine learning-based approaches, often face challenges in handling complex, high-dimensional data distributions. In this study, we explore the potential of diffusion models for anomaly detection, proposing a novel framework that leverages the strengths of diffusion probabilistic models (DPMs) to effectively identify anomalies in both image and audio data. The proposed method models the distribution of normal data through a diffusion process and reconstructs input data via reverse diffusion, using a combination of reconstruction errors and semantic discrepancies as anomaly indicators. To enhance the framework's performance, we introduce multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, enabling the model to capture fine-grained structures and global dependencies in the data. Extensive experiments on benchmark datasets, including MVTec AD and UrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly detection techniques, achieving superior accuracy and robustness across diverse data modalities. This research highlights the effectiveness of diffusion models in anomaly detection and provides a robust and efficient solution for real-world applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Based Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.05195</link>
<guid>https://arxiv.org/abs/2505.05195</guid>
<content:encoded><![CDATA[
arXiv:2505.05195v1 Announce Type: cross 
Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by explaining predictions through human-understandable concepts but typically assume that training and test data share the same distribution. This assumption often fails under domain shifts, leading to degraded performance and poor generalization. To address these limitations and improve the robustness of CBMs, we propose the Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed to: (1) align concept representations across domains using adversarial training, (2) introduce a relaxation threshold to allow minor domain-specific differences in concept distributions, thereby preventing performance drop due to over-constraints of these distributions, (3) infer concepts directly in the target domain without requiring labeled concept data, enabling CBMs to adapt to diverse domains, and (4) integrate concept learning into conventional domain adaptation (DA) with theoretical guarantees, improving interpretability and establishing new benchmarks for DA. Experiments demonstrate that our approach significantly outperforms the state-of-the-art CBM and DA methods on real-world datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning</title>
<link>https://arxiv.org/abs/2505.05208</link>
<guid>https://arxiv.org/abs/2505.05208</guid>
<content:encoded><![CDATA[
arXiv:2505.05208v1 Announce Type: cross 
Abstract: Early detection and accurate diagnosis are essential to improving patient outcomes. The use of convolutional neural networks (CNNs) for tumor detection has shown promise, but existing models often suffer from overparameterization, which limits their performance gains. In this study, fuzzy sigmoid convolution (FSC) is introduced along with two additional modules: top-of-the-funnel and middle-of-the-funnel. The proposed methodology significantly reduces the number of trainable parameters without compromising classification accuracy. A novel convolutional operator is central to this approach, effectively dilating the receptive field while preserving input data integrity. This enables efficient feature map reduction and enhances the model's tumor detection capability. In the FSC-based model, fuzzy sigmoid activation functions are incorporated within convolutional layers to improve feature extraction and classification. The inclusion of fuzzy logic into the architecture improves its adaptability and robustness. Extensive experiments on three benchmark datasets demonstrate the superior performance and efficiency of the proposed model. The FSC-based architecture achieved classification accuracies of 99.17%, 99.75%, and 99.89% on three different datasets. The model employs 100 times fewer parameters than large-scale transfer learning architectures, highlighting its computational efficiency and suitability for detecting brain tumors early. This research offers lightweight, high-performance deep-learning models for medical imaging applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.05223</link>
<guid>https://arxiv.org/abs/2505.05223</guid>
<content:encoded><![CDATA[
arXiv:2505.05223v1 Announce Type: cross 
Abstract: Human drivers exhibit individual preferences regarding driving style. Adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. However, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. We propose a novel approach using multi-objective reinforcement learning (MORL) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. Preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\unicode{x2013}$without requiring policy retraining. Our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the CARLA simulator. Experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White Light Specular Reflection Data Augmentation for Deep Learning Polyp Detection</title>
<link>https://arxiv.org/abs/2505.05248</link>
<guid>https://arxiv.org/abs/2505.05248</guid>
<content:encoded><![CDATA[
arXiv:2505.05248v1 Announce Type: cross 
Abstract: Colorectal cancer is one of the deadliest cancers today, but it can be prevented through early detection of malignant polyps in the colon, primarily via colonoscopies. While this method has saved many lives, human error remains a significant challenge, as missing a polyp could have fatal consequences for the patient. Deep learning (DL) polyp detectors offer a promising solution. However, existing DL polyp detectors often mistake white light reflections from the endoscope for polyps, which can lead to false positives.To address this challenge, in this paper, we propose a novel data augmentation approach that artificially adds more white light reflections to create harder training scenarios. Specifically, we first generate a bank of artificial lights using the training dataset. Then we find the regions of the training images that we should not add these artificial lights on. Finally, we propose a sliding window method to add the artificial light to the areas that fit of the training images, resulting in augmented images. By providing the model with more opportunities to make mistakes, we hypothesize that it will also have more chances to learn from those mistakes, ultimately improving its performance in polyp detection. Experimental results demonstrate the effectiveness of our new data augmentation method.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTL-UE: Learning to Learn Nothing for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.05279</link>
<guid>https://arxiv.org/abs/2505.05279</guid>
<content:encoded><![CDATA[
arXiv:2505.05279v1 Announce Type: cross 
Abstract: Most existing unlearnable strategies focus on preventing unauthorized users from training single-task learning (STL) models with personal data. Nevertheless, the paradigm has recently shifted towards multi-task data and multi-task learning (MTL), targeting generalist and foundation models that can handle multiple tasks simultaneously. Despite their growing importance, MTL data and models have been largely neglected while pursuing unlearnable strategies. This paper presents MTL-UE, the first unified framework for generating unlearnable examples for multi-task data and MTL models. Instead of optimizing perturbations for each sample, we design a generator-based structure that introduces label priors and class-wise feature embeddings which leads to much better attacking performance. In addition, MTL-UE incorporates intra-task and inter-task embedding regularization to increase inter-class separation and suppress intra-class variance which enhances the attack robustness greatly. Furthermore, MTL-UE is versatile with good supports for dense prediction tasks in MTL. It is also plug-and-play allowing integrating existing surrogate-dependent unlearnable methods with little adaptation. Extensive experiments show that MTL-UE achieves superior attacking performance consistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5 MTL task-weighting strategies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</title>
<link>https://arxiv.org/abs/2505.05291</link>
<guid>https://arxiv.org/abs/2505.05291</guid>
<content:encoded><![CDATA[
arXiv:2505.05291v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n=587) of DFIs with AMD labels from Brazil.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Deep Contexts for Spatially Embedded Video Coding</title>
<link>https://arxiv.org/abs/2505.05309</link>
<guid>https://arxiv.org/abs/2505.05309</guid>
<content:encoded><![CDATA[
arXiv:2505.05309v1 Announce Type: cross 
Abstract: Most Neural Video Codecs (NVCs) only employ temporal references to generate temporal-only contexts and latent prior. These temporal-only NVCs fail to handle large motions or emerging objects due to limited contexts and misaligned latent prior. To relieve the limitations, we propose a Spatially Embedded Video Codec (SEVC), in which the low-resolution video is compressed for spatial references. Firstly, our SEVC leverages both spatial and temporal references to generate augmented motion vectors and hybrid spatial-temporal contexts. Secondly, to address the misalignment issue in latent prior and enrich the prior information, we introduce a spatial-guided latent prior augmented by multiple temporal latent representations. At last, we design a joint spatial-temporal optimization to learn quality-adaptive bit allocation for spatial references, further boosting rate-distortion performance. Experimental results show that our SEVC effectively alleviates the limitations in handling large motions or emerging objects, and also reduces 11.9% more bitrate than the previous state-of-the-art NVC while providing an additional low-resolution bitstream. Our code and model are available at https://github.com/EsakaK/SEVC.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields</title>
<link>https://arxiv.org/abs/2505.05356</link>
<guid>https://arxiv.org/abs/2505.05356</guid>
<content:encoded><![CDATA[
arXiv:2505.05356v1 Announce Type: cross 
Abstract: We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OcularAge: A Comparative Study of Iris and Periocular Images for Pediatric Age Estimation</title>
<link>https://arxiv.org/abs/2505.05374</link>
<guid>https://arxiv.org/abs/2505.05374</guid>
<content:encoded><![CDATA[
arXiv:2505.05374v1 Announce Type: cross 
Abstract: Estimating a child's age from ocular biometric images is challenging due to subtle physiological changes and the limited availability of longitudinal datasets. Although most biometric age estimation studies have focused on facial features and adult subjects, pediatric-specific analysis, particularly of the iris and periocular regions, remains relatively unexplored. This study presents a comparative evaluation of iris and periocular images for estimating the ages of children aged between 4 and 16 years. We utilized a longitudinal dataset comprising more than 21,000 near-infrared (NIR) images, collected from 288 pediatric subjects over eight years using two different imaging sensors. A multi-task deep learning framework was employed to jointly perform age prediction and age-group classification, enabling a systematic exploration of how different convolutional neural network (CNN) architectures, particularly those adapted for non-square ocular inputs, capture the complex variability inherent in pediatric eye images. The results show that periocular models consistently outperform iris-based models, achieving a mean absolute error (MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These results mark the first demonstration that reliable age estimation is feasible from children's ocular images, enabling privacy-preserving age checks in child-centric applications. This work establishes the first longitudinal benchmark for pediatric ocular age estimation, providing a foundation for designing robust, child-focused biometric systems. The developed models proved resilient across different imaging sensors, confirming their potential for real-world deployment. They also achieved inference speeds of less than 10 milliseconds per image on resource-constrained VR headsets, demonstrating their suitability for real-time applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based assignment decision network for multiple object tracking</title>
<link>https://arxiv.org/abs/2208.03571</link>
<guid>https://arxiv.org/abs/2208.03571</guid>
<content:encoded><![CDATA[
arXiv:2208.03571v3 Announce Type: replace 
Abstract: Data association is a crucial component for any multiple object tracking (MOT) method that follows the tracking-by-detection paradigm. To generate complete trajectories such methods employ a data association process to establish assignments between detections and existing targets during each timestep. Recent data association approaches try to solve either a multi-dimensional linear assignment task or a network flow minimization problem or tackle it via multiple hypotheses tracking. However, during inference an optimization step that computes optimal assignments is required for every sequence frame inducing additional complexity to any given solution. To this end, in the context of this work we introduce Transformer-based Assignment Decision Network (TADN) that tackles data association without the need of any explicit optimization during inference. In particular, TADN can directly infer assignment pairs between detections and active targets in a single forward pass of the network. We have integrated TADN in a rather simple MOT framework, designed a novel training strategy for efficient end-to-end training and demonstrated the high potential of our approach for online visual tracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and UA-DETRAC. Our proposed approach demonstrates strong performance in most evaluation metrics despite its simple nature as a tracker lacking significant auxiliary components such as occlusion handling or re-identification. The implementation of our method is publicly available at https://github.com/psaltaath/tadn-mot.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2303.12484</link>
<guid>https://arxiv.org/abs/2303.12484</guid>
<content:encoded><![CDATA[
arXiv:2303.12484v5 Announce Type: replace 
Abstract: Deep learning has significantly advanced medical imaging analysis (MIA), achieving state-of-the-art performance across diverse clinical tasks. However, its success largely depends on large-scale, high-quality labeled datasets, which are costly and time-consuming to obtain due to the need for expert annotation. To mitigate this limitation, label-efficient deep learning methods have emerged to improve model performance under limited supervision by leveraging labeled, unlabeled, and weakly labeled data. In this survey, we systematically review over 350 peer-reviewed studies and present a comprehensive taxonomy of label-efficient learning methods in MIA. These methods are categorized into four labeling paradigms: no label, insufficient label, inexact label, and label refinement. For each category, we analyze representative techniques across imaging modalities and clinical applications, highlighting shared methodological principles and task-specific adaptations. We also examine the growing role of health foundation models (HFMs) in enabling label-efficient learning through large-scale pre-training and transfer learning, enhancing the use of limited annotations in downstream tasks. Finally, we identify current challenges and future directions to facilitate the translation of label-efficient learning from research promise to everyday clinical care.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USTEP: Spatio-Temporal Predictive Learning under A Unified View</title>
<link>https://arxiv.org/abs/2310.05829</link>
<guid>https://arxiv.org/abs/2310.05829</guid>
<content:encoded><![CDATA[
arXiv:2310.05829v2 Announce Type: replace 
Abstract: Spatio-temporal predictive learning plays a crucial role in self-supervised learning, with wide-ranging applications across a diverse range of fields. Previous approaches for temporal modeling fall into two categories: recurrent-based and recurrent-free methods. The former, while meticulously processing frames one by one, neglect short-term spatio-temporal information redundancies, leading to inefficiencies. The latter naively stack frames sequentially, overlooking the inherent temporal dependencies. In this paper, we re-examine the two dominant temporal modeling approaches within the realm of spatio-temporal predictive learning, offering a unified perspective. Building upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive learning), an innovative framework that reconciles the recurrent-based and recurrent-free methods by integrating both micro-temporal and macro-temporal scales. Extensive experiments on a wide range of spatio-temporal predictive learning demonstrate that USTEP achieves significant improvements over existing temporal modeling approaches, thereby establishing it as a robust solution for a wide range of spatio-temporal applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency</title>
<link>https://arxiv.org/abs/2311.12421</link>
<guid>https://arxiv.org/abs/2311.12421</guid>
<content:encoded><![CDATA[
arXiv:2311.12421v3 Announce Type: replace 
Abstract: Deducing a 3D human pose from a single 2D image is inherently challenging because multiple 3D poses can correspond to the same 2D representation. 3D data can resolve this pose ambiguity, but it is expensive to record and requires an intricate setup that is often restricted to controlled lab environments. We propose a method that improves the performance of deep learning-based monocular 3D human pose estimation models by using multiview data only during training, but not during inference. We introduce a novel loss function, consistency loss, which operates on two synchronized views. This approach is simpler than previous models that require 3D ground truth or intrinsic and extrinsic camera parameters. Our consistency loss penalizes differences in two pose sequences after rigid alignment. We also demonstrate that our consistency loss substantially improves performance for fine-tuning without requiring 3D data. Furthermore, we show that using our consistency loss can yield state-of-the-art performance when training models from scratch in a semi-supervised manner. Our findings provide a simple way to capture new data, e.g in a new domain. This data can be added using off-the-shelf cameras with no calibration requirements. We make all our code and data publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in Field Robotics</title>
<link>https://arxiv.org/abs/2403.08142</link>
<guid>https://arxiv.org/abs/2403.08142</guid>
<content:encoded><![CDATA[
arXiv:2403.08142v2 Announce Type: replace 
Abstract: Shadows significantly hinder computer vision tasks in outdoor environments, particularly in field robotics, where varying lighting conditions complicate object detection and localisation. We present FieldNet, a novel deep learning framework for real-time shadow removal, optimised for resource-constrained hardware. FieldNet introduces a probabilistic enhancement module and a novel loss function to address challenges of inconsistent shadow boundary supervision and artefact generation, achieving enhanced accuracy and simplicity without requiring shadow masks during inference. Trained on a dataset of 10,000 natural images augmented with synthetic shadows, FieldNet outperforms state-of-the-art methods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed improvements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality (PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture robotics demonstrate the practical impact of FieldNet in enhancing weed detection accuracy. These advancements establish FieldNet as a robust, efficient solution for real-time vision tasks in field robotics and beyond.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Human Gaussians from Single-View Image</title>
<link>https://arxiv.org/abs/2406.06050</link>
<guid>https://arxiv.org/abs/2406.06050</guid>
<content:encoded><![CDATA[
arXiv:2406.06050v5 Announce Type: replace 
Abstract: In this work, we tackle the task of learning 3D human Gaussians from a single image, focusing on recovering detailed appearance and geometry including unobserved regions. We introduce a single-view generalizable Human Gaussian Model (HGM), which employs a novel generate-then-refine pipeline with the guidance from human body prior and diffusion prior. Our approach uses a ControlNet to refine rendered back-view images from coarse predicted human Gaussians, then uses the refined image along with the input image to reconstruct refined human Gaussians. To mitigate the potential generation of unrealistic human poses and shapes, we incorporate human priors from the SMPL-X model as a dual branch, propagating image features from the SMPL-X volume to the image Gaussians using sparse convolution and attention mechanisms. Given that the initial SMPL-X estimation might be inaccurate, we gradually refine it with our HGM model. We validate our approach on several publicly available datasets. Our method surpasses previous methods in both novel view synthesis and surface reconstruction. Our approach also exhibits strong generalization for cross-dataset evaluation and in-the-wild images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive Learning</title>
<link>https://arxiv.org/abs/2408.05956</link>
<guid>https://arxiv.org/abs/2408.05956</guid>
<content:encoded><![CDATA[
arXiv:2408.05956v3 Announce Type: replace 
Abstract: Currently, most crowd counting methods have outstanding performance under normal weather conditions. However, our experimental validation reveals two key obstacles limiting the accuracy improvement of crowd counting models: 1) the domain gap between the adverse weather and the normal weather images; 2) the weather class imbalance in the training set. To address the problems, we propose a two-stage crowd counting method named Multi-queue Contrastive Learning (MQCL). Specifically, in the first stage, our target is to equip the backbone network with weather-awareness capabilities. In this process, a contrastive learning method named multi-queue MoCo designed by us is employed to enable representation learning under weather class imbalance. After the first stage is completed, the backbone model is "mature" enough to extract weather-related representations. On this basis, we proceed to the second stage, in which we propose to refine the representations under the guidance of contrastive learning, enabling the conversion of the weather-aware representations to the normal weather domain. Through such representation and conversion, the model achieves robust counting performance under both normal and adverse weather conditions. Extensive experimental results show that, compared to the baseline, MQCL reduces the counting error under adverse weather conditions by 22%, while introducing only about 13% increase in computational burden, which achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-based ecological analysis of camera trap images is impacted by training data quality and quantity</title>
<link>https://arxiv.org/abs/2408.14348</link>
<guid>https://arxiv.org/abs/2408.14348</guid>
<content:encoded><![CDATA[
arXiv:2408.14348v2 Announce Type: replace 
Abstract: Large image collections generated from camera traps offer valuable insights into species richness, occupancy, and activity patterns, significantly aiding biodiversity monitoring. However, the manual processing of these datasets is time-consuming, hindering analytical processes. To address this, deep neural networks have been adopted to automate image labelling, but the impact of classification error on ecological metrics remains unclear. Here, we analyse data from camera trap collections in an African savannah (82,300 images, 47 species) and an Asian sub-tropical dry forest (40,308 images, 29 species) to compare ecological metrics derived from expert-generated species identifications with those generated by deep learning classification models. We specifically assess the impact of deep learning model architecture, the proportion of label noise in the training data, and the size of the training dataset on three ecological metrics: species richness, occupancy, and activity patterns. Overall, ecological metrics derived from deep neural networks closely match those calculated from expert labels and remain robust to manipulations in the training pipeline. We found that the choice of deep learning model architecture does not impact ecological metrics, and ecological metrics related to the overall community (species richness, community occupancy) were resilient to up to 10% noise in the training dataset and a 50% reduction in the training dataset size. However, we caution that less common species are disproportionately affected by a reduction in deep neural network accuracy, and this has consequences for species-specific metrics (occupancy, diel activity patterns). To ensure the reliability of their findings, practitioners should prioritize creating large, clean training sets with balanced representation across species over exploring numerous deep learning model architectures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2408.15994</link>
<guid>https://arxiv.org/abs/2408.15994</guid>
<content:encoded><![CDATA[
arXiv:2408.15994v2 Announce Type: replace 
Abstract: Existing All-in-One image restoration methods often fail to perceive degradation types and severity levels simultaneously, overlooking the importance of fine-grained quality perception. Moreover, these methods often utilize highly customized backbones, which hinder their adaptability and integration into more advanced restoration networks. To address these limitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image restoration framework designed for fine-grained quality control across various degradation types and severity levels. Its modular structure allows core components to function independently of specific backbones, enabling seamless integration into advanced restoration models without significant modifications. Specifically, Perceive-IR operates in two key stages: 1) multi-level quality-driven prompt learning stage, where a fine-grained quality perceiver is meticulously trained to discern three tier quality levels by optimizing the alignment between prompts and images within the CLIP perception space. This stage ensures a nuanced understanding of image quality, laying the groundwork for subsequent restoration; 2) restoration stage, where the quality perceiver is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a quality-aware learning strategy. This strategy not only dynamically differentiates sample learning difficulty but also achieves fine-grained quality control by driving the restored image toward the ground truth while pulling it away from both low- and medium-quality samples.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2409.03757</link>
<guid>https://arxiv.org/abs/2409.03757</guid>
<content:encoded><![CDATA[
arXiv:2409.03757v3 Announce Type: replace 
Abstract: Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. Code: https://github.com/YunzeMan/Lexicon3D
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization</title>
<link>https://arxiv.org/abs/2409.07967</link>
<guid>https://arxiv.org/abs/2409.07967</guid>
<content:encoded><![CDATA[
arXiv:2409.07967v3 Announce Type: replace 
Abstract: Dense-localization Audio-Visual Events (DAVE) aims to identify time boundaries and corresponding categories for events that are both audible and visible in a long video, where events may co-occur and exhibit varying durations. However, complex audio-visual scenes often involve asynchronization between modalities, making accurate localization challenging. Existing DAVE solutions extract audio and visual features through unimodal encoders, and fuse them via dense cross-modal interaction. However, independent unimodal encoding struggles to emphasize shared semantics between modalities without cross-modal guidance, while dense cross-modal attention may over-attend to semantically unrelated audio-visual features. To address these problems, we present LoCo, a Locality-aware cross-modal Correspondence learning framework for DAVE. LoCo leverages the local temporal continuity of audio-visual events as important guidance to filter irrelevant cross-modal signals and enhance cross-modal alignment throughout both unimodal and cross-modal encoding stages. i) Specifically, LoCo applies Local Correspondence Feature (LCF) Modulation to enforce unimodal encoders to focus on modality-shared semantics by modulating agreement between audio and visual features based on local cross-modal coherence. ii) To better aggregate cross-modal relevant features, we further customize Local Adaptive Cross-modal (LAC) Interaction, which dynamically adjusts attention regions in a data-driven manner. This adaptive mechanism focuses attention on local event boundaries and accommodates varying event durations. By incorporating LCF and LAC, LoCo provides solid performance gains and outperforms existing DAVE methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthetic Texture Datasets: Challenges, Creation, and Curation</title>
<link>https://arxiv.org/abs/2409.10297</link>
<guid>https://arxiv.org/abs/2409.10297</guid>
<content:encoded><![CDATA[
arXiv:2409.10297v2 Announce Type: replace 
Abstract: The influence of textures on machine learning models has been an ongoing investigation, specifically in texture bias/learning, interpretability, and robustness. However, due to the lack of large and diverse texture data available, the findings in these works have been limited, as more comprehensive evaluations have not been feasible. Image generative models are able to provide data creation at scale, but utilizing these models for texture synthesis has been unexplored and poses additional challenges both in creating accurate texture images and validating those images. In this work, we introduce an extensible methodology and corresponding new dataset for generating high-quality, diverse texture images capable of supporting a broad set of texture-based tasks. Our pipeline consists of: (1) developing prompts from a range of descriptors to serve as input to text-to-image models, (2) adopting and adapting Stable Diffusion pipelines to generate and filter the corresponding images, and (3) further filtering down to the highest quality images. Through this, we create the Prompted Textures Dataset (PTD), a dataset of 362,880 texture images that span 56 textures. During the process of generating images, we find that NSFW safety filters in image generation pipelines are highly sensitive to texture (and flag up to 60\% of our texture images), uncovering a potential bias in these models and presenting unique challenges when working with texture data. Through both standard metrics and a human evaluation, we find that our dataset is high quality and diverse. Our dataset is available for download at https://zenodo.org/records/15359142.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[
arXiv:2409.11686v3 Announce Type: replace 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2410.03577</link>
<guid>https://arxiv.org/abs/2410.03577</guid>
<content:encoded><![CDATA[
arXiv:2410.03577v2 Announce Type: replace 
Abstract: Despite their impressive capabilities, multimodal large language models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to "amnesia" about visual information. To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as "key-value memory" at the middle trigger layer. This "look-twice" mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead. The implementation is available from https://github.com/1zhou-Wang/MemVR
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion</title>
<link>https://arxiv.org/abs/2410.03825</link>
<guid>https://arxiv.org/abs/2410.03825</guid>
<content:encoded><![CDATA[
arXiv:2410.03825v2 Announce Type: replace 
Abstract: Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneCraft: Layout-Guided 3D Scene Generation</title>
<link>https://arxiv.org/abs/2410.09049</link>
<guid>https://arxiv.org/abs/2410.09049</guid>
<content:encoded><![CDATA[
arXiv:2410.09049v3 Announce Type: replace 
Abstract: The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: https://orangesodahub.github.io/SceneCraft
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation</title>
<link>https://arxiv.org/abs/2411.14423</link>
<guid>https://arxiv.org/abs/2411.14423</guid>
<content:encoded><![CDATA[
arXiv:2411.14423v4 Announce Type: replace 
Abstract: Realistic simulation of dynamic scenes requires accurately capturing diverse material properties and modeling complex object interactions grounded in physical principles. However, existing methods are constrained to basic material types with limited predictable parameters, making them insufficient to represent the complexity of real-world materials. We introduce PhysFlow, a novel approach that leverages multi-modal foundation models and video diffusion to achieve enhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to identify material types and initialize material parameters through image queries, while simultaneously inferring 3D Gaussian splats for detailed scene representation. We further refine these material parameters using video diffusion with a differentiable Material Point Method (MPM) and optical flow guidance rather than render loss or Score Distillation Sampling (SDS) loss. This integrated framework enables accurate prediction and realistic simulation of dynamic interactions in real-world scenarios, advancing both accuracy and flexibility in physics-based simulations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2412.00626</link>
<guid>https://arxiv.org/abs/2412.00626</guid>
<content:encoded><![CDATA[
arXiv:2412.00626v2 Announce Type: replace 
Abstract: Harnessing low-light enhancement and domain adaptation, nighttime UAV tracking has made substantial strides. However, over-reliance on image enhancement, limited high-quality nighttime data, and a lack of integration between daytime and nighttime trackers hinder the development of an end-to-end trainable framework. Additionally, current ViT-based trackers demand heavy computational resources due to their reliance on the self-attention mechanism. In this paper, we propose a novel pure Mamba-based tracking framework (MambaNUT) that employs a state space model with linear complexity as its backbone, incorporating a single-stream architecture that integrates feature learning and template-search coupling within Vision Mamba. We introduce an adaptive curriculum learning (ACL) approach that dynamically adjusts sampling strategies and loss weights, thereby improving the model's ability of generalization. Our ACL is composed of two levels of curriculum schedulers: (1) sampling scheduler that transforms the data distribution from imbalanced to balanced, as well as from easier (daytime) to harder (nighttime) samples; (2) loss scheduler that dynamically assigns weights based on the size of the training data and IoU of individual instances. Exhaustive experiments on multiple nighttime UAV tracking benchmarks demonstrate that the proposed MambaNUT achieves state-of-the-art performance while requiring lower computational costs. The code will be available at https://github.com/wuyou3474/MambaNUT.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Event Modality Applications through a Robust CLIP-Based Encoder</title>
<link>https://arxiv.org/abs/2412.03093</link>
<guid>https://arxiv.org/abs/2412.03093</guid>
<content:encoded><![CDATA[
arXiv:2412.03093v2 Announce Type: replace 
Abstract: This paper introduces a powerful encoder that transfers CLIP`s capabilities to event-based data, enhancing its utility and expanding its applicability across diverse domains. While large-scale datasets have significantly advanced image-based models, the scarcity of comprehensive event datasets has limited performance potential in event modality. To address this challenge, we adapt CLIP`s architecture to align event embeddings with image embeddings, supporting zero-shot learning and preserving text alignment while mitigating catastrophic forgetting. Our encoder achieves strong performance in object recognition, with competitive results in zero-shot and few-shot learning tasks. Notably, it generalizes effectively to events extracted from video data without requiring additional training, highlighting its versatility. Additionally, we integrate this encoder within a cross-modality framework that facilitates interaction across five modalities-Image, Event, Text, Sound, and Depth-expanding the possibilities for cross-modal applications. Overall, this work underscores the transformative potential of a robust event encoder, broadening the scope and utility of event-based data across various fields.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2412.07825</link>
<guid>https://arxiv.org/abs/2412.07825</guid>
<content:encoded><![CDATA[
arXiv:2412.07825v3 Announce Type: replace 
Abstract: 3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction</title>
<link>https://arxiv.org/abs/2412.09507</link>
<guid>https://arxiv.org/abs/2412.09507</guid>
<content:encoded><![CDATA[
arXiv:2412.09507v2 Announce Type: replace 
Abstract: Indoor pathloss prediction is a fundamental task in wireless network planning, yet it remains challenging due to environmental complexity and data scarcity. In this work, we propose a deep learning-based approach utilizing a vision transformer (ViT) architecture with DINO-v2 pretrained weights to model indoor radio propagation. Our method processes a floor map with additional features of the walls to generate indoor pathloss maps. We systematically evaluate the effects of architectural choices, data augmentation strategies, and feature engineering techniques. Our findings indicate that extensive augmentation significantly improves generalization, while feature engineering is crucial in low-data regimes. Through comprehensive experiments, we demonstrate the robustness of our model across different generalization scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions</title>
<link>https://arxiv.org/abs/2412.16698</link>
<guid>https://arxiv.org/abs/2412.16698</guid>
<content:encoded><![CDATA[
arXiv:2412.16698v3 Announce Type: replace 
Abstract: For efficient human-agent interaction, an agent should proactively recognize their target user and prepare for upcoming interactions. We formulate this challenging problem as the novel task of jointly forecasting a person's intent to interact with the agent, their attitude towards the agent and the action they will perform, from the agent's (egocentric) perspective. So we propose \emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task dependencies through a hierarchical multitask learning approach. SocialEgoNet uses whole-body skeletons (keypoints from face, hands and body) extracted from only 1 second of video input for high inference speed. For evaluation, we augment an existing egocentric human-agent interaction dataset with new class labels and bounding box annotations. Extensive experiments on this augmented dataset, named JPL-Social, demonstrate \emph{real-time} inference and superior performance (average accuracy across all tasks: 83.15\%) of our model outperforming several competitive baselines. The additional annotations and code will be available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling</title>
<link>https://arxiv.org/abs/2412.17378</link>
<guid>https://arxiv.org/abs/2412.17378</guid>
<content:encoded><![CDATA[
arXiv:2412.17378v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. However, training a 3DGS model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and Gaussian spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS, a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS training process, perfectly solving load-imbalance issues. First, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which constitutes the foundation of load balancing. Second, we are the first to propose the Gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. Based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all SMs, which boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we present a self-adaptive render kernel selection strategy during the 3DGS training process based on different load-balance situations, which effectively improves training efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image Reconstruction</title>
<link>https://arxiv.org/abs/2501.02180</link>
<guid>https://arxiv.org/abs/2501.02180</guid>
<content:encoded><![CDATA[
arXiv:2501.02180v2 Announce Type: replace 
Abstract: Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images, demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texture Image Synthesis Using Spatial GAN Based on Vision Transformers</title>
<link>https://arxiv.org/abs/2502.01842</link>
<guid>https://arxiv.org/abs/2502.01842</guid>
<content:encoded><![CDATA[
arXiv:2502.01842v2 Announce Type: replace 
Abstract: Texture synthesis is a fundamental task in computer vision, whose goal is to generate visually realistic and structurally coherent textures for a wide range of applications, from graphics to scientific simulations. While traditional methods like tiling and patch-based techniques often struggle with complex textures, recent advancements in deep learning have transformed this field. In this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to address the limitations of previous methods. By incorporating specialized texture descriptors such as mean-variance (mu, sigma) and textons into the self-attention mechanism of ViTs, our model achieves superior texture synthesis. This approach enhances the model's capacity to capture complex spatial dependencies, leading to improved texture quality that is superior to state-of-the-art models, especially for regular and irregular textures. Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS demonstrate the substantial improvement of ViT-SGAN, which underlines its efficiency in generating diverse realistic textures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps</title>
<link>https://arxiv.org/abs/2502.08821</link>
<guid>https://arxiv.org/abs/2502.08821</guid>
<content:encoded><![CDATA[
arXiv:2502.08821v2 Announce Type: replace 
Abstract: The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at https://github.com/Noodulz/dejAIvu.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces</title>
<link>https://arxiv.org/abs/2503.01894</link>
<guid>https://arxiv.org/abs/2503.01894</guid>
<content:encoded><![CDATA[
arXiv:2503.01894v2 Announce Type: replace 
Abstract: We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a benchmark for multi-criteria alignment, developed through a two-year participatory process with 30 community organizations to support the pluralistic alignment of text-to-image (T2I) models in inclusive urban planning. The dataset encodes 37,710 pairwise comparisons across 13,462 images, structured along six criteria - Accessibility, Safety, Comfort, Invitingness, Inclusivity, and Diversity - derived from 634 community-defined concepts. Using Direct Preference Optimization (DPO), we fine-tune Stable Diffusion XL to reflect multi-criteria spatial preferences and evaluate the LIVS dataset and the fine-tuned model through four case studies: (1) DPO increases alignment with annotated preferences, particularly when annotation volume is high; (2) preference patterns vary across participant identities, underscoring the need for intersectional data; (3) human-authored prompts generate more distinctive visual outputs than LLM-generated ones, influencing annotation decisiveness; and (4) intersectional groups assign systematically different ratings across criteria, revealing the limitations of single-objective alignment. While DPO improves alignment under specific conditions, the prevalence of neutral ratings indicates that community values are heterogeneous and often ambiguous. LIVS provides a benchmark for developing T2I models that incorporate local, stakeholder-driven preferences, offering a foundation for context-aware alignment in spatial design.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment</title>
<link>https://arxiv.org/abs/2503.03355</link>
<guid>https://arxiv.org/abs/2503.03355</guid>
<content:encoded><![CDATA[
arXiv:2503.03355v4 Announce Type: replace 
Abstract: In this work, we rethink the approach to video super-resolution by introducing a method based on the Diffusion Posterior Sampling framework, combined with an unconditional video diffusion transformer operating in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2503.05423</link>
<guid>https://arxiv.org/abs/2503.05423</guid>
<content:encoded><![CDATA[
arXiv:2503.05423v2 Announce Type: replace 
Abstract: Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn from distinct categories without retaining exemplars but easily suffers from catastrophic forgetting of learned knowledge. While existing EFCIL methods leverage knowledge distillation to alleviate forgetting, they still face two critical challenges: semantic shift and decision bias. Specifically, the embeddings of old tasks shift in the embedding space after learning new tasks, and the classifier becomes biased towards new tasks due to training solely with new data, thereby hindering the balance between old and new knowledge. To address these issues, we propose the Dual-Projection Shift Estimation and Classifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic shift through a dual-projection, which combines a learnable transformation with a row-space projection to capture both task-wise and category-wise shifts. Furthermore, to mitigate decision bias, DPCR employs ridge regression to reformulate classifier training as a reconstruction process. This reconstruction exploits previous information encoded in covariance and prototype of each class after calibration with estimated shift, thereby reducing decision bias. Extensive experiments demonstrate that, across various datasets, DPCR effectively balances old and new tasks, outperforming state-of-the-art EFCIL methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in Body Embeddings for Recognition and Identification</title>
<link>https://arxiv.org/abs/2503.06451</link>
<guid>https://arxiv.org/abs/2503.06451</guid>
<content:encoded><![CDATA[
arXiv:2503.06451v3 Announce Type: replace 
Abstract: Person Re-identification (ReID) systems that match individuals across images or video frames are essential in many real-world applications. However, existing methods are often influenced by attributes such as gender, pose, and body mass index (BMI), which vary in unconstrained settings and raise concerns related to fairness and generalization. To address this, we extend the notion of expressivity, defined as the mutual information between learned features and specific attributes, using a secondary neural network to quantify how strongly attributes are encoded. Applying this framework to three ReID models, we find that BMI consistently shows the highest expressivity in the final layers, indicating its dominant role in recognition. In the last attention layer, attributes are ranked as BMI > Pitch > Gender > Yaw, revealing their relative influences in representation learning. Expressivity values also evolve across layers and training epochs, reflecting a dynamic encoding of attributes. These findings demonstrate the central role of body attributes in ReID and establish a principled approach for uncovering attribute driven correlations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game</title>
<link>https://arxiv.org/abs/2503.10042</link>
<guid>https://arxiv.org/abs/2503.10042</guid>
<content:encoded><![CDATA[
arXiv:2503.10042v2 Announce Type: replace 
Abstract: The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaskAttn-UNet: A Mask Attention-Driven Framework for Universal Low-Resolution Image Segmentation</title>
<link>https://arxiv.org/abs/2503.10686</link>
<guid>https://arxiv.org/abs/2503.10686</guid>
<content:encoded><![CDATA[
arXiv:2503.10686v2 Announce Type: replace 
Abstract: Low-resolution image segmentation is crucial in real-world applications such as robotics, augmented reality, and large-scale scene understanding, where high-resolution data is often unavailable due to computational constraints. To address this challenge, we propose MaskAttn-UNet, a novel segmentation framework that enhances the traditional U-Net architecture via a mask attention mechanism. Our model selectively emphasizes important regions while suppressing irrelevant backgrounds, thereby improving segmentation accuracy in cluttered and complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet effectively balances local feature extraction with broader contextual awareness, making it particularly well-suited for low-resolution inputs. We evaluate our approach on three benchmark datasets with input images rescaled to 128x128 and demonstrate competitive performance across semantic, instance, and panoptic segmentation tasks. Our results show that MaskAttn-UNet achieves accuracy comparable to state-of-the-art methods at significantly lower computational cost than transformer-based models, making it an efficient and scalable solution for low-resolution segmentation in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search is All You Need for Few-shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2504.11895</link>
<guid>https://arxiv.org/abs/2504.11895</guid>
<content:encoded><![CDATA[
arXiv:2504.11895v2 Announce Type: replace 
Abstract: Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging task in industrial inspection, where normal distribution modeling must be accomplished with only a few normal images. While existing approaches typically employ multi-modal foundation models combining language and vision modalities for prompt-guided anomaly detection, these methods often demand sophisticated prompt engineering and extensive manual tuning. In this paper, we demonstrate that a straightforward nearest-neighbor search framework can surpass state-of-the-art performance in both single-class and multi-class FSAD scenarios. Our proposed method, VisionAD, consists of four simple yet essential components: (1) scalable vision foundation models that extract universal and discriminative features; (2) dual augmentation strategies - support augmentation to enhance feature matching adaptability and query augmentation to address the oversights of single-view prediction; (3) multi-layer feature integration that captures both low-frequency global context and high-frequency local details with minimal computational overhead; and (4) a class-aware visual memory bank enabling efficient one-for-all multi-class detection. Extensive evaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate VisionAD's exceptional performance. Using only 1 normal images as support, our method achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8% respectively, outperforming current state-of-the-art approaches by significant margins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior few-shot capabilities of VisionAD make it particularly appealing for real-world applications where samples are scarce or expensive to obtain. Code is available at https://github.com/Qiqigeww/VisionAD.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections</title>
<link>https://arxiv.org/abs/2504.16612</link>
<guid>https://arxiv.org/abs/2504.16612</guid>
<content:encoded><![CDATA[
arXiv:2504.16612v2 Announce Type: replace 
Abstract: Purpose: In this study, we investigate the training of foundation models using federated learning to address data-sharing limitations and enable collaborative model training without data transfer for minimally invasive surgery. Methods: Inspired by the EndoViT study, we adapt the Masked Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is pretrained on the Endo700k dataset collection and later fine-tuned and evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition, and Surgical Phase Recognition. Results: Our findings demonstrate that integrating adaptive FedSAM into the federated MAE approach improves pretraining, leading to a reduction in reconstruction loss per patch. The application of FL-EndoViT in surgical downstream tasks results in performance comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over CEN-EndoViT in surgical scene segmentation when data is limited and in action triplet recognition when large datasets are used. Conclusion: These findings highlight the potential of federated learning for privacy-preserving training of surgical foundation models, offering a robust and generalizable solution for surgical data science. Effective collaboration requires adapting federated learning methods, such as the integration of FedSAM, which can accommodate the inherent data heterogeneity across institutions. In future, exploring FL in video-based models may enhance these capabilities by incorporating spatiotemporal dynamics crucial for real-world surgical environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointBA: Towards Backdoor Attacks in 3D Point Cloud</title>
<link>https://arxiv.org/abs/2103.16074</link>
<guid>https://arxiv.org/abs/2103.16074</guid>
<content:encoded><![CDATA[
arXiv:2103.16074v4 Announce Type: replace-cross 
Abstract: 3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits the unique properties of 3D data and networks. In particular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean-label backdoor attack (PointCBA). The first one is straightforward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples suggesting the vulnerability of deep models under spatial transformation; 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. Extensive experiments show the efficacy of the PointPBA with over 95% success rate across various 3D datasets and models, and the more stealthy PointCBA with around 50% success rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the robustness of 3D deep models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules</title>
<link>https://arxiv.org/abs/2305.00046</link>
<guid>https://arxiv.org/abs/2305.00046</guid>
<content:encoded><![CDATA[
arXiv:2305.00046v2 Announce Type: replace-cross 
Abstract: Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive rate. The performance of both networks of the proposed framework was compared with other studies and found to outperform them regarding segmentation and detection accuracy. Additionally, our proposed Vision transformer network obtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art networks. Our proposed end-to-end deep learning-based framework can effectively segment lungs, and detect and classify lung nodules, specifically in low-resource settings with limited access to radiologists. The proposed framework outperforms existing studies regarding all the respective evaluation metrics. The proposed framework can potentially improve the accuracy and efficiency of lung cancer screening in low-resource settings, ultimately leading to better patient outcomes.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns</title>
<link>https://arxiv.org/abs/2309.14630</link>
<guid>https://arxiv.org/abs/2309.14630</guid>
<content:encoded><![CDATA[
arXiv:2309.14630v3 Announce Type: replace-cross 
Abstract: Sharp, multidimensional changepoints-abrupt shifts in a regression surface whose locations and magnitudes are unknown-arise in settings as varied as gene-expression profiling, financial covariance breaks, climate-regime detection, and urban socioeconomic mapping. Despite their prevalence, there are no current approaches that jointly estimate the location and size of the discontinuity set in a one-shot approach with statistical guarantees. We therefore introduce Free Discontinuity Regression (FDR), a fully nonparametric estimator that simultaneously (i) smooths a regression surface, (ii) segments it into contiguous regions, and (iii) provably recovers the precise locations and sizes of its jumps. By extending a convex relaxation of the Mumford-Shah functional to random spatial sampling and correlated noise, FDR overcomes the fixed-grid and i.i.d. noise assumptions of classical image-segmentation approaches, thus enabling its application to real-world data of any dimension. This yields the first identification and uniform consistency results for multivariate jump surfaces: under mild SBV regularity, the estimated function, its discontinuity set, and all jump sizes converge to their true population counterparts. Hyperparameters are selected automatically from the data using Stein's Unbiased Risk Estimate, and large-scale simulations up to three dimensions validate the theoretical results and demonstrate good finite-sample performance. Applying FDR to an internet shutdown in India reveals a 25-35% reduction in economic activity around the estimated shutdown boundaries-much larger than previous estimates. By unifying smoothing, segmentation, and effect-size recovery in a general statistical setting, FDR turns free-discontinuity ideas into a practical tool with formal guarantees for modern multivariate data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deep Learning Models for Breast Cancer Classification: A Comparative Study</title>
<link>https://arxiv.org/abs/2408.16859</link>
<guid>https://arxiv.org/abs/2408.16859</guid>
<content:encoded><![CDATA[
arXiv:2408.16859v2 Announce Type: replace-cross 
Abstract: This study evaluates the effectiveness of deep learning models in classifying histopathological images for early and accurate detection of breast cancer. Eight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision Transformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and SqueezeNet, were compared using a dataset of 277,524 image patches. The Vision Transformer (ViT) model, with its attention-based mechanisms, achieved the highest validation accuracy of 94%, outperforming conventional CNNs. The study demonstrates the potential of advanced machine learning methods to enhance precision and efficiency in breast cancer diagnosis in clinical settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A nonlinear elasticity model in computer vision</title>
<link>https://arxiv.org/abs/2408.17237</link>
<guid>https://arxiv.org/abs/2408.17237</guid>
<content:encoded><![CDATA[
arXiv:2408.17237v3 Announce Type: replace-cross 
Abstract: The purpose of this paper is to analyze a nonlinear elasticity model introduced by the authors for comparing two images, regarded as bounded open subsets of $\R^n$ together with associated vector-valued intensity maps. Optimal transformations between the images are sought as minimisers of an integral functional among orientation-preserving homeomorphisms. The existence of minimisers is proved under natural coercivity and polyconvexity conditions, assuming only that the intensity functions are bounded measurable. Variants of the existence theorem are also proved, first under the constraint that finite sets of landmark points in the two images are mapped one to the other, and second when one image is to be compared to an unknown part of another.
  The question is studied as to whether for images related by an affine mapping the unique minimiser is given by that affine mapping. For a natural class of functional integrands an example is given guaranteeing that this property holds for pairs of images in which the second is a scaling of the first by a constant factor. However for the property to hold for arbitrary pairs of affinely related images it is shown that the integrand has to depend on the gradient of the transformation as a convex function of its determinant alone. This suggests a new model in which the integrand depends also on second derivatives of the transformation, and an example is given for which both existence of minimisers is assured and the above property holds for all pairs of affinely related images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning</title>
<link>https://arxiv.org/abs/2409.09085</link>
<guid>https://arxiv.org/abs/2409.09085</guid>
<content:encoded><![CDATA[
arXiv:2409.09085v2 Announce Type: replace-cross 
Abstract: Structured pruning is one of the most popular approaches to effectively compress the heavy deep neural networks (DNNs) into compact sub-networks while retaining performance. The existing methods suffer from multi-stage procedures along with significant engineering efforts and human expertise. The Only-Train-Once (OTO) series has been recently proposed to resolve the many pain points by streamlining the workflow by automatically conducting (i) search space generation, (ii) structured sparse optimization, and (iii) sub-network construction. However, the built-in sparse optimizers in the OTO series, i.e., the Half-Space Projected Gradient (HSPG) family, have limitations that require hyper-parameter tuning and the implicit controls of the sparsity exploration, consequently requires intervening by human expertise. To address such limitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO). HESSO could automatically and efficiently train a DNN to produce a high-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys user-friendly integration for generic training applications. To address another common issue of irreversible performance collapse observed in pruning DNNs, we further propose a Corrective Redundant Identification Cycle (CRIC) for reliably identifying indispensable structures. We numerically demonstrate the efficacy of HESSO and its enhanced version HESSO-CRIC on a variety of applications ranging from computer vision to natural language processing, including large language model. The numerical results showcase that HESSO can achieve competitive even superior performance to varying state-of-the-arts and support most DNN architectures. Meanwhile, CRIC can effectively prevent the irreversible performance collapse and further enhance the performance of HESSO on certain applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CloudTrack: Scalable UAV Tracking with Cloud Semantics</title>
<link>https://arxiv.org/abs/2409.16111</link>
<guid>https://arxiv.org/abs/2409.16111</guid>
<content:encoded><![CDATA[
arXiv:2409.16111v3 Announce Type: replace-cross 
Abstract: Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and rescue scenarios to gather information in the search area. The automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person's chances of survival. In this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of UAV hardware. Our approach has several advantages. It can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission and can efficiently track a potentially moving person. Our experimental results demonstrate the versatility and efficacy of our approach.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title>
<link>https://arxiv.org/abs/2410.12705</link>
<guid>https://arxiv.org/abs/2410.12705</guid>
<content:encoded><![CDATA[
arXiv:2410.12705v5 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Scale MRI Collection and Segmentation of Cirrhotic Liver</title>
<link>https://arxiv.org/abs/2410.16296</link>
<guid>https://arxiv.org/abs/2410.16296</guid>
<content:encoded><![CDATA[
arXiv:2410.16296v2 Announce Type: replace-cross 
Abstract: Liver cirrhosis represents the end stage of chronic liver disease, characterized by extensive fibrosis and nodular regeneration that significantly increases mortality risk. While magnetic resonance imaging (MRI) offers a non-invasive assessment, accurately segmenting cirrhotic livers presents substantial challenges due to morphological alterations and heterogeneous signal characteristics. Deep learning approaches show promise for automating these tasks, but progress has been limited by the absence of large-scale, annotated datasets. Here, we present CirrMRI600+, the first comprehensive dataset comprising 628 high-resolution abdominal MRI scans (310 T1-weighted and 318 T2-weighted sequences, totaling nearly 40,000 annotated slices) with expert-validated segmentation labels for cirrhotic livers. The dataset includes demographic information, clinical parameters, and histopathological validation where available. Additionally, we provide benchmark results from 11 state-of-the-art deep learning experiments to establish performance standards. CirrMRI600+ enables the development and validation of advanced computational methods for cirrhotic liver analysis, potentially accelerating progress toward automated Cirrhosis visual staging and personalized treatment planning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Convolution-based Unlearnable Datasets</title>
<link>https://arxiv.org/abs/2411.01742</link>
<guid>https://arxiv.org/abs/2411.01742</guid>
<content:encoded><![CDATA[
arXiv:2411.01742v2 Announce Type: replace-cross 
Abstract: The construction of large datasets for deep learning has raised concerns regarding unauthorized use of online data, leading to increased interest in protecting data from third-parties who want to use it for training. The Convolution-based Unlearnable DAtaset (CUDA) method aims to make data unlearnable by applying class-wise blurs to every image in the dataset so that neural networks learn relations between blur kernels and labels, as opposed to informative features for classifying clean data. In this work, we evaluate whether CUDA data remains unlearnable after image sharpening and frequency filtering, finding that this combination of simple transforms improves the utility of CUDA data for training. In particular, we observe a substantial increase in test accuracy over adversarial training for models trained with CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training models to high accuracy using unlearnable data, we underscore the need for ongoing refinement in data poisoning techniques to ensure data privacy. Our method opens new avenues for enhancing the robustness of unlearnable datasets by highlighting that simple methods such as sharpening and frequency filtering are capable of breaking convolution-based unlearnable datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation</title>
<link>https://arxiv.org/abs/2411.07848</link>
<guid>https://arxiv.org/abs/2411.07848</guid>
<content:encoded><![CDATA[
arXiv:2411.07848v3 Announce Type: replace-cross 
Abstract: Large scale scenes such as multifloor homes can be robustly and efficiently mapped with a 3D graph of landmarks estimated jointly with robot poses in a factor graph, a technique commonly used in commercial robots such as drones and robot vacuums. In this work, we propose Language-Inferred Factor Graph for Instruction Following (LIFGIF), a zero-shot method to ground natural language instructions in such a map. LIFGIF also includes a policy for following natural language navigation instructions in a novel environment while the map is constructed, enabling robust navigation performance in the physical world. To evaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in order to evaluate grounding of object-centric natural language navigation instructions. We compare to two state-of-the-art zero-shot baselines from related tasks, Object Goal Navigation and Vision Language Navigation, to demonstrate that LIFGIF outperforms them across all our evaluation metrics on OCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for performing zero-shot object-centric instruction following in the real world on a Boston Dynamics Spot robot.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud Occupancy Functions</title>
<link>https://arxiv.org/abs/2411.08777</link>
<guid>https://arxiv.org/abs/2411.08777</guid>
<content:encoded><![CDATA[
arXiv:2411.08777v4 Announce Type: replace-cross 
Abstract: Accurately determining the shape of objects and the location of their internal structures within deformable objects is crucial for medical tasks that require precise targeting, such as robotic biopsies. We introduce LUDO, a method for accurate low-latency understanding of deformable objects. LUDO reconstructs objects in their deformed state, including their internal structures, from a single-view point cloud observation in under 30 ms using occupancy networks. LUDO provides uncertainty estimates for its predictions. Additionally, it provides explainability by highlighting key features in its input observations. Both uncertainty and explainability are important for safety-critical applications such as surgical interventions. We demonstrate LUDO's abilities for autonomous targeting of internal regions of interest (ROIs) in deformable objects. We evaluate LUDO in real-world robotic experiments, achieving a success rate of 98.9% for puncturing various ROIs inside deformable objects. LUDO demonstrates the potential to interact with deformable objects without the need for deformable registration methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway Analysis</title>
<link>https://arxiv.org/abs/2412.11039</link>
<guid>https://arxiv.org/abs/2412.11039</guid>
<content:encoded><![CDATA[
arXiv:2412.11039v2 Announce Type: replace-cross 
Abstract: Accurate anatomical labeling and analysis of the pulmonary structure and its surrounding anatomy from thoracic CT is getting increasingly important for understanding the etilogy of abnormalities or supporting targetted therapy and early interventions. Whilst lung and airway cell atlases have been attempted, there is a lack of fine-grained morphological atlases that are clinically deployable. In this work, we introduce AirMorph, a robust, end-to-end deep learning pipeline enabling fully automatic and comprehensive airway anatomical labeling at lobar, segmental, and subsegmental resolutions that can be used to create digital atlases of the lung. Evaluated across large-scale multi-center datasets comprising diverse pulmonary conditions, the AirMorph consistently outperformed existing segmentation and labeling methods in terms of accuracy, topological consistency, and completeness. To simplify clinical interpretation, we further introduce a compact anatomical signature quantifying critical morphological airway features, including stenosis, ectasia, tortuosity, divergence, length, and complexity. When applied to various pulmonary diseases such as pulmonary fibrosis, emphysema, atelectasis, consolidation, and reticular opacities, it demonstrates strong discriminative power, revealing disease-specific morphological patterns with high interpretability and explainability. Additionally, AirMorph supports efficient automated branching pattern analysis, potentially enhancing bronchoscopic navigation planning and procedural safety, offering a valuable clinical tool for improved diagnosis, targeted treatment, and personalized patient care.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Rate Control for Deep Video Compression with Rate-Distortion Prediction</title>
<link>https://arxiv.org/abs/2412.18834</link>
<guid>https://arxiv.org/abs/2412.18834</guid>
<content:encoded><![CDATA[
arXiv:2412.18834v2 Announce Type: replace-cross 
Abstract: Deep video compression has made significant progress in recent years, achieving rate-distortion performance that surpasses that of traditional video compression methods. However, rate control schemes tailored for deep video compression have not been well studied. In this paper, we propose a neural network-based $\lambda$-domain rate control scheme for deep video compression, which determines the coding parameter $\lambda$ for each to-be-coded frame based on the rate-distortion-$\lambda$ (R-D-$\lambda$) relationships directly learned from uncompressed frames, achieving high rate control accuracy efficiently without the need for pre-encoding. Moreover, this content-aware scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt changes in video content. Specifically, we introduce two neural network-based predictors to estimate the relationship between bitrate and $\lambda$, as well as the relationship between distortion and $\lambda$ for each frame. Then we determine the coding parameter $\lambda$ for each frame to achieve the target bitrate. Experimental results demonstrate that our approach achieves high rate control accuracy at the mini-GOP level with low time overhead and mitigates inter-frame quality fluctuations across video content of varying resolutions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrontierNet: Learning Visual Cues to Explore</title>
<link>https://arxiv.org/abs/2501.04597</link>
<guid>https://arxiv.org/abs/2501.04597</guid>
<content:encoded><![CDATA[
arXiv:2501.04597v2 Announce Type: replace-cross 
Abstract: Exploration of unknown environments is crucial for autonomous robots; it allows them to actively reason and decide on what new data to acquire for different tasks, such as mapping, object discovery, and environmental assessment. Existing solutions, such as frontier-based exploration approaches, rely heavily on 3D map operations, which are limited by map quality and, more critically, often overlook valuable context from visual cues. This work aims at leveraging 2D visual cues for efficient autonomous exploration, addressing the limitations of extracting goal poses from a 3D map. We propose a visual-only frontier-based exploration system, with FrontierNet as its core component. FrontierNet is a learning-based model that (i) proposes frontiers, and (ii) predicts their information gain, from posed RGB images enhanced by monocular depth priors. Our approach provides an alternative to existing 3D-dependent goal-extraction approaches, achieving a 15\% improvement in early-stage exploration efficiency, as validated through extensive simulations and real-world experiments. The project is available at https://github.com/cvg/FrontierNet.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework</title>
<link>https://arxiv.org/abs/2503.13309</link>
<guid>https://arxiv.org/abs/2503.13309</guid>
<content:encoded><![CDATA[
arXiv:2503.13309v2 Announce Type: replace-cross 
Abstract: Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer remains one of the leading causes of cancer-related deaths among women worldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown significant promise in development of advanced Deep Learning (DL) architectures for breast cancer diagnosis through mammography. In this context, the paper focuses on the integration of AI within a Human-Centric workflow to enhance breast cancer diagnostics. Key challenges are, however, largely overlooked such as reliance on detailed tumor annotations and susceptibility to missing views, particularly during test time. To address these issues, we propose a hybrid, multi-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that enhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework is designed to work as a decision-support tool, helping radiologists analyze multi-view mammograms more effectively. More specifically, the MSMV-Swin framework leverages the Segment Anything Model (SAM) to isolate the breast lobe, reducing background noise and enabling comprehensive feature extraction. The multi-scale nature of the proposed MSMV-Swin framework accounts for tumor-specific regions as well as the spatial characteristics of tissues surrounding the tumor, capturing both localized and contextual information. The integration of contextual and localized data ensures that MSMV-Swin's outputs align with the way radiologists interpret mammograms, fostering better human-AI interaction and trust. A hybrid fusion structure is then designed to ensure robustness against missing views, a common occurrence in clinical practice when only a single mammogram view is available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment</title>
<link>https://arxiv.org/abs/2504.08603</link>
<guid>https://arxiv.org/abs/2504.08603</guid>
<content:encoded><![CDATA[
arXiv:2504.08603v2 Announce Type: replace-cross 
Abstract: Geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. In this paper we present FindAnything, an open-world mapping and exploration framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything bridges the gap between pure geometric and open-vocabulary semantic information for a higher level of understanding while allowing to explore any environment without the help of any external source of ground-truth pose information. We represent the environment as a series of volumetric occupancy submaps, resulting in a robust and accurate map representation that deforms upon pose updates when the underlying SLAM system corrects its drift, allowing for a locally consistent representation between submaps. Pixel-wise vision-language features are aggregated from efficient SAM (eSAM)-generated segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. The open-vocabulary map representation of FindAnything achieves state-of-the-art semantic accuracy in closed-set evaluations on the Replica dataset. This level of scene understanding allows a robot to explore environments based on objects or areas of interest selected via natural language queries. Our system is the first of its kind to be deployed on resource-constrained devices, such as MAVs, leveraging vision-language information for real-world robotic tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models</title>
<link>https://arxiv.org/abs/2505.03821</link>
<guid>https://arxiv.org/abs/2505.03821</guid>
<content:encoded><![CDATA[
<div> Vision Language Models, visual perspective taking, spatial configurations, humanoid minifigure, spatial reasoning <br />
Summary:<br />
The study investigates Vision Language Models' (VLMs) ability in visual perspective taking through novel visual tasks inspired by human tests. Controlled scenes with humanoid minifigures and objects were used to create 144 tasks with varying spatial configurations. Diagnostic questions assessed scene understanding, spatial reasoning, and visual perspective taking. State-of-the-art models like GPT-4-Turbo showed strong scene understanding but struggled with spatial reasoning and perspective-taking. The gap between object recognition and deeper reasoning suggests a need for geometric representations and specialized training in VLM development. <div>
arXiv:2505.03821v1 Announce Type: new 
Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN &amp; BNN) and Digital Image Colorimetry</title>
<link>https://arxiv.org/abs/2505.03826</link>
<guid>https://arxiv.org/abs/2505.03826</guid>
<content:encoded><![CDATA[
<div> machine learning, semiconductor manufacturing, etch depth prediction, in-situ monitoring, digital image colorimetry

Summary:
- The study focuses on the precise monitoring of etch depth and insulating material thickness in semiconductor manufacturing using a non-contact, in-situ approach based on machine learning (ML) techniques. 
- Two scenarios are explored: the use of an artificial neural network (ANN) to predict etch depth from process parameters and a Bayesian Neural Network (BNN) to incorporate variability from repeated measurements. 
- The BNN was found to provide reliable uncertainty estimates, enhancing the accuracy of etch depth prediction. 
- Additionally, the feasibility of using RGB data from digital image colorimetry (DIC) for etch depth prediction was demonstrated, showing strong performance even without explicit process parameters. 
- The integration of DIC and ML offers a cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to improved process stability and manufacturing efficiency.

<br /><br />Summary: <div>
arXiv:2505.03826v1 Announce Type: new 
Abstract: Precise monitoring of etch depth and the thickness of insulating materials, such as Silicon dioxide and silicon nitride, is critical to ensuring device performance and yield in semiconductor manufacturing. While conventional ex-situ analysis methods are accurate, they are constrained by time delays and contamination risks. To address these limitations, this study proposes a non-contact, in-situ etch depth prediction framework based on machine learning (ML) techniques. Two scenarios are explored. In the first scenario, an artificial neural network (ANN) is trained to predict average etch depth from process parameters, achieving a significantly lower mean squared error (MSE) compared to a linear baseline model. The approach is then extended to incorporate variability from repeated measurements using a Bayesian Neural Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage analysis confirms the BNN's capability to provide reliable uncertainty estimates. In the second scenario, we demonstrate the feasibility of using RGB data from digital image colorimetry (DIC) as input for etch depth prediction, achieving strong performance even in the absence of explicit process parameters. These results suggest that the integration of DIC and ML offers a viable, cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to enhanced process stability, and manufacturing efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoLLM Benchmarks and Evaluation: A Survey</title>
<link>https://arxiv.org/abs/2505.03829</link>
<guid>https://arxiv.org/abs/2505.03829</guid>
<content:encoded><![CDATA[
<div> benchmarks, evaluation methodologies, Video Large Language Models, performance trends, challenges

Summary:
This survey explores benchmarks and evaluation methodologies for Video Large Language Models (VideoLLMs), assessing current benchmarks, evaluation protocols, and limitations. It delves into various evaluation methods like closed-set, open-set, temporal, and spatiotemporal understanding tasks. The survey also showcases the trends in state-of-the-art VideoLLMs' performance across benchmarks and identifies challenges within current evaluation frameworks. Future research directions are proposed to enhance benchmark design, evaluation metrics, and protocols, emphasizing the necessity for more diverse, multimodal, and interpretability-focused benchmarks. This analysis equips researchers with structured knowledge on effectively evaluating VideoLLMs and suggests promising avenues for video understanding advancement with large language models. <br /><br />Summary: <div>
arXiv:2505.03829v1 Announce Type: new 
Abstract: The rapid development of Large Language Models (LLMs) has catalyzed significant advancements in video understanding technologies. This survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for Video Large Language Models (VideoLLMs). We examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. The paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. We highlight the performance trends of state-of-the-art VideoLLMs across these benchmarks and identify key challenges in current evaluation frameworks. Additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. This survey aims to equip researchers with a structured understanding of how to effectively evaluate VideoLLMs and identify promising avenues for advancing the field of video understanding with large language models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Forgery Detection for Surveillance Cameras: A Review</title>
<link>https://arxiv.org/abs/2505.03832</link>
<guid>https://arxiv.org/abs/2505.03832</guid>
<content:encoded><![CDATA[
<div> Forensic Techniques, Video Forgery, Surveillance Footage, Authenticity, Video Analysis<br />
<br />Summary: The article discusses the increasing accessibility of video recording technology and the potential for manipulation through advanced editing tools. Surveillance footage is crucial for security and judicial processes, but concerns about authenticity have risen due to the ease of tampering. Various forensic techniques, including compression-based analysis, frame duplication detection, and machine learning approaches, are reviewed for their effectiveness in detecting video forgery. The need for stronger forensic capabilities to combat evolving forgery methods is emphasized to maintain the credibility and admissibility of surveillance recordings as legal evidence. <div>
arXiv:2505.03832v1 Announce Type: new 
Abstract: The widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. Surveillance footage plays a crucial role in security, law enforcement, and judicial processes. However, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. Ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. This paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. Various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. The findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. Strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2505.03833</link>
<guid>https://arxiv.org/abs/2505.03833</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, Parkinson's disease, PointExplainer, interpretability, diagnosis

Summary:
PointExplainer is a new explainable diagnostic strategy using deep neural networks to analyze hand-drawn signals for early diagnosis of Parkinson's disease. By assigning attribution values to hand-drawn segments, PointExplainer identifies the regions driving the model's diagnosis, providing clear interpretability. The system encodes hand-drawn signals into 3D point clouds and trains an interpretable surrogate model to approximate the black-box diagnostic model's behavior. Consistency measures ensure faithfulness in explanations. Extensive experiments on benchmark datasets show that PointExplainer offers intuitive explanations without sacrificing diagnostic performance. The source code is available for further exploration. <div>
arXiv:2505.03833v1 Announce Type: new 
Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of Parkinson's disease. However, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. In this paper, we propose PointExplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. Specifically, PointExplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. Its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3D point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. We also introduce consistency measures to further address the issue of faithfulness in explanations. Extensive experiments on two benchmark datasets and a newly constructed dataset show that PointExplainer can provide intuitive explanations with no diagnostic performance degradation. The source code is available at https://github.com/chaoxuewang/PointExplainer.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Face Recognition via Improved Localization</title>
<link>https://arxiv.org/abs/2505.03837</link>
<guid>https://arxiv.org/abs/2505.03837</guid>
<content:encoded><![CDATA[
<div> Keywords: Biometric authentication, Deep learning, Face recognition, Explainable AI, Scaled Directed Divergence

Summary: 
This paper discusses the importance of explainable face recognition systems in the age of artificial intelligence. While deep learning-based face recognition systems are widely used, the lack of transparency and justification for their decisions can lead to a lack of trust from users. To address this issue, the paper introduces a method called Scaled Directed Divergence (SDD) for explainable face recognition. By using Class Activation Mapping (CAM), the SDD technique allows for the fine localization of relevant face features used by the deep learning model for its predictions. The experiments demonstrate that the SDD Class Activation Map provides specific and accurate visual explanations compared to traditional CAM. By providing transparent and visually clear explanations with narrow localization of relevant features, deep learning-based face recognition systems can enhance transparency and trust among users. <div>
arXiv:2505.03837v1 Announce Type: new 
Abstract: Biometric authentication has become one of the most widely used tools in the current technological era to authenticate users and to distinguish between genuine users and imposters. Face is the most common form of biometric modality that has proven effective. Deep learning-based face recognition systems are now commonly used across different domains. However, these systems usually operate like black-box models that do not provide necessary explanations or justifications for their decisions. This is a major disadvantage because users cannot trust such artificial intelligence-based biometric systems and may not feel comfortable using them when clear explanations or justifications are not provided. This paper addresses this problem by applying an efficient method for explainable face recognition systems. We use a Class Activation Mapping (CAM)-based discriminative localization (very narrow/specific localization) technique called Scaled Directed Divergence (SDD) to visually explain the results of deep learning-based face recognition systems. We perform fine localization of the face features relevant to the deep learning model for its prediction/decision. Our experiments show that the SDD Class Activation Map (CAM) highlights the relevant face features very specifically compared to the traditional CAM and very accurately. The provided visual explanations with narrow localization of relevant features can ensure much-needed transparency and trust for deep learning-based face recognition systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation</title>
<link>https://arxiv.org/abs/2505.03846</link>
<guid>https://arxiv.org/abs/2505.03846</guid>
<content:encoded><![CDATA[
<div> Graph-Augmented Multimodal Encoder, facial graph, Geo Two-Stream Network, Graph Convolutional Networks, Convolutional Neural Networks, attention mechanisms, ResNet18, VGGFace, BiGRU, VGGish, XLM-Roberta

Summary:
GAME, a Graph-Augmented Multimodal Encoder, is proposed to analyze personality from short videos. It combines visual, auditory, and textual cues through a facial graph and a dual-branch network incorporating Graph Convolutional Networks, Convolutional Neural Networks, and attention mechanisms for facial cue analysis. Temporal dynamics are captured by a BiGRU with temporal attention modules. Audio features come from the VGGish network, while linguistic semantics are derived from the XLM-Roberta transformer. A Channel Attention-based Fusion module integrates the multimodal features, followed by a Multi-Layer Perceptron for personality prediction. Experiments demonstrate GAME's superiority over existing methods across various benchmarks, confirming its efficacy and versatility.<br /><br />Summary:GAME, a Graph-Augmented Multimodal Encoder, integrates visual, auditory, and textual cues to predict personality traits from short videos. It utilizes advanced techniques such as dual-branch networks, attention mechanisms, and temporal dynamics modeling to extract facial, audio, and linguistic features. A Channel Attention-based Fusion module effectively integrates these features, leading to improved performance compared to existing methods.GAME demonstrates superior performance across multiple benchmarks, affirming its effectiveness and generalizability in automatic personality prediction from short videos. <div>
arXiv:2505.03846v1 Announce Type: new 
Abstract: Apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. In this paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to robustly model and fuse multi-source features for automatic personality prediction. For the visual stream, we construct a facial graph and introduce a dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks (GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to capture both structural and appearance-based facial cues. Complementing this, global context and iden-tity features are extracted using pretrained ResNet18 and VGGFace back-bones. To capture temporal dynamics, frame-level features are processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio representations are derived from the VGGish network, and linguistic se-mantics are captured via the XLM-Roberta transformer. To achieve effective multimodal integration, we propose a Channel Attention-based Fusion module, followed by a Multi-Layer Perceptron (MLP) regression head for predicting personality traits. Extensive experiments show that GAME con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques</title>
<link>https://arxiv.org/abs/2505.03848</link>
<guid>https://arxiv.org/abs/2505.03848</guid>
<content:encoded><![CDATA[
<div> Keywords: semiconductor manufacturing, image data, clustering, Topological Data Analysis, self-supervised learning, transfer learning

Summary: 
This paper introduces an advanced clustering framework for semiconductor image data analysis. The framework combines deep Topological Data Analysis (TDA), self-supervised learning, and transfer learning techniques to enhance unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data. Transfer learning enables adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. The framework was validated on synthetic and open-source semiconductor image datasets, successfully identifying clusters aligned with defect patterns and process variations. By integrating TDA, self-supervised learning, and transfer learning, this framework offers a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other industries with large-scale image datasets.

<br /><br />Summary: <div>
arXiv:2505.03848v1 Announce Type: new 
Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for defect identification and yield optimization, yet often exceeds manual inspection capabilities. Traditional clustering techniques struggle with high-dimensional, unlabeled data, limiting their effectiveness in capturing nuanced patterns. This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Inference Model of Covert and Overt Visual Attention</title>
<link>https://arxiv.org/abs/2505.03856</link>
<guid>https://arxiv.org/abs/2505.03856</guid>
<content:encoded><![CDATA[
<div> Model, covert attention, overt attention, active inference, sensory precisions<br />
Summary:<br />
This paper presents a model of covert and overt visual attention within the framework of active inference, focusing on optimizing sensory precisions to minimize free-energy. The model considers both current environmental beliefs and sensory input to determine visual sensory precisions, guiding attentional allocation in covert and overt modalities. In experiments using the Posner cueing task and a target focus task with 2D visual data, reaction times are measured to study the interaction between exogenous and endogenous attention, as well as valid and invalid cueing. Results indicate that exogenous and valid cues lead to faster reaction times, with behavior similar to inhibition of return observed. Additionally, the study investigates different aspects of overt attention, finding that involuntary saccades are quicker than intentional ones but lack adaptability. Overall, the model demonstrates how dynamic optimization of sensory precisions can enhance attentional processes in complex sensory environments. <br /><br />Summary: <div>
arXiv:2505.03856v1 Announce Type: new 
Abstract: The ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. This paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. The model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. To test the effectiveness of the model, we analyze its behavior in the Posner cueing task and a simple target focus task using two-dimensional(2D) visual data. Reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. The results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. Furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. Lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2505.03896</link>
<guid>https://arxiv.org/abs/2505.03896</guid>
<content:encoded><![CDATA[
<div> Keywords: Retinal vessel segmentation, Neural Networks, Attention Gates, Kolmogorov-Arnold Network, Pixel-wise Contrastive Loss 

Summary: 
Retinal vessel segmentation is crucial for detecting ocular diseases early. The proposed AttUKAN model combines Attention Gates with Kolmogorov-Arnold Networks to enhance sensitivity and interpretability. A novel Label-guided Pixel-wise Contrastive Loss is introduced to extract more discriminative features. Experiment results on public datasets and a private dataset demonstrate that AttUKAN achieves the highest F1 and MIoU scores compared to other networks. The model outperforms existing methods in both quantitative and qualitative evaluations, showcasing its state-of-the-art performance. The code for AttUKAN will be available on GitHub for further research and implementation. <br /><br />Summary: <div>
arXiv:2505.03896v1 Announce Type: new 
Abstract: Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov-Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at https://github.com/stevezs315/AttUKAN.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces</title>
<link>https://arxiv.org/abs/2505.03974</link>
<guid>https://arxiv.org/abs/2505.03974</guid>
<content:encoded><![CDATA[
<div> Keywords: drones, infrastructure asset management, super-resolution, convolutional neural network, efficient sub-pixel convolutional neural network

Summary: 
This study addresses the challenge of low-resolution infrastructure images captured by drones for asset management. A framework combining convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN) was developed. CNN accurately classifies distress classes in infrastructure images, while ESPCNN efficiently generates high-resolution images of positive distress. The ESPCNN outperforms bicubic interpolation in super-resolution evaluation metrics. The combination of CNN and ESPCNN effectively preprocesses images with negative distress, reducing computational costs and false alarms. Visual inspection shows that ESPCNN captures crack propagation and complex geometry. This framework is expected to aid highway agencies in accurate distress detection and improve asset management practices.<br /><br />Summary: <div>
arXiv:2505.03974v1 Announce Type: new 
Abstract: Recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. However, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. A few studies used super-resolution techniques to address the problem of low-resolution images. Nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. In order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately classified both the classes. ESPCNN, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the evaluation metrics for super-resolution. Based on the performance metrics, the combination of CNN and ESPCNN was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. The visual inspection showed that EPSCNN is able to capture crack propagation, complex geometry of even minor cracks. The proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges</title>
<link>https://arxiv.org/abs/2505.03991</link>
<guid>https://arxiv.org/abs/2505.03991</guid>
<content:encoded><![CDATA[
<div> Keywords: Video event detection, deep learning, sports analytics, Convolutional Neural Networks, Transformers

Summary: 
This survey delves into the advancements in video event detection in sports analytics, focusing on Temporal Action Localization, Action Spotting, and Precise Event Spotting tasks. It explores the evolution of methodological approaches and evaluates existing datasets and evaluation metrics tailored for sports contexts. State-of-the-art techniques such as multi-modal approaches, self-supervised learning, and knowledge distillation are analyzed. The survey also discusses methods for generalizing across multiple sports and outlines key challenges and future research directions. The goal is to develop more efficient, generalizable, and robust event detection frameworks applicable to a variety of sports. Overall, this survey lays the groundwork for future research in multi-modal sports event detection. 

Summary:<br /><br />Keywords: Video event detection, deep learning, sports analytics, Convolutional Neural Networks, Transformers<br />This survey delves into the advancements in video event detection in sports analytics, focusing on Temporal Action Localization, Action Spotting, and Precise Event Spotting tasks. It explores the evolution of methodological approaches and evaluates existing datasets and evaluation metrics tailored for sports contexts. State-of-the-art techniques such as multi-modal approaches, self-supervised learning, and knowledge distillation are analyzed. The survey also discusses methods for generalizing across multiple sports and outlines key challenges and future research directions. The goal is to develop more efficient, generalizable, and robust event detection frameworks applicable to a variety of sports. Overall, this survey lays the groundwork for future research in multi-modal sports event detection. <div>
arXiv:2505.03991v1 Announce Type: new 
Abstract: Video event detection has become an essential component of sports analytics, enabling automated identification of key moments and enhancing performance analysis, viewer engagement, and broadcast efficiency. Recent advancements in deep learning, particularly Convolutional Neural Networks (CNNs) and Transformers, have significantly improved accuracy and efficiency in Temporal Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting (PES). This survey provides a comprehensive overview of these three key tasks, emphasizing their differences, applications, and the evolution of methodological approaches. We thoroughly review and categorize existing datasets and evaluation metrics specifically tailored for sports contexts, highlighting the strengths and limitations of each. Furthermore, we analyze state-of-the-art techniques, including multi-modal approaches that integrate audio and visual information, methods utilizing self-supervised learning and knowledge distillation, and approaches aimed at generalizing across multiple sports. Finally, we discuss critical open challenges and outline promising research directions toward developing more generalized, efficient, and robust event detection frameworks applicable to diverse sports. This survey serves as a foundation for future research on efficient, generalizable, and multi-modal sports event detection.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics</title>
<link>https://arxiv.org/abs/2505.04006</link>
<guid>https://arxiv.org/abs/2505.04006</guid>
<content:encoded><![CDATA[
<div> retinal imaging, Artificial Intelligence, oculomics, systemic health, early detection <br />
Summary: 
The article discusses the significance of the human eye's vascularized anatomy in providing insights into human health, with the retina serving as a crucial window for detecting and monitoring diseases. Advancements in imaging technology, particularly Artificial Intelligence, have revolutionized the field by enabling the analysis of retinal images for systemic health assessment. Oculomics, a novel approach in ophthalmology, combines retinal imaging with AI-driven analysis to identify non-invasive markers for timely intervention. The paper traces the evolution of retinal imaging techniques and underscores the importance of integrating AI in this domain. It also addresses potential challenges in the adoption of oculomics, highlighting research gaps and proposing future directions for this innovative field.<br /><br /> <div>
arXiv:2505.04006v1 Announce Type: new 
Abstract: The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoodTrack: Estimating Handheld Food Portions with Egocentric Video</title>
<link>https://arxiv.org/abs/2505.04055</link>
<guid>https://arxiv.org/abs/2505.04055</guid>
<content:encoded><![CDATA[
<div> Keywords: FoodTrack, hand-held food items, egocentric video, food volume measurement, accuracy

Summary:
FoodTrack is a new framework proposed for accurately tracking and measuring the volume of hand-held food items using egocentric video. Unlike traditional approaches that rely on specific camera angles or gesture recognition, FoodTrack is robust to hand occlusions and flexible with varying camera and object poses. It estimates food volume directly, without making assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. The framework achieved an absolute percentage loss of approximately 7.01% on a handheld food object, outperforming a previous approach that had a mean absolute percentage error of 16.40% in its best case scenario under less flexible conditions.<br /><br />Summary: FoodTrack is a novel framework for accurately tracking and measuring hand-held food items' volume using egocentric video. It provides a flexible and robust solution that directly estimates food volume without relying on intake gestures or making assumptions about bite size, resulting in a more accurate and adaptable method for tracking food consumption. <div>
arXiv:2505.04055v1 Announce Type: new 
Abstract: Accurately tracking food consumption is crucial for nutrition and health monitoring. Traditional approaches typically require specific camera angles, non-occluded images, or rely on gesture recognition to estimate intake, making assumptions about bite size rather than directly measuring food volume. We propose the FoodTrack framework for tracking and measuring the volume of hand-held food items using egocentric video which is robust to hand occlusions and flexible with varying camera and object poses. FoodTrack estimates food volume directly, without relying on intake gestures or fixed assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. We achieve absolute percentage loss of approximately 7.01% on a handheld food object, improving upon a previous approach that achieved a 16.40% mean absolute percentage error in its best case, under less flexible conditions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2505.04058</link>
<guid>https://arxiv.org/abs/2505.04058</guid>
<content:encoded><![CDATA[
<div> 3D visual grounding, 2D-assisted framework, semantic-spatial scene graphs, referred object discrimination, relationship perception <br />
Summary:<br />
The article introduces a novel approach for 3D visual grounding by incorporating a 2D-assisted framework that focuses on discriminating referred objects for relationship perception in complex scenes. The proposed method utilizes 2D pre-trained attributes to guide multi-modal object encoding and employs a graph attention mechanism for relationship-oriented information fusion. This approach enhances object representation and facilitates iterative relational learning, leading to effective alignment between 3D vision and referential descriptions. Experimental results on benchmark datasets demonstrate the superior performance of the model, particularly in distinguishing multiple similar distractors. The dual-branch visual encoder and cross-modal interaction module play crucial roles in achieving accurate localization of unique targets in 3D scenes based on natural language descriptions. <br /> <div>
arXiv:2505.04058v1 Announce Type: new 
Abstract: 3D visual grounding aims to localize the unique target described by natural languages in 3D scenes. The significant gap between 3D and language modalities makes it a notable challenge to distinguish multiple similar objects through the described spatial relationships. Current methods attempt to achieve cross-modal understanding in complex scenes via a target-centered learning mechanism, ignoring the perception of referred objects. We propose a novel 2D-assisted 3D visual grounding framework that constructs semantic-spatial scene graphs with referred object discrimination for relationship perception. The framework incorporates a dual-branch visual encoder that utilizes 2D pre-trained attributes to guide the multi-modal object encoding. Furthermore, our cross-modal interaction module uses graph attention to facilitate relationship-oriented information fusion. The enhanced object representation and iterative relational learning enable the model to establish effective alignment between 3D vision and referential descriptions. Experimental results on the popular benchmarks demonstrate our superior performance compared to state-of-the-art methods, especially in addressing the challenges of multiple similar distractors.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2505.04087</link>
<guid>https://arxiv.org/abs/2505.04087</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time adaptation, robustness, data augmentations, entropy loss, selection mechanism

Summary:<br />
Test-Time adaptation (TTA) is a method used to improve model robustness against distribution shifts during inference. Existing TTA methods often rely on entropy-based unsupervised training but struggle to effectively utilize reliable samples due to a single round of training. This paper introduces Single-step Ensemble of Vicinal Augmentations (SEVA), a novel TTA approach that optimizes an upper bound of entropy loss to incorporate the effects of multiple augmentations in a single step. By using this efficient loss and a selection strategy, SEVA can enhance the potential of reliable samples and meet real-time requirements. The proposed method outperforms on challenging testing scenarios and various network architectures, demonstrating its effectiveness and adaptability. The code for SEVA will be made publicly available. <br />Summary: <div>
arXiv:2505.04087v1 Announce Type: new 
Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. While existing TTA methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. In this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. To address this limitation, we propose a novel TTA approach named Single-step Ensemble of Vicinal Augmentations (SEVA), which can take advantage of data augmentations without increasing the computational burden. Specifically, instead of explicitly utilizing the augmentation strategy to generate new data, SEVA develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. Furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. Combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of TTA. The comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of SEVA. The code will be publicly available.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking</title>
<link>https://arxiv.org/abs/2505.04088</link>
<guid>https://arxiv.org/abs/2505.04088</guid>
<content:encoded><![CDATA[
<div> Siamese Motion Mamba Tracker, thermal infrared object tracking, bidirectional state-space model, self-attention mechanism, motion features, edge details, computational efficiency, parameter-sharing strategy, motion edge-aware regression loss. 
<br />
Summary: 
The paper introduces the Siamese Motion Mamba Tracker (SMMT) for thermal infrared object tracking, addressing challenges like occlusion and background clutter. SMMT integrates bidirectional modeling and a self-attention mechanism to extract motion features and recover edge details. The tracker utilizes a Siamese parameter-sharing strategy to reduce computational redundancy while maintaining strong feature representation. Additionally, a motion edge-aware regression loss is proposed to enhance tracking accuracy for motion-blurred targets. Extensive experiments on four TIR tracking benchmarks demonstrate the superior performance of SMMT in thermal infrared target tracking. <div>
arXiv:2505.04088v1 Announce Type: new 
Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</title>
<link>https://arxiv.org/abs/2505.04105</link>
<guid>https://arxiv.org/abs/2505.04105</guid>
<content:encoded><![CDATA[
<div> Motion-Aware Image SYnthesis, GAN-based methods, SSIM loss, anatomical details, artifact correction  
Motion-Aware Image SYnthesis (MAISY) addresses the limitations of current image correction algorithms by focusing on localized features and introducing the Variance-Selective SSIM (VS-SSIM) loss function. By leveraging the Segment Anything Model (SAM) to identify motion patterns along anatomical boundaries and using VS-SSIM to preserve details in regions with high pixel variance, MAISY outperforms existing models in correcting motion artifacts in medical images. Experimental results on chest and head CT datasets show significant improvements in Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Dice scores, demonstrating the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2505.04105v1 Announce Type: new 
Abstract: Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging.Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One2Any: One-Reference 6D Pose Estimation for Any Object</title>
<link>https://arxiv.org/abs/2505.04109</link>
<guid>https://arxiv.org/abs/2505.04109</guid>
<content:encoded><![CDATA[
<div> Keywords: 6D object pose estimation, single RGB-D image, pose encoding-decoding, reference object pose embedding, U-Net-based pose decoding<br />
Summary: <br />
- The paper introduces a novel method called One2Any for 6D object pose estimation using a single reference-single query RGB-D image, without the need for complete 3D models or multi-view images.
- The approach involves encoding the object shape, orientation, and texture into a Reference Object Pose Embedding (ROPE) from a single reference view, which is then used for pose decoding and generating Reference Object Coordinates (ROC) for new views.
- By treating object pose estimation as an encoding-decoding process and training the model on pair-wise pose data, the model exhibits excellent generalization to novel objects, outperforming methods that require multi-view or CAD inputs.
- Experimental results on benchmark datasets demonstrate that the proposed method achieves state-of-the-art accuracy and robustness, while being computationally efficient.
- The simplicity and scalability of the encoding-decoding framework allow for large-scale training and promising performance without the need for elaborate 3D models or category constraints. <br /> 
Summary: <div>
arXiv:2505.04109v1 Announce Type: new 
Abstract: 6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</title>
<link>https://arxiv.org/abs/2505.04119</link>
<guid>https://arxiv.org/abs/2505.04119</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained models, 3D vision, point cloud, parameter-efficient fine-tuning, geometric cues

Summary: 
Geometry-Aware Point Cloud Prompt (GAPrompt) is proposed to enhance the adaptability of 3D vision models by leveraging geometric cues. The approach includes a Point Prompt for fine-grained geometric details, a Point Shift Prompter for global shape information extraction, and Prompt Propagation for integrating shape information into the feature extraction process. GAPrompt outperforms existing parameter-efficient fine-tuning methods and achieves competitive results compared to full fine-tuning on various benchmarks. The approach utilizes only 2.19% of trainable parameters and is available on GitHub for further research.  <br /><br />Summary: <div>
arXiv:2505.04119v1 Announce Type: new 
Abstract: Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Graph Prompting via Semantic Low-Rank Decomposition</title>
<link>https://arxiv.org/abs/2505.04121</link>
<guid>https://arxiv.org/abs/2505.04121</guid>
<content:encoded><![CDATA[
<div> Vision Graph Prompting, low-rank properties, semantic features, global structural patterns, fine-grained semantic dependencies <br />
Summary: <br />
The paper introduces Vision Graph Prompting (VGP), a framework designed for vision graph structures, leveraging the low-rank properties of semantically connected components. By decomposing low-rank semantic features and integrating them with prompts on vision graph topologies, VGP captures both global structural patterns and fine-grained semantic dependencies. The proposed method significantly improves transfer performance on various downstream tasks for Vision GNN (ViG) models, achieving results comparable to full fine-tuning while maintaining parameter efficiency. This innovative approach fills the gap in existing prompting methods tailored for Transformer-based models but limited for graph-based representations, showcasing the potential of semantic low-rank prompting in enhancing the adaptability and performance of vision graph models. <div>
arXiv:2505.04121v1 Announce Type: new 
Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as graph structures, providing a more natural way to capture irregular semantic patterns beyond traditional grid or sequence-based representations. To efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning techniques like visual prompting become increasingly essential. However, existing prompting methods are primarily designed for Transformer-based models, neglecting the rich topological relationships among nodes and edges in graph-based representations, limiting their capacity to model complex semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG's transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R^3-VQA: "Read the Room" by Video Social Reasoning</title>
<link>https://arxiv.org/abs/2505.04147</link>
<guid>https://arxiv.org/abs/2505.04147</guid>
<content:encoded><![CDATA[
<div> Keywords: social reasoning, video dataset, mental states, social events, vision-language models

Summary: 
The paper introduces a new video dataset called R^3-VQA that focuses on social reasoning capabilities in complex social scenarios. This dataset includes annotations of social events, mental states (belief, intent, desire, emotion), and social causal chains. The dataset also includes human-annotated and model-generated QAs. The task R^3-VQA involves aspects such as Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. The study evaluates the performance of large vision-language models (LVLMs) on social reasoning tasks and finds that they are still far from achieving human-level consistent social reasoning in complex scenarios. The results show that prompting LVLMs with Theory of Mind (ToM) can improve their performance on social reasoning tasks. Supplementary materials containing some dataset and codes are provided, with plans to release the full dataset and codes upon acceptance.

<br /><br />Summary: <div>
arXiv:2505.04147v1 Announce Type: new 
Abstract: "Read the room" is a significant social reasoning capability in human daily life. Humans can infer others' mental states from subtle social cues. Previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. In this paper, we contribute a valuable, high-quality, and comprehensive video dataset named R^3-VQA with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. Moreover, we include human-annotated and model-generated QAs. Our task R^3-VQA includes three aspects: Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. As a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs are still far from human-level consistent social reasoning in complex social scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on social reasoning tasks. We provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages</title>
<link>https://arxiv.org/abs/2505.04150</link>
<guid>https://arxiv.org/abs/2505.04150</guid>
<content:encoded><![CDATA[
<div> Keywords: muscle tissue regeneration, machine learning, Learning from Label Proportions, Ordinal Scale Learning, skeletal muscle recovery stages 

Summary: 
The article presents a new method, Ordinal Scale Learning from Similarity Proportion (OSLSP), to automate the evaluation of muscle tissue regeneration. The current approach relies on visual inspection, but OSLSP utilizes machine learning to provide a quantitative and objective analysis. Unlike existing methods, OSLSP can adapt the feature extractor for muscle tissues and considers the ordinal information of recovery stages and cell morphological changes. By using a similarity proportion loss derived from two bag combinations, OSLSP can update the feature extractor based on class proportion attention. In classification tasks of skeletal muscle recovery stages, the model with OSLSP outperforms pre-trained and fine-tuning models. The proposed method addresses limitations in current weakly supervised learning methods and offers a more effective approach to analyzing muscle tissue regeneration. 

<br /><br />Summary: <div>
arXiv:2505.04150v1 Announce Type: new 
Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.04175</link>
<guid>https://arxiv.org/abs/2505.04175</guid>
<content:encoded><![CDATA[
<div> Keywords: Text recognition, OCR, ResNet, Vision Transformer, Deformable Convolutions

Summary: 
This paper introduces a novel end-to-end framework for text recognition in natural images. The framework combines ResNet and Vision Transformer backbones with advanced methodologies such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). Deformable Convolutions are used to enhance feature representation, adaptive dropout for regularization, and CRF for sequence modeling. The proposed method achieves high accuracies on benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80, with an average accuracy of 77.77%. The results establish a new state-of-the-art for text recognition, showcasing the robustness of the approach across diverse and challenging datasets. This study demonstrates the effectiveness of combining different techniques to improve OCR performance in natural images. 

<br /><br />Summary: <div>
arXiv:2505.04175v1 Announce Type: new 
Abstract: Text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. This paper introduces a novel end-to-end framework that combines ResNet and Vision Transformer backbones with advanced methodologies, including Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations collectively enhance feature representation and improve Optical Character Recognition (OCR) performance. Specifically, the framework substitutes standard convolution layers in the third and fourth blocks with Deformable Convolutions, leverages adaptive dropout for regularization, and incorporates CRF for more refined sequence modeling. Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy of 77.77%. These results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3D: Sketch-Driven 3D Model Generation</title>
<link>https://arxiv.org/abs/2505.04185</link>
<guid>https://arxiv.org/abs/2505.04185</guid>
<content:encoded><![CDATA[
<div> U-Net-based architecture, 2D sketches, 3D models, face segmentation masks, style alignment loss <br />
Summary: <br />
Generating high-quality 3D models from simple hand-drawn 2D sketches presents challenges due to data ambiguity and sparsity. The S3D framework addresses this by converting sketches into face segmentation masks using a U-Net-based encoder-decoder architecture. These masks are then utilized to create detailed 3D models that can be viewed from different angles. A novel style-alignment loss enhances reconstruction fidelity by aligning bottleneck features with initial 3D generation module outputs. The network's robustness is further improved with augmentation techniques applied to the sketch dataset. The S3D framework offers a streamlined approach that effectively generates high-quality 3D models from sketch inputs. Source code for the project is available on GitHub for public access at https://github.com/hailsong/S3D. <div>
arXiv:2505.04185v1 Announce Type: new 
Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models. Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views. To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity. To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs. The source code for this project is publicly available at https://github.com/hailsong/S3D.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.04192</link>
<guid>https://arxiv.org/abs/2505.04192</guid>
<content:encoded><![CDATA[
<div> Keywords: VideoPath-LLaVA, multimodal model, computational pathology, diagnostic reasoning, histopathology

Summary:
VideoPath-LLaVA is the first large multimodal model in computational pathology that integrates single patch images, keyframe-extracted clips, and manually segmented video pathology images to mimic the diagnostic process of pathologists. It is trained on the VideoPath-Instruct dataset, which consists of video and diagnosis-specific instructional pairs from educational histopathology videos. By transferring knowledge from existing single-image instruction datasets and fine-tuning on manually segmented videos, VideoPath-LLaVA sets a new benchmark in pathology video analysis. The model generates detailed histological descriptions and definitive sign-out diagnoses, bridging visual narratives with diagnostic reasoning. It offers a foundation for future AI systems supporting clinical decision-making. The code, data, and model are publicly available on GitHub at https://github.com/trinhvg/VideoPath-LLaVA. 

Summary: <br /><br /> <div>
arXiv:2505.04192v1 Announce Type: new 
Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube. Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning. Our code, data, and model are publicly available at https://github.com/trinhvg/VideoPath-LLaVA.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios</title>
<link>https://arxiv.org/abs/2505.04201</link>
<guid>https://arxiv.org/abs/2505.04201</guid>
<content:encoded><![CDATA[
<div> Keywords: tactile sensing, multimodal reasoning, commonsense reasoning, Mixture of Experts, physical properties<br />
Summary:<br />
This paper discusses the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly for enabling commonsense reasoning about the physical world. Two key challenges are identified: modality discrepancy and open-ended tactile data scarcity. To address these challenges, the authors introduce SToLa, a Self-Adaptive Touch-Language framework that uses Mixture of Experts to manage tactile and language modalities. They also present a new tactile commonsense reasoning dataset and benchmark with diverse knowledge and questions. Experiments show that SToLa performs competitively on the PhysiCLeAR benchmark and self-created datasets, demonstrating the effectiveness of the Mixture of Experts architecture for multimodal management and the advantages it offers for tactile commonsense reasoning tasks.<br /><br />Summary: <div>
arXiv:2505.04201v1 Announce Type: new 
Abstract: This paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world. We identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning. To overcome these challenges, we introduce SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of Experts (MoE) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics. Crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge. Experiments show SToLa exhibits competitive performance compared to existing models on the PhysiCLeAR benchmark and self-constructed datasets, proving the effectiveness of the Mixture of Experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement</title>
<link>https://arxiv.org/abs/2505.04207</link>
<guid>https://arxiv.org/abs/2505.04207</guid>
<content:encoded><![CDATA[
<div> Dataset, RGB-D images, Pothole detection, Physical features analysis, YOLOv8 <br />
Summary:
A new dataset, PothRGBD, consisting of RGB-D images was created for accurate pothole detection. An improved YOLOv8-based model was developed for analyzing physical characteristics of potholes. The model, YOLOv8n-seg, integrated Dynamic Snake Convolution, Simple Attention Module, and Gaussian Error Linear Unit for enhanced segmentation of potholes with irregular edges. It achieved 91.9% precision, 85.2% recall, and 91.9% mAP@50, with the proposed model improving to 93.7% precision, 90.4% recall, and 93.8% mAP. The model accurately measured perimeter and depth on depth maps, showing a 1.96% increase in precision, 6.13% in recall, and 2.07% in mAP. This lightweight and effective model is suitable for real-time applications in intelligent transportation solutions. <br /><br />Summary: <div>
arXiv:2505.04207v1 Announce Type: new 
Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models</title>
<link>https://arxiv.org/abs/2505.04214</link>
<guid>https://arxiv.org/abs/2505.04214</guid>
<content:encoded><![CDATA[
<div> Keyphrases: Key-value information extraction, Handwritten documents, Large Vision Language Models, Few-shot learning, CM1 dataset

Summary:
The article discusses the challenge of automatically extracting key-value information from handwritten documents and the potential of Large Vision Language Models (LVLM) in addressing this issue. A new dataset, CM1, is introduced to evaluate the few-shot capabilities of LVLMs using historic forms from post-World War Two Europe. Three benchmarks are established for extracting name and birthdate information, considering different training set sizes. Baseline results for two LVLMs show that they outperform traditional full-page extraction models when only a few training samples are available. Despite competitive performance by the full-page model, the LVLMs benefit from their size and heavy pretraining in low-data scenarios. This suggests the potential of LVLMs in efficiently extracting key-value information from handwritten documents with limited training data.

<br /><br />Summary: <div>
arXiv:2505.04214v1 Announce Type: new 
Abstract: The automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. A reliable extraction is a prerequisite for the mass digitization efforts of many archives. Large Vision Language Models (LVLM) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. In this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of LVLMs. The CM1 documents are a historic collection of forms with handwritten entries created in Europe to administer the Care and Maintenance program after World War Two. The dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. We provide baseline results for two different LVLMs and compare performances to an established full-page extraction model. While the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered LVLMs benefit from their size and heavy pretraining and outperform the classical approach.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation</title>
<link>https://arxiv.org/abs/2505.04229</link>
<guid>https://arxiv.org/abs/2505.04229</guid>
<content:encoded><![CDATA[
<div> Keywords: weak supervision, parking lot occupancy, satellite imagery, urban mobility analysis, vulnerable communities 

Summary: 
The study introduces a novel weak supervision framework for estimating parking lot occupancy using 3m resolution satellite imagery. By utilizing coarse temporal labels based on the assumption of parking lot usage patterns in Germany, the model achieves high accuracy in predicting parking lot occupancy levels. This approach reduces the dependency on costly high-resolution images, making it more accessible for applications in low-income regions. The framework shows potential for scalable urban mobility analysis and could be adapted to assess transit patterns and resource allocation in underserved communities. By providing a data-driven basis for decision-making, this method offers opportunities to improve the well-being of vulnerable populations through informed urban planning and resource distribution strategies. <br /><br />Summary: <div>
arXiv:2505.04229v1 Announce Type: new 
Abstract: The scarcity and high cost of labeled high-resolution imagery have long challenged remote sensing applications, particularly in low-income regions where high-resolution data are scarce. In this study, we propose a weak supervision framework that estimates parking lot occupancy using 3m resolution satellite imagery. By leveraging coarse temporal labels -- based on the assumption that parking lots of major supermarkets and hardware stores in Germany are typically full on Saturdays and empty on Sundays -- we train a pairwise comparison model that achieves an AUC of 0.92 on large parking lots. The proposed approach minimizes the reliance on expensive high-resolution images and holds promise for scalable urban mobility analysis. Moreover, the method can be adapted to assess transit patterns and resource allocation in vulnerable communities, providing a data-driven basis to improve the well-being of those most in need.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.04262</link>
<guid>https://arxiv.org/abs/2505.04262</guid>
<content:encoded><![CDATA[
<div> optimization, Gaussian Splatting, 3D generation, multi-view correlation, geometric consistency
<br />
Summary:<br />
The paper introduces Coupled Score Distillation (CSD), a framework that enhances text-to-3D generation by incorporating multi-view joint distribution priors to ensure geometrically consistent results. By optimizing as a multi-view joint optimization problem, CSD effectively couples multi-view priors to guide optimization across different viewpoints, leading to diverse 3D content generation. The framework directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization for geometrically consistent results. Additionally, a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, produces high-quality meshes. Experimental results demonstrate the efficiency and competitive quality of the proposed approach, addressing issues such as geometric inconsistencies and multi-face artifacts commonly encountered in 3D content generation. <div>
arXiv:2505.04262v1 Announce Type: new 
Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object-Shot Enhanced Grounding Network for Egocentric Video</title>
<link>https://arxiv.org/abs/2505.04270</link>
<guid>https://arxiv.org/abs/2505.04270</guid>
<content:encoded><![CDATA[
<div> Object-Shot, Grounding Network, egocentric video, embodied intelligence, OSGNet <br />
<br />
Summary: OSGNet is introduced as a novel approach for egocentric video grounding, which differs from traditional exocentric video moment localization. The model leverages object information extracted from videos to enhance video representations, especially for objects mentioned in textual queries but not directly captured in video features. Additionally, OSGNet analyzes shot movements typical of egocentric videos to capture the wearer's attention information and improve modality alignment. Experimental results on three datasets demonstrate that OSGNet achieves state-of-the-art performance, showcasing its effectiveness in addressing the unique characteristics and challenges of egocentric video grounding tasks. The code for OSGNet is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2505.04270v1 Announce Type: new 
Abstract: Egocentric video grounding is a crucial task for embodied intelligence applications, distinct from exocentric video moment localization. Existing methods primarily focus on the distributional differences between egocentric and exocentric videos but often neglect key characteristics of egocentric videos and the fine-grained information emphasized by question-type queries. To address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding Network for egocentric video. Specifically, we extract object information from videos to enrich video representation, particularly for objects highlighted in the textual query but not directly captured in the video features. Additionally, we analyze the frequent shot movements inherent to egocentric videos, leveraging these features to extract the wearer's attention information, which enhances the model's ability to perform modality alignment. Experiments conducted on three datasets demonstrate that OSGNet achieves state-of-the-art performance, validating the effectiveness of our approach. Our code can be found at https://github.com/Yisen-Feng/OSGNet.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2505.04276</link>
<guid>https://arxiv.org/abs/2505.04276</guid>
<content:encoded><![CDATA[
<div> Transformer, Graph Convolutional Network, diffusion model, 3D Human Pose Estimation, state-of-the-art performance <br />
<br />
Summary: <br />
The article introduces HDiffTG, a novel 3D Human Pose Estimation method that combines Transformer, Graph Convolutional Network (GCN), and diffusion model in a unified framework. The Transformer captures global spatiotemporal dependencies, GCN models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, striking a balance between global and local features. The integrated model demonstrates improved accuracy and robustness in pose estimation, particularly in occluded and complex scenarios. Lightweight optimizations and refined objective function design reduce computational overhead without compromising performance. Evaluation on Human3.6M and MPI-INF-3DHP datasets shows that HDiffTG achieves state-of-the-art performance on the latter while excelling in accuracy and computational efficiency, showcasing exceptional robustness in noisy and occluded environments. Source codes and models are available for further exploration. <div>
arXiv:2505.04276v1 Announce Type: new 
Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that integrates Transformer, Graph Convolutional Network (GCN), and diffusion model into a unified framework. HDiffTG leverages the strengths of these techniques to significantly improve pose estimation accuracy and robustness while maintaining a lightweight design. The Transformer captures global spatiotemporal dependencies, the GCN models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, achieving a complementary balance between global and local features. This integration enhances the model's ability to handle pose estimation under occlusions and in complex scenarios. Furthermore, we introduce lightweight optimizations to the integrated model and refine the objective function design to reduce computational overhead without compromising performance. Evaluation results on the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling in both accuracy and computational efficiency. Additionally, the model exhibits exceptional robustness in noisy and occluded environments. Source codes and models are available at https://github.com/CirceJie/HDiffTG
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement</title>
<link>https://arxiv.org/abs/2505.04281</link>
<guid>https://arxiv.org/abs/2505.04281</guid>
<content:encoded><![CDATA[
<div> Keywords: Two-Stage Diffusion Model, Camera Feature Integration, Color Corrector, QID dataset, Low-light imaging

Summary: 
The paper introduces the Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. TS-Diff consists of a pre-training stage where noisy images are synthesized and Camera Feature Integration (CFI) modules learn generalizable features across different virtual cameras. In the aligning stage, a target-specific CFI$^T$ is fine-tuned using a small amount of real RAW data to adapt to specific camera noise characteristics. A color corrector is used to maintain color consistency during the diffusion process by adjusting global color distributions dynamically. The researchers also construct a new dataset, QID, for training and evaluation purposes, featuring quantifiable illumination levels and a wide dynamic range. Experimental results demonstrate the superior performance of TS-Diff on various datasets, excelling in denoising, generalization, and color consistency across different cameras and lighting conditions. The robustness and versatility of TS-Diff make it a practical solution for low-light imaging applications. <br /><br />Summary: <div>
arXiv:2505.04281v1 Announce Type: new 
Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at https://github.com/CircccleK/TS-Diff
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition</title>
<link>https://arxiv.org/abs/2505.04306</link>
<guid>https://arxiv.org/abs/2505.04306</guid>
<content:encoded><![CDATA[
<div> diffusion-based, generative expert, face recognition, occluded faces, identity-gated network

Summary:
The paper introduces a novel approach called identity-gated mixture of diffusion experts (MoDE) for occluded face recognition (OFR). Current OFR algorithms struggle with varying types and severity of occlusions, impacting daily life convenience. MoDE consists of diffusion-based generative experts that estimate complete images for occluded faces. An identity-gating network evaluates and integrates information from multiple reconstructed faces based on their contribution to identity. MoDE can be easily integrated into existing face recognition models. Experimental results on multiple datasets demonstrate the superior performance of MoDE in handling various occlusions compared to existing methods. <div>
arXiv:2505.04306v1 Announce Type: new 
Abstract: With the continuous impact of epidemics, people have become accustomed to wearing masks. However, most current occluded face recognition (OFR) algorithms lack prior knowledge of occlusions, resulting in poor performance when dealing with occluded faces of varying types and severity in reality. Recognizing occluded faces is still a significant challenge, which greatly affects the convenience of people's daily lives. In this paper, we propose an identity-gated mixture of diffusion experts (MoDE) for OFR. Each diffusion-based generative expert estimates one possible complete image for occluded faces. Considering the random sampling process of the diffusion model, which introduces inevitable differences and variations between the inpainted faces and the real ones. To ensemble effective information from multi-reconstructed faces, we introduce an identity-gating network to evaluate the contribution of each reconstructed face to the identity and adaptively integrate the predictions in the decision space. Moreover, our MoDE is a plug-and-play module for most existing face recognition models. Extensive experiments on three public face datasets and two datasets in the wild validate our advanced performance for various occlusions in comparison with the competing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-turn Consistent Image Editing</title>
<link>https://arxiv.org/abs/2505.04320</link>
<guid>https://arxiv.org/abs/2505.04320</guid>
<content:encoded><![CDATA[
<div> Keywords: image editing, iterative refinement, flow matching, Linear Quadratic Regulators (LQR), adaptive attention highlighting <br />
Summary: <br />
The article introduces a multi-turn image editing framework for achieving flexible and iterative modifications in real-world applications. Existing editing methods often struggle with user intent ambiguity and producing consistent outcomes. The proposed framework utilizes flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, reducing error accumulation. An adaptive attention highlighting method is introduced to enhance editability and maintain multi-turn coherence by analyzing the role of transformers at different layers. Extensive experiments demonstrate that the framework significantly improves edit success rates and visual fidelity compared to current methods. <div>
arXiv:2505.04320v1 Announce Type: new 
Abstract: Many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. However, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. As a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. To address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. Our approach leverages flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, effectively mitigating error accumulation. Additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. Extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion</title>
<link>https://arxiv.org/abs/2505.04347</link>
<guid>https://arxiv.org/abs/2505.04347</guid>
<content:encoded><![CDATA[
<div> CountDiffusion, text-to-image synthesis, object quantity, diffusion model, counting model
<br />
Summary:
CountDiffusion is a framework designed to address the challenge of generating images with accurate object quantity from textual descriptions. It comprises two stages: the first stage involves generating an intermediate denoising result using a diffusion model and a counting model to determine object quantity, while the second stage employs a correction module to adjust the object quantity based on universal guidance. CountDiffusion can be seamlessly integrated into existing diffusion-based text-to-image generation models without the need for additional training. Experimental results demonstrate that CountDiffusion significantly enhances the ability of text-to-image models to generate images with precise object quantities. <div>
arXiv:2505.04347v1 Announce Type: new 
Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. In this paper, we propose CountDiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. CountDiffusion consists of two stages. In the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. In the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. The proposed CountDiffusion can be plugged into any diffusion-based text-to-image (T2I) generation models without further training. Experiment results demonstrate the superiority of our proposed CountDiffusion, which improves the accurate object quantity generation ability of T2I models by a large margin.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing</title>
<link>https://arxiv.org/abs/2505.04369</link>
<guid>https://arxiv.org/abs/2505.04369</guid>
<content:encoded><![CDATA[
<div> wavelet transform analysis, low-frequency components, dehazing framework, Mamba blocks, self-guided contrastive regularization 

Summary: 
This paper introduces a novel dehazing framework called WDMamba, which leverages wavelet transform analysis to identify that haze-related information is mainly found in low-frequency components. The framework consists of two stages: low-frequency restoration using Mamba blocks to remove overall haze and detail enhancement to restore fine-grained information. The model incorporates self-guided contrastive regularization during training to improve detail retention and achieve more natural dehazing results. By using the coarse restored output as a hard negative example, the model learns discriminative representations leading to enhanced dehazing performance. Extensive evaluations on public dehazing benchmarks demonstrate the superiority of WDMamba over existing state-of-the-art approaches, both qualitatively and quantitatively. The code for the framework is also made publicly available for further research and implementation. <br /><br /> <div>
arXiv:2505.04369v1 Announce Type: new 
Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior observed through wavelet transform analysis, which shows that haze-related information predominantly resides in low-frequency components. Exploiting this insight, we propose a novel dehazing framework, WDMamba, which decomposes the image dehazing task into two sequential stages: low-frequency restoration followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to effectively capture features specific to each stage of the dehazing process, resulting in high-quality restored images. Specifically, in the low-frequency restoration stage, we integrate Mamba blocks to reconstruct global structures with linear complexity, efficiently removing overall haze and producing a coarse restored image. Thereafter, the detail enhancement stage reinstates fine-grained information that may have been overlooked during the previous phase, culminating in the final dehazed output. Furthermore, to enhance detail retention and achieve more natural dehazing, we introduce a self-guided contrastive regularization during network training. By utilizing the coarse restored output as a hard negative example, our model learns more discriminative representations, substantially boosting the overall dehazing performance. Extensive evaluations on public dehazing benchmarks demonstrate that our method surpasses state-of-the-art approaches both qualitatively and quantitatively. Code is available at https://github.com/SunJ000/WDMamba.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise</title>
<link>https://arxiv.org/abs/2505.04375</link>
<guid>https://arxiv.org/abs/2505.04375</guid>
<content:encoded><![CDATA[
<div> Exploration, Vision Transformers, Model Size Impact, Label Noise, Active Learning<br />
Summary: <br />
This study investigates the impact of model size on the performance of vision transformers in active learning scenarios with label noise. Larger ViT models, particularly ViTl32, outperform smaller models in accuracy and calibration, even under moderate to high label noise levels. Swin Transformers show weaker robustness across all noise levels. Smaller patch sizes do not always lead to better performance, as ViTl16 performs worse than ViTl32 with higher computational cost. Information-based Active Learning strategies improve accuracy at moderate label noise rates but result in poorer calibration compared to models trained on randomly acquired labels, especially at high noise rates. These findings offer practical guidance for deploying vision transformers in resource-constrained environments, emphasizing the balance between model complexity, label noise, and compute efficiency in fine-tuning or distillation processes. <br /> <div>
arXiv:2505.04375v1 Announce Type: new 
Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for downstream tasks is well-established. Still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. Given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. We explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label noise rates. Our findings show that larger ViT models (ViTl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while Swin Transformers exhibit weaker robustness across all noise levels. We find that smaller patch sizes do not always lead to better performance, as ViTl16 performs consistently worse than ViTl32 while incurring a higher computational cost. We also find that information-based Active Learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. We hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-efficient Single Photon Images Classification via Active Learning</title>
<link>https://arxiv.org/abs/2505.04376</link>
<guid>https://arxiv.org/abs/2505.04376</guid>
<content:encoded><![CDATA[
<div> LiDAR, 3D imaging, single-photon, active learning, image classification <br />
<br />
Summary: 
This paper introduces an active learning framework for single-photon image classification in LiDAR technology. It addresses the challenge of semantic interpretation in single-photon images by proposing a sampling strategy that combines synthetic augmentation to capture variability in imaging conditions. The method selectively annotates informative examples by prioritizing samples where the model is uncertain and sensitive to imaging conditions. Experimental results demonstrate superior performance compared to baseline methods, achieving high classification accuracy with minimal labeled samples. The proposed approach achieves 97% accuracy on synthetic data with only 1.5% labeled samples and maintains 90.63% accuracy on real-world data with just 8% labeled samples, outperforming existing methods. This study highlights the potential of active learning in enhancing classification performance in single-photon images, paving the way for broader integration of such data in real-world applications. <br /> <div>
arXiv:2505.04376v1 Announce Type: new 
Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme environments through quantum-level photon detection technology. Current research primarily focuses on reconstructing 3D scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. This paper presents the first active learning framework for single-photon image classification. The core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. By identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. Experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. Specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples. On real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline. This illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tetrahedron-Net for Medical Image Registration</title>
<link>https://arxiv.org/abs/2505.04380</link>
<guid>https://arxiv.org/abs/2505.04380</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, convolutional backbone, Tetrahedron-Net, encoder-decoder architecture, feature extraction.

Summary:
- Medical image registration is essential in image processing, and improving representation quality is key.
- Traditional U-Net networks use skip connections to enhance representation capacity.
- A new method called Tetrahedron-Net adds an extra decoder to interact with the encoder and original decoder.
- This simple yet effective approach improves registration results by reusing features and enhancing interactions.
- Tetrahedron-Net outperforms existing methods on medical image registration benchmarks.
- The Tetrahedron design can be incorporated into popular architectures like VoxelMorph, ViT-V-Net, and TransMorph for consistent performance gains.

<br /><br />Summary: 
Medical image registration's quality is crucial, with U-Net networks using skip connections to improve representation. The Tetrahedron-Net method enhances this by adding an extra decoder for improved interactions and reuse of features, leading to better registration results. It surpasses existing methods on benchmarks and can be integrated into various architectures for consistent performance boosts. <div>
arXiv:2505.04380v1 Announce Type: new 
Abstract: Medical image registration plays a vital role in medical image processing. Extracting expressive representations for medical images is crucial for improving the registration quality. One common practice for this end is constructing a convolutional backbone to enable interactions with skip connections among feature extraction layers. The de facto structure, U-Net-like networks, has attempted to design skip connections such as nested or full-scale ones to connect one single encoder and one single decoder to improve its representation capacity. Despite being effective, it still does not fully explore interactions with a single encoder and decoder architectures. In this paper, we embrace this observation and introduce a simple yet effective alternative strategy to enhance the representations for registrations by appending one additional decoder. The new decoder is designed to interact with both the original encoder and decoder. In this way, it not only reuses feature presentation from corresponding layers in the encoder but also interacts with the original decoder to corporately give more accurate registration results. The new architecture is concise yet generalized, with only one encoder and two decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net. Three instantiations of Tetrahedron-Net are further constructed regarding the different structures of the appended decoder. Our extensive experiments prove that superior performance can be obtained on several representative benchmarks of medical image registration. Finally, such a ``Tetrahedron'' design can also be easily integrated into popular U-Net-like architectures including VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution</title>
<link>https://arxiv.org/abs/2505.04384</link>
<guid>https://arxiv.org/abs/2505.04384</guid>
<content:encoded><![CDATA[
<div> deepfake attribution, multi-DisentAnglement, contrastive learning, novel classes, OSS-DFA task

Summary:
The paper introduces an innovative framework called DATA for deepfake attribution, focusing on open-world semi-supervised scenarios. It introduces the concept of 'Orthonormal Deepfake Basis' to disentangle method-specific features and reduce overfitting. An augmented-memory mechanism aids in novel class discovery and contrastive learning, improving class boundaries. Bases contrastive loss and center contrastive loss enhance feature standardization and discrimination. Experimental results show that DATA outperforms existing methods, achieving notable accuracy improvements in the OSS-DFA benchmark. <div>
arXiv:2505.04384v1 Announce Type: new 
Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of 'Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle</title>
<link>https://arxiv.org/abs/2505.04392</link>
<guid>https://arxiv.org/abs/2505.04392</guid>
<content:encoded><![CDATA[
<div> novel approach, road surface anomalies, visual tracking, preceding vehicle, predictive detection<br />
<br />
Summary: 
A novel approach for detecting road surface anomalies using visual tracking of a preceding vehicle is proposed. The method is versatile, able to predict different types of anomalies without the need for specific visual detectors. It can operate in low visibility or dense traffic conditions, where anomalies are often occluded. By detecting anomalies predictively, the method allows for pre-configuration of vehicle systems or planning avoidance maneuvers in autonomous driving scenarios. The challenge of weak and disturbed camera signals is addressed through an efficient method to compensate for camera pitch rotation. Experimental results demonstrate reliable detection of road anomalies in various conditions, including imperfect road surfaces, with real-time performance on standard consumer hardware. <div>
arXiv:2505.04392v1 Announce Type: new 
Abstract: A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer</title>
<link>https://arxiv.org/abs/2505.04394</link>
<guid>https://arxiv.org/abs/2505.04394</guid>
<content:encoded><![CDATA[
<div> Transformer, lip reading, SwinLip, visual speech encoder, computational load<br />
Summary:<br />
- The paper introduces SwinLip, a lightweight scale of the Swin Transformer for efficient visual speech encoding in lip reading.
- Traditional ResNet-based models have high computational complexity, while SwinLip integrates Conformer temporal embeddings with spatial embeddings to reduce computational load.
- The hierarchical structure and window self-attention of Swin Transformer enhance the performance and inference speed of the lip reading network.
- SwinLip outperforms existing models in English LRW and Mandarin LRW-1000 datasets for word and sentence recognition.
- The SwinLip model achieves state-of-the-art performance on the Mandarin LRW-1000 dataset with reduced computation, making it a promising solution for efficient lip reading tasks. <br /> <div>
arXiv:2505.04394v1 Announce Type: new 
Abstract: This paper presents an efficient visual speech encoder for lip reading. While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading. We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep residual learning with product units</title>
<link>https://arxiv.org/abs/2505.04397</link>
<guid>https://arxiv.org/abs/2505.04397</guid>
<content:encoded><![CDATA[
<div> product-unit, residual neural network, convolutional networks, parameter efficiency, multiplicative feature interactions

Summary:
The proposed deep product-unit residual neural network (PURe) integrates product units into residual blocks to enhance the expressiveness and parameter efficiency of deep convolutional networks. By incorporating product units, which allow for multiplicative feature interactions, PURe offers a more powerful representation of complex patterns compared to traditional networks. The use of 2D product units in the second layer of each residual block eliminates the need for nonlinear activation functions, preserving structural information. Experimental results on Galaxy10 DECaLS, ImageNet, and CIFAR-10 datasets demonstrate that PURe achieves high test accuracy, surpassing deeper ResNet models while using fewer parameters and computational resources. PURe exhibits faster convergence, robustness to noise, and competitive classification performance, highlighting its potential for scalable and reliable deep learning in computer vision. 

<br /><br />Summary: <div>
arXiv:2505.04397v1 Announce Type: new 
Abstract: We propose a deep product-unit residual neural network (PURe) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. Unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. PURe replaces conventional convolutional layers with 2D product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS, PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper ResNet152, while converging nearly five times faster and demonstrating strong robustness to Poisson noise. On ImageNet, PURe architectures outperform standard ResNet models at similar depths, with PURe34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet variants (ResNet50, ResNet101) while utilizing significantly fewer parameters and computational resources. On CIFAR-10, PURe consistently outperforms ResNet variants across varying depths, with PURe272 reaching 95.01% test accuracy, comparable to ResNet1001 but at less than half the model size. These results demonstrate that PURe achieves a favorable balance between accuracy, efficiency, and robustness. Compared to traditional residual networks, PURe not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. Its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFSeg: Efficient Multi-frame 3D Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.04408</link>
<guid>https://arxiv.org/abs/2505.04408</guid>
<content:encoded><![CDATA[
<div> Efficient multi-frame 3D semantic segmentation framework, Point cloud sequences, Feature aggregation, Regularized feature extraction, Lightweight MLP-based point decoder, nuScenes dataset, Waymo dataset <br />
Summary: <br />
MFSeg is introduced as an efficient multi-frame 3D semantic segmentation framework that aggregates point cloud sequences at the feature level and regulates the feature extraction and aggregation process. This reduces computational overhead while maintaining high accuracy. The method also utilizes a lightweight MLP-based point decoder, eliminating the need to upsample redundant points from past frames. Experiments conducted on the nuScenes and Waymo datasets demonstrate that MFSeg surpasses existing methods, showcasing its effectiveness and efficiency in the field of semantic segmentation for 3D point clouds. <div>
arXiv:2505.04408v1 Announce Type: new 
Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation framework. By aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, MFSeg reduces computational overhead while maintaining high accuracy. Moreover, by employing a lightweight MLP-based point decoder, our method eliminates the need to upsample redundant points from past frames. Experiments on the nuScenes and Waymo datasets show that MFSeg outperforms existing methods, demonstrating its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</title>
<link>https://arxiv.org/abs/2505.04410</link>
<guid>https://arxiv.org/abs/2505.04410</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense visual prediction, Vision-Language Models, CLIP, DeCLIP, Object detection, Semantic segmentation

Summary:
DeCLIP addresses limitations in dense visual prediction tasks by enhancing CLIP through a new framework. The observation that CLIP struggles to aggregate information effectively from related regions is addressed by decoupling the self-attention module into "content" and "context" features. The "content" features align with image crop representations to improve local discriminability, while the "context" features learn to retain spatial correlations guided by vision foundation models like DINO. Extensive experiments show that DeCLIP outperforms existing methods in open-vocabulary dense prediction tasks, including object detection and semantic segmentation. This approach improves local feature representation, leading to enhanced performance in tasks where visual concepts are unbounded. The code for DeCLIP is available on GitHub for further exploration and implementation.  

<br /><br />Summary: <div>
arXiv:2505.04410v1 Announce Type: new 
Abstract: Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation</title>
<link>https://arxiv.org/abs/2505.04424</link>
<guid>https://arxiv.org/abs/2505.04424</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, arbitrary style transfer, lightweight model, uncertainty-aware multi-task learning, high-quality results

Summary: 
RLMiniStyler is a novel reinforcement learning-based framework for arbitrary style transfer, aiming to generate diverse stylized results in a lightweight manner. It uses a unified reinforcement learning policy to guide the style transfer process, resulting in smooth sequences of stylized images. Additionally, an uncertainty-aware multi-task learning strategy adjusts loss weights dynamically to achieve a balance between content and style requirements, improving model convergence. Experimental results demonstrate the superiority of RLMiniStyler over existing methods in generating high-quality artistic image sequences at a lower computational cost. The code for RLMiniStyler is available on GitHub for further exploration. <div>
arXiv:2505.04424v1 Announce Type: new 
Abstract: Arbitrary style transfer aims to apply the style of any given artistic image to another content image. Still, existing deep learning-based methods often require significant computational costs to generate diverse stylized results. Motivated by this, we propose a novel reinforcement learning-based framework for arbitrary style transfer RLMiniStyler. This framework leverages a unified reinforcement learning policy to iteratively guide the style transfer process by exploring and exploiting stylization feedback, generating smooth sequences of stylized results while achieving model lightweight. Furthermore, we introduce an uncertainty-aware multi-task learning strategy that automatically adjusts loss weights to adapt to the content and style balance requirements at different training stages, thereby accelerating model convergence. Through a series of experiments across image various resolutions, we have validated the advantages of RLMiniStyler over other state-of-the-art methods in generating high-quality, diverse artistic image sequences at a lower cost. Codes are available at https://github.com/fengxiaoming520/RLMiniStyler.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Real Facial Concepts for Independent Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.04460</link>
<guid>https://arxiv.org/abs/2505.04460</guid>
<content:encoded><![CDATA[
<div> concept capture module, real faces, deepfake detection, forgery artifacts, generalization

Summary:
- The article introduces RealID, a novel approach to improve deepfake detection models' generalization ability.
- RealID consists of two key modules: the Real Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier (IDC).
- The RealC2 module uses a MultiReal Memory to maintain various prototypes for real faces, helping the model develop a comprehensive understanding of real faces.
- The IDC module redefines the classification strategy by making independent decisions based on real face concepts and forgery artifacts.
- Extensive experiments on five popular datasets show that RealID outperforms existing methods by achieving a 1.74% improvement in average accuracy.<br /><br />Summary: <div>
arXiv:2505.04460v1 Announce Type: new 
Abstract: Deepfake detection models often struggle with generalization to unseen datasets, manifesting as misclassifying real instances as fake in target domains. This is primarily due to an overreliance on forgery artifacts and a limited understanding of real faces. To address this challenge, we propose a novel approach RealID to enhance generalization by learning a comprehensive concept of real faces while assessing the probabilities of belonging to the real and fake classes independently. RealID comprises two key modules: the Real Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier (IDC). With the assistance of a MultiReal Memory, RealC2 maintains various prototypes for real faces, allowing the model to capture a comprehensive concept of real class. Meanwhile, IDC redefines the classification strategy by making independent decisions based on the concept of the real class and the presence of forgery artifacts. Through the combined effect of the above modules, the influence of forgery-irrelevant patterns is alleviated, and extensive experiments on five widely used datasets demonstrate that RealID significantly outperforms existing state-of-the-art methods, achieving a 1.74% improvement in average accuracy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation</title>
<link>https://arxiv.org/abs/2505.04481</link>
<guid>https://arxiv.org/abs/2505.04481</guid>
<content:encoded><![CDATA[
<div> parametric sequences, computer-aided design, Large Language Models, CAD-Llama, 3D shapes <br />
Summary: <br />
This study explores the use of Large Language Models (LLMs) for generating parametric sequences for computer-aided design (CAD) models. The research presents CAD-Llama, a framework that enhances LLMs for creating parametric 3D CAD models. A hierarchical annotation pipeline and code-like format are utilized to translate parametric CAD command sequences into Structured Parametric CAD Code (SPCC). An adaptive pretraining approach using SPCC and instruction tuning aligned with CAD guidelines is proposed to imbue LLMs with spatial knowledge. Experimental results show that CAD-Llama outperforms previous autoregressive methods and LLM baselines, showcasing its effectiveness in generating parametric 3D CAD models. <br /> <div>
arXiv:2505.04481v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
<link>https://arxiv.org/abs/2505.04485</link>
<guid>https://arxiv.org/abs/2505.04485</guid>
<content:encoded><![CDATA[
arXiv:2505.04485v1 Announce Type: new 
Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural network architecture built on top of the well-known KPConv, a widely adopted backbone for 3D point cloud analysis. Even though invariance and/or equivariance to Euclidean transformations are required for many common tasks, KPConv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations. Using Frame Averaging, we allow to flexibly customize point cloud neural networks built with KPConv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds. By simply wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information. We showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Flow Matching using Latent Variables</title>
<link>https://arxiv.org/abs/2505.04486</link>
<guid>https://arxiv.org/abs/2505.04486</guid>
<content:encoded><![CDATA[
arXiv:2505.04486v1 Announce Type: new 
Abstract: Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\sim 50\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments</title>
<link>https://arxiv.org/abs/2505.04488</link>
<guid>https://arxiv.org/abs/2505.04488</guid>
<content:encoded><![CDATA[
arXiv:2505.04488v1 Announce Type: new 
Abstract: The visually impaired population, especially the severely visually impaired, is currently large in scale, and daily activities pose significant challenges for them. Although many studies use large language and vision-language models to assist the blind, most focus on static content and fail to meet real-time perception needs in dynamic and complex environments, such as daily activities. To provide them with more effective intelligent assistance, it is imperative to incorporate advanced visual understanding technologies. Although real-time vision and speech interaction VideoLLMs demonstrate strong real-time visual understanding, no prior work has systematically evaluated their effectiveness in assisting visually impaired individuals. In this work, we conduct the first such evaluation. First, we construct a benchmark dataset (VisAssistDaily), covering three categories of assistive tasks for visually impaired individuals: Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that GPT-4o achieves the highest task success rate. Next, we conduct a user study to evaluate the models in both closed-world and open-world scenarios, further exploring the practical challenges of applying VideoLLMs in assistive contexts. One key issue we identify is the difficulty current models face in perceiving potential hazards in dynamic environments. To address this, we build an environment-awareness dataset named SafeVid and introduce a polling mechanism that enables the model to proactively detect environmental risks. We hope this work provides valuable insights and inspiration for future research in this field.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining and Quantifying Creative Behavior in Popular Image Generators</title>
<link>https://arxiv.org/abs/2505.04497</link>
<guid>https://arxiv.org/abs/2505.04497</guid>
<content:encoded><![CDATA[
arXiv:2505.04497v1 Announce Type: new 
Abstract: Creativity of generative AI models has been a subject of scientific debate in the last years, without a conclusive answer. In this paper, we study creativity from a practical perspective and introduce quantitative measures that help the user to choose a suitable AI model for a given task. We evaluated our measures on a number of popular image-to-image generation models, and the results of this suggest that our measures conform to human intuition.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition</title>
<link>https://arxiv.org/abs/2505.04502</link>
<guid>https://arxiv.org/abs/2505.04502</guid>
<content:encoded><![CDATA[
arXiv:2505.04502v1 Announce Type: new 
Abstract: Video face detection and recognition in public places at the edge is required in several applications, such as security reinforcement and contactless access to authorized venues. This paper aims to maximize the simultaneous usage of hardware engines available in edge GPUs nowadays by leveraging the concurrency and pipelining of tasks required for face detection and recognition. This also includes the video decoding task, which is required in most face monitoring applications as the video streams are usually carried via Gbps Ethernet network. This constitutes an improvement over previous works where the tasks are usually allocated to a single engine due to the lack of a unified and automated framework that simultaneously explores all hardware engines. In addition, previously, the input faces were usually embedded in still images or within raw video streams that overlook the burst delay caused by the decoding stage. The results on real-life video streams suggest that simultaneously using all the hardware engines available in the recent NVIDIA edge Orin GPU, higher throughput, and a slight saving of power consumption of around 300 mW, accounting for around 5%, have been achieved while satisfying the real-time performance constraint. The performance gets even higher by considering several video streams simultaneously. Further performance improvement could have been obtained if the number of shuffle layers that were created by the tensor RT framework for the face recognition task was lower. Thus, the paper suggests some hardware improvements to the existing edge GPU processors to enhance their performance even higher.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation</title>
<link>https://arxiv.org/abs/2505.04512</link>
<guid>https://arxiv.org/abs/2505.04512</guid>
<content:encoded><![CDATA[
arXiv:2505.04512v1 Announce Type: new 
Abstract: Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model</title>
<link>https://arxiv.org/abs/2505.04522</link>
<guid>https://arxiv.org/abs/2505.04522</guid>
<content:encoded><![CDATA[
arXiv:2505.04522v1 Announce Type: new 
Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration</title>
<link>https://arxiv.org/abs/2505.04524</link>
<guid>https://arxiv.org/abs/2505.04524</guid>
<content:encoded><![CDATA[
arXiv:2505.04524v1 Announce Type: new 
Abstract: Cost-effective machine vision systems dedicated to real-time and accurate face detection and recognition in public places are crucial for many modern applications. However, despite their high performance, which could be reached using specialized edge or cloud AI hardware accelerators, there is still room for improvement in throughput and power consumption. This paper aims to suggest a combined hardware-software approach that optimizes face detection and recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX Orin. First, it leverages the simultaneous usage of all its hardware engines to improve processing time. This offers an improvement over previous works where these tasks were mainly allocated automatically and exclusively to the CPU or, to a higher extent, to the GPU core. Additionally, the paper suggests integrating a face tracker module to avoid redundantly running the face recognition algorithm for every frame but only when a new face appears in the scene. The results of extended experiments suggest that simultaneous usage of all the hardware engines that are available in the Orin GPU and tracker integration into the pipeline yield an impressive throughput of 290 FPS (frames per second) on 1920 x 1080 input size frames containing in average of 6 faces/frame. Additionally, a substantial saving of power consumption of around 800 mW was achieved when compared to running the task on the CPU/GPU engines only and without integrating a tracker into the Orin GPU\'92s pipeline. This hardware-codesign approach can pave the way to design high-performance machine vision systems at the edge, critically needed in video monitoring in public places where several nearby cameras are usually deployed for a same scene.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once</title>
<link>https://arxiv.org/abs/2505.04526</link>
<guid>https://arxiv.org/abs/2505.04526</guid>
<content:encoded><![CDATA[
arXiv:2505.04526v1 Announce Type: new 
Abstract: Visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. However, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. To this end, a Darkness-Free network is proposed to handle Visible and infrared image disentanglement and fusion all at Once (DFVO), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. Specifically, we construct a latent-common feature extractor (LCFE) to obtain latent features for the cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised to acquire high-frequency semantic information. Secondly, we design a hyper cross-attention module (HCAM) to extract low-frequency information and preserve texture features from source images. Finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. Particularly, DFVO can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the LLVIP dataset with 63.258 dB PSNR and 0.724 CC, providing more effective information for high-level vision tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAFT: Robust Augmentation of FeaTures for Image Segmentation</title>
<link>https://arxiv.org/abs/2505.04529</link>
<guid>https://arxiv.org/abs/2505.04529</guid>
<content:encoded><![CDATA[
arXiv:2505.04529v1 Announce Type: new 
Abstract: Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real "SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Registration of 3D Point Sets Using Exponential-based Similarity Matrix</title>
<link>https://arxiv.org/abs/2505.04540</link>
<guid>https://arxiv.org/abs/2505.04540</guid>
<content:encoded><![CDATA[
arXiv:2505.04540v1 Announce Type: new 
Abstract: Point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3D point sets captured from varying viewpoints using depth sensors such as LiDAR or structured light. In modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. However, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. These challenges can lead to misalignments and, consequently, to inaccurate or distorted 3D reconstructions. In this work, we address both these limitations by proposing a robust modification to the classic Iterative Closest Point (ICP) algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP), integrates a Gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. This matrix facilitates improved estimation of both rotational and translational components during alignment. We demonstrate the robustness of ESM-ICP in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show that ESM-ICP outperforms traditional geometric registration techniques as well as several recent learning-based methods. To encourage reproducibility and community engagement, our full implementation is made publicly available on GitHub. https://github.com/aralab-unr/ESM_ICP
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Componential Prompt-Knowledge Alignment for Domain Incremental Learning</title>
<link>https://arxiv.org/abs/2505.04575</link>
<guid>https://arxiv.org/abs/2505.04575</guid>
<content:encoded><![CDATA[
arXiv:2505.04575v1 Announce Type: new 
Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt. Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Sampling for MRI-based Sequential Decision Making</title>
<link>https://arxiv.org/abs/2505.04586</link>
<guid>https://arxiv.org/abs/2505.04586</guid>
<content:encoded><![CDATA[
arXiv:2505.04586v1 Announce Type: new 
Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.04594</link>
<guid>https://arxiv.org/abs/2505.04594</guid>
<content:encoded><![CDATA[
arXiv:2505.04594v1 Announce Type: new 
Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning</title>
<link>https://arxiv.org/abs/2505.04601</link>
<guid>https://arxiv.org/abs/2505.04601</guid>
<content:encoded><![CDATA[
arXiv:2505.04601v1 Announce Type: new 
Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>